const data_for_modality_reinforcement_learning = 
[
	{"name":"HeatAlertsRL-Data","keyword":"reinforcement-learning","description":"mauriciogtec/HeatAlertsRL-Data dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/mauriciogtec/HeatAlertsRL-Data","creator_name":"Mauricio","creator_url":"https://huggingface.co/mauriciogtec","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","tabular-regression","time-series-forecasting","mit","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"CartPole-v1","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tCartPole-v1 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\nEach entry consists of:\nobs (list): observation with length 4.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/CartPole-v1.","url":"https://huggingface.co/datasets/NathanGavenski/CartPole-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"pong_muzero_2episodes_gsl400_v0.0.4","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for pong_muzero_2episodes_gsl400_v0.0.4\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n","url":"https://huggingface.co/datasets/puyuan1996/pong_muzero_2episodes_gsl400_v0.0.4","creator_name":"Yuan Pu","creator_url":"https://huggingface.co/puyuan1996","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","10M<n<100M","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"hh-rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/hh-rlhf.","url":"https://huggingface.co/datasets/Anthropic/hh-rlhf","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF Better Uncensored\n\t\n\n\n\t\n\t\t\n\t\tBetter Uncensored Summary\n\t\n\nThis is the Better Uncensored version of the famous Anthropic preference dataset Anthropic/hh-rlhf\nOnly the train files were processed with the rlhf uncensor script in this manner:\nfind ../hh-rlhf/ -type f -name 'train.jsonl' | xargs -I {} python uncensor_rlhf.py --in-file {}\n\nThis should work as a drop in replacement of the original dataset for training uncensored models. About 10% to 25% of the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/betteruncensored/hh-rlhf.","url":"https://huggingface.co/datasets/betteruncensored/hh-rlhf","creator_name":"Better Uncensored","creator_url":"https://huggingface.co/betteruncensored","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-summarisation-preferences","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHuman feedback data\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nSee https://github.com/openai/summarize-from-feedback for original details of the dataset.\nHere the data is formatted to enable huggingface transformers sequence classification models to be trained as reward functions.\n","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","crowdsourced","crowdsourced","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"helpful-self-instruct-raw","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for \"helpful-self-instruct-raw\"\n\t\n\nThis dataset is derived from the finetuning subset of Self-Instruct, with some light formatting to remove trailing spaces and <|endoftext|> tokens.\n","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-self-instruct-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar.","url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/berkeley-nest/Nectar.","url":"https://huggingface.co/datasets/berkeley-nest/Nectar","creator_name":"Berkeley-Nest","creator_url":"https://huggingface.co/berkeley-nest","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"hh_rlhf-chinese-zhtw","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for \"hh_rlhf-chinese-zhtw\"\n\t\n\nÊ≠§Êï∏ÊìöÈõÜÂêà‰Ωµ‰∫Ü‰∏ãÂàóÁöÑË≥áÊñô:\n\nÈóúÊñºÊúâÁî®‰∏îÁÑ°ÂÆ≥ÁöÑ‰∫∫È°ûÂÅèÂ•ΩÊï∏ÊìöÔºå‰æÜËá™ Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback„ÄÇÈÄô‰∫õÊï∏ÊìöÊó®Âú®ÁÇ∫ÂæåÁ∫å RLHF Ë®ìÁ∑¥Ë®ìÁ∑¥ÂÅèÂ•ΩÔºàÊàñÁçéÂãµÔºâÊ®°Âûã„ÄÇÈÄô‰∫õË≥áÊñô‰∏çÁî®ÊñºÂ∞çË©±‰ª£ÁêÜ‰∫∫ÁöÑÁõ£Áù£Ë®ìÁ∑¥„ÄÇÊ†πÊìöÈÄô‰∫õË≥áÊñôË®ìÁ∑¥Â∞çË©±‰ª£ÁêÜÂèØËÉΩÊúÉÂ∞éËá¥ÊúâÂÆ≥ÁöÑÊ®°ÂûãÔºåÈÄôÁ®ÆÊÉÖÊ≥ÅÊáâË©≤ÈÅøÂÖç„ÄÇ\n‰∫∫Â∑•ÁîüÊàê‰∏¶Â∏∂Ë®ªÈáãÁöÑÁ¥ÖÈöäÂ∞çË©±Ôºå‰æÜËá™Ê∏õÂ∞ëÂç±ÂÆ≥ÁöÑÁ¥ÖÈöäË™ûË®ÄÊ®°ÂûãÔºöÊñπÊ≥ï„ÄÅÊì¥Â±ïË°åÁÇ∫ÂíåÁ∂ìÈ©óÊïôË®ì„ÄÇÈÄô‰∫õÊï∏ÊìöÊó®Âú®‰∫ÜËß£ÁúæÂåÖÁ¥ÖÈöäÂ¶Ç‰ΩïÂª∫Ê®°‰ª•ÂèäÂì™‰∫õÈ°ûÂûãÁöÑÁ¥ÖÈöäÊîªÊìäÊàêÂäüÊàñÂ§±Êïó„ÄÇÈÄô‰∫õÊï∏Êìö‰∏çÁî®ÊñºÂæÆË™øÊàñÂÅèÂ•ΩÂª∫Ê®°Ôºà‰ΩøÁî®‰∏äÈù¢ÁöÑÊï∏ÊìöÈÄ≤Ë°åÂÅèÂ•ΩÂª∫Ê®°Ôºâ„ÄÇÈÄô‰∫õÊï∏ÊìöÊòØÂæû‰∏äËø∞ÁÑ°ÂÆ≥ÂÅèÂ•ΩÂª∫Ê®°Êï∏ÊìöÂ∞éÂá∫ÁöÑÂ∞çË©±ÁöÑÂÆåÊï¥ËΩâÈåÑÊú¨ÔºåÂÖ∂‰∏≠ÂÉÖÂ∞áÊâÄÈÅ∏ÈüøÊáâÂêà‰ΩµÂà∞Êï¥ÂÄãËΩâÈåÑÊú¨‰∏≠„ÄÇÊ≠§Â§ñÔºåÊñáÂ≠óË®òÈåÑ‰πüÈÄèÈÅé‰∫∫Â∑•ÂíåËá™ÂãïÊ∏¨Èáè‰æÜÊ®ôË®ªÊï¥ÂÄãÂ∞çË©±ÁöÑÂç±ÂÆ≥Á®ãÂ∫¶„ÄÇ\n\n\n\t\n\t\t\n\t\n\t\n\t\tÁâπÂà•Ê≥®ÊÑè‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw.","url":"https://huggingface.co/datasets/erhwenkuo/hh_rlhf-chinese-zhtw","creator_name":"Erhwen, Kuo","creator_url":"https://huggingface.co/erhwenkuo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","Chinese","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-7k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tArgilla DPO Mix 7K Dataset\n\t\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses. \n\n\n    \n\n\n\n\n  \n    \n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasets mixed\n\t\n\nAs already mentioned, this dataset mixes the following datasets:\n\nargilla/distilabel-capybara-dpo-7k-binarized: random sample of highly scored chosen responses (>=4).\nargilla/distilabel-intel-orca-dpo-pairs:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/dpo-mix-7k.","url":"https://huggingface.co/datasets/argilla/dpo-mix-7k","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Tutorbot-Spock-Bio-Dataset","keyword":"rlhf","description":"Mock conversations between a student and a tutor to train a chatbot for educational purposes as suggested in the paper \nCLASS Meet SPOCK: An Education Tutoring Chatbot based on Learning Science Principles.\nDataset generated from OpenStax Biology 2e textbook.\nProblem, Subproblem, Hints, and Feedback is generated using the prompt.\nMock Conversations is generated using the prompt.\nFor any queries, contact Shashank Sonkar (ss164  AT rice dot edu)\nIf you use this model, please cite:\nCLASS Meet‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset.","url":"https://huggingface.co/datasets/luffycodes/Tutorbot-Spock-Bio-Dataset","creator_name":"Shashank Sonkar","creator_url":"https://huggingface.co/luffycodes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"minari_d4rl","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\ttransfer from d4rl dataset to minari dataset\n\t\n\ntransfer scripts and validation are in transfer.py\n\nclone the repo\n\n$ git clone https://huggingface.co/datasets/im-Kitsch/minari_d4rl\n\n\ncopy the file to minari root (default is ~/.minari)\n\nmv minari_d4rl/datasets ~/.minari/datasets\n\n\n\t\n\t\t\n\t\ttodo\n\t\n\ninfos like infos/qvel are not saved since the interface is not stable yet and those infos cannot be read directly.\n","url":"https://huggingface.co/datasets/im-Kitsch/minari_d4rl","creator_name":"Hu","creator_url":"https://huggingface.co/im-Kitsch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tCapybara-KTO 15K binarized\n\t\n\n\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n\t\n\t\t\n\t\tWhy KTO?\n\t\n\nThe KTO paper states:\n\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is, taking a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar.","url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tCapybara-KTO 15K binarized\n\t\n\n\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n\t\n\t\t\n\t\tWhy KTO?\n\t\n\nThe KTO paper states:\n\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is, taking a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Nectar","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tDataset Card for Nectar\n\t\n\n\nDeveloped by: Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\nLicense: Apache-2.0 license under the condition that the dataset is not used to compete with OpenAI\n\nNectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Nectar.","url":"https://huggingface.co/datasets/agicorp/Nectar","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"rlhf_reward_dataset","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tRLHF Reward Model Dataset\n\t\n\nÂ•ñÂä±Ê®°ÂûãÊï∞ÊçÆÈõÜ„ÄÇ\nÊï∞ÊçÆÈõÜ‰ªéÁΩë‰∏äÊî∂ÈõÜÊï¥ÁêÜÂ¶Ç‰∏ã:\n\n\t\n\t\t\nÊï∞ÊçÆ\nËØ≠Ë®Ä\nÂéüÂßãÊï∞ÊçÆ/È°πÁõÆÂú∞ÂùÄ\nÊ†∑Êú¨‰∏™Êï∞\nÂéüÂßãÊï∞ÊçÆÊèèËø∞\nÊõø‰ª£Êï∞ÊçÆ‰∏ãËΩΩÂú∞ÂùÄ\n\n\n\t\t\nbeyond\nchinese\nbeyond/rlhf-reward-single-round-trans_chinese\n24858\n\n\n\n\nhelpful_and_harmless\nchinese\ndikw/hh_rlhf_cn\nharmless train 42394 Êù°Ôºåharmless test 2304 Êù°Ôºåhelpful train 43722 Êù°Ôºåhelpful test 2346 Êù°ÔºåÂü∫‰∫é Anthropic ËÆ∫Êñá Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback ÂºÄÊ∫êÁöÑ helpful Âíåharmless Êï∞ÊçÆÔºå‰ΩøÁî®ÁøªËØëÂ∑•ÂÖ∑ËøõË°å‰∫ÜÁøªËØë„ÄÇ\nAnthropic/hh-rlhf\n\n\nzhihu_3k\nchinese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qgyd2021/rlhf_reward_dataset.","url":"https://huggingface.co/datasets/qgyd2021/rlhf_reward_dataset","creator_name":"Êô¥ËÄïÈõ®ËØª","creator_url":"https://huggingface.co/qgyd2021","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","text-generation","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"franka_panda_serl_example","keyword":"rl","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"panda\",\n    \"total_episodes\": 6,\n    \"total_frames\": 1001,\n    \"total_tasks\": 1,\n    \"total_videos\": 12,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 10,\n    \"splits\": {\n        \"train\": \"0:6\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andypeng05/franka_panda_serl_example.","url":"https://huggingface.co/datasets/andypeng05/franka_panda_serl_example","creator_name":"Andy Peng","creator_url":"https://huggingface.co/andypeng05","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"HelpSteer","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHelpSteer: Helpfulness SteerLM Dataset\n\t\n\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nLeveraging this dataset and SteerLM, we train a Llama 2 70B to reach 7.54 on MT Bench, the highest among models trained on open-source datasets based on MT Bench Leaderboard as of 15 Nov 2023.\nThis model is available on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer.","url":"https://huggingface.co/datasets/nvidia/HelpSteer","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"drone-test","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tliftoff_drone_dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains telemetry and control data from the Liftoff drone simulator, recorded for robot learning and imitation learning tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nRobot Type: liftoff_drone_simulator\nTotal Episodes: 1\nTotal Frames: 1357\nRecording FPS: 30.0 Hz\nVideo FPS: 30.0 Hz\nLeRobot Version: v3.0\nChunk Size: 100 episodes per chunk\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nThe dataset is organized into chunks for efficient storage and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/radiuson/drone-test.","url":"https://huggingface.co/datasets/radiuson/drone-test","creator_name":"runqian","creator_url":"https://huggingface.co/radiuson","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tOpen Assistant Conversations Dataset Release 2 (OASST2)\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains message trees. Each message tree has an initial prompt message as the root node, \nwhich can have multiple child messages as replies, and these child messages can have multiple replies. \nAll messages have a role property: this can either be \"assistant\" or \"prompter\". The roles in \nconversation threads from prompt to leaf node strictly alternate between \"prompter\" and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst2.","url":"https://huggingface.co/datasets/OpenAssistant/oasst2","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"API-Bank-RLVR","keyword":"rl","description":"\n\t\n\t\t\n\t\tAPI-Bank-RLVR\n\t\n\nReinforcement Learning with Verifiable Rewards dataset derived from API-Bank stepwise tool-call tasks.\nSplits:\n\ntrain\nvalidation\n\nColumns (typical):\n\nprompt: User-facing prompt text (string)\nreward_model: JSON string or object containing ground truth (e.g., {\"style\": \"rule\", \"ground_truth\": {\"name\": str, \"parameters\": dict}})\n\nNotes:\n\nPrompts are prepared for tool-call generation.\nGround truth matches API-Bank evaluate semantics (name + parameters).\n\n","url":"https://huggingface.co/datasets/Simu-Env/API-Bank-RLVR","creator_name":"Evolving Environment Simulation for LLM Agents","creator_url":"https://huggingface.co/Simu-Env","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"openassistant-llama-style","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Llama 2 Style\n\t\n\nThis dataset allows for fine-tuning chat models using [INST] AND [/INST] to wrap user messages.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then filtered to:\n\n\nreplace instances of '### Human:' with '[INST]'\nreplace‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-llama-style.","url":"https://huggingface.co/datasets/Trelis/openassistant-llama-style","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch\n\t\n\n\n[!TIP]\nI recommend using the cleaned, deduplicated version. https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch.","url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"dpo","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   Êîπ‰∫Üquestion„ÄÅresponse_rejected„ÄÅresponse_chosenÂ≠óÊÆµÔºåÊñπ‰æøORPO„ÄÅDPOÊ®°ÂûãËÆ≠ÁªÉÊó∂‰ΩøÁî®train usage:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference.","url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k-Preference","keyword":"rlhf","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nrefer: https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k   Êîπ‰∫Üquestion„ÄÅresponse_rejected„ÄÅresponse_chosenÂ≠óÊÆµÔºåÊñπ‰æøORPO„ÄÅDPOÊ®°ÂûãËÆ≠ÁªÉÊó∂‰ΩøÁî®train usage:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference.","url":"https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dpo-contrast-sample","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO Contrast Sample\n\t\n\nThis dataset provides a direct comparison between high-quality and low-quality instruction-output pairs evaluated by a LLaMA-2-7B-Chat model in a DPO-style workflow.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\ninstruction: The generated prompt (x).\noutput: The associated output (y).\nscore: Quality rating assigned by LLaMA2 (1 = high, 5 = low).\n\n\n\t\n\t\t\n\t\tSamples\n\t\n\n5 examples with score = 1 (excellent), and 5 with score = 5 (poor).\n","url":"https://huggingface.co/datasets/helloTR/dpo-contrast-sample","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"PRISM-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPRISM: Principled Reasoning for Integrated Safety in Multimodality Datasets\n\t\n\nThis repository provides access to the datasets developed for PRISM (Principled Reasoning for Integrated Safety in Multimodality), a system2-like framework that aligns Vision-Language Models (VLMs) by embedding a structured, safety-aware reasoning process.\n\nPaper: PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality\nCode: https://github.com/SaFoLab-WISC/PRISM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyc03/PRISM-DPO.","url":"https://huggingface.co/datasets/andyc03/PRISM-DPO","creator_name":"Nanxi Li","creator_url":"https://huggingface.co/andyc03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","Image","arxiv:2508.18649"],"keywords_longer_than_N":true},
	{"name":"Big-Math-RL-Verified-Subset","keyword":"rl","description":"Default Math dataset used for https://github.com/RD211/actors.\nThe dataset is a subset of https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified.\nOnly includes problems with solve rate between 0.05 and 0.6 and from aops_forum and olympiads.\n","url":"https://huggingface.co/datasets/rl-actors/Big-Math-RL-Verified-Subset","creator_name":"RL Actors","creator_url":"https://huggingface.co/rl-actors","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.2\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"itorca_dpo_vi","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_vi.","url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Vietnamese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Electricity_dataset_sample","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tElectricity Consumption and Pricing Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains electricity consumption monitoring data generated for the ICOS Use Case related to Energy Consumption Recommendation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nSource: Simulated using python\nPurpose: Training data for Reinforcement Learning\nUpdate Frequency: Electricity consumption and cost at every 30 minute intervals\n\n\n\t\n\t\t\n\t\tData Schema\n\t\n\n\n\t\n\t\t\nColumn\nType\nDescription\n\n\n\t\t\ntimestamp\nfloat\nISO 8601 format‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ICOS-AI/Electricity_dataset_sample.","url":"https://huggingface.co/datasets/ICOS-AI/Electricity_dataset_sample","creator_name":"ICOS AI Catalogue","creator_url":"https://huggingface.co/ICOS-AI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"1k-ranked-videos-coherence","keyword":"rl","description":"\n\t\n\t\t\n\t\t1k Ranked Videos\n\t\n\nThis dataset contains approximately one thousand videos, ranked from most preferred to least preferred based on human feedback from over 25k pairwise comparisons. The videos are rated solely on coherence as evaluated by human annotators, without considering the specific prompt used for generation. Each video is associated with the model name that generated it.\nThe videos are sampled from our benchmark dataset text-2-video-human-preferences-pika2.2. Follow us to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence.","url":"https://huggingface.co/datasets/Rapidata/1k-ranked-videos-coherence","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Tabular"],"keywords_longer_than_N":true},
	{"name":"synth-apigen-qwen","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for argilla-warehouse/synth-apigen-qwen\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\nsynth_apigen.py.\n\n\t\n\t\t\n\t\tDataset creation\n\t\n\nThis dataset is a replica in distilabel of the framework\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-tools,\nthe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen.","url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-qwen","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"rlhf-medical-diagnosis-dummy-dataset","keyword":"rlhf","description":"nuriyev/rlhf-medical-diagnosis-dummy-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/nuriyev/rlhf-medical-diagnosis-dummy-dataset","creator_name":"Mahammad Nuriyev","creator_url":"https://huggingface.co/nuriyev","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Hidream_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Hidream I1 full Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 38k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hidream I1 full across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"mimicgen59","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMimicgen 59\n\t\n\nThis repository contains the generated Mimicgen datasets as used in \"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\": \n\ntrajectories for each task are stored in a separate .tar.gz\nevery trajectory is stored in a separate .hdf5 file.\n\nFor more information on Mimicgen, we refer to the original documentation: https://mimicgen.github.io/docs/introduction/overview.html.\nDownload the dataset using:\nhuggingface-cli download ml-jku/mimicgen59‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/mimicgen59.","url":"https://huggingface.co/datasets/ml-jku/mimicgen59","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"Aya-Aya.23.8B-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"Aya-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO.","url":"https://huggingface.co/datasets/2A2I/Aya-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multiturn-capybara-preferences-filtered-binarized","keyword":"dpo","description":"This dataset has been created with distilabel, plus some extra post-processing steps described below.\nDataset Summary\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\n\nRemove responses from the assistant, not only in the last turn, but also in intermediate turns where the assistant responds with a potential refusal and/or some ChatGPT-isms such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized.","url":"https://huggingface.co/datasets/arcee-ai/multiturn-capybara-preferences-filtered-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orcaratgen","keyword":"dpo","description":"An rationale-enhanced version of the paired preference learning dataset Intel-ORCA-DPO.\nThese rationales are general, high-level explanation of why the chosen response is preferred over the rejected response.\nThe dataset was generated according to this paper: Data-Centric Human Preference Optimization with\nRationales.\n","url":"https://huggingface.co/datasets/redsgnaoh/orcaratgen","creator_name":"Hoang Anh Just","creator_url":"https://huggingface.co/redsgnaoh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\n\n\t\n\t\t\n\t\tOur Methods for Cleaning Vision Feedback Data\n\t\n\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred ones, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized-cleaned","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"CRUDE_OIL_PRICES","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\t‚õΩ Global Crude Oil Data\n\t\n\nThis repository contains a comprehensive dataset on global crude oil prices, imports, exports, and net imports, aggregated from various public sources. The data is compiled into a single file for convenient analysis and research.\n\n\t\n\t\t\n\t\tüìÅ Dataset Description\n\t\n\nThe final dataset combines daily and monthly time series data, providing a unified view of key metrics within the oil industry. The primary focus is on crude oil from various geographic regions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MaxPrestige/CRUDE_OIL_PRICES.","url":"https://huggingface.co/datasets/MaxPrestige/CRUDE_OIL_PRICES","creator_name":"brian perez","creator_url":"https://huggingface.co/MaxPrestige","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","https://www.eia.gov/","https://markets.businessinsider.com/","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"audio-alpaca","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAudio-alpaca: A preference dataset for aligning text-to-audio models\n\t\n\nAudio-alpaca is a pairwise preference dataset containing about 15k (prompt,chosen, rejected) triplets where given a textual prompt, chosen is the preferred generated audio and rejected is the undesirable audio.\n\n\t\n\t\t\n\t\tField details\n\t\n\nprompt: Given textual prompt\nchosen: The preferred audio sample\nrejected: The rejected audio sample\n","url":"https://huggingface.co/datasets/declare-lab/audio-alpaca","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"Seedream-3_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Seedream 3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~30'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"lyrical_Ru2En_translation_Soviet_rock_songs_DPO_ORPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tLyrics to songs by seminal Soviet and Russian songwriters, poets, and bands.\n\t\n\nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts' phrasal/phonetic, rhythmic, metric, syllabic, melodic, and other lyrical/performance-catered features, whilst retaining adequate semantic/significational fidelity.  \nThis dataset samples months and years of inspired‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/lyrical_Ru2En_translation_Soviet_rock_songs_DPO_ORPO.","url":"https://huggingface.co/datasets/AlekseyCalvin/lyrical_Ru2En_translation_Soviet_rock_songs_DPO_ORPO","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPTS DPO Dataset\n\t\n\nA Direct Preference Optimization (DPO) dataset created using the Pivotal Token Search (PTS) technique.\n\n\t\n\t\t\n\t\tDetails\n\t\n\n\nSource: Generated using the PTS tool\nModel: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example in the dataset consists of:\n\nprompt: The context leading up to the pivotal token\nchosen: The preferred token that increases success probability\nrejected: The alternative token that decreases success probability\nmetadata:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs.","url":"https://huggingface.co/datasets/codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts-dpo-pairs","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K<n<10K","üá∫üá∏ Region: US","dpo"],"keywords_longer_than_N":true},
	{"name":"anthropic-hh-rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/giovannioliveira/anthropic-hh-rlhf.","url":"https://huggingface.co/datasets/giovannioliveira/anthropic-hh-rlhf","creator_name":"Giovanni Oliveira","creator_url":"https://huggingface.co/giovannioliveira","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","arxiv:2204.05862","üá∫üá∏ Region: US","human-feedback"],"keywords_longer_than_N":false},
	{"name":"tactical-military-reasoning-v.1.0","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tTactical Military Reasoning Dataset v1.0\n\t\n\n\nA curated collection of 150 rich tactical military scenarios with LLM-generated reasoning strategies for both attacking and defending forces.\n\n\n \n\n\n\n\n\t\n\t\t\n\t\tüìù Preface\n\t\n\nOncologists do not study cancer because they love cancer and wish for it to occur more frequently. They study cancer to better understand its causes, progression, and consequences in order to therefore eradicate it from the earth more effectively. A distaste for something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/tactical-military-reasoning-v.1.0.","url":"https://huggingface.co/datasets/ZennyKenny/tactical-military-reasoning-v.1.0","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"alpaca-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAlpaca DPO\n\t\n\nThis dataset is a curated subset of vicgalle/alpaca-gpt4.\nThe original LLM-generated answers have been revised for clarity, conciseness, and relevance using the following models:  \n\nQwen/Qwen2.5-14B-Instruct  \nibm-granite/granite-3.3-2b-instruct\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nFine-tune language models using supervised learning on the chosen column  \nApply Direct Preference Optimization (DPO) to encourage models to generate responses aligned with chosen answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/alpaca-dpo.","url":"https://huggingface.co/datasets/agentlans/alpaca-dpo","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tMathematical Reasoning DPO Dataset\n\t\n\nThis dataset contains mathematical reasoning problems with chosen and rejected responses, designed for Direct Preference Optimization (DPO) and preference learning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the ShareGPT format for DPO training with three main fields:\n\nconversations: List of conversation turns leading up to the response\nchosen: Preferred response with detailed reasoning and correct solution\nrejected: Less‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-dpo.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-dpo","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"math-reasoning-dpo","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tMathematical Reasoning DPO Dataset\n\t\n\nThis dataset contains mathematical reasoning problems with chosen and rejected responses, designed for Direct Preference Optimization (DPO) and preference learning of language models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset follows the ShareGPT format for DPO training with three main fields:\n\nconversations: List of conversation turns leading up to the response\nchosen: Preferred response with detailed reasoning and correct solution\nrejected: Less‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/est-ai/math-reasoning-dpo.","url":"https://huggingface.co/datasets/est-ai/math-reasoning-dpo","creator_name":"ESTsoft AI","creator_url":"https://huggingface.co/est-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SauerkrautLM-Fermented-Irrelevance-GER-DPO","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tSauerkrautLM-Fermented-Irrelevance-GER-DPO Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSauerkrautLM-Fermented-Irrelevance-GER-DPO  is a specialized dataset designed for training language models in function calling irrelevance detection using Direct Preference Optimization (DPO). The dataset consists of 2,000 carefully evaluated instruction-response pairs, specifically curated to help models recognize situations where function calls are unnecessary and direct responses are more appropriate.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO.","url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-Irrelevance-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","German","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Fast-Math-R1-GRPO","keyword":"reinforcement-learning","description":"This repository contains the second-stage GRPO dataset for the paper A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning.\nThis dataset is crucial for the second stage of the training recipe, aiming to improve token efficiency while preserving peak mathematical reasoning performance in Large Language Models (LLMs) through Reinforcement Learning from online inference (GRPO).\nWe extracted the answers from the 2nd stage SFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-GRPO.","url":"https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-GRPO","creator_name":"Hiroshi Yoshihara","creator_url":"https://huggingface.co/RabotniKuma","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"reinforcement-learning","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"anthropic-hh-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAnthropic Helpful/Harmful Dataset for Llama 3 Instruct\n\t\n\nThis is a formatted version of the Anthropic helpful/harmful dataset, preprocessed to work with the Llama 3 instruct template.\n\n\t\n\t\t\n\t\tUsage with HuggingFace Transformers\n\t\n\nIf you are using the HuggingFace transformers library, ensure you are using the default chat template. This should add the <|begin_of_text|> token to the start of the input, but nothing else.\n\n\t\n\t\t\n\t\tVerifying the Format\n\t\n\nTo make sure the format is correct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo.","url":"https://huggingface.co/datasets/eborgnia/anthropic-hh-dpo","creator_name":"Eitan Borgnia","creator_url":"https://huggingface.co/eborgnia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"WebInstructSub-prometheus","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for WebInstructSub-prometheus\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTIGER-Lab/WebInstructSub evaluated for logical and effective reasoning using prometheus-7b-v2.0.\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus.","url":"https://huggingface.co/datasets/chargoddard/WebInstructSub-prometheus","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"creative-rubrics-preferences","keyword":"dpo","description":"\n\t\n\t\t\n\t\tcreative-rubrics-preferences üéè\n\t\n\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\nThis dataset was used in the paper Configurable Preference Tuning with Rubric-Guided Synthetic Data.\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences.","url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"oasst2_dpo_pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"oasst2_dpo_pairs\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDataset transferred into the structure for trainig with DPO and can be used with the Alignment Handbook\nThe structure follows mostly the same scheme as HuggingFaceH4/ultrafeedback_binarized\n\n\t\n\t\t\n\t\tUsage\n\t\n\nTo load the dataset, run:\nfrom datasets import load_dataset\n\nds = load_dataset(\"alexredna/oasst2_dpo_pairs\")\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nBase dataset filtered to only contain: German, English, Spanish and Frensh‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs.","url":"https://huggingface.co/datasets/alexredna/oasst2_dpo_pairs","creator_name":"Alexander Gruhl","creator_url":"https://huggingface.co/alexredna","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","German","Spanish","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-formatted","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tReformatted version of Anthropic's hh-rlhf dataset\n\t\n\nOriginal available at https://huggingface.co/datasets/Anthropic/hh-rlhf. (Does not include red teaming data)\nRLHF datasets are in general defined as a collection of triples D={(x,y_1,y_2)_n} where x is the prompt, y_1 the chosen reponse and y_2 the rejected response. \nThe original dataset provides two columns, \"chosen\"=x+y_1 and \"rejected\"=x+y_2.\nVarious RLHF setups may require either format, so in this dataset we keep the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KHuss/hh-rlhf-formatted.","url":"https://huggingface.co/datasets/KHuss/hh-rlhf-formatted","creator_name":"Karim El Husseini","creator_url":"https://huggingface.co/KHuss","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","reinforcement-learning","token-classification","English"],"keywords_longer_than_N":true},
	{"name":"OpenHumnoidActuatedFaceData","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tü§ñ Open¬†Humanoid¬†Actuated¬†Face¬†Dataset\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\tDataset¬†Summary\n\t\n\nThe Open‚ÄØHumanoid‚ÄØActuated‚ÄØFace Dataset is designed for researchers working onfacial‚Äëactuation control, robotics, reinforcement learning, and human‚Äìcomputer interaction.\n\nOrigin ‚Äì collected during a reinforcement‚Äëlearning (RL) training loop whose objective was to reproduce human facial expressions.\nPlatform ‚Äì a modified i2Head InMoov humanoid head with a silicone skin.\nControl ‚Äì 16 actuators driving facial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/infosys/OpenHumnoidActuatedFaceData.","url":"https://huggingface.co/datasets/infosys/OpenHumnoidActuatedFaceData","creator_name":"Infosys Limited","creator_url":"https://huggingface.co/infosys","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-mlx","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k for direct use with mlx-lm-lora, specifically tailored to be compatible with ORPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx.","url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Hinglish-Preference-Humanized-DPO","keyword":"dpo","description":"![Cover Image](data:image/webp;base64‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO.","url":"https://huggingface.co/datasets/fhai50032/Hinglish-Preference-Humanized-DPO","creator_name":"Low IQ Gen AI","creator_url":"https://huggingface.co/fhai50032","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Hindi","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"rc1","keyword":"rl","description":"\n\t\n\t\t\n\t\tReasoning Core ‚óâ\n\t\n\nPaper: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning\nCode: GitHub Repository\nreasoning-core is a text-based RLVR for LLM reasoning training.\nIt is centered on expressive symbolic tasks, including full fledged FOL, formal mathematics with TPTP, formal planning with novel domains, and syntax tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nWe introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reasoning-core/rc1.","url":"https://huggingface.co/datasets/reasoning-core/rc1","creator_name":"Reasoning Core","creator_url":"https://huggingface.co/reasoning-core","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Notus_Aegis-v1","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataSet Overview:\n\t\n\n\n\n\t\n\t\t\n\t\tDataSet Name:Notus_Aegis-v1\n\t\n\n\n\n\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures:\n\t\n\n\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses are coherent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1.","url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Notus_Aegis-v1","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataSet Overview:\n\t\n\n\n\n\t\n\t\t\n\t\tDataSet Name:Notus_Aegis-v1\n\t\n\n\n\n\n This starter dataset is designed to facilitate research and development in the field of natural language processing, specifically focusing on instruction-response pairs.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures:\n\t\n\n\nDiversity of Instructions: The dataset encompasses a broad spectrum of topics, ensuring applicability across different domains.\nHigh-Quality Responses: Leveraging the notus-7b-v1 model ensures that the responses are coherent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1.","url":"https://huggingface.co/datasets/AINovice2005/Notus_Aegis-v1","creator_name":"Parag Ekbote","creator_url":"https://huggingface.co/AINovice2005","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"cosmochat","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card for CosmoChat\n\t\n\n\n  \n    \n  \n\n\nThis dataset has been created with distilabel. It is a WIP and is likely to change whenever I have some spare time to work on this! Feel free to follow my harebrained ideas for improving this here: https://huggingface.co/datasets/davanstrien/cosmochat/discussions\n\n\t\n\t\n\t\n\t\tCan we create pedagogically valuable multi-turn synthetic datasets from Cosmopedia?\n\t\n\nSynthetic datasets are increasingly helping to push forward the quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/cosmochat.","url":"https://huggingface.co/datasets/davanstrien/cosmochat","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","cc0-1.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Turkish-Legislation-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tTurkish-Legislation-DPO Dataset\n\t\n\nTurkish-Legislation-DPO is a large-scale Direct Preference Optimization (DPO) training dataset specifically designed to align Turkish language models with expertise in the fields of law and regulation.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Turkish-Legislation-DPO dataset contains 23,596 carefully selected preference pairs, all generated by google/gemma-2-2b-it model and improved through systematic quality assessment protocols. Each example consists of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yusufbaykaloglu/Turkish-Legislation-DPO.","url":"https://huggingface.co/datasets/yusufbaykaloglu/Turkish-Legislation-DPO","creator_name":"Yusuf  Baykaloƒülu","creator_url":"https://huggingface.co/yusufbaykaloglu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","table-question-answering","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VL-RewardBench","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/VL-RewardBench.","url":"https://huggingface.co/datasets/MMInstruction/VL-RewardBench","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"GRAM-pre-training-566k","keyword":"rlhf","description":"This is the dataset for Per-Training GRAM.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach item of the dataset includes following keys:\n\ninstruction: any prompt in following template:[User Question]\n{your prompt here}\n\n\ninput: the input for above prompt, can be empty if there is not.\noutput: two responses in following template:[The Start of Assistant A's Answer]\n{answer of assistant A}\n[The End of Assistant A's Answer]\n\n[The Start of Assistant B's Answer]\n{answer of assistant B}\n[The End of Assistant B's Answer]\n\n\n\nAn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/GRAM-pre-training-566k.","url":"https://huggingface.co/datasets/NiuTrans/GRAM-pre-training-566k","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"uplimit-synthetic-data-week-1-with-seed","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for uplimit-synthetic-data-week-1-with-seed\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed.","url":"https://huggingface.co/datasets/marotosg/uplimit-synthetic-data-week-1-with-seed","creator_name":"Sergio Garcia Maroto","creator_url":"https://huggingface.co/marotosg","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","n<1K","Distilabel","arxiv:2212.10560","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"dpo-llm-judge-preferences-llama3","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO Preference Dataset - LLM Judge\n\t\n\nThis dataset contains preference pairs for Direct Preference Optimization (DPO) training.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nBase Model: Llama-3.2-1B-Instruct\nNumber of Samples: 150\nCreation Method: LLM Judge\nTask: Preference learning for instruction following\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"pyamy/dpo-llm judge-preferences-llama3\")\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach sample contains:\n\nprompt: The instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pyamy/dpo-llm-judge-preferences-llama3.","url":"https://huggingface.co/datasets/pyamy/dpo-llm-judge-preferences-llama3","creator_name":"Priyam Choksi","creator_url":"https://huggingface.co/pyamy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"maze-curriculum-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMaze Navigation Curriculum Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 60,000 ASCII maze puzzles designed for training language models on spatial reasoning and navigation tasks. The dataset implements curriculum learning with progressively larger mazes.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach instance contains:\n\nprompt: Instruction for the navigation task\nmaze: ASCII representation of the maze with '#' for walls, 'S' for start, 'E' for end‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimaktas/maze-curriculum-dataset.","url":"https://huggingface.co/datasets/selimaktas/maze-curriculum-dataset","creator_name":"Slm","creator_url":"https://huggingface.co/selimaktas","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dm_control_10M","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDMControl 10M\n\t\n\nThis repository contains the DMControl datasets as used in \"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks\": \n\nTrajectories for every task are stored as separate .tar.gz files. \nEvery .tar.gz file contains 10M transitions.\nEvery trajectory is stored as a separate .hdf5 file.\n\nDownload the dataset using the huggingface-cli:\nhuggingface-cli download ml-jku/dm_control_10M --local-dir=./dm_control_10M --repo-type dataset\n\nFor dataloading we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/dm_control_10M.","url":"https://huggingface.co/datasets/ml-jku/dm_control_10M","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pairs Dutch Cleaned\n\t\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\n@misc{vanroy2024geitje7bultraconversational,\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \n      author={Bram Vanroy},\n      year={2024},\n      eprint={2412.04092},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.04092}, \n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned.","url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-1.5-RL-Verifiable","keyword":"rl","description":"\n\t\n\t\t\n\t\tDataset Card for NuminaMath-1.5-RL-Verifiable\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNuminaMath-1.5-RL-Verifiable is a curated subset of the NuminaMath-1.5 dataset, specifically filtered to support reinforcement learning applications requiring verifiable outcomes. This collection consists of 131,063 math word problems from the original dataset that meet strict filtering criteria: all problems have definitive numerical answers, validated problem statements and solutions, and come from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable.","url":"https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"asciinema_terminal_recordings","keyword":"reinforcement-learning","description":"Public terminal recordings from asciinema.org\nTotal 79329 recordings.\nDue to restrictions, converted GIF images are uploaded to Kaggle Dataset:\nYou can convert asciicast recordings into GIF using agg\nScraper code: https://github.com/James4Ever0/agi_computer_control/blob/master/scrape_asciinema_terminal_recordings\nFile structure:\n./recordings.7z\n  |_ ./recordings\n          |_ <asciinema_public_record_id>\n                 |_ record.<cast|json> // v2 and v3 uses .cast, v1 uses .json‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/James4Ever0/asciinema_terminal_recordings.","url":"https://huggingface.co/datasets/James4Ever0/asciinema_terminal_recordings","creator_name":"James Brown","creator_url":"https://huggingface.co/James4Ever0","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","robotics","English","unlicense","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"auryn_dpo_orpo_english","keyword":"dpo","description":"celsowm/auryn_dpo_orpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"after_visit_summary_simulated_edits","keyword":"rlhf","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card: AVS edits Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nThe AVS edits dataset is designed to support human feedback research in for clinical summarization. It contains synthetic edit feedback generated by large language models (LLMs) to improve the factual consistency and quality of summaries. The dataset includes training, evaluation, and test splits with specific fields for modeling and evaluation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTrain Split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits.","url":"https://huggingface.co/datasets/PrabhakarSai/after_visit_summary_simulated_edits","creator_name":"Sai","creator_url":"https://huggingface.co/PrabhakarSai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","reinforcement-learning","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"gsm8k-base-vs-verl-comparison","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tGSM8K: BASE vs VERL-trained Model Comparison Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset provides a comprehensive comparison between a base language model and its VERL (Reinforcement Learning from Human Feedback) fine-tuned version on mathematical reasoning tasks from GSM8K.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Samples: 50 GSM8K test problems\nBASE Model: Qwen/Qwen2.5-0.5B-Instruct (22.0% accuracy)  \nVERL Model: karthik/verl-qwen2.5-0.5b-gsm8k-ppo-step360 (28.0% accuracy)\nImprovement: +6.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/karthik/gsm8k-base-vs-verl-comparison.","url":"https://huggingface.co/datasets/karthik/gsm8k-base-vs-verl-comparison","creator_name":"karthik","creator_url":"https://huggingface.co/karthik","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"example-dataset-3","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset-3\n\t\n\nThis dataset has been created with distilabel.\nWhile certain attempts were successful in making a new prompt that provides the desired outcome, there are multpiple instances where it appears that the token limit was reached before the entire response was recorded.\nIn addition there was 1/10 that the LLM did not properly understand the request and commented on the source prompt topic.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/djackson-proofpoint/example-dataset-3.","url":"https://huggingface.co/datasets/djackson-proofpoint/example-dataset-3","creator_name":"Dan Jackson","creator_url":"https://huggingface.co/djackson-proofpoint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"HelpSteer3","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tHelpSteer3\n\t\n\nHelpSteer3 is an open-source dataset (CC-BY-4.0) that supports aligning models to become more helpful in responding to user prompts.\nHelpSteer3-Preference can be used to train Llama 3.3 Nemotron Super 49B v1 (for Generative RMs) and Llama 3.3 70B Instruct Models (for Bradley-Terry RMs) to produce Reward Models that score as high as 85.5% on RM-Bench and 78.6% on JudgeBench, which substantially surpass existing Reward Models on these benchmarks.\nHelpSteer3-Feedback and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer3.","url":"https://huggingface.co/datasets/nvidia/HelpSteer3","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Korean","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"twinviews-13k","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for TwinViews-13k\n\t\n\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k.","url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"HelpSteer3","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHelpSteer3\n\t\n\nHelpSteer3 is an open-source dataset (CC-BY-4.0) that supports aligning models to become more helpful in responding to user prompts.\nHelpSteer3-Preference can be used to train Llama 3.3 Nemotron Super 49B v1 (for Generative RMs) and Llama 3.3 70B Instruct Models (for Bradley-Terry RMs) to produce Reward Models that score as high as 85.5% on RM-Bench and 78.6% on JudgeBench, which substantially surpass existing Reward Models on these benchmarks.\nHelpSteer3-Feedback and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer3.","url":"https://huggingface.co/datasets/nvidia/HelpSteer3","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","Korean","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"MATH-500-Overall","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tMATH-500-Overall\n\t\n\n\n\t\n\t\t\n\t\tAbout the dataset\n\t\n\nThis dataset of only 500 examples combines mathematics, physics and logic in English with reasoning and step-by-step problem solving, the dataset was created synthetically, CoT of Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct.\n\n\t\n\t\t\n\t\tBrief information\n\t\n\n\nNumber of rows: 500\nType of dataset files: parquet\nType of dataset: text, alpaca with system prompts\nLanguage: English\nLicense: MIT\n\nStructure:\nmath¬Ø¬Ø¬Ø¬Ø¬Ø‚åâ\n   school-level (100 rows)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/MATH-500-Overall.","url":"https://huggingface.co/datasets/fluently-sets/MATH-500-Overall","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-classification","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"untested-WiP-dpo","keyword":"dpo","description":"CultriX/untested-WiP-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/untested-WiP-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"NoRobots-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-SambaLingo.Arabic.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO.","url":"https://huggingface.co/datasets/2A2I/NoRobots-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-jondurbin","keyword":"dpo","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenRM/gutenberg-dpo-v0.1-jondurbin.","url":"https://huggingface.co/datasets/GenRM/gutenberg-dpo-v0.1-jondurbin","creator_name":"GenRM: Generative Reward Models","creator_url":"https://huggingface.co/GenRM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"autoux","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAutoUX Dataset\n\t\n\nThis dataset contains desktop interaction recordings from the autoux project. Each recording captures screen captures, cursor positions, and user input events in MCAP format for training desktop automation agents.\n\n\t\n\t\t\n\t\tüìä Dataset Summary\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal Episodes\n10\n\n\nTotal Duration\n5min 44sec\n\n\nTotal Size\n310.0 MB\n\n\nDate Range\nJuly 3-4, 2025\n\n\n\t\n\n\n\t\n\t\t\n\t\tüóÇÔ∏è Data Format\n\t\n\nEach MCAP file contains the following topics:\n\n\t\n\t\t\nTopic\nFormat\nDescription‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aditya-shriwastava/autoux.","url":"https://huggingface.co/datasets/aditya-shriwastava/autoux","creator_name":"Aditya Shriwastava","creator_url":"https://huggingface.co/aditya-shriwastava","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","mit","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"web_agents_google_flight_trajectories","keyword":"dpo","description":"\n\t\n\t\t\n\t\tWeb Agent Google Flight Trajectories\n\t\n\nThis dataset was originally created on Nov 23 2024 during EF's Ai On Edge Hackathon.\nThe purpose of this dataset is to give both positive and negative image web-agent trajectories to finetune small edge-models on web agentic tasks.\n","url":"https://huggingface.co/datasets/anonx3247/web_agents_google_flight_trajectories","creator_name":"Anas lecaillon","creator_url":"https://huggingface.co/anonx3247","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"itorca_dpo_en","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/itorca_dpo_en.","url":"https://huggingface.co/datasets/lamhieu/itorca_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"2k-ranked-images-open-image-preferences-v1","keyword":"rl","description":"\n\t\n\t\t\n\t\t2k Ranked Images\n\t\n\nThis dataset contains roughly two thousand images ranked from most preferred to least preferred based on human feedback on pairwise comparisons (>25k responses). \nThe generated images, which are a sample from the open-image-preferences-v1 dataset \nfrom the team @data-is-better-together, are rated purely based on aesthetic preference, disregarding the prompt used for generation.\nWe provide the categories of the original dataset for easy filtering.\nThis is a new‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1.","url":"https://huggingface.co/datasets/Rapidata/2k-ranked-images-open-image-preferences-v1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"chembench-rlvr-test4","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tChemBench-RLVR: Comprehensive Chemistry Dataset for Reinforcement Learning from Verifiable Rewards\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChemBench-RLVR is a high-quality, balanced dataset containing 16,699 question-answer pairs across 14 chemistry task types. This dataset is specifically designed for training language models using Reinforcement Learning from Verifiable Rewards (RLVR), where all answers are computationally verifiable using established cheminformatics tools.\n\n\t\n\t\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/summykai/chembench-rlvr-test4.","url":"https://huggingface.co/datasets/summykai/chembench-rlvr-test4","creator_name":"Sumner Marston","creator_url":"https://huggingface.co/summykai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"rushhour4x4-rl","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRush Hour 4x4 RL Evaluation Dataset\n\t\n\nThis dataset contains 3000 4x4 Rush Hour puzzles generated using reinforcement learning techniques for model evaluation.\n\n\t\n\t\t\n\t\tFormat\n\t\n\n\npuzzle_id: Unique identifier (puzzle151, puzzle1000, ...)\nprompt: Full formatted prompt as used in model inference\nsolution: Optimal solution with proper formatting\noptimal_moves: Number of moves in optimal solution\ndifficulty: Difficulty level based on puzzle complexity\n\n\n\t\n\t\t\n\t\tDifficulty Distribution‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mustafaah/rushhour4x4-rl.","url":"https://huggingface.co/datasets/mustafaah/rushhour4x4-rl","creator_name":"Mustafa Anis Hussain","creator_url":"https://huggingface.co/mustafaah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SHP-2-dpo-100k_sample","keyword":"dpo","description":"\n\t\n\t\t\n\t\t\n\t\n\n>>> dataset = load_dataset(\"pszemraj/SHP-2-dpo-100k_sample\")\n>>> dataset\nDatasetDict({\n    train: Dataset({\n        features: ['post_id', 'domain', 'upvote_ratio', 'seconds_difference', 'score_ratio', 'prompt', 'chosen', 'rejected', 'score_chosen', 'score_rejected'],\n        num_rows: 89971\n    })\n    validation: Dataset({\n        features: ['post_id', 'domain', 'upvote_ratio', 'seconds_difference', 'score_ratio', 'prompt', 'chosen', 'rejected', 'score_chosen', 'score_rejected']‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/SHP-2-dpo-100k_sample.","url":"https://huggingface.co/datasets/pszemraj/SHP-2-dpo-100k_sample","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["odc-by","100K - 1M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"Neural-DPO-ko","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"Neural-DPO\"\n\t\n\nTranslated NeuralNovel/Neural-DPO using nayohan/llama3-instrucTrans-enko-8b.\n","url":"https://huggingface.co/datasets/nayohan/Neural-DPO-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"gsm8k-reasoning","keyword":"rlaif","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for gsm8k-reasoning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nGSM8K Reasoning is a dataset derived from the openai/gsm8k dataset, focusing on enhancing math problem-solving through reasoning-based prompts and solutions.\nThis version emphasizes logical reasoning and step-by-step thought processes in mathematics, pushing models to generate solutions that reflect human-like deductive reasoning.\nThe dataset is curated using a specialized pipeline designed to encourage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/gsm8k-reasoning.","url":"https://huggingface.co/datasets/thesven/gsm8k-reasoning","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"vgc-battle-logs","keyword":"reinforcement-learning","description":"Dataset from the paper VGC-Bench: A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pok√©mon.\nCode: vgc-bench\nContains data for 14 Gen 9 Pok√©mon VGC formats, all filtered for Open Team Sheets (OTS). See:\n\nscrape_logs.py for the code that scraped these 531,475 logs from the Pok√©mon Showdown replay database\nlogs2trajs.py for the code that reads the logs from player 1 and 2's perspective, yielding 1,062,821 trajectories (99.99% successful log read rate), or 9,863‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.","url":"https://huggingface.co/datasets/cameronangliss/vgc-battle-logs","creator_name":"Cameron Angliss","creator_url":"https://huggingface.co/cameronangliss","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K<n<1M","arxiv:2506.10326","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"dpo-orpo-mix-45k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-45k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"okapi-ranking","keyword":"dpo","description":"Saugatkafley/okapi-ranking dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Saugatkafley/okapi-ranking","creator_name":"Saugat Kafley","creator_url":"https://huggingface.co/Saugatkafley","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Nepali","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LyricalSongsVerses_Ver4_forUnslothORPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nEDITED VARIANT 4 \nStyled after the RecipeResearch Dolphin Preference set, for use with the unsloth Llama3 (8B) ORPO Colab notebook\nRe-balanced, edited, substantially abridged/consolidated, somewhat re-expanded \nJsonl variant with excessive separators ('/' symbols) removed \nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/LyricalSongsVerses_Ver4_forUnslothORPO.","url":"https://huggingface.co/datasets/AlekseyCalvin/LyricalSongsVerses_Ver4_forUnslothORPO","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"league-of-legends-decoded-replay-packets","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis work isn‚Äôt endorsed by Riot Games and doesn‚Äôt reflect the views or opinions of Riot Games or anyone officially involved in producing or managing League of Legends. League of Legends and Riot Games are trademarks or registered trademarks of Riot Games, Inc.\n\n\t\n\t\t\n\t\tLeague of Legends Replays Dataset\n\t\n\nThis dataset contains over 1TB+ (700k+ replays) of League of Legends game replay data for research in gaming analytics, behavioral modeling, and reinforcement learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maknee/league-of-legends-decoded-replay-packets.","url":"https://huggingface.co/datasets/maknee/league-of-legends-decoded-replay-packets","creator_name":"maknee","creator_url":"https://huggingface.co/maknee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","time-series-forecasting","other","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Chatgpt","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RajChat/Chatgpt.","url":"https://huggingface.co/datasets/RajChat/Chatgpt","creator_name":"Rajdeep Chatterjee ","creator_url":"https://huggingface.co/RajChat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"WikiPrefs","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for WikiPrefs\n\t\n\n\n\nThe WikiPrefs dataset is a human preferences dataset created using the EditPrefs method. It was constructed from historical edits of Wikipedia featured articles.\nThe code used for creating the dataset is available on GitHub: https://github.com/jmajkutewicz/EditPrefs\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nLanguage: English\nLicense: Apache 2.0 Note that:\nthe text comes from Wikipedia and is subjected to CC BY-SA 4.0 license\nthe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmajkutewicz/WikiPrefs.","url":"https://huggingface.co/datasets/jmajkutewicz/WikiPrefs","creator_name":"Jan Majkutewicz","creator_url":"https://huggingface.co/jmajkutewicz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"vision-feedback-mix-binarized","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for Vision-Feedback-Mix-Binarized\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset aims to provide large-scale vision feedback data. \nIt is a combination of the following high-quality vision feedback datasets:\n\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\nMMInstruction/VLFeedback: 80,258 samples\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\nopenbmb/RLHF-V-Dataset: 5,733 samples\nopenbmb/RLAIF-V-Dataset: 83,132 samples\n\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized.","url":"https://huggingface.co/datasets/NiuTrans/vision-feedback-mix-binarized","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"poemma-10k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPoemma 10K\n\t\n\nDataset for naturalisations of language model outputs in the domain of poems.\n","url":"https://huggingface.co/datasets/0x7o/poemma-10k","creator_name":"Danil Kononyuk","creator_url":"https://huggingface.co/0x7o","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-nosafe","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maywell/hh-rlhf-nosafe.","url":"https://huggingface.co/datasets/maywell/hh-rlhf-nosafe","creator_name":"Jeonghwan Park","creator_url":"https://huggingface.co/maywell","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Atsunori-HelpSteer2-DPO","keyword":"human-feedback","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO.","url":"https://huggingface.co/datasets/Triangle104/Atsunori-HelpSteer2-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"KAIROS_EVAL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tKAIROS_EVAL Dataset\n\t\n\nPaper: LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions | Code (GitHub)\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nKAIROS is a benchmark dataset designed to evaluate the robustness of large language models (LLMs) in multi-agent, socially interactive scenarios. Unlike static QA datasets, KAIROS dynamically constructs evaluation settings for each model by capturing its original belief (answer + confidence) and then simulating peer influence through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/declare-lab/KAIROS_EVAL.","url":"https://huggingface.co/datasets/declare-lab/KAIROS_EVAL","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","multiple-choice","multiple-choice-qa","monolingual"],"keywords_longer_than_N":true},
	{"name":"ChatML-H4rmony_dpo","keyword":"reinforcement-learning","description":"neovalle/H4rmony_dpo in ChatML format, ready to use in HuggingFace TRL's DPO Trainer.\nPython code used for conversion:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"neovalle/H4rmony_dpo\", split=\"train\")\n\ndef format(columns):\n    return {\n        \"prompt\": f\"<|im_start|>user\\n{columns['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        \"chosen\": f\"{columns['chosen']}<|im_end|>\",\n        \"rejected\": f\"{columns['rejected']}<|im_end|>\",\n    }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo.","url":"https://huggingface.co/datasets/Felladrin/ChatML-H4rmony_dpo","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","reinforcement-learning","text-generation","mit"],"keywords_longer_than_N":true},
	{"name":"DeepTheorem","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning üöÄ\n\t\n\nWelcome to the GitHub repository for DeepTheorem üéâ, a comprehensive framework for enhancing large language model (LLM) mathematical reasoning through informal, natural language-based theorem proving. This project introduces a novel approach to automated theorem proving (ATP) by leveraging the informal reasoning strengths of LLMs, moving beyond traditional formal proof‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jiahao004/DeepTheorem.","url":"https://huggingface.co/datasets/Jiahao004/DeepTheorem","creator_name":"Jiahao Xu","creator_url":"https://huggingface.co/Jiahao004","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Smart-shower-usage-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSmart Spa Usage Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains real usage patterns from a smart spa system based on Kohler Anthem+ technology. It includes comprehensive user interaction data, device usage sequences, environmental parameters, and temporal patterns collected for machine learning research in smart home automation and predictive device control.\nThe dataset captures natural user behavior patterns in a smart spa environment with 20 different devices‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zachzzz33/Smart-shower-usage-dataset.","url":"https://huggingface.co/datasets/Zachzzz33/Smart-shower-usage-dataset","creator_name":"Sarguna Narayana Sachin S","creator_url":"https://huggingface.co/Zachzzz33","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["time-series-forecasting","reinforcement-learning","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"short-metaworld-vla","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tShort-MetaWorld Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nShort-MetaWorld is a curated dataset from Meta-World containing Multi-Task 10 (MT10) and Meta-Learning 10 (ML10) tasks with 100 successful trajectories per task and 20 steps per trajectory. This dataset is specifically designed for multi-task robot learning, imitation learning, and vision-language robotics research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom short_metaworld_loader import load_short_metaworld\nfrom torch.utils.data import DataLoader\n\n#‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hz1919810/short-metaworld-vla.","url":"https://huggingface.co/datasets/hz1919810/short-metaworld-vla","creator_name":"H.Z","creator_url":"https://huggingface.co/hz1919810","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"wmdp-cyber-corpus_unpaired-preference","keyword":"reinforcement-learning","description":"This is the \"cyber\" data from https://huggingface.co/datasets/cais/wmdp-corpora repackaged into the format of \"unpaired preference\". https://huggingface.co/docs/trl/v0.11.1/en/dataset_formats#unpaired-preference\nData to retain is considered good so it is mapped to \"True\", while forget data is mapped to \"False.\"\nPrompt is left empty since the text from WMDP doesn't come with a prompt.\nI have no idea whether this would really work for Reinforcement Learning, but I plan to try it out. Use at your‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference.","url":"https://huggingface.co/datasets/AMindToThink/wmdp-cyber-corpus_unpaired-preference","creator_name":"Matthew Khoriaty","creator_url":"https://huggingface.co/AMindToThink","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RLSTACK","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK.","url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Orca DPO Pair\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a pre-processed version of the OpenOrca dataset.\nThe original OpenOrca dataset is a collection of augmented FLAN data that aligns, as best as possible, with the distributions outlined in the Orca paper.\nIt has been instrumental in generating high-performing preference-tuned model checkpoints and serves as a valuable resource for all NLP researchers and developers!\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe OrcaDPO Pair‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs.","url":"https://huggingface.co/datasets/HuggingFaceH4/orca_dpo_pairs","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"social-reasoning-rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\n\nunderstanding of emotions\nintent recognition\nsocial norms\nsocial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf.","url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"social-reasoning-rlhf","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to a social reasoning dataset that aims to provide signal to how humans navigate social situations, how they reason about them and how they understand each other. It contains questions probing people's thinking and understanding of various social situations.\nThis dataset was created by collating a set of questions within the following social reasoning tasks:\n\nunderstanding of emotions\nintent recognition\nsocial norms\nsocial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf.","url":"https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf","creator_name":"Prolific","creator_url":"https://huggingface.co/ProlificAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"catboros-3.2-dpo","keyword":"rlhf","description":"\nCatboros-3.2 DPO Dataset\n  \n    \n      \n        Original Dataset\n        \n      \n      The creation of this catgirl personality DPO dataset was enabled by Jon Durbin's work on airoboros-3.2, which served as the foundational basis. Jon's dataset is accessible at jondurbin/airoboros-3.2. \n    \n    \n      \n        The Idea\n        \n      \n      The concept of a catgirl assistant was inspired by Sao's NatsumiV1 project, available at ( Sao10K/NatsumiV1). \n    \n    \n      \n        DPO Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo.","url":"https://huggingface.co/datasets/antiven0m/catboros-3.2-dpo","creator_name":"antiven0m","creator_url":"https://huggingface.co/antiven0m","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"hhh_alignment","keyword":"human-feedback","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses.","url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"hive-player-games","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains recorded match histories for the board game Hive, collected from the online platforms BoardGameArena and BoardSpace. All matches were played between human players; no AI or bot games are included.\nEach record represents a complete Hive game encoded in the Universal Hive Protocol (UHP) GameString format, describing every move taken throughout the match.\nGames include both base Hive and variations with different combinations of expansion pieces‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rhstephens/hive-player-games.","url":"https://huggingface.co/datasets/rhstephens/hive-player-games","creator_name":"Ryan Stephens","creator_url":"https://huggingface.co/rhstephens","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"synthetic-instruct-gptj-pairwise-ru\"\n\t\n\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","Dahoas/synthetic-instruct-gptj-pairwise","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"synthetic-instruct-gptj-pairwise-ru","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for \"synthetic-instruct-gptj-pairwise-ru\"\n\t\n\nThis is translated version of Dahoas/synthetic-instruct-gptj-pairwise dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/synthetic-instruct-gptj-pairwise-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","Dahoas/synthetic-instruct-gptj-pairwise","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-Processed","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tShareGPT-Processed\n\t\n\nThe RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.\n\n\t\n\t\t\n\t\tAcknowledgements\n\t\n\n\nvinta/pangu.js ‚Äî To insert whitespace between CJK (Chinese, Japanese, Korean) and half-width characters (alphabetical letters, numerical digits and symbols).\nmatthewwithanm/python-markdownify ‚Äî Provides a starting point to convert HTML to Markdown.\nBYVoid/OpenCC ‚Äî Conversions between Traditional Chinese and Simplified Chinese.\naboSamoor/polyglot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zetavg/ShareGPT-Processed.","url":"https://huggingface.co/datasets/zetavg/ShareGPT-Processed","creator_name":"Pokai Chang","creator_url":"https://huggingface.co/zetavg","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","Spanish","Japanese"],"keywords_longer_than_N":true},
	{"name":"human-style-preferences-images","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human preference datasets for text-to-image models, this release contains over 1,200,000 human preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-style-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-style-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"stratified-kmeans-diverse-reasoning-100K-1M","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tStratified K-Means Diverse Reasoning Dataset (100K-1M)\n\t\n\nA carefully balanced subset of NVIDIA's Llama-Nemotron Post-Training Dataset, featuring square-root rebalanced sampling across math, code, science, instruction-following, chat, and safety tasks at multiple scales.\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüë• Follow the Authors\n\t\n\nAman Priyanshu\n\n\nSupriti Vijay\n\n\n\n\n\n\n\t\t\n\t\n\t\tOverview\n\t\n\nThis dataset provides stratified subsets at 50k, 100k, 250k, 500k, and 1M scales from the Llama-Nemotron‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/stratified-kmeans-diverse-reasoning-100K-1M.","url":"https://huggingface.co/datasets/AmanPriyanshu/stratified-kmeans-diverse-reasoning-100K-1M","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CultriX-dpo","keyword":"dpo","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Open_Assistant_Conversation_Chains","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\n\n\nThis dataset is a reformatting of OpenAssistant Conversations (OASST1), which is\n\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.\n\nIt was modified‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains.","url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Conversation_Chains","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"stratified-kmeans-diverse-reasoning-100K-1M","keyword":"rl","description":"\n\t\n\t\t\n\t\tStratified K-Means Diverse Reasoning Dataset (100K-1M)\n\t\n\nA carefully balanced subset of NVIDIA's Llama-Nemotron Post-Training Dataset, featuring square-root rebalanced sampling across math, code, science, instruction-following, chat, and safety tasks at multiple scales.\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüë• Follow the Authors\n\t\n\nAman Priyanshu\n\n\nSupriti Vijay\n\n\n\n\n\n\n\t\t\n\t\n\t\tOverview\n\t\n\nThis dataset provides stratified subsets at 50k, 100k, 250k, 500k, and 1M scales from the Llama-Nemotron‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/stratified-kmeans-diverse-reasoning-100K-1M.","url":"https://huggingface.co/datasets/AmanPriyanshu/stratified-kmeans-diverse-reasoning-100K-1M","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PetraAI","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tPETRA\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nPETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nData is separated by language into /ar and /en directories\nWithin each language directory, data is separated by task into subdirectories  \nTasks include:\nTranslation\nSummarization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PetraAI/PetraAI.","url":"https://huggingface.co/datasets/PetraAI/PetraAI","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"KoopmanRL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for KoopmanRL\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains the collected experimental data used for the results of Koopman-Assisted Reinforcement Learning allowing for the full reproduction, and further use of the paper's results. To reproduce the results by running the experiments yourself, please see the source code of KoopmanRL.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset of the reinforcement learning experiments for KoopmanRL contains roughly 461MB of Tensorboard‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dynamicslab/KoopmanRL.","url":"https://huggingface.co/datasets/dynamicslab/KoopmanRL","creator_name":"Dynamicslab","creator_url":"https://huggingface.co/dynamicslab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","code","cc-by-4.0","doi:10.57967/hf/1825","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"dpo","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlhf","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ultradistil-intel-orca-dpo-de","keyword":"rlaif","description":"(WIP)\nCurrently this dataset is WIP - there seem to be some translation tasks in the dataset that may not be completly accurate. \nIn the next days, they will be filtered out. To do so manually, just look for \"√ºbersetz\" in the columns \"input\", \"chosen\" or \"rejected\"\nand exclude them from your training pipeline.\n\n\t\n\t\t\n\t\tULTRA Distilabel Intel Orca DPO (German):\n\t\n\nThis is the machine-translated German version of Intel's Orca DPO pairs, distilabeled by argilla.\nThe provided dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de.","url":"https://huggingface.co/datasets/aari1995/ultradistil-intel-orca-dpo-de","creator_name":"Aaron Chibb","creator_url":"https://huggingface.co/aari1995","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["German","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"H4rmony","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset H4rmony\n\t\n\n\n**** There is a simplified version, specifically curated for DPO training here: \n***** https://huggingface.co/datasets/neovalle/H4rmony_dpo\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe H4rmony dataset is a collection of prompts and completions aimed at integrating ecolinguistic principles into AI Large Language Models (LLMs). \nDeveloped with collaborative efforts from ecolinguistics enthusiasts and experts, it offers a series of prompts and corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/H4rmony.","url":"https://huggingface.co/datasets/neovalle/H4rmony","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-classification","question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"xlam-function-calling-60k","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAPIGen Function-Calling Datasets\n\t\n\nPaper | Website | Models\nThis repo contains 60,000 data collected by APIGen, an automated data generation pipeline designed to produce verifiable high-quality datasets for function-calling applications. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness. \nWe conducted human evaluation over 600 sampled data points, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lockon/xlam-function-calling-60k.","url":"https://huggingface.co/datasets/lockon/xlam-function-calling-60k","creator_name":"Junlong Li","creator_url":"https://huggingface.co/lockon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","reinforcement-learning","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"helpful_instructions","keyword":"human-feedback","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"instruct_me","keyword":"human-feedback","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \"chatty\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform.","url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","Text"],"keywords_longer_than_N":true},
	{"name":"OpenX-Embodiment","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOpen X-Embodiment Dataset (unofficial)\n\t\n\nThis is an unofficial Dataset Repo. This Repo is set up to make Open X-Embodiment Dataset (55 in 1) more accessible for people who love huggingfaceü§ó.\nOpen X-Embodiment Dataset is the largest open-source real robot dataset to date. It contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds.\nMore information is located on RT-X website‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jxu124/OpenX-Embodiment.","url":"https://huggingface.co/datasets/jxu124/OpenX-Embodiment","creator_name":"Jie Xu","creator_url":"https://huggingface.co/jxu124","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","cc-by-4.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"RyokoAI_ShareGPT52K","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K.","url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"deas_robocasa","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDEAS-RoboCasa\n\t\n\nRobocasa dataset used for fine-tuning GR00T-N1.5 in DEAS\n\n\t\n\t\t\n\t\tDataset Owner(s):\n\t\n\nChangyeon Kim\n\n\t\n\t\t\n\t\tDataset Creation Date:\n\t\n\nOctober 14, 2025\n\n\t\n\t\t\n\t\tDataset Format:\n\t\n\nLeRobot-style dataset\n\n\t\n\t\t\n\t\tDataset Quantification\n\t\n\ndemos: 300 Mimicgen generated expert trajectories for each of the 4 tabletop tasksrollouts: 300 rollouts (success + failure) for each of the 4 tabletop taskssuccess_rollouts: subset of rollouts (only success rollouts) for each of the 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/changyeon/deas_robocasa.","url":"https://huggingface.co/datasets/changyeon/deas_robocasa","creator_name":"changyeon kim","creator_url":"https://huggingface.co/changyeon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"HOT","keyword":"rl","description":"\n\t\n\t\t\n\t\tHOT Dataset: HVAC Operations Transfer\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe HOT (HVAC Operations Transfer) dataset is the first large-scale open-source dataset purpose-built for transfer learning research in building control systems. Buildings account for approximately 10-15% of global energy consumption through HVAC systems, making intelligent control optimization critical for energy efficiency and climate change mitigation.\nBeyond technical advances, the field needs standardized evaluation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BuildingBench/HOT.","url":"https://huggingface.co/datasets/BuildingBench/HOT","creator_name":"BuildingBench","creator_url":"https://huggingface.co/BuildingBench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K<n<1M","üá∫üá∏ Region: US","engineering","RL"],"keywords_longer_than_N":true},
	{"name":"processed-hh-rlhf","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for Processed-Hh-RLHF\n\t\n\nThis is a dataset that processes hh-rlhf into an easy-to-use conversational and human-preference form.\n","url":"https://huggingface.co/datasets/PKU-Alignment/processed-hh-rlhf","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"lave-human-feedback","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tLAVE human judgments\n\t\n\nThis repository contains the human judgment data for Improving Automatic VQA Evaluation Using Large Language Models. Details about the data collection process and crowdworker population can be found in our paper, specifically in section 5.2 and appendix A.1.\nFields:\n\ndataset: VQA dataset of origin for this example (vqav2, vgqa, okvqa).\nmodel: VQA model that generated the predicted answer (blip2, promptcap, blip_vqa, blip_vg).\nqid: question ID coming from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mair-lab/lave-human-feedback.","url":"https://huggingface.co/datasets/mair-lab/lave-human-feedback","creator_name":"MAIR Lab","creator_url":"https://huggingface.co/mair-lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"security-verifiers-e2-metadata","keyword":"rl","description":"\n\t\n\t\t\n\t\tüîí Security Verifiers E2: Security Configuration Verification (Public Metadata)\n\t\n\n\n‚ö†Ô∏è This is a PUBLIC metadata-only repository. The full datasets are hosted privately to prevent training contamination. See below for access instructions.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nE2 is a tool-grounded configuration auditing environment for Kubernetes and Terraform. This repository contains only the sampling metadata that describes how the private datasets were constructed.\n\n\t\n\t\t\n\t\tWhy Private Datasets?‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intertwine-ai/security-verifiers-e2-metadata.","url":"https://huggingface.co/datasets/intertwine-ai/security-verifiers-e2-metadata","creator_name":"Intertwine","creator_url":"https://huggingface.co/intertwine-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","English","mit","n<1K"],"keywords_longer_than_N":true},
	{"name":"security-verifiers-e1-metadata","keyword":"rl","description":"\n\t\n\t\t\n\t\tüîí Security Verifiers E1: Network Log Anomaly Detection (Public Metadata)\n\t\n\n\n‚ö†Ô∏è This is a PUBLIC metadata-only repository. The full datasets are hosted privately to prevent training contamination. See below for access instructions.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nE1 is a network log anomaly detection environment with calibrated classification and abstention. This repository contains only the sampling metadata that describes how the private datasets were constructed.\n\n\t\n\t\t\n\t\tWhy Private Datasets?‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intertwine-ai/security-verifiers-e1-metadata.","url":"https://huggingface.co/datasets/intertwine-ai/security-verifiers-e1-metadata","creator_name":"Intertwine","creator_url":"https://huggingface.co/intertwine-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"HelpSteer_binarized","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tBinarized version of HelpSteer\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA binarized version of https://huggingface.co/datasets/nvidia/HelpSteer ready for DPO using https://github.com/huggingface/alignment-handbook or similar.\nFor each unique prompt, we take the best and worst scoring (average of helpfulness and correctness) responses. These are converted into MessagesList format in the 'chosen' and 'rejected' columns.\n\nCreated by: dctanner and the team at Sablo AI\nLicense: CC BY 4.0\n\n","url":"https://huggingface.co/datasets/sablo/HelpSteer_binarized","creator_name":"Sablo AI","creator_url":"https://huggingface.co/sablo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"periodontal-reasoning-40k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPeriodontal-Reasoning-40k\n\t\n\n40,000 periodontal clinical reasoning examples for off-policy RLHF (KTO/DPO).\nFormat (JSONL, one per line): prompt, completion, label ‚àà {1,-1}\nExample:\n{\"prompt\": \"A patient's plaque score was 35% at baseline and 1% at follow‚Äëup. Determine whether the improvement is favourable according to BSP criteria (‚â§20% plaque or ‚â•50% reduction).\", \"completion\": \"The improvement is favourable.\", \"label\": 1}\nSplit: train: 40,000 (data/train.jsonl)\nIntended use: KTO/DPO;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k.","url":"https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"periodontal-reasoning-40k","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tPeriodontal-Reasoning-40k\n\t\n\n40,000 periodontal clinical reasoning examples for off-policy RLHF (KTO/DPO).\nFormat (JSONL, one per line): prompt, completion, label ‚àà {1,-1}\nExample:\n{\"prompt\": \"A patient's plaque score was 35% at baseline and 1% at follow‚Äëup. Determine whether the improvement is favourable according to BSP criteria (‚â§20% plaque or ‚â•50% reduction).\", \"completion\": \"The improvement is favourable.\", \"label\": 1}\nSplit: train: 40,000 (data/train.jsonl)\nIntended use: KTO/DPO;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k.","url":"https://huggingface.co/datasets/Wildstash/periodontal-reasoning-40k","creator_name":"ArnavS","creator_url":"https://huggingface.co/Wildstash","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","expert-generated","monolingual","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"rag_qa_embedding_questions_0_60_0","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for rag_qa_embedding_questions_0_60_0\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0.","url":"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0","creator_name":"ZenML","creator_url":"https://huggingface.co/zenml","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"swedish_healthcare_dpo_sft_dataset","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSwedish Healthcare DPO/SFT Dataset\n\t\n\nThis dataset contains Swedish conversational prompts paired with chosen (preferred) and rejected (dispreferred) responses, along with English translations. It is designed for fine-tuning Large Language Models (LLMs) using preference-based methods. The (prompt, chosen, rejected) structure makes it particularly well-suited for Direct Preference Optimization (DPO) and similar algorithms.\nSweden is renowned for its high standards in healthcare and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset.","url":"https://huggingface.co/datasets/rzgar/swedish_healthcare_dpo_sft_dataset","creator_name":"Rzgar","creator_url":"https://huggingface.co/rzgar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Swedish","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Reverse-Text-RL","keyword":"rl","description":"\n\t\n\t\t\n\t\tReverse-Text-RL\n\t\n\nA small, scrappy RL dataset used in prime-rl's CI to debug RL training asking a model to reverse small sentences character-by-character. Follows the general format of PrimeIntellect/Reverse-Text-SFT\nThe following script was used to generate the dataset.\nfrom datasets import Dataset, load_dataset\n\ndataset = load_dataset(\"willcb/R1-reverse-wikipedia-paragraphs-v1-1000\", split=\"train\")\nprompt = \"Reverse the text character-by-character. Put your answer in <reversed_text>‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrimeIntellect/Reverse-Text-RL.","url":"https://huggingface.co/datasets/PrimeIntellect/Reverse-Text-RL","creator_name":"Prime Intellect","creator_url":"https://huggingface.co/PrimeIntellect","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"chembench-rlvr-test2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tChemBench-RLVR: Comprehensive Chemistry Dataset for Reinforcement Learning from Verifiable Rewards\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChemBench-RLVR is a high-quality, balanced dataset containing 10,381 question-answer pairs across 14 chemistry task types. This dataset is specifically designed for training language models using Reinforcement Learning from Verifiable Rewards (RLVR), where all answers are computationally verifiable using established cheminformatics tools.\n\n\t\n\t\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/summykai/chembench-rlvr-test2.","url":"https://huggingface.co/datasets/summykai/chembench-rlvr-test2","creator_name":"Sumner Marston","creator_url":"https://huggingface.co/summykai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"gaia2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tGaia2\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGaia2 is a benchmark dataset for evaluating AI agent capabilities in simulated environments. The dataset contains 800 scenarios that test agent performance in environments where time flows continuously and events occur dynamically.\nThe dataset evaluates seven core capabilities: Execution (multi-step planning and state changes), Search (information gathering and synthesis), Adaptability (dynamic response to environmental‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/meta-agents-research-environments/gaia2.","url":"https://huggingface.co/datasets/meta-agents-research-environments/gaia2","creator_name":"Meta Agents Research Environments","creator_url":"https://huggingface.co/meta-agents-research-environments","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","task-planning","dialogue-modeling","dialogue-generation","conversational"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_format","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts' phrasal/phonetic, rhythmic, metric, syllabic, melodic, and other lyrical/performance-catered features, whilst retaining adequate semantic/significational fidelity.  \nThis dataset samples months and years‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_format.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_format","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-X","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nShareGPT-X is an expanded, snapshot of ~92K (ChatGPT) one-to-one human & LLM conversations harvested from X.com (formerly Twitter).The corpus spans January 2024 ‚Üí present (last ingest 2025-05) and is built entirely from public \"share\" links that users posted to their timelines.Each thread contains the original user prompt plus the assistant‚Äôs reply; no system prompts or metadata are exposed.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DSULT-Core/ShareGPT-X.","url":"https://huggingface.co/datasets/DSULT-Core/ShareGPT-X","creator_name":"DeSULT","creator_url":"https://huggingface.co/DSULT-Core","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","multilingual","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"rl","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tüìñ Overview\n\t\n\nDeepMath-103K is meticulously curated to push the boundaries of mathematical reasoning in language models. Key features include:1. Challenging Problems: DeepMath-103K has a strong focus on difficult mathematical problems (primarily Levels 5-9), significantly raising the complexity bar compared to many existing open datasets.\n \n\nDifficulty‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swpdd/test.","url":"https://huggingface.co/datasets/swpdd/test","creator_name":"weipeng","creator_url":"https://huggingface.co/swpdd","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","English","mit","100K<n<1M","arxiv:2504.11456"],"keywords_longer_than_N":true},
	{"name":"OpenGVLab_Lumina_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Lumina Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Lumina across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"fantasiq","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüêâ FantastiQ\n\t\n\n\n    \n\n\n\nFantastiQ: A fictional reasoning benchmark for evaluating inference and logical capabilities beyond memorization.\n\n\n\t\n\t\t\n\t\tWhat is FantastiQ?\n\t\n\nFantastiQ üêâ is a synthetic benchmark consisting of question-answer pairs crafted around fictional yet internally consistent scenarios. It is designed specifically to assess logical reasoning, inference skills, and robustness against explicit memorization by Large Language Models (LLMs).\nFantastiQ includes multiple‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sumukshashidhar-archive/fantasiq.","url":"https://huggingface.co/datasets/sumukshashidhar-archive/fantasiq","creator_name":"Sumuk's Archived Content","creator_url":"https://huggingface.co/sumukshashidhar-archive","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used with a Llama 3 70B Base Model, we achieve 88.8% on RewardBench, which makes it the 4th best Reward Model as of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/HelpSteer2.","url":"https://huggingface.co/datasets/H-D-T/HelpSteer2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"OmniRetarget_Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOmniRetarget Dataset: Humanoid Loco-Manipulation & Scene Interaction\n\t\n\nThis dataset contains motion trajectories of a G1 humanoid robot interacting with objects and complex terrains. It was generated by OMNIRETARGET, an interaction-preserving data generation engine that produces high-quality, kinematically feasible trajectories free of common artifacts like foot-skating and penetration.\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nDue to licensing restrictions, we cannot release the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omniretarget/OmniRetarget_Dataset.","url":"https://huggingface.co/datasets/omniretarget/OmniRetarget_Dataset","creator_name":"OmniRetarget","creator_url":"https://huggingface.co/omniretarget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["English","mit","üá∫üá∏ Region: US","robotics","motion-retargeting"],"keywords_longer_than_N":true},
	{"name":"SauerkrautLM-Fermented-GER-DPO","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tSauerkrautLM-Fermented-GER-DPO Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSauerkrautLM-Fermented-GER-DPO is a high-quality German instruction-response dataset specifically designed for Direct Preference Optimization (DPO) training. The dataset consists of 3,305 instruction-response pairs. Rather than being merged from existing German datasets, it was carefully created through a sophisticated augmentation process, transforming curated English instructions and responses into culturally adapted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO.","url":"https://huggingface.co/datasets/VAGOsolutions/SauerkrautLM-Fermented-GER-DPO","creator_name":"VAGO solutions","creator_url":"https://huggingface.co/VAGOsolutions","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","German","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"dpo","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mmlu_preferences","keyword":"rlhf","description":"reformat of MMLU to be in DPO (paired) format\nexamples:\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\n  college_chemistry\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences.","url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Agentic-Long-Context-Understanding-QA","keyword":"reinforcement-learning","description":" üìñ Agentic Long Context Understanding üìñ \n Self-Taught Agentic Long Context Understanding  (Arxiv). \n\n\n\n  \n  \n  \n\n AgenticLU refines complex, long-context queries through self-clarifications and contextual grounding, enabling robust long-document understanding in a single pass.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tInstallation Requirements\n\t\n\nThis codebase is largely based on OpenRLHF and Helmet, kudos to them.\nThe requirements are the same\npip install openrlhf\npip install -r ./HELMET/requirements.txt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA.","url":"https://huggingface.co/datasets/yzhuang/Agentic-Long-Context-Understanding-QA","creator_name":"Yufan Zhuang","creator_url":"https://huggingface.co/yzhuang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"dpo","description":"\n\t\n\t\t\n\t\tüíéüÜöüíé Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"tak-stack-dpo","keyword":"dpo","description":"\n\n\n\t\n\t\t\n\t\ttak-stack-dpo üß†\n\t\n\nA DPO alignment dataset for fine tuning open-source LLMs, taking sample preference pairs from a variety of datasets for diversity.  \nPrepared in the \"standard\" instruction, chosen, and rejected format, with a source feature indicating from which dataset the row was extracted.\nSource datasets:\n\nargilla/distilabel-math-preference-dpo\njondurbin/truthy-dpo-v0.1\nargilla/distilabel-intel-orca-dpo-pairs\nargilla/OpenHermes2.5-dpo-binarized-alpha\n\n","url":"https://huggingface.co/datasets/CorticalStack/tak-stack-dpo","creator_name":"JP Boyd","creator_url":"https://huggingface.co/CorticalStack","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gemma-vs-gemma-preferences","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tüíéüÜöüíé Gemma vs Gemma Preferences\n\t\n\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\n\nThe training would be off-policy for your model.\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\n\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences.","url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"yawp_thinking","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAmerican Yawp Enriched: A Reasoning Dataset for US History\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 1,000 high-quality question-and-answer pairs focused on US History, enriched with an AI-generated \"chain-of-thought\" reasoning step. The primary goal of this dataset is to provide a specialized resource for fine-tuning conversational language models to not only answer historical questions but also to explain the process of arriving at that answer.\nThe foundation of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ambrosfitz/yawp_thinking.","url":"https://huggingface.co/datasets/ambrosfitz/yawp_thinking","creator_name":"Christopher Smith","creator_url":"https://huggingface.co/ambrosfitz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Aya-AceGPT.13B.Chat-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-AceGPT.13B.Chat-DPO\" ü§ó\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO.","url":"https://huggingface.co/datasets/2A2I/Aya-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"KTO-mix-14k-vietnamese-groq","keyword":"rl","description":"Original dataset: https://huggingface.co/datasets/trl-lib/kto-mix-14k\nThis dataset is a KTO-formatted version of argilla/dpo-mix-7k. Please cite the original dataset if you find it useful in your work.\n\nTranslated to Vietnamese with context-aware using Groq Llama3.3 70B* via this repo:\nhttps://github.com/vTuanpham/Large_dataset_translator.\nRoughly 9 hours for 2k examples.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\nkto_mix_14k_vi =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq.","url":"https://huggingface.co/datasets/1TuanPham/KTO-mix-14k-vietnamese-groq","creator_name":"Pham Minh Tuan","creator_url":"https://huggingface.co/1TuanPham","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","Vietnamese","English"],"keywords_longer_than_N":true},
	{"name":"pitfall","keyword":"reinforcement-learning","description":"kimic/pitfall dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/kimic/pitfall","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"VQA-Verify","keyword":"reinforcement-learning","description":"This is the VQA-Verify dataset, introduced in the paper SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards.\nArxiv Here | Github\nVQA-Verify is a 12k dataset annotated with answer-aligned captions and bounding boxes. It's designed to facilitate training models for Visual Question Answering (VQA) tasks, particularly those employing free-form reasoning. The dataset addresses limitations in existing VQA datasets by providing verifiable intermediate steps and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/justairr/VQA-Verify.","url":"https://huggingface.co/datasets/justairr/VQA-Verify","creator_name":"Chuming Shen","creator_url":"https://huggingface.co/justairr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat-mlx","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a split version of orpo-dpo-mix-40k-flat for direct use with mlx-lm-lora, specifically tailored to be compatible with DPO and CPO training.\nThe dataset has been divided into three parts:\n\nTrain Set: 90%\nValidation Set: 6% \nTest Set: 4%\n\n\n\t\n\t\t\n\t\n\t\n\t\tExample Usage\n\t\n\nTo train a model using this dataset, you can use the following command:\nmlx_lm_lora.train \\\n--model Qwen/Qwen2.5-3B-Instruct \\\n--train \\\n--test \\\n--num-layers 8 \\\n--data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx.","url":"https://huggingface.co/datasets/mlx-community/orpo-dpo-mix-40k-flat-mlx","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"dpo","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"rl","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-chat-template","keyword":"rlhf","description":"Rexhaif/hh-rlhf-chat-template dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Rexhaif/hh-rlhf-chat-template","creator_name":"Daniil Larionov","creator_url":"https://huggingface.co/Rexhaif","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"GoDatas","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Go Game Dataset for Neural Network Training\n\t\n\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGF‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas.","url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GoDatas","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Go Game Dataset for Neural Network Training\n\t\n\nThis is a high-quality dataset designed for Go neural network training, containing board positions extracted from curated SGF game records. The dataset is divided into three strength categories: Standard, Strong, and Elite, with approximately 1,000 samples per category.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Go board positions and corresponding moves extracted from high-quality SGF‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/GoDatas.","url":"https://huggingface.co/datasets/Karesis/GoDatas","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VL-PRM300K","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for VL-PRM300K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nVL-PRM300K is a dataset of 300,000 samples of step-level solutions to a set of diverse and difficult visual reasoning tasks for training Vision Language Process Reward Models (VL-PRMs) with distilled reasoning traces from GPT-4.1 and judge solutions from o4-mini. Refer to the VL-PRMs paper for more details.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n# pip install -q datasets\nfrom datasets import load_dataset\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ob11/VL-PRM300K.","url":"https://huggingface.co/datasets/ob11/VL-PRM300K","creator_name":"Brandon Ong","creator_url":"https://huggingface.co/ob11","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","reinforcement-learning","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OmniEAR","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOmniEAR Expert Trajectory Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe OmniEAR Expert Trajectory SFT Dataset is a comprehensive collection of high-quality expert demonstration trajectories specifically designed for supervised fine-tuning (SFT) of embodied reasoning models. This dataset contains 1,982 instruction-following examples across single-agent and multi-agent scenarios, focusing on physical interactions, tool usage, and collaborative reasoning in embodied environments.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wangzx1210/OmniEAR.","url":"https://huggingface.co/datasets/wangzx1210/OmniEAR","creator_name":"wangzixuan","creator_url":"https://huggingface.co/wangzx1210","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","robotics","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"campus-ride-hailing","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nThis dataset simulates realistic ride hailing demand patterns in a university campus environment by transforming real-world bike sharing trip data into ride hailing scenarios. The synthetic generation preserves temporal demand patterns and spatial distributions while adapting the characteristics for ride hailing services. This makes it valuable for transportation research, demand forecasting, and mobility optimization algorithms without privacy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/oemer450/campus-ride-hailing.","url":"https://huggingface.co/datasets/oemer450/campus-ride-hailing","creator_name":"√ñmer Erduran","creator_url":"https://huggingface.co/oemer450","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","German","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"beyond_dpo_en","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_en.","url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_en","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"CHI","keyword":"dpo","description":"This repository outlines the methodology for creating training sets aimed at aligning a language model with a specific character and persona. \nThe process involves utilizing a Direct Preference Optimization (DPO) dataset to steer the model towards embodying the defined character and persona traits. \nFollowing this, a Unified Neutral Alignment (UNA) dataset is employed to moderate any excessive sentiments resulting from the DPO training. \nThe final step involves merging the model realigned with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-B/CHI.","url":"https://huggingface.co/datasets/AI-B/CHI","creator_name":"AI-B","creator_url":"https://huggingface.co/AI-B","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["unlicense","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Hopper-v3","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tHopper-v3 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 3608.1871.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v3.","url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v3","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"OpenR1-Math-220k-paired","keyword":"dpo","description":"\n\t\n\t\t\n\t\t!!! Is there anyone can help me? https://github.com/huggingface/trl/issues/2994\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset is built by filtering the open-r1/OpenR1-Math-220k dataset according to the following rules:\n\nFirst, filter all of rows with only correct answers\nThe chosen contains the shortest and correct generation, the rejected contains the wrong generation.\nAll data with a prompt+chosen length exceeding 16k are filtered out.\nWe provide the length for both chosen and rejected‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired.","url":"https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"OpenAI-4o_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata OpenAI 4o Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 200'000 human responses from over ~45,000 individual annotators, collected in less than half a day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"LONG-DPO","keyword":"dpo","description":"This dataset is a DPO version of the hassanjbara/LONG  dataset. The preference model used for choosing and rejecting responses is Hello-SimpleAI/chatgpt-detector-roberta, with the idea being creating a dataset for training a model to \"beat\" that detector. Full script for generating the dataset included in the scripts directory.\nThe chosen responses were generated by mistralai/Mistral-Nemo-Instruct-2407 and gpt-3.5-turbo.\n","url":"https://huggingface.co/datasets/hassanjbara/LONG-DPO","creator_name":"Hassan Jbara","creator_url":"https://huggingface.co/hassanjbara","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"customer-support-finetuning-dataset","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for customer-support-finetuning-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset.","url":"https://huggingface.co/datasets/debabrata-ai/customer-support-finetuning-dataset","creator_name":"Debabrata Bordoloi","creator_url":"https://huggingface.co/debabrata-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ConnectFour","keyword":"reinforcement-learning","description":"Connect 4 Solver Outputs\nAbout 100M + 60M observations and targets generated from selfplay with a solver [https://github.com/PascalPons/connect4] with temperature.\nObservations from different depths are roughly uniformly distributed, altough later positions are reached less frequently.\nAs a consequence early positions are duplicated and there is a small overlap between the train and test split (less than 3%).\nObservations are of shape (2,6,7) with binary (0 or 255) data.\nThe first channel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TonyCWang/ConnectFour.","url":"https://huggingface.co/datasets/TonyCWang/ConnectFour","creator_name":"Tony Congqian Wang","creator_url":"https://huggingface.co/TonyCWang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100M - 1B","parquet","Time-series"],"keywords_longer_than_N":true},
	{"name":"chembench-rlvr-test5","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tChemBench-RLVR: Comprehensive Chemistry Dataset for Reinforcement Learning from Verifiable Rewards\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChemBench-RLVR is a high-quality, balanced dataset containing 16,699 question-answer pairs across 14 chemistry task types. This dataset is specifically designed for training language models using Reinforcement Learning from Verifiable Rewards (RLVR), where all answers are computationally verifiable using established cheminformatics tools.\n\n\t\n\t\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/summykai/chembench-rlvr-test5.","url":"https://huggingface.co/datasets/summykai/chembench-rlvr-test5","creator_name":"Sumner Marston","creator_url":"https://huggingface.co/summykai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Hopper-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tHopper-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Hopper-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 3760.6908.\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v2.","url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"RAMPART-FL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRAMPART-FL: A Dataset for Offline Reinforcement Learning in Federated Participant Selection\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository contains a dataset package generated by the RAMPART-FL framework, a system for researching Reinforcement Learning (RL) based participant selection in Federated Learning (FL) for intrusion detection. The data originates from a 400-round, 25-client simulation where an RL agent used a Multi-Criteria strategy to select clients.\nThis package provides two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LCS-FS/RAMPART-FL.","url":"https://huggingface.co/datasets/LCS-FS/RAMPART-FL","creator_name":"Lucas Sousa","creator_url":"https://huggingface.co/LCS-FS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","üá∫üá∏ Region: US","federated-learning"],"keywords_longer_than_N":true},
	{"name":"RAMPART-FL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRAMPART-FL: A Dataset for Offline Reinforcement Learning in Federated Participant Selection\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository contains a dataset package generated by the RAMPART-FL framework, a system for researching Reinforcement Learning (RL) based participant selection in Federated Learning (FL) for intrusion detection. The data originates from a 400-round, 25-client simulation where an RL agent used a Multi-Criteria strategy to select clients.\nThis package provides two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LCS-FS/RAMPART-FL.","url":"https://huggingface.co/datasets/LCS-FS/RAMPART-FL","creator_name":"Lucas Sousa","creator_url":"https://huggingface.co/LCS-FS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","üá∫üá∏ Region: US","federated-learning"],"keywords_longer_than_N":true},
	{"name":"contextual-dpo-v0.1-ko","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"contextual-dpo-v0.1-ko\"\n\t\n\nTranslated jondurbin/contextual-dpo-v0.1 using nayohan/llama3-instrucTrans-enko-8b.\n","url":"https://huggingface.co/datasets/nayohan/contextual-dpo-v0.1-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Korean","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"GymnasiumRecording__SuperMarioBros_Nes","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSuperMarioBros-Nes Gameplay Dataset\n\t\n\nThis dataset contains 244 frames recorded from the Gymnasium environment SuperMarioBros-Nes across 1 episodes.\nEnvironment ID: SuperMarioBros-Nes\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains the following columns:\n\nepisode_id: Value(dtype='int64', id=None)\ntimestamp: Value(dtype='float64', id=None)\nimage: Image(mode=None, decode=True, id=None)\nstep: Value(dtype='int64', id=None)\naction: Sequence(feature=Value(dtype='int64', id=None)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tsilva/GymnasiumRecording__SuperMarioBros_Nes.","url":"https://huggingface.co/datasets/tsilva/GymnasiumRecording__SuperMarioBros_Nes","creator_name":"Tiago Silva","creator_url":"https://huggingface.co/tsilva","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Resumes","keyword":"reinforcement-learning","description":"LithiVR/Resumes dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/LithiVR/Resumes","creator_name":"Litheesh V R","creator_url":"https://huggingface.co/LithiVR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","< 1K","Document"],"keywords_longer_than_N":true},
	{"name":"Agentic-DPO-V0.1","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAgentic DPO V1.0\n\t\n\n\n\nThe Capx Agentic DPO (Direct Prompt Optimization) Dataset is a unique collection of prompts, chosen answers, and rejected answers designed to train and optimize AI models for agentic and intuitive processing. \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset covers a wide range of topics, including but not limited to problem-solving, creativity, analysis, and general knowledge. The prompts are specifically crafted to elicit agentic responses from the AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1.","url":"https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1","creator_name":"Capx AI","creator_url":"https://huggingface.co/Capx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"mypo-4k-rfc","keyword":"dpo","description":"\n\t\n\t\t\n\t\tmypo\n\t\n\nmypy + DPO = mypo\n\nThis is a preview version of what I'll be calling the mypo dataset, a DPO dataset focused on Python code quality. It is derived from iamtarun/python_code_instructions_18k_alpaca.\nmypo-4k-rfc is a DPO dataset with three columns:\n\nprompt\nfrom the original dataset\n\n\nrejected\ncode from the original dataset, found to have linting errors\n\n\nchosen\ncode from the original dataset, rewritten by codellama/CodeLlama-7b-Python-hf to address linting errors\n\n\n\nThe plan is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc.","url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"human-alignment-preferences-images","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated alignment datasets for text-to-image models, this release contains over 1,200,000 human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","reinforcement-learning","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"oaast_rm_full_jieba","keyword":"human-feedback","description":"Â∞ùËØïËß£ÂÜ≥\"llm repetition problem\"Ôºå‰ΩøÁî®ÂàÜËØçÊ®°ÂûãÂØπoaastËØ≠ÊñôËøõË°å‚ÄúÁªìÂ∑¥Âåñ‚ÄùÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈáçÂ§çÂÜÖÂÆπÊãíÁªùÊïàÊûú„ÄÇ\nAttempts to solve the \"llm repetition problem\" by using a segmentation model to enhance the oaast corpus with \"stuttering\" data to provide stronger rejection of duplicate content.\nÂÖ∂Ê¨°ÔºåËøòËøáÊª§Êéâ‰∫ÜÊâÄÊúâËá™ÊàëËÆ§Áü•ÁöÑÂæÆË∞ÉÊ†∑Êú¨„ÄÇ\nSecond, it also filters out all the fine-tuned samples of self-cognition.\nfiles:\n\noaast_rm_full_jieba.jsonl : word level repeat\noaast_rm_full_sent_jieba.jsonl : sentence level repeat\n\n","url":"https://huggingface.co/datasets/lenML/oaast_rm_full_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Ru2En_DPO_v3_RWKVcategories_CSV","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nEDITED VARIANT 3 \nRe-balanced, edited, substantially abridged/consolidated, somewhat re-expanded \nCSV Version, no more excessive separators, category titles altered for the RWKV LM RLHF trainer \nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_v3_RWKVcategories_CSV.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_v3_RWKVcategories_CSV","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMPR-Tiny","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMMPR-Tiny\n\t\n\nThis is the training data used during the online RL stage of InternVL3.5, which greatly improves the overall performance of InternVL3.5 across all scales. Our training code is also open-sourced.\nBased on MMPR-v1.2, we compute the accuracy of each query using the provided rollouts and select those whose model accuracy falls between 0.2 and 0.8 for online RL.\nWe further extend the dataset with recent multimodal datasets to enhance diversity.\nPlease refer to our paper for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Ru2En_DPO_songs_poems_v3_NoSeparators","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nEDITED VARIANT 3 \nRe-balanced, edited, substantially abridged/consolidated, somewhat re-expanded \nJsonl variant with excessive separators ('/' symbols) removed \nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts' phrasal/phonetic, rhythmic, metric, syllabic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_songs_poems_v3_NoSeparators.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_songs_poems_v3_NoSeparators","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"persian-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPersian Alpaca Preference Dataset\n\t\n\n\nThis repository contains the Persian translation of the original Alpaca dataset, along with additional preference data generated using the LLama3 70B model. The dataset has been prepared for language model alignment using Direct Preference Optimization (DPO) or similar methods. It consists of approximately 39,000 Persian records.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal Alpaca Dataset\n\t\n\nThe Alpaca dataset is a collection of text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-dpo.","url":"https://huggingface.co/datasets/myrkur/persian-dpo","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Persian","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated DPO datasets used to align Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe first stage of the Aloe-Beta alignment process. We curated data from many publicly available data sources, including three different types of data:\n\nMedical preference data: TsinghuaC3I/UltraMedical-Preference\n\nGeneral preference data:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO.","url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-7k-SHORT","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSHORTENED Argilla DPO Mix 7K Dataset\n\t\n\nThis is is a shortened version of the argilla/dpo-mix-7k dataset, shortened in two ways:\n\nFilter out all rows with chosen content exceeding 2,000 characters.\nFilter out all rows with the final assistant message of content exceeding 500 characters.\n\nThe original dataset card follows below.\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT.","url":"https://huggingface.co/datasets/Trelis/dpo-mix-7k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"synth-priv-v0.1","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for synth-priv-v0.1\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\npipeline.py.\nIt can be run directly using the CLI:\ndistilabel pipeline run --script \"https://huggingface.co/datasets/narodr/synth-priv-v0.1/raw/main/pipeline.py\"\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/narodr/synth-priv-v0.1.","url":"https://huggingface.co/datasets/narodr/synth-priv-v0.1","creator_name":"Nicol√°s","creator_url":"https://huggingface.co/narodr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"webui-dom-snapshots","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for WebUI DOM snapshots\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Gary Benson\nLanguages: Mostly English (87%);\nDutch, French, Chinese, Japanese (1-2% each); 30+ others (<1% each)\nLicense: CC0 1.0 Universal\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gbenson/webui-dom-snapshots.","url":"https://huggingface.co/datasets/gbenson/webui-dom-snapshots","creator_name":"Gary Benson","creator_url":"https://huggingface.co/gbenson","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","reinforcement-learning","text-classification","multilingual","biglab/webui-7k"],"keywords_longer_than_N":true},
	{"name":"MetaCognition-Preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMetaCognition Preference Dataset\n\t\n\nThe MetaCognition Preference Dataset is a structured dataset for analyzing and evaluating model reasoning through a cognitive decomposition lens. It is built on top of the NVIDIA HelpSteer2 dataset and extends it with annotations that reflect the meta-cognitive and tactical structure of language model outputs.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nEach sample in this dataset consists of:\n\nAn instruction or query.\nTwo model outputs (output_a, output_b) responding to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Death-Raider/MetaCognition-Preference.","url":"https://huggingface.co/datasets/Death-Raider/MetaCognition-Preference","creator_name":"Darsh Kachroo","creator_url":"https://huggingface.co/Death-Raider","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"chembench-rlvr-test3","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tChemBench-RLVR: Comprehensive Chemistry Dataset for Reinforcement Learning from Verifiable Rewards\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChemBench-RLVR is a high-quality, balanced dataset containing 16,699 question-answer pairs across 14 chemistry task types. This dataset is specifically designed for training language models using Reinforcement Learning from Verifiable Rewards (RLVR), where all answers are computationally verifiable using established cheminformatics tools.\n\n\t\n\t\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/summykai/chembench-rlvr-test3.","url":"https://huggingface.co/datasets/summykai/chembench-rlvr-test3","creator_name":"Sumner Marston","creator_url":"https://huggingface.co/summykai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"earl-datasets","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tEARL Tokenized Datasets\n\t\n\nThis dataset collection contains the VQ-tokenized versions of all datasets used in the paper The Promise of RL for Autoregressive Image Editing. Tokenization was performed using the official scripts from the EARL GitHub repository, converting images into discrete VQ tokens compatible with EARL‚Äôs autoregressive models.\nEach sample includes the tokenized representation of the original and edited images, the text instruction describing the edit, and, when‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mair-lab/earl-datasets.","url":"https://huggingface.co/datasets/mair-lab/earl-datasets","creator_name":"MAIR Lab","creator_url":"https://huggingface.co/mair-lab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","arxiv:2508.01119","arxiv:2411.07199","arxiv:2412.04280"],"keywords_longer_than_N":true},
	{"name":"chess-evaluations","keyword":"rl","description":"\n\t\n\t\t\n\t\tChess Evaluations Dataset\n\t\n\nThis dataset contains chess positions represented in FEN (Forsyth-Edwards Notation) along with their evaluations and next moves for tactical evals. The dataset is divided into three configurations:\n\ntactics: Includes chess positions, their evaluations, and the best move in the position.\nrandoms: Contains random chess positions and their evaluations.\nchess_data: General chess positions with evaluations.\n\nThis is an in progress dataset which contains millions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ssingh22/chess-evaluations.","url":"https://huggingface.co/datasets/ssingh22/chess-evaluations","creator_name":"Somesh Singh","creator_url":"https://huggingface.co/ssingh22","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","token-classification","mit","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"R-PRM","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüìò R-PRM Dataset (SFT + DPO)\n\t\n\nThis dataset is developed for training Reasoning-Driven Process Reward Models (R-PRM), proposed in our ACL 2025 paper. It consists of two stages:\n\nSFT (Supervised Fine-Tuning): collected from strong LLMs prompted with limited annotated examples, enabling reasoning-style evaluation.\nDPO (Direct Preference Optimization): constructed by sampling multiple reasoning trajectories and forming preference pairs without additional labels.\n\nThese datasets are used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kevinpro/R-PRM.","url":"https://huggingface.co/datasets/kevinpro/R-PRM","creator_name":"Shuaijie She","creator_url":"https://huggingface.co/kevinpro","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Qwen3-0.6B-icm-dpo-pairs","keyword":"dpo","description":"codelion/Qwen3-0.6B-icm-dpo-pairs dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/codelion/Qwen3-0.6B-icm-dpo-pairs","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Ant-v4","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tAnt-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 5913.2959.\nEach entry consists of:\nobs (list): observation with length 27.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v4.","url":"https://huggingface.co/datasets/NathanGavenski/Ant-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"stock-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüìà S&P 500 Comprehensive Stock Market Dataset\n\t\n\n\n  \n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tüéØ Dataset Overview\n\t\n\nThis comprehensive dataset contains 620,095 daily observations of S&P 500 companies with 73 meticulously engineered features spanning the last 5 years. Designed specifically for time series forecasting, stock price prediction, and advanced financial modeling tasks.\n\n\t\n\t\t\n\t\tüìä Key Statistics\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal Records\n620,095 daily observations\n\n\nFeatures\n73 comprehensive features‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/stock-dataset.","url":"https://huggingface.co/datasets/Adilbai/stock-dataset","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","reinforcement-learning","tabular-regression","English","mit"],"keywords_longer_than_N":true},
	{"name":"stock-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüìà S&P 500 Comprehensive Stock Market Dataset\n\t\n\n\n  \n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tüéØ Dataset Overview\n\t\n\nThis comprehensive dataset contains 620,095 daily observations of S&P 500 companies with 73 meticulously engineered features spanning the last 5 years. Designed specifically for time series forecasting, stock price prediction, and advanced financial modeling tasks.\n\n\t\n\t\t\n\t\tüìä Key Statistics\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal Records\n620,095 daily observations\n\n\nFeatures\n73 comprehensive features‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/stock-dataset.","url":"https://huggingface.co/datasets/Adilbai/stock-dataset","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["time-series-forecasting","reinforcement-learning","tabular-regression","English","mit"],"keywords_longer_than_N":true},
	{"name":"PFT-MME","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\n\t\n\n\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \nThrough a tallied voting mechanism, average scores are calculated to identify the \"worst\" and \"best\" answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME.","url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"nvidia-HelpSteer2","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Reward Model as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2.","url":"https://huggingface.co/datasets/Triangle104/nvidia-HelpSteer2","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Recraft-v3-24-7-25_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Recraft v3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft v3 (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"vigorl_datasets","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tViGoRL Datasets\n\t\n\nThis repository contains the official datasets associated with the paper \"Grounded Reinforcement Learning for Visual Reasoning (ViGoRL)\", by Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, and Katerina Fragkiadaki.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThese datasets are designed for training and evaluating visually grounded vision-language models (VLMs).\nDatasets are organized by the visual reasoning tasks described in the ViGoRL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gsarch/vigorl_datasets.","url":"https://huggingface.co/datasets/gsarch/vigorl_datasets","creator_name":"Gabriel H Sarch","creator_url":"https://huggingface.co/gsarch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"arc-loan-underwriting-trinity-rft-v2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tArc Loan Underwriting Trinity-RFT Dataset (v2.0)\n\t\n\nProduction-ready multi-agent loan underwriting dataset for Reinforcement Fine tuning training, featuring 200 loan applications with temperature-varied responses.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2\")\n\n# Each entry contains:\n# - prompt: The loan application and task description\n# - responses: 4 agent trajectories with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2.","url":"https://huggingface.co/datasets/Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arc-loan-underwriting-trinity-rft-v2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tArc Loan Underwriting Trinity-RFT Dataset (v2.0)\n\t\n\nProduction-ready multi-agent loan underwriting dataset for Reinforcement Fine tuning training, featuring 200 loan applications with temperature-varied responses.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2\")\n\n# Each entry contains:\n# - prompt: The loan application and task description\n# - responses: 4 agent trajectories with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2.","url":"https://huggingface.co/datasets/Jarrodbarnes/arc-loan-underwriting-trinity-rft-v2","creator_name":"Jarrod Barnes","creator_url":"https://huggingface.co/Jarrodbarnes","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-DPO-Atsunori","keyword":"human-feedback","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GenRM/HelpSteer2-DPO-Atsunori.","url":"https://huggingface.co/datasets/GenRM/HelpSteer2-DPO-Atsunori","creator_name":"GenRM: Generative Reward Models","creator_url":"https://huggingface.co/GenRM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"rule-reasoning","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRule Reasoning Datasets\n\t\n\nThis repository contains datasets for rule-based reasoning tasks, organized into two main categories:\n\n\t\n\t\t\n\t\tIn-Distribution (ID) Datasets\n\t\n\n\nar_lsat: Analytical Reasoning from LSAT\nclutrr: CLUTtRR (Compositional Language Understanding and Text-based Relational Reasoning)\nfolio: FOLIO (First-Order Logic in Natural Language)\nlogic_nli: Logic-based Natural Language Inference\nlogical_deduction: Logical Deduction tasks\nlogiqa: LogiQA (Logical Reasoning QA)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RuleReasoner/rule-reasoning.","url":"https://huggingface.co/datasets/RuleReasoner/rule-reasoning","creator_name":"RuleReasoner","creator_url":"https://huggingface.co/RuleReasoner","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"dpo","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"helpsteer3_preference","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer3. HelpSteer3 contains 40,476 preference samples, each containing a domain, language, context, two responses, an overall preference score between the responses as well as individual preferences from up to 3 annotators. Each individual preference contains a preference score in addition to a concise reasoning for their preference in 1-2 sentences. Data is split into 95% train and 5% validation.\nI processed the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer3_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer3_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Command.R-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO.","url":"https://huggingface.co/datasets/2A2I/NoRobots-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Arc-ATLAS-Teach-v1","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tArc-ATLAS-Teach\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis revision bundles 624 high-quality adaptive teaching examples that were generated and validated with the latest five-pass pipeline. Every dialogue walks through the full instructional arc‚Äîprobe, draft plan, checkpoint feedback, revised plan, and final solution‚Äîso the teaching policy observes the complete adjustment process without ever seeing the canonical answer. Probe turns capture the student‚Äôs diagnostic attempt, teacher plans and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1.","url":"https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v1","creator_name":"Arc Intelligence","creator_url":"https://huggingface.co/Arc-Intelligence","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"synthetic-domain-text-classification","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-b845cf19\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/davidberenstein1957/my-distiset-b845cf19/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/synthetic-domain-text-classification.","url":"https://huggingface.co/datasets/argilla/synthetic-domain-text-classification","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ngit-cli-fc-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSynthetic Function Calls\n\t\n\nA small dataset for ngit-cli pypi package, with query and structured responses for testing function calling capabilities in LLMs.\n\n\t\n\t\t\n\t\tDataset Format\n\t\n\nEach record contains:\n\nid: integer\nquery: natural language instruction\nanswers: stringified JSON of function name and arguments\ntools: task domain\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n{\n  \"id\": 0,\n  \"query\": \"Set git username to johnsmith\",\n  \"answers\": \"[{\\\"name\\\": \\\"git_config_username\\\", \\\"arguments\\\": {\\\"username\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/afmjoaa/ngit-cli-fc-dataset.","url":"https://huggingface.co/datasets/afmjoaa/ngit-cli-fc-dataset","creator_name":"A F M Mohimenul Joaa","creator_url":"https://huggingface.co/afmjoaa","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"h4rmony_dpo_multilingual","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual.","url":"https://huggingface.co/datasets/neovalle/h4rmony_dpo_multilingual","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","Spanish","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-binarized","keyword":"human-feedback","description":"juyoungml/HelpSteer2-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/juyoungml/HelpSteer2-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Vl-RewardBench","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhihui/Vl-RewardBench.","url":"https://huggingface.co/datasets/Zhihui/Vl-RewardBench","creator_name":"Xie","creator_url":"https://huggingface.co/Zhihui","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"dpo","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"rlhf","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"dpo","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-alt","keyword":"rlhf","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Swimmer-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tSwimmer-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Swimmer-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 4 episodes with an average episodic reward of 259.5244.\nEach entry‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Swimmer-v2.","url":"https://huggingface.co/datasets/NathanGavenski/Swimmer-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"xAI_Aurora_t2i_human_preferences","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Aurora Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Aurora across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"NoRobots-Aya.23.8B-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-Aya.23.8B-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-Aya.23.8B-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO.","url":"https://huggingface.co/datasets/2A2I/NoRobots-Aya.23.8B-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-50k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-50k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"HunyuanImage-2.1_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Hunyuan Image 2.1 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hunyuan Image 2.1 (version from 19.9.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-50k","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-50k\n\t\n\nThis dataset is designed for DPO or ORPO training.\nThis dataset combines samples of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\nargilla/distilabel-math-preference-dpo: highly scored chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-50k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"chembench-rlvr-test","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tChemBench-RLVR: Comprehensive Chemistry Dataset for Reinforcement Learning from Verifiable Rewards\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChemBench-RLVR is a high-quality, balanced dataset containing 7,001 question-answer pairs across 14 chemistry task types. This dataset is specifically designed for training language models using Reinforcement Learning from Verifiable Rewards (RLVR), where all answers are computationally verifiable using established cheminformatics tools.\n\n\t\n\t\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/summykai/chembench-rlvr-test.","url":"https://huggingface.co/datasets/summykai/chembench-rlvr-test","creator_name":"Sumner Marston","creator_url":"https://huggingface.co/summykai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"dpo","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"rlhf","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\n\nin main branch there is a custom scoring correct > helpful > -verbosity\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\n\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose.","url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"dpo","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"reinforcement-learning","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo.","url":"https://huggingface.co/datasets/data-is-better-together/aya_dutch_dpo","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"APASI-SI-dataset","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAPASI-SI-Dataset: Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations\n\t\n\nThis repository hosts the APASI Self-Injection (SI) Dataset, presented in the paper Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations.\nCode Repository: https://github.com/davidluciolu/APASI\nThe APASI (Autonomous Preference Alignment via Self-Injection) method proposes a novel approach to mitigate hallucinations in Large Vision-Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lucio36/APASI-SI-dataset.","url":"https://huggingface.co/datasets/lucio36/APASI-SI-dataset","creator_name":"david lu","creator_url":"https://huggingface.co/lucio36","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Ling-Coder-DPO","keyword":"dpo","description":"\n    \n\n\n\n          ü§ó Hugging Face\n          ü§ñ ModelScope\n          üñ•Ô∏è GitHub\n\n\n\n\t\n\t\t\n\t\tLing-Coder Dataset\n\t\n\nThe Ling-Coder Dataset comprises the following components:\n\nLing-Coder-SFT: A subset of SFT data used for training Ling-Coder Lite, containing more than 5 million samples.\nLing-Coder-DPO: A subset of DPO data used for training Ling-Coder Lite, containing 250k samples.\nLing-Coder-SyntheticQA: A subset of synthetic data used for annealing training of Ling-Coder Lite, containing more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO.","url":"https://huggingface.co/datasets/inclusionAI/Ling-Coder-DPO","creator_name":"inclusionAI","creator_url":"https://huggingface.co/inclusionAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-binarized","keyword":"dpo","description":"This is the binarized version of distilabel Orca Pairs for DPO and ORPO. \nReference: https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs?row=0\n","url":"https://huggingface.co/datasets/arcee-ai/distilabel-intel-orca-dpo-pairs-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Runway_Frames_t2i_human_preferences","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Frames Preference\n\t\n\n\n\n\n\nThis T2I dataset contains roughly 400k human responses from over 82k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Frames across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"oasst2_uzbek","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tOpen Assistant Conversations Dataset Release 2 (OASST2) in Uzbek language\n\t\n\nThis dataset is an Uzbek translated version of OASST2 dataset.\nLlama3 chat template + thread formatted dataset based on this translation is also available for model fine-tuning here. \nThe Uzbek translation was completed in 45 hours using a single T4 GPU and nllb-200-3.3B model.\nBased on nllb metrics, you might want to only filter out records that were not originally in English or Russian since English-Uzbek‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek.","url":"https://huggingface.co/datasets/MLDataScientist/oasst2_uzbek","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","Uzbek","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1","keyword":"dpo","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1.","url":"https://huggingface.co/datasets/jondurbin/gutenberg-dpo-v0.1","creator_name":"Jon Durbin","creator_url":"https://huggingface.co/jondurbin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MMPR-v1.2","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tMMPR-v1.2\n\t\n\n[üìÇ GitHub] | [üåê Project Page] | [üìú Paper (InternVL3.5)] | [üìú Paper (MMPR/MPO)] | [üÜï Blog (MPO)] | [üìñ Documents]\nThis is a newer version of MMPR and MMPR-v1.1, which includes additional data sources to enhance the data diversity and greatly improves the overall performance of InternVL3.5 across all scales. The prompts used to build this dataset is released in MMPR-v1.2-prompts.\nTo unzip the archive of images, please first run cat images.zip_* > images.zip and then run‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"og-marl","keyword":"reinforcement-learning","description":"\n@misc{formanek2024puttingdatacentreoffline,\n      title={Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning}, \n      author={Claude Formanek and Louise Beyers and Callum Rhys Tilbury and Jonathan P. Shock and Arnu Pretorius},\n      year={2024},\n      eprint={2409.12001},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2409.12001}, \n}\n\n","url":"https://huggingface.co/datasets/InstaDeepAI/og-marl","creator_name":"InstaDeep Ltd","creator_url":"https://huggingface.co/InstaDeepAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","< 1K","Datasets","Croissant"],"keywords_longer_than_N":true},
	{"name":"24-game","keyword":"rl","description":"\n\t\n\t\t\n\t\tMath Twenty Four (24s Game) Dataset\n\t\n\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, √ó, /) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game.","url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","other","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"synth-apigen-llama","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for argilla-warehouse/synth-apigen-llama\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\nsynth_apigen.py.\n\n\t\n\t\t\n\t\tDataset creation\n\t\n\nThis dataset is a replica in distilabel of the framework\ndefined in: APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets.\nUsing the seed dataset of synthetic python functions in argilla-warehouse/python-seed-tools,\nthe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama.","url":"https://huggingface.co/datasets/argilla-warehouse/synth-apigen-llama","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHub‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tORPO-DPO-Mix-TR-20k\n\t\n\n\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\n\n\t\n\t\t\n\t\n\t\n\t\tTranslation Process\n\t\n\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the GitHub‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k.","url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","text-generation","Turkish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"oaast_rm_zh_jieba","keyword":"human-feedback","description":"Â∞ùËØïËß£ÂÜ≥\"llm repetition problem\"Ôºå‰ΩøÁî®ÂàÜËØçÊ®°ÂûãÂØπoaastËØ≠ÊñôËøõË°å‚ÄúÁªìÂ∑¥Âåñ‚ÄùÊï∞ÊçÆÂ¢ûÂº∫ÔºåÊèê‰æõÊõ¥Âº∫ÁöÑÈáçÂ§çÂÜÖÂÆπÊãíÁªùÊïàÊûú„ÄÇ\nAttempts to solve the \"llm repetition problem\" by using a segmentation model to enhance the oaast corpus with \"stuttering\" data to provide stronger rejection of duplicate content.\nÂÖ∂Ê¨°ÔºåËøòËøáÊª§Êéâ‰∫ÜÊâÄÊúâËá™ÊàëËÆ§Áü•ÁöÑÂæÆË∞ÉÊ†∑Êú¨„ÄÇ\nSecond, it also filters out all the fine-tuned samples of self-cognition.\nfiles:\n\noaast_rm_zh_jieba.jsonl : word level repeat\noaast_rm_zh_sent_jieba.jsonl : sentence level repeat\n\n","url":"https://huggingface.co/datasets/lenML/oaast_rm_zh_jieba","creator_name":"len","creator_url":"https://huggingface.co/lenML","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ARPO-SFT-54K","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAgentic Reinforced Policy Optimization (ARPO) Dataset\n\t\n\nThis repository contains the datasets associated with the paper Agentic Reinforced Policy Optimization (ARPO).\nARPO proposes a novel agentic Reinforcement Learning algorithm designed for training multi-turn Large Language Model (LLM)-based agents. It addresses the challenge of balancing intrinsic long-horizon reasoning capabilities with proficiency in multi-turn tool interactions, particularly noting the increased uncertainty in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dongguanting/ARPO-SFT-54K.","url":"https://huggingface.co/datasets/dongguanting/ARPO-SFT-54K","creator_name":"KABI","creator_url":"https://huggingface.co/dongguanting","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"safe-pair-data","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tSafe Pair Data\n\t\n\nA locally curated dataset with train and test splits for preference-based training.\nThis is a filtered subset of PKU-Alignment/PKU-SafeRLHF-30K.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset('Mingyin0312/safe-pair-data')\nprint(ds)\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\n\ntrain/\ntest/\n\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\nSaved with datasets.save_to_disk; reloadable via load_from_disk.\n\n","url":"https://huggingface.co/datasets/Mingyin0312/safe-pair-data","creator_name":"Ming Yin","creator_url":"https://huggingface.co/Mingyin0312","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","PKU-Alignment/PKU-SafeRLHF-30K","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"data-science-sentetic-data","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for data-science-sentetic-data\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data.","url":"https://huggingface.co/datasets/emredeveloper/data-science-sentetic-data","creator_name":"C. Emre Karata≈ü","creator_url":"https://huggingface.co/emredeveloper","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Ant-v2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAnt-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Ant-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 5514.0229.\nEach entry consists‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v2.","url":"https://huggingface.co/datasets/NathanGavenski/Ant-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"CoProv2-SDXL","keyword":"dpo","description":"This repository contains CoProV2, a synthetically generated dataset of harmful and safe image-text pairs. It was introduced in the paper AlignGuard: Scalable Safety Alignment for Text-to-Image Generation.\nCoProV2 is specifically designed to enable the application of Direct Preference Optimization (DPO) for safety purposes in Text-to-Image (T2I) models. It facilitates the training of \"safety experts\" to guide the generative process away from specific safety-related concepts, enabling scalable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Visualignment/CoProv2-SDXL.","url":"https://huggingface.co/datasets/Visualignment/CoProv2-SDXL","creator_name":"Visualignment","creator_url":"https://huggingface.co/Visualignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Ant-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tAnt-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on Ant-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 5514.0229.\nEach entry consists‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Ant-v2.","url":"https://huggingface.co/datasets/NathanGavenski/Ant-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"ESC-Pro","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for ESC-Pro\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nESC-Pro is a high-quality preference dataset designed for training and evaluating dialogue models using preference-based alignment methods such as Direct Preference Optimization (DPO). Each turn in the dialogue contains one optimal response (preferred) and multiple non-preferred responses, enabling the construction of preference pairs for learning from human or algorithmic feedback.\nThe dataset is derived from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XingYuSSS/ESC-Pro.","url":"https://huggingface.co/datasets/XingYuSSS/ESC-Pro","creator_name":"sss","creator_url":"https://huggingface.co/XingYuSSS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ESC-Pro","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for ESC-Pro\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nESC-Pro is a high-quality preference dataset designed for training and evaluating dialogue models using preference-based alignment methods such as Direct Preference Optimization (DPO). Each turn in the dialogue contains one optimal response (preferred) and multiple non-preferred responses, enabling the construction of preference pairs for learning from human or algorithmic feedback.\nThe dataset is derived from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/XingYuSSS/ESC-Pro.","url":"https://huggingface.co/datasets/XingYuSSS/ESC-Pro","creator_name":"sss","creator_url":"https://huggingface.co/XingYuSSS","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"tron-dataset-v.1.0","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMetareasoning: To Reason or Not, Zero-Shot Classification for Reasoning Tasks\n\t\n\nThe To Reason or Not (TRON) Dataset represents a tangible advancement in the use of reasoning models by way of an architectural paradigm that we will refer to as Metareasoning. Metareasoning is the practice by which we ask a reasoning model to reason whether or not further reasoning is required to respond to a given prompt.\n\n\t\n\t\t\n\t\tPurpose and Scope\n\t\n\nThe TRON Dataset is designed to train lightweight‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/tron-dataset-v.1.0.","url":"https://huggingface.co/datasets/ZennyKenny/tron-dataset-v.1.0","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset.","url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jat-project/jat-dataset.","url":"https://huggingface.co/datasets/jat-project/jat-dataset","creator_name":"Jack of All Trades project","creator_url":"https://huggingface.co/jat-project","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"Code_Vulnerability_Security_DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tCybernative.ai Code Vulnerability and Security Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Cybernative.ai Code Vulnerability and Security Dataset is a dataset of synthetic Data Programming by Demonstration (DPO) pairs, focusing on the intricate relationship between secure and insecure code across a variety of programming languages. This dataset is meticulously crafted to serve as a pivotal resource for researchers, cybersecurity professionals, and AI developers who are keen on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO.","url":"https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO","creator_name":"Byte","creator_url":"https://huggingface.co/CyberNative","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\n\t\n\nExport of oasst2 only top 1 threads in huggingface chat format\n\n\t\n\t\t\n\t\tScript\n\t\n\nThe convert script can be find here\n","url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"Auto-Rubric","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tAuto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling\n\t\n\nThis is the official dataset release for the paper: \"Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling\".\nThis repository contains query-specific rubrics datasets where each preference pair is annotated with its own specific rubric, along with generation metadata. This dataset is used for training, analysis, and reproduction of the Auto-Rubric methodology.\n\n\t\n\t\t\n\t\n\t\n\t\tQuery-Specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentscope-ai/Auto-Rubric.","url":"https://huggingface.co/datasets/agentscope-ai/Auto-Rubric","creator_name":"agentscope-ai","creator_url":"https://huggingface.co/agentscope-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","RLHF","Reward Modeling","Preference Dataset"],"keywords_longer_than_N":true},
	{"name":"H4rmony_dpo","keyword":"reinforcement-learning","description":"This dataset is based on neovalle/H4rmony, and optimised to the format required by DPOTrainer from the trl library.\n","url":"https://huggingface.co/datasets/neovalle/H4rmony_dpo","creator_name":"Jorge Vallego","creator_url":"https://huggingface.co/neovalle","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","reinforcement-learning","text-generation","mit"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered-queries","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\nThis is the version of the dataset with only filtering on the queries, and hence there is more data than in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries.","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\nRead more about Argilla's approach towards UltraFeedback binarization at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MATH-Beyond","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMATH-Beyond\n\t\n\nA benchmark dataset for evaluating reinforcement learning methods on challenging mathematical problems that base models struggle with.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nMATH-Beyond contains 181 carefully selected mathematical problems that are unsolved by at least one of 21 base language models. This dataset is designed to evaluate the effectiveness of RL methods in pushing the boundaries of mathematical reasoning capabilities.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n181 challenging problems‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/brendel-group/MATH-Beyond.","url":"https://huggingface.co/datasets/brendel-group/MATH-Beyond","creator_name":"Robust Machine Learning Group","creator_url":"https://huggingface.co/brendel-group","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"D4RL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tD4RL Dataset on HuggingFace\n\t\n\nThis repository hosts the pre-downloaded D4RL dataset on HuggingFace. It is designed to provide accelerated data downloading for users, eliminating the need to download the dataset from scratch.\n\n\t\n\t\t\n\t\tInstallation\n\t\n\nTo use this dataset, you need to clone it into your local .d4rl directory. Here are the steps to do so:\n\nNavigate to your .d4rl directory:\n\ncd ~/.d4rl\n\n\nClone the dataset repository from HuggingFace:\n\ngit clone‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imone/D4RL.","url":"https://huggingface.co/datasets/imone/D4RL","creator_name":"One","creator_url":"https://huggingface.co/imone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"ultrafeedback-sample","keyword":"dpo","description":"pgurazada1/ultrafeedback-sample dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/pgurazada1/ultrafeedback-sample","creator_name":"Pavankumar Gurazada","creator_url":"https://huggingface.co/pgurazada1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"robot_demos_with_state_reset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRobot Demonstrations with Environment States\n\t\n\nDataset of robot demonstrations for various benchmarks formatted to contain environment states following the ManiSkill2 trajectory format: https://haosulab.github.io/ManiSkill2/concepts/demonstrations.html#format\nBenchmarks supported:\n\nManiSkill2\nAdroit (D4RL)\nMetaworld\n\nFor code to load and reset to states, as well as formatting the origianl demonstration datasets in supported benchmarks, see this repo:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stonet2000/robot_demos_with_state_reset.","url":"https://huggingface.co/datasets/stonet2000/robot_demos_with_state_reset","creator_name":"Stone Tao","creator_url":"https://huggingface.co/stonet2000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"rlvr-guru-raw-data-extended","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRLVR GURU Extended: Compiling a 150K Cross-Domain Dataset for RLVR\n\t\n\nA comprehensive cross-domain reasoning dataset containing 150,000 training samples and 221,332 test samples across diverse reasoning-intensive domains. This dataset extends the foundational work from the GURU dataset (Cheng et al., 2025) by incorporating additional STEM reasoning domains (MedMCQA and CommonsenseQA) while maintaining rigorous quality standards and verification mechanisms essential for reinforcement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/rlvr-guru-raw-data-extended.","url":"https://huggingface.co/datasets/AmanPriyanshu/rlvr-guru-raw-data-extended","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","100K - 1M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"rlvr-guru-raw-data-extended","keyword":"rl","description":"\n\t\n\t\t\n\t\tRLVR GURU Extended: Compiling a 150K Cross-Domain Dataset for RLVR\n\t\n\nA comprehensive cross-domain reasoning dataset containing 150,000 training samples and 221,332 test samples across diverse reasoning-intensive domains. This dataset extends the foundational work from the GURU dataset (Cheng et al., 2025) by incorporating additional STEM reasoning domains (MedMCQA and CommonsenseQA) while maintaining rigorous quality standards and verification mechanisms essential for reinforcement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/rlvr-guru-raw-data-extended.","url":"https://huggingface.co/datasets/AmanPriyanshu/rlvr-guru-raw-data-extended","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","odc-by","100K - 1M","arrow","Text"],"keywords_longer_than_N":true},
	{"name":"reescritura-textos-administrativos","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for reescritura-textos-administrativos\n\t\n\nThis dataset has been created with Argilla.\nAs shown in the sections below, this dataset can be loaded into Argilla as explained in Load with Argilla, or used directly with the datasets library in Load with datasets.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains:\n\nA dataset configuration file conforming to the Argilla dataset format named argilla.yaml. This configuration file will be used to configure the dataset when using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos.","url":"https://huggingface.co/datasets/somosnlp/reescritura-textos-administrativos","creator_name":"SomosNLP","creator_url":"https://huggingface.co/somosnlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Spanish","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"music-arena-dataset","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tMusic Arena Dataset\n\t\n\nThis is the official dataset from Music Arena, an open platform for evaluating text-to-music (TTM) models.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nLoad the dataset with a single line of code. The audio will be decoded automatically for public models. Use streaming=True to avoid downloading all data at once.\nfrom datasets import load_dataset\n\n# Stream the dataset (recommended)\nds = load_dataset(\"music-arena/music-arena-dataset\", name=\"2025_jul_aug\", split=\"train\", streaming=True)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/music-arena/music-arena-dataset.","url":"https://huggingface.co/datasets/music-arena/music-arena-dataset","creator_name":"Music Arena","creator_url":"https://huggingface.co/music-arena","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","json","Audio","Text"],"keywords_longer_than_N":true},
	{"name":"configurable-system-prompt-multitask","keyword":"dpo","description":"\n\t\n\t\t\n\t\tConfigurable System Prompt Multi-task Dataset üõû\n\t\n\nWe release the synthetic dataset for the multi-task experiments from the paper \"Configurable Safety Tuning of Language Models with Synthetic Preference Data\", https://huggingface.co/papers/2404.00495. This dataset has two sources for the examples:\n\nSelf-critique on a safety task from Harmful Behaviours, using the SOLAR-Instruct model. It employs two system prompts to learn the different behaviors:\nYou are a helpful yet harmless‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask.","url":"https://huggingface.co/datasets/vicgalle/configurable-system-prompt-multitask","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"openassistant-falcon","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant Falcon\n\t\n\nThis dataset allows for fine-tuning chat models using '\\Human:' AND '\\nAssistant:' to wrap user messages.\nIt still uses <|endoftext|> as EOS and BOS token, as per Falcon.\nSample \nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-falcon.","url":"https://huggingface.co/datasets/Trelis/openassistant-falcon","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"oasst1","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tOpenAssistant Conversations Dataset (OASST1)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nIn an effort to democratize research on large-scale alignment, we release OpenAssistant \nConversations (OASST1), a human-generated, human-annotated assistant-style conversation \ncorpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 \nquality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus \nis a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1.","url":"https://huggingface.co/datasets/OpenAssistant/oasst1","creator_name":"OpenAssistant","creator_url":"https://huggingface.co/OpenAssistant","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"genies_preferences","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for \"genie_dpo\"\n\t\n\nA conversion of the distribution from GENIES to open_pref_eval format.\nConversion code\n","url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for HH-RLHF\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis repository provides access to two different kinds of data:\n\nHuman preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/polinaeterna/hh-rlhf.","url":"https://huggingface.co/datasets/polinaeterna/hh-rlhf","creator_name":"Polina Kazakova","creator_url":"https://huggingface.co/polinaeterna","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"Mistral-EN-DPO-9K","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"Mistral-EN-DPO-9K\"\n\t\n\n\n\t\n\t\t\n\t\tInfo\n\t\n\nWe used snorkelai/Snorkel-Mistral-PairRM-DPO-Dataset dataset.We selected train_iteration_1 part.  \n\n\t\n\t\t\n\t\tPre-processing\n\t\n\n\nRemove coding task\n\nFiltering words: ['[Latex]', 'java', 'SQL', 'C#', 'nextjs', 'react', 'Ruby', 'Lua', 'Unity', 'XML', 'qrcode', 'jest', 'const', \n                    'python', 'Python', 'R code', 'Next.js', 'Node.js', 'Typescript', 'HTML', 'php', 'skeleton code', \n                    'MATLAB', 'using js'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K.","url":"https://huggingface.co/datasets/kyujinpy/Mistral-EN-DPO-9K","creator_name":"KyujinHan","creator_url":"https://huggingface.co/kyujinpy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"reinforcement-learning","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"rlhf","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"opin-pref","keyword":"rlaif","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\nhf.co/swaroop-nath/prompt-opin-summ dataset.\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref.","url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","summarization","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ORPRO-Spider-SQL-Feedback","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for ORPRO-Spider-SQL-Feedback\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback.","url":"https://huggingface.co/datasets/mjerome89/ORPRO-Spider-SQL-Feedback","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-1.5-Pro","keyword":"rl","description":"\n\t\n\t\t\n\t\tNuminaMath-1.5-Pro\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nNuminaMath-1.5-Pro targets post-training and verifiable reasoning scenarios. It applies strict filtering, judge-based consistency checks, and staged solution regeneration on top of the upstream NuminaMath-1.5 dataset.\nAll data processing and synthesis for this dataset is executed with the BlossomData framework, covering the full pipeline‚Äîloading, filtering, judging, generation, retry, and export‚Äîwith an emphasis on reproducibility‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Azure99/NuminaMath-1.5-Pro.","url":"https://huggingface.co/datasets/Azure99/NuminaMath-1.5-Pro","creator_name":"Azure99","creator_url":"https://huggingface.co/Azure99","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer2. HelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses. This dataset has been created in partnership with Scale AI.\nI processed the raw data by prioritizing helpfulness, correctness, and coherence to determine which responses were chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer2_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer2_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_preference","keyword":"dpo","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer2. HelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses. This dataset has been created in partnership with Scale AI.\nI processed the raw data by prioritizing helpfulness, correctness, and coherence to determine which responses were chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer2_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer2_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"UF_DPO","keyword":"dpo","description":"Each row has chosen and rejected string fields containing the linearized multi-turn dialogue in the form:\n\nHuman: ...\nAssistant: ...\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\n\ndata/train.jsonl\ndata/test.jsonl\n\nGenerated on 2025-08-08.\n","url":"https://huggingface.co/datasets/kamandmesbah/UF_DPO","creator_name":"kamand mesbah","creator_url":"https://huggingface.co/kamandmesbah","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"helpsteer2_preference","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a binarized preference datasets from nvidia/HelpSteer2. HelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses. This dataset has been created in partnership with Scale AI.\nI processed the raw data by prioritizing helpfulness, correctness, and coherence to determine which responses were chosen‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/helpsteer2_preference.","url":"https://huggingface.co/datasets/AIR-hl/helpsteer2_preference","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"DATA-AI_Chat","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDATA-AI: Il Modello di IA di M.INC.\n\t\n\n\n\t\n\t\t\n\t\tüìå Introduzione\n\t\n\nDATA-AI √® un avanzato modello di intelligenza artificiale sviluppato da *M.INC., un'azienda italiana fondata da *Mattimax (M. Marzorati).Questo modello √® basato sull'architettura ELNS (Elaborazione del Linguaggio Naturale Semplice), un sistema innovativo progettato per rendere l'IA accessibile su quasi qualsiasi dispositivo, garantendo prestazioni ottimali anche su hardware limitato.  \nDATA-AI √® stato addestrato su un‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mattimax/DATA-AI_Chat.","url":"https://huggingface.co/datasets/Mattimax/DATA-AI_Chat","creator_name":"M.INC.","creator_url":"https://huggingface.co/Mattimax","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Italian","English","Spanish","Russian","German"],"keywords_longer_than_N":true},
	{"name":"Qwen3-0.6B-pts-dpo-pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPTS DPO Dataset\n\t\n\nA Direct Preference Optimization (DPO) dataset created using the Pivotal Token Search (PTS) technique.\n\n\t\n\t\t\n\t\tDetails\n\t\n\n\nSource: Generated using the PTS tool\nModel: Qwen/Qwen3-0.6B\n\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach example in the dataset consists of:\n\nprompt: The context leading up to the pivotal token\nchosen: The preferred token that increases success probability\nrejected: The alternative token that decreases success probability\nmetadata: Additional information about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts-dpo-pairs.","url":"https://huggingface.co/datasets/codelion/Qwen3-0.6B-pts-dpo-pairs","creator_name":"Asankhaya Sharma","creator_url":"https://huggingface.co/codelion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ElectricalDeviceFeedbackBalanced","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tElectricalDeviceFeedbackBalanced\n\t\n\nThis dataset contains balanced feedback on electrical devices focusing on smart meters, solar panels, circuit breakers, inverters etc. It extends the original Electrical Device Feedback dataset with additional data generated through Large Language Models (LLMs) and careful prompt engineering.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nTrain: 11,552 samples with balanced class distribution \n\nMixed: 3,143\nNegative: 2,975\nNeutral: 2,762\nPositive: 2,672\nNote: Balance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced.","url":"https://huggingface.co/datasets/disham993/ElectricalDeviceFeedbackBalanced","creator_name":"Doula Isham Rashik Hasan","creator_url":"https://huggingface.co/disham993","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"dpo","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tü•áü•à Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita-reranked","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tEvol DPO Ita Reranked\n\t\n\n\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\n\n\t\n\t\t\n\t\n\t\n\t\tü•áü•à Reranking process\n\t\n\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\nChoosing the response from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked.","url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"reinforcement-learning","description":"\n  \n\n\n[üìñ arXiv Paper] \n[üìä R1-Reward Code] \n[üìù R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nüî• We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"AnswerCarefully_DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tAnswerCarefully Translated and Augmented Dataset\n\t\n\nThis dataset is a preprocessed version of llm-jp/AnswerCarefully, adapted for DPO (Direct Preference Optimization) training.\n\n\t\n\t\t\n\t\tDataset Creation Process\n\t\n\n\nTranslation: The original llm-jp/AnswerCarefully dataset, which is in English, was translated to Japanese using the Qwen3-32B model.\nRejection Sampling: A rejected response was generated for each question using the Qwen3-14B model. This provides a contrastive pair (chosen vs.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/AnswerCarefully_DPO.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/AnswerCarefully_DPO","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","machine-generated","machine-generated","monolingual","llm-jp/AnswerCarefully"],"keywords_longer_than_N":true},
	{"name":"HalfCheetah-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tHalfCheetah-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on HalfCheetah-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of 7581.5527.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2.","url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-SetTable","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB SetTable Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB SetTable.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB SetTable. Each subtask/object combination (e.g pick 013_apple) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered for safe robot behavior with a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-SetTable.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-SetTable","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-SetTable","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB SetTable Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB SetTable.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB SetTable. Each subtask/object combination (e.g pick 013_apple) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered for safe robot behavior with a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-SetTable.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-SetTable","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"pacman","keyword":"reinforcement-learning","description":"This dataset contains state-action pairs of three different Pacman levels, each located in its own tar.gz.\nEach tar.gz contains output/, dataset.csv, and results.txt.\n\noutput/ contains all the images/frames generated by an RL agent based on Taylor Downey's RL Agent.\ndataset.csv contains state-action pairs that match each frame to an action of either North, South, East, West, or Stop. There is also a column indicating whether a new game has started.\nresults.txt contains the results of either‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimic/pacman.","url":"https://huggingface.co/datasets/kimic/pacman","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Aya-SambaLingo.Arabic.Chat-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"Aya-SambaLingo.Arabic.Chat-DPO\" ü§ó\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-SambaLingo.Arabic.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO.","url":"https://huggingface.co/datasets/2A2I/Aya-SambaLingo.Arabic.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Retargeted_AMASS_for_FourierN1","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRetargeted AMASS for Robotics\n\t\n\n\n\t\n\t\t\n\t\tProject Overview\n\t\n\nThis project aims to retarget motion data from the AMASS dataset to various robot models and open-source the retargeted data to facilitate research and applications in robotics and human-robot interaction. AMASS (Archive of Motion Capture as Surface Shapes) is a high-quality human motion capture dataset, and the SMPL-X model is a powerful tool for generating realistic human motion data.\nBy adapting the motion data from AMASS‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fleaven/Retargeted_AMASS_for_FourierN1.","url":"https://huggingface.co/datasets/fleaven/Retargeted_AMASS_for_FourierN1","creator_name":"Kun Zhao","creator_url":"https://huggingface.co/fleaven","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"reinforcement-learning","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Orsta-Data-47k","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOrsta-Data-47k Dataset\n\t\n\n\nüêô GitHub Repo: MiniMax-AI/One-RL-to-See-Them-All\nüìú Paper (arXiv): V-Triune: One RL to See Them All (arXiv:2505.18129)\n\n\n\t\n\t\t\n\t\tDataset Description üìñ\n\t\n\nOrsta-Data-47k is a specialized dataset curated for the post-training of Vision-Language Models (VLMs) using our V-Triune unified reinforcement learning system. Its primary purpose is to enable robust joint training across a diverse spectrum of both visual reasoning and visual perception tasks, powering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k.","url":"https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k","creator_name":"One-RL-to-See-Them-All","creator_url":"https://huggingface.co/One-RL-to-See-Them-All","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K<n<100K","arxiv:2505.18129"],"keywords_longer_than_N":true},
	{"name":"Hanabi_dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tHanabi LLM Evaluation Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains evaluations of 13 state-of-the-art (SoTA) Large Language Models (LLMs) playing the cooperative card game Hanabi. The dataset supports research into LLM strategic reasoning, cooperative behavior, and multi-agent coordination in interactive environments.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of 5 JSONL files containing game transcripts and model interactions:\n\n\t\n\t\t\n\t\tGame Transcript Files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mahesh111000/Hanabi_dataset.","url":"https://huggingface.co/datasets/Mahesh111000/Hanabi_dataset","creator_name":"Mahesh Ramesh","creator_url":"https://huggingface.co/Mahesh111000","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","other","English","mit"],"keywords_longer_than_N":true},
	{"name":"MoSim_Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüóÇÔ∏è MoSim Dataset\n\t\n\nOfficial release of the dataset from the paper:Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning  \nThis dataset contains sequential state-action trajectories for training and evaluating MoSim (Neural Motion Simulator) world models.All trajectories are collected from random policies in classical control and locomotion environments.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüì¶ Dataset Overview\n\t\n\n\nFormat: .npz (NumPy compressed arrays)  \nContents:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wujiss1/MoSim_Dataset.","url":"https://huggingface.co/datasets/wujiss1/MoSim_Dataset","creator_name":"chenjie ","creator_url":"https://huggingface.co/wujiss1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K<n<10K","arxiv:2504.07095"],"keywords_longer_than_N":true},
	{"name":"MoSim_Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüóÇÔ∏è MoSim Dataset\n\t\n\nOfficial release of the dataset from the paper:Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning  \nThis dataset contains sequential state-action trajectories for training and evaluating MoSim (Neural Motion Simulator) world models.All trajectories are collected from random policies in classical control and locomotion environments.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüì¶ Dataset Overview\n\t\n\n\nFormat: .npz (NumPy compressed arrays)  \nContents:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wujiss1/MoSim_Dataset.","url":"https://huggingface.co/datasets/wujiss1/MoSim_Dataset","creator_name":"chenjie ","creator_url":"https://huggingface.co/wujiss1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K<n<10K","arxiv:2504.07095"],"keywords_longer_than_N":true},
	{"name":"TruthGen","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for TruthGen\n\t\n\nTruthGen is a dataset of generated political statements, created to assess the relationship between truthfulness and political bias in reward models and language models. It consists of non-repetitive, non-political factual statements paired with false statements, designed to evaluate models for their ability to distinguish true from false information while minimizing political content. The dataset was generated using GPT-3.5, GPT-4 and Gemini, with a focus‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/TruthGen.","url":"https://huggingface.co/datasets/wwbrannon/TruthGen","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","reinforcement-learning","machine-generated","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"SRPO_RL_datasets","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSRPO Dataset: Reflection-Aware RL Training Data\n\t\n\nThis repository provides the multimodal reasoning dataset used in the paper:\nSRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning\nWe release two versions of the dataset:\n\n39K version (modified_39Krelease.jsonl + images.zip)  \nEnhanced 47K+ version (47K_release_plus.jsonl + 47K_release_plus.zip)\n\nBoth follow the same unified format, containing multimodal (image‚Äìtext) reasoning data with self-reflection‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bruce360568/SRPO_RL_datasets.","url":"https://huggingface.co/datasets/bruce360568/SRPO_RL_datasets","creator_name":"zhongwei666","creator_url":"https://huggingface.co/bruce360568","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"EmotionAlignQA","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tEmpathic Dialogue Choices\n\t\n\nThis is a small dataset to support training and evaluation of conversational AI in emotionally sensitive contexts.\nEach sample contains:\n\na user input\ntwo assistant responses\na human preference\noptional rubric scoring\nmetadata such as tone, formality, and topic\n\nUseful for tasks like:\n\nsupervised fine-tuning (SFT)\npreference modeling (for RLHF or DPO)\nsafe response generation\ntone- or style-controlled generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nApache 2.0 ‚Äî free for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA.","url":"https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA","creator_name":"Nguy·ªÖn Ho√†ng H·∫£i","creator_url":"https://huggingface.co/hoanghai2110","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"oasst2_dpo_pairs_enth","keyword":"dpo","description":"\n\t\n\t\t\n\t\tOASST2 DPO Pairs English and Thai\n\t\n\nThis dataset contains message ChatML. It was create from Open Assistant Conversations Dataset Release 2 (OASST2). You can use to do human preference optimization (DPO, ORPO, and other).\n\n\t\n\t\t\n\t\tSelect Thai only\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"pythainlp/oasst2_dpo_pairs_enth\",split=\"train\")\nthai_dataset = dataset.filter(lambda example: example['lang']==\"th\") # if you want to use English only, change to \"en\".\n\nlicense:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth.","url":"https://huggingface.co/datasets/pythainlp/oasst2_dpo_pairs_enth","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Thai","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"EmotionAlignQA","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tEmpathic Dialogue Choices\n\t\n\nThis is a small dataset to support training and evaluation of conversational AI in emotionally sensitive contexts.\nEach sample contains:\n\na user input\ntwo assistant responses\na human preference\noptional rubric scoring\nmetadata such as tone, formality, and topic\n\nUseful for tasks like:\n\nsupervised fine-tuning (SFT)\npreference modeling (for RLHF or DPO)\nsafe response generation\ntone- or style-controlled generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nApache 2.0 ‚Äî free for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA.","url":"https://huggingface.co/datasets/hoanghai2110/EmotionAlignQA","creator_name":"Nguy·ªÖn Ho√†ng H·∫£i","creator_url":"https://huggingface.co/hoanghai2110","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"count_letters_in_word_base","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tDataset Card for count_letters_in_word\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was generated using the nltk words corpus and a custom script to create tasks that require counting letters in words. Each example prompts the language model to count occurrences of specific letters within words. For added complexity, in 10% of cases, a letter that does not appear in the word is included. \nThe dataset is structured to provide both chosen (correct counts) and rejected (incorrect counts)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base.","url":"https://huggingface.co/datasets/JeanIbarz/count_letters_in_word_base","creator_name":"Jean Ibarz","creator_url":"https://huggingface.co/JeanIbarz","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["English","unlicense","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"prm_dpo_pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tprm_dpo_pairs\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nprm_dpo_pairs is a curated version of the PRM800K dataset designed for ease of use when fine-tuning a language model using the DPO (Direct Preference Optimization) technique. The dataset contains pairs of prompts and completions, with labels indicating which completion was preferred by the original language model.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of the following features:\n\nprompt: The input prompt or question posed to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/M4-ai/prm_dpo_pairs.","url":"https://huggingface.co/datasets/M4-ai/prm_dpo_pairs","creator_name":"M4-ai","creator_url":"https://huggingface.co/M4-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_raw_csv","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nCSV VARIANT \nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts' phrasal/phonetic, rhythmic, metric, syllabic, melodic, and other lyrical/performance-catered features, whilst retaining adequate semantic/significational fidelity.  \nThis dataset samples‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_raw_csv.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_MT_SongsPoems_Set2_contrastive_tri-column_raw_csv","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"expert-connect4-mcts-expert_mcts_dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tExpert Connect 4 MCTS Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains Connect 4 game positions generated using expert-level Monte Carlo Tree Search (MCTS) self-play between two strong agents.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nNumber of games: 1000\nNumber of positions: 36905\nMCTS simulations per move: 1000\nExploration constant: 1.5\nBoard size: 6x7 (standard Connect 4)\n\n\n\t\n\t\t\n\t\tData Format\n\t\n\nEach example contains:\n\nstates: 3x6x7 tensor representing the board state\nChannel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/theGhoul21/expert-connect4-mcts-expert_mcts_dataset.","url":"https://huggingface.co/datasets/theGhoul21/expert-connect4-mcts-expert_mcts_dataset","creator_name":"Luca Simonetti","creator_url":"https://huggingface.co/theGhoul21","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"medical-reasoning-orpo_preprocess","keyword":"dpo","description":"\n\t\n\t\t\n\t\tMedical Reasoning ORPO Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of SURESHBEEKHANI/medical-reasoning-orpo, formatted for preference tuning tasks like DPO or ORPO.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nThe dataset contains three columns:\n\nquestion: A combination of the original instruction and Input fields.\naccepted: The preferred response, formatted with thinking process and final answer tags.\nrejected: The dispreferred response, also formatted with tags.\n\n\n\t\n\t\t\n\t\tAnswer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical-reasoning-orpo_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical-reasoning-orpo_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"unstacked","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked.","url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1B<n<10B","arxiv:2403.08763","arxiv:2310.05914"],"keywords_longer_than_N":true},
	{"name":"pacman-small","keyword":"reinforcement-learning","description":"Important Note: The new_game_flag column in dataset.csv is incorrect. Please refer to my bigger Pacman dataset for accurate information.\n","url":"https://huggingface.co/datasets/kimic/pacman-small","creator_name":"Kimi Chen","creator_url":"https://huggingface.co/kimic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"dpo-merged-binarized","keyword":"dpo","description":"CultriX/dpo-merged-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-merged-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tSHORTENED - ORPO-DPO-mix-40k v1.1\n\t\n\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\n\nThe original dataset card follows below.\n\n\n\t\n\t\t\n\t\tORPO-DPO-mix-40k v1.1\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nIt is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT.","url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"my_smolvla123456789","keyword":"reinforcement-learning","description":"Amanpatel81/my_smolvla123456789 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Amanpatel81/my_smolvla123456789","creator_name":"Amankumar Patel","creator_url":"https://huggingface.co/Amanpatel81","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","table-question-answering","question-answering","translation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"ether0-benchmark","keyword":"reinforcement-learning","description":"\n\n\n\t\n\t\t\n\t\tether0-benchmark\n\t\n\nQA benchmark (test set) for the ether0 reasoning language model:\nhttps://huggingface.co/futurehouse/ether0\nThis benchmark is made from commonly used tasks - like reaction prediction in USPTO/ORD,\nmolecular captioning from PubChem, or predicting GHS classification.\nIt's unique from other benchmarks in that all answers are a molecule.\nIt's balanced so that each task is about 25 questions,\na reasonable amount for frontier model evaluations.\nThe tasks generally follow‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/futurehouse/ether0-benchmark.","url":"https://huggingface.co/datasets/futurehouse/ether0-benchmark","creator_name":"Future House","creator_url":"https://huggingface.co/futurehouse","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"openai-summarize-tldr","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tSummarize TL;DR Filtered Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2009.01325.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://huggingface.co/datasets/webis/tldr-17.\n","url":"https://huggingface.co/datasets/martimfasantos/openai-summarize-tldr","creator_name":"Martim Santos","creator_url":"https://huggingface.co/martimfasantos","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","summarization","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"UltraFeedback","keyword":"human-feedback","description":"juyoungml/UltraFeedback dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/juyoungml/UltraFeedback","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"warren-buffett-letters-qna-r1-enhanced-1998-2024","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüß† Warren Buffett Letters Q&A Dataset Pipeline\n\t\n\nThis project extracts question-answer-reasoning triplets from Warren Buffett's annual shareholder letters using OCR and LLMs. The pipeline is modular and divided into the following stages:\nYou can clone the repo here.\n\n\n\t\n\t\t\n\t\t1. Setup\n\t\n\nCreate a virtual environment and install dependencies using requirements.txt.\n\n\n\t\n\t\t\n\t\t2. Data Curation (curate_data.py)\n\t\n\n\nLoad a list of PDF URLs from the Berkshire Hathaway website.\nUse Mistral's‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024.","url":"https://huggingface.co/datasets/eagle0504/warren-buffett-letters-qna-r1-enhanced-1998-2024","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"InvertedPendulum-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tInvertedPendulum-v2 - Continuous Imitation Learning from Observation\n\t\n\nThis dataset was created for the paper Explorative imitation learning: A path signature approach for continuous environments.\nIt is based on InvertedPendulum-v2, which is an older version for the MuJoCo environment.\nIf you would like to use newer version, be sure to check: IL-Datasets repository for the updated list.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 10 episodes with an average episodic reward of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2.","url":"https://huggingface.co/datasets/NathanGavenski/InvertedPendulum-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset.","url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"jat-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tJAT Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Jack of All Trades (JAT) dataset combines a wide range of individual datasets. It includes expert demonstrations by expert RL agents, image and caption pairs, textual data and more. The JAT dataset is part of the JAT project, which aims to build a multimodal generalist agent.\nPaper: https://huggingface.co/papers/2402.09844\n\n\t\n\t\t\n\t\tUsage\n\t\n\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"jat-project/jat-dataset\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jobs-git/jat-dataset.","url":"https://huggingface.co/datasets/jobs-git/jat-dataset","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","question-answering","found","machine-generated"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"demons_megaten_fandom_orpo_dpo_english","keyword":"dpo","description":"celsowm/demons_megaten_fandom_orpo_dpo_english dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/celsowm/demons_megaten_fandom_orpo_dpo_english","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Recraft-V2_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Recraft-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 47k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"formatted-hh-rlhf","keyword":"rlhf","description":"Estwld/formatted-hh-rlhf dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Estwld/formatted-hh-rlhf","creator_name":"zhangyiqun","creator_url":"https://huggingface.co/Estwld","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"example-dataset","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/sharonav123/example-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sharonav123/example-dataset.","url":"https://huggingface.co/datasets/sharonav123/example-dataset","creator_name":"Sharon AV","creator_url":"https://huggingface.co/sharonav123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"IFDecorator","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tIFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards\n\t\n\nProject page | Paper | Code\nHigh-quality synthetic datasets engineered for Reinforcement Learning with Verifiable Rewards (RLVR)\n\n\t\n\t\t\n\t\n\t\n\t\tüåü Why This Dataset?\n\t\n\nThis repository contains two complementary datasets with different synthesis approaches and difficulty distributions:\n\n\t\n\t\n\t\n\t\tüìä Core Dataset (train.jsonl + val.jsonl)\n\t\n\n\nüéØ Controlled difficulty: 3,625 training + 200 validation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guox18/IFDecorator.","url":"https://huggingface.co/datasets/guox18/IFDecorator","creator_name":"guox18","creator_url":"https://huggingface.co/guox18","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"Multifaceted-Collection-ORPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Multifaceted Collection ORPO\n\t\n\n\n\t\n\t\t\n\t\tLinks for Reference\n\t\n\n\nHomepage: https://lklab.kaist.ac.kr/Janus/ \nRepository: https://github.com/kaistAI/Janus \nPaper: https://arxiv.org/abs/2405.17977 \nPoint of Contact: suehyunpark@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\n\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from five existing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO.","url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-ORPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"oasst2_french_dpo_pairs","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for oasst2_french_dpo_pairs\n\t\n\nThis dataset was created from OpenAssistant/oasst2 by keeping only the french data and producing dpo pairs with their rank.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/oasst2_french_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"DPO-uz-9k","keyword":"dpo","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \nNote that each prompt has two answers. The first answer should be the 'selected'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k.","url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"dpo","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"DPO-uz-9k","keyword":"rlhf","description":"This is DPO Uzbek translated dataset with 9k chat pairs.\nOriginal English dataset comes from DPO-En-Zh-20k (commit 9ad5f7428419d3cf78493cf3f4be832cf5346ba8. File:  dpo_en.json).\nI translated 10k pairs of chat examples into Uzbek using NLLB 3.3B model.\nAfter translation was completed, I used local lilac instance to remove records with coding examples since NLLB is not good at translating text with coding examples. \nNote that each prompt has two answers. The first answer should be the 'selected'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k.","url":"https://huggingface.co/datasets/MLDataScientist/DPO-uz-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Uzbek","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_preferences","keyword":"rlhf","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"human-coherence-preferences-images","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Coherence Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated coherence datasets for text-to-image models, this release contains over 1,200,000 human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","question-answering","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"carla-autopilot-multimodal-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tCARLA Autopilot Multimodal Dataset\n\t\n\nThis dataset contains synchronized multimodal driving data collected in the CARLA simulator using the autopilot feature. It provides RGB images from multiple cameras, semantic segmentation, LiDAR point clouds, 2D bounding boxes, and ego-vehicle state/control signals across varied weather, maps, and traffic densities.\nThe dataset is designed for research in autonomous driving, sensor fusion, imitation learning, and self-driving evaluation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/immanuelpeter/carla-autopilot-multimodal-dataset.","url":"https://huggingface.co/datasets/immanuelpeter/carla-autopilot-multimodal-dataset","creator_name":"Immanuel Peter","creator_url":"https://huggingface.co/immanuelpeter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-classification","image-segmentation","depth-estimation","video-classification"],"keywords_longer_than_N":true},
	{"name":"MinecraftSkillDiscovery","keyword":"reinforcement-learning","description":"This is the segmented datasets of the project presented in the paper Open-World Skill Discovery from Unsegmented Demonstrations.\n\nCode: https://github.com/CraftJarvis/SkillDiscovery\nProject Page: https://craftjarvis.github.io/SkillDiscovery\n\nEach line of the jsonl file consists of the video file name and the boundaries [begin1, end1], [begin2, end2], ...\nEvents information is also included in the \"with info\" file.\nThe video files can be downloaded here. Notice that we use the 7.x version.\n","url":"https://huggingface.co/datasets/fatty-belly/MinecraftSkillDiscovery","creator_name":"DJW","creator_url":"https://huggingface.co/fatty-belly","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Advance","keyword":"reinforcement-learning","description":"Groovy-123/Advance dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/Advance","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"dual-arm-robot-dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDual-Arm Robot Dataset\n\t\n\nA LeRobot-compatible dataset containing joint position data from a dual-arm robotic system with 34 degrees of freedom.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains robot manipulation data recorded from a dual-arm robotic system. The data includes joint positions for both arms and their associated grippers, captured at 100Hz for precise motion analysis and learning.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Episodes: 1\nTotal Frames: 3,105\nDuration: 31.05‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sraghvi/dual-arm-robot-dataset.","url":"https://huggingface.co/datasets/Sraghvi/dual-arm-robot-dataset","creator_name":"Sraghvi Anchaliya","creator_url":"https://huggingface.co/Sraghvi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Deep_math","keyword":"rl","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\tüî• News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Compumacy/Deep_math.","url":"https://huggingface.co/datasets/Compumacy/Deep_math","creator_name":"Compumacy AI","creator_url":"https://huggingface.co/Compumacy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-PrepareGroceries","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB PrepareGroceries Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB PrepareGroceries.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB PrepareGroceries. Each subtask/object combination (e.g pick 002_master_chef_can) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-PrepareGroceries.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-PrepareGroceries","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-PrepareGroceries","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB PrepareGroceries Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB PrepareGroceries.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB PrepareGroceries. Each subtask/object combination (e.g pick 002_master_chef_can) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-PrepareGroceries.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-PrepareGroceries","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized","keyword":"dpo","description":"Ultrafeedback binarized dataset using the mean of preference ratings by Argilla. \nThey addressed TruthfulQA-related data contamination in this version.\nSteps:\n\nCompute mean of preference ratings (honesty, instruction-following, etc.)\nPick the best mean rating as the chosen\nPick random rejected with lower mean (or another random if equal to chosen rating)\nFilter out examples with chosen rating == rejected rating\n\nReference:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized.","url":"https://huggingface.co/datasets/arcee-ai/ultrafeedback-binarized","creator_name":"Arcee AI","creator_url":"https://huggingface.co/arcee-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_dpo_dataset_HelpSteer3","keyword":"dpo","description":"\n\t\n\t\t\n\t\tCS-552 Stochastic Parrots M2 DPO Dataset\n\t\n\nThis dataset contains preference pairs for Direct Preference Optimization (DPO) training with a focus on STEM and code domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains the following splits:\n\ntrain: 36693 examples\nvalidation: 4078 examples\ntest: 908 examples\n\n\n\t\n\t\t\n\t\tData Format\n\t\n\nEach example contains:\n\nprompt: The input query or question\nchosen: The preferred response\nrejected: The less preferred response\ndomain: Domain of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3.","url":"https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3","creator_name":"Rizhong Lin","creator_url":"https://huggingface.co/RizhongLin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"carla-autopilot-images","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tCARLA Autopilot Images Dataset\n\t\n\n\nNote: A newer, extended version of this dataset is available.ü§ó CARLA Autopilot Multimodal Dataset ü§óIt includes semantic segmentation, LiDAR, 2D bounding boxes, and additional environment metadata.Use it if your research requires multimodal signals beyond the RGB images and vehicle state/control data provided here.\n\nThis dataset contains autonomous driving data collected from CARLA simulator using autopilot.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/immanuelpeter/carla-autopilot-images.","url":"https://huggingface.co/datasets/immanuelpeter/carla-autopilot-images","creator_name":"Immanuel Peter","creator_url":"https://huggingface.co/immanuelpeter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","any-to-any","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"german_tlr_gold_14k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tüß† German TLR Gold Dataset (14.5k)\n\t\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\nEin hochwertiger deutschsprachiger Datensatz mit 14.500 Samples im Think-Learn-Respond (TLR) Format f√ºr das Training von reasoning-f√§higen Large Language Models.\nFormat: Jede Antwort ist strukturiert in:\n\n<think>: Strukturierter Denkprozess und Reasoning\n<answer>: Finale, klare Antwort\n\n\n\t\n\t\t\n\t\tüéØ Anwendung\n\t\n\nDieses Dataset wurde speziell entwickelt f√ºr:\n\nSupervised Fine-Tuning (SFT) von deutschen LLMs\nTraining von‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k.","url":"https://huggingface.co/datasets/arnomatic/german_tlr_gold_14k","creator_name":"arnomatic","creator_url":"https://huggingface.co/arnomatic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","German","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"french_orca_dpo_pairs","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for french_orca_dpo_pairs\n\t\n\nThis dataset offers a french translation of the 12k DPO Intel/orca_dpo_pairs pairs made from Open-Orca/OpenOrca.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_orca_dpo_pairs","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","French","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Retargeted_AMASS_for_bxi_elf2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRetargeted AMASS for Robotics\n\t\n\n\n\t\n\t\t\n\t\tProject Overview\n\t\n\nThis project aims to retarget motion data from the AMASS dataset to various robot models and open-source the retargeted data to facilitate research and applications in robotics and human-robot interaction. AMASS (Archive of Motion Capture as Surface Shapes) is a high-quality human motion capture dataset, and the SMPL-X model is a powerful tool for generating realistic human motion data.\nBy adapting the motion data from AMASS‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fleaven/Retargeted_AMASS_for_bxi_elf2.","url":"https://huggingface.co/datasets/fleaven/Retargeted_AMASS_for_bxi_elf2","creator_name":"Kun Zhao","creator_url":"https://huggingface.co/fleaven","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"HyperThink-X-Nvidia-Opencode-Reasoning-200K","keyword":"reinforcement-learning","description":"\n  \n\n\n\n\t\n\t\t\n\t\tüîÆ HyperThink\n\t\n\nHyperThink is a premium, best-in-class dataset series capturing deep reasoning interactions between users and an advanced Reasoning AI system. Designed for training and evaluating next-gen language models on complex multi-step tasks, the dataset spans a wide range of prompts and guided thinking outputs.\n\n\n\t\n\t\t\n\t\tüöÄ Dataset Tiers\n\t\n\nHyperThink is available in three expertly curated versions, allowing flexible scaling based on compute resources and training goals:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NukeverseAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K.","url":"https://huggingface.co/datasets/NukeverseAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K","creator_name":"NukeverseAi","creator_url":"https://huggingface.co/NukeverseAi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"dpo","description":"\n    \n\n\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\nLANGUAGES:\nARABIC\nCHINESE\nFRENCH\nGERMAN\nRUSSIAN\nSPANISH\nTURKISH\n(WIP)\n","url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","German","French"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs","keyword":"rlhf","description":"\n    \n\n\nmLLM IMPLEMENTATION OF Intel/orca_dpo_pairs.\nLANGUAGES:\nARABIC\nCHINESE\nFRENCH\nGERMAN\nRUSSIAN\nSPANISH\nTURKISH\n(WIP)\n","url":"https://huggingface.co/datasets/multilingual/orca_dpo_pairs","creator_name":"mLLM multilingual","creator_url":"https://huggingface.co/multilingual","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","German","French"],"keywords_longer_than_N":true},
	{"name":"Gomoku","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDatacard: Gomoku (Five in a Row) AI Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku.","url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","mit","10K<n<100K","doi:10.57967/hf/4816"],"keywords_longer_than_N":true},
	{"name":"Gomoku","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDatacard: Gomoku (Five in a Row) AI Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Gomoku (Five in a Row) AI Dataset contains board states and moves from 875 self-played Gomoku games, totaling 26,378 training examples. The data was generated using WinePy, a Python implementation of the Wine Gomoku AI engine. Each example consists of a board state and the corresponding optimal next move as determined by an alpha-beta search algorithm with pattern recognition.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Karesis/Gomoku.","url":"https://huggingface.co/datasets/Karesis/Gomoku","creator_name":"Êù®‰∫¶Èîã","creator_url":"https://huggingface.co/Karesis","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","feature-extraction","mit","10K<n<100K","doi:10.57967/hf/4816"],"keywords_longer_than_N":true},
	{"name":"Fast-Math-R1-SFT","keyword":"reinforcement-learning","description":"This repository contains the First stage SFT dataset as presented in the paper A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning.\nThis dataset is used for the intensive Supervised Fine-Tuning (SFT) phase, crucial for pushing the model's mathematical accuracy.\nProject GitHub Repository: https://github.com/RabotniKuma/Kaggle-AIMO-Progress-Prize-2-9th-Place-Solution\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Construction\n\t\n\nThis dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT.","url":"https://huggingface.co/datasets/RabotniKuma/Fast-Math-R1-SFT","creator_name":"Hiroshi Yoshihara","creator_url":"https://huggingface.co/RabotniKuma","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Spark-Data","keyword":"rl","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSpark-Data\n\t\n\n\n\t\n\t\t\n\t\tData Introduction\n\t\n\nThis repository stores the datasets used for training ü§óSpark-VL-7B and Spark-VL-32B, as well as a collection of multiple mathematical benchmarks covered in the Spark paper.\ninfer_data_ViRL_19k_h.json is used for training Spark-VL-7B.\ninfer_data_ViRL_hard_24k_h.json is used for training Spark-VL-32B.\nbenchmark_combine.json and benchmark_combine_v2.json is a combination of multiple mathematical benchmarks.\nThe training dataset is derived‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/internlm/Spark-Data.","url":"https://huggingface.co/datasets/internlm/Spark-Data","creator_name":"Intern Large Models","creator_url":"https://huggingface.co/internlm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K<n<100K","arxiv:2509.22624","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"example-dataset","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for example-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/CoffeeDoodle/example-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CoffeeDoodle/example-dataset.","url":"https://huggingface.co/datasets/CoffeeDoodle/example-dataset","creator_name":"Shalini Sundaram","creator_url":"https://huggingface.co/CoffeeDoodle","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"flammenai-Prude-Phi3-DPO","keyword":"dpo","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\n","url":"https://huggingface.co/datasets/Triangle104/flammenai-Prude-Phi3-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"collabllm-20q-interactive","keyword":"rlhf","description":"aditijb/collabllm-20q-interactive dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/aditijb/collabllm-20q-interactive","creator_name":"Aditi","creator_url":"https://huggingface.co/aditijb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"TelePlanNet","keyword":"reinforcement-learning","description":"CaiYujie/TelePlanNet dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CaiYujie/TelePlanNet","creator_name":"caiyujie","creator_url":"https://huggingface.co/CaiYujie","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"ARPO-RL-DeepSearch-1K","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tARPO Dataset: Agentic Reinforced Policy Optimization\n\t\n\nThis repository contains the datasets used in the paper Agentic Reinforced Policy Optimization.\nPaper Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. To bridge this gap‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dongguanting/ARPO-RL-DeepSearch-1K.","url":"https://huggingface.co/datasets/dongguanting/ARPO-RL-DeepSearch-1K","creator_name":"KABI","creator_url":"https://huggingface.co/dongguanting","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"unanswerable-math-question","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Synthetic Unanswerable Math (SUM)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSynthetic Unanswerable Math (SUM) is a dataset of high-quality, implicitly unanswerable math problems constructed to probe and improve the refusal behavior of large language models (LLMs). The goal is to teach models to identify when a problem cannot be answered due to incomplete, ambiguous, or contradictory information, and respond with epistemic humility (e.g., \\boxed{I don't know}).\nEach entry in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ayushnangia/unanswerable-math-question.","url":"https://huggingface.co/datasets/Ayushnangia/unanswerable-math-question","creator_name":"Ayush Nangia","creator_url":"https://huggingface.co/Ayushnangia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","mit","arxiv:2505.13988","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"ebaluatoia","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tEbaluatoia\n\t\n\n\nüíª Repository: https://github.com/hitz-zentroa/Latxa-Instruct\nüìí Blog Post: To be published\nüìñ Paper: Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque\nüìß Point of Contact: hitz@ehu.eus\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nEbaluatoia is a human preference dataset for Basque language models, collected through a community-driven arena-style evaluation. The dataset contains over 12,000 preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/ebaluatoia.","url":"https://huggingface.co/datasets/HiTZ/ebaluatoia","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Basque","cc0-1.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"TelePlanNet","keyword":"rl","description":"CaiYujie/TelePlanNet dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CaiYujie/TelePlanNet","creator_name":"caiyujie","creator_url":"https://huggingface.co/CaiYujie","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"boxoban-astar-solutions","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\n\t\n\t\tA* solutions to Boxoban levels\n\t\n\nFor some levels we were not able to find solutions within the allotted A* budget. These have solution\nSEARCH_STATE_FAILED or NOT_FOUND. These are the ones labeled \"Unsolved levels\" below.\nThe search budget was 5 million nodes to expand for medium-difficulty levels, vs. 1 million nodes for\nunfiltered-difficulty levels. The heuristic was the sum of Manhattan distances of each box to its closest target.\n\n\t\n\t\t\n\t\n\t\n\t\tSummary table:\n\t\n\n\n\t\n\t\t\nLevel file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions.","url":"https://huggingface.co/datasets/AlignmentResearch/boxoban-astar-solutions","creator_name":"FAR AI","creator_url":"https://huggingface.co/AlignmentResearch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","apache-2.0","1M<n<10M","üá∫üá∏ Region: US","sokoban"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2-DPO","keyword":"human-feedback","description":"This dataset is a conversion of nvidia/HelpSteer2 into preference pairs based on the helpfulness score for training DPO.\nHelpSteer2-DPO is also licensed under CC-BY-4.0.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nIn accordance with the following paper, HelpSteer2: Open-source dataset for training top-performing reward models\nwe converted nvidia/HelpSteer2 dataset into a preference dataset by taking the response with the higher helpfulness score as the chosen response, with the remaining response being the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO.","url":"https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO","creator_name":"Atsunori Fujita","creator_url":"https://huggingface.co/Atsunori","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"reinforcement-learning","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tHelpSteer2: Open-source dataset for training top-performing reward models\n\t\n\nHelpSteer2 is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\nThis dataset has been created in partnership with Scale  AI. \nWhen used to tune a Llama 3.1 70B Instruct Model, we achieve 94.1% on RewardBench, which makes it the best Reward Model as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/HelpSteer2.","url":"https://huggingface.co/datasets/nvidia/HelpSteer2","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"my-distiset-38ebca4b","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-38ebca4b\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b.","url":"https://huggingface.co/datasets/kritsanan/my-distiset-38ebca4b","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Thai","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"elkins_dataset_trial","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for elkins_dataset_trial\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial.","url":"https://huggingface.co/datasets/elkinsqiu/elkins_dataset_trial","creator_name":"elkins.qiu","creator_url":"https://huggingface.co/elkinsqiu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"dpo","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ExplainableAI-emotions-DPO-ORPO-RLHF","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tPreference Dataset for Explainable Multi-Label Emotion Classification\n\t\n\nThis repository contains a preference dataset compiled to compare two model-generated responses for explaining multi-label emotion classifications on Tweets. The dataset is accompanied by human annotations indicating which response was preferred, based on a set of defined dimensions (clarity, correctness, helpfulness, and verbosity). The annotation guidelines are included to describe how these preference judgments‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF.","url":"https://huggingface.co/datasets/imhmdf/ExplainableAI-emotions-DPO-ORPO-RLHF","creator_name":"Hammad Fahim","creator_url":"https://huggingface.co/imhmdf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Ru2En_DPO_songs_poems_v3_Rebalanced","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nEDITED VARIANT 3 \nRe-balanced, edited, substantially abridged/consolidated, somewhat re-expanded \nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, rejected)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts' phrasal/phonetic, rhythmic, metric, syllabic, melodic, and other lyrical/performance-catered features‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_songs_poems_v3_Rebalanced.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Ru2En_DPO_songs_poems_v3_Rebalanced","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Prude-Phi3-DPO","keyword":"dpo","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\n","url":"https://huggingface.co/datasets/flammenai/Prude-Phi3-DPO","creator_name":"flammen.ai","creator_url":"https://huggingface.co/flammenai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"HelpSteer2","keyword":"human-feedback","description":"juyoungml/HelpSteer2 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/juyoungml/HelpSteer2","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"RealStories-Micro-MRL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for ReactiveAI/RealStories-Micro-MRL\n\t\n\nFirst synthetic Memory Reinforcement Learning dataset for Proof-of-Concept Reactive Transformer models.\nDataset is divided into subsets, used in different Curriculum Stage of MRL training - each subset have\ndifferent number of follow-up interactions, could use different strategy, and have train and validation\nsplits.\n\n\t\n\t\t\n\t\tSubsets\n\t\n\n\nsteps-1: ~2300 train (~4600 interactions) / ~340 validation (~680 interactions) - Single-Step‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReactiveAI/RealStories-Micro-MRL.","url":"https://huggingface.co/datasets/ReactiveAI/RealStories-Micro-MRL","creator_name":"Reactive AI","creator_url":"https://huggingface.co/ReactiveAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-retrieval","text-generation","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1","keyword":"dpo","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dans-DiscountModels/gutenberg-dpo-v0.1.","url":"https://huggingface.co/datasets/Dans-DiscountModels/gutenberg-dpo-v0.1","creator_name":"Dan's Discount Model Emporium","creator_url":"https://huggingface.co/Dans-DiscountModels","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"hermes_reasoning_tool_use","keyword":"rl","description":"\n\t\n\t\t\n\t\tTL;DR\n\t\n\n51 004 ShareGPT conversations that teach LLMs when, how and whether to call tools.Built with the Nous Research Atropos RL stack in Atropos using a custom MultiTurnToolCallingEnv, and aligned with BFCL v3 evaluation scenarios.Released by @interstellarninja under Apache-2.0.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1‚ÄÇDataset Highlights\n\t\n\n\n\t\n\t\t\nCount\nSplit\nScenarios covered\nSize\n\n\n\t\t\n51 004\ntrain\nsingle-turn ¬∑ multi-turn ¬∑ multi-step ¬∑ relevance\n392 MB\n\n\n\t\n\n\nEach row: OpenAI-style conversations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use.","url":"https://huggingface.co/datasets/interstellarninja/hermes_reasoning_tool_use","creator_name":"interstellarninja","creator_url":"https://huggingface.co/interstellarninja","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SentimentSynth","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSentimentSynth Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe SentimentSynth dataset is a collection of text samples expressing various sentiments, ranging from joy and excitement to stress and sadness. These samples are generated to simulate human-like expressions of emotions in different contexts.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use the SentimentSynth dataset in your work, please cite it as:\n@misc {helpingai_2024,\n    author       = { {HelpingAI} },\n    title        = { SentimentSynth (Revision‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OEvortex/SentimentSynth.","url":"https://huggingface.co/datasets/OEvortex/SentimentSynth","creator_name":"HelpingAI","creator_url":"https://huggingface.co/OEvortex","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"osiris","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOSIRIS: Bridging Analog Layout Circuit Design and Machine Learning with Scalable Dataset Generation\n\t\n\nOSIRIS is an end-to-end analog circuits design pipeline capable of producing, validating, and evaluating layouts for generic analog circuits.\nThe Osiris ü§ó HuggingFace repository hosts the OSIRIS code and dataset generated following random exploration discussed in the paper.\n\nCurated by: Anonymous\nLicense: Open Data Commons License cc-by-4.0\n\n\n\t\n\t\n\t\n\t\tHow to Download\n\t\n\nThe code is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anonymousUser2/osiris.","url":"https://huggingface.co/datasets/anonymousUser2/osiris","creator_name":"Anon","creator_url":"https://huggingface.co/anonymousUser2","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","üá∫üá∏ Region: US","eda"],"keywords_longer_than_N":true},
	{"name":"human-writing-dpo","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\thuman-writing-dpo\n\t\n\nA high-quality creative writing preference dataset derived from nothingiisreal/Reddit-Dirty-And-WritingPrompts.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\nprompt: Writing prompts from r/WritingPrompts and r/DirtyWritingPrompts post titles\nchosen: High-quality human-written story\nrejected: Lower-quality story completion generated by nbeerbower/Dumpling-Qwen2.5-7B-1k-r16\nreddit_score: Number of upvotes on Reddit\noverall_score: Overall grade from LLM judge (GPT 4.1-mini)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nbeerbower/human-writing-dpo.","url":"https://huggingface.co/datasets/nbeerbower/human-writing-dpo","creator_name":"Nicholas Beerbower","creator_url":"https://huggingface.co/nbeerbower","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["apache-2.0","üá∫üá∏ Region: US","Not-For-All-Audiences","writing","dpo"],"keywords_longer_than_N":false},
	{"name":"osiris","keyword":"rl","description":"\n\t\n\t\t\n\t\tOSIRIS: Bridging Analog Layout Circuit Design and Machine Learning with Scalable Dataset Generation\n\t\n\nOSIRIS is an end-to-end analog circuits design pipeline capable of producing, validating, and evaluating layouts for generic analog circuits.\nThe Osiris ü§ó HuggingFace repository hosts the OSIRIS code and dataset generated following random exploration discussed in the paper.\n\nCurated by: Anonymous\nLicense: Open Data Commons License cc-by-4.0\n\n\n\t\n\t\n\t\n\t\tHow to Download\n\t\n\nThe code is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anonymousUser2/osiris.","url":"https://huggingface.co/datasets/anonymousUser2/osiris","creator_name":"Anon","creator_url":"https://huggingface.co/anonymousUser2","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","üá∫üá∏ Region: US","eda"],"keywords_longer_than_N":true},
	{"name":"Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Alignment_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Preference dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset.","url":"https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"Lyrical_Rus2Eng_ver4_forRWKV_RLtrainer_jsonl","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSong-lyrics & Poems by seminal & obscure Soviet & Russian songwriters, bands, & poets.\n\t\n\nEDITED VARIANT 4 \nRe-balanced, edited, substantially abridged/consolidated, somewhat re-expanded \nJSONLines Version, no more excessive separators, category titles altered for the RWKV LM RLHF trainer \nALTERNATE VERSION OF THE DATASET (3 columns: prompt, chosen, reject)\nManually translated to English by Aleksey Calvin, with a painstaking effort to cross-linguistically reproduce source texts'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_ver4_forRWKV_RLtrainer_jsonl.","url":"https://huggingface.co/datasets/AlekseyCalvin/Lyrical_Rus2Eng_ver4_forRWKV_RLtrainer_jsonl","creator_name":"Aleksey Calvin Tsukanov (A.C.T. SOON¬Æ)","creator_url":"https://huggingface.co/AlekseyCalvin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","Russian","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"UltraFeedback-relabeled-binarized","keyword":"human-feedback","description":"juyoungml/UltraFeedback-relabeled-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/juyoungml/UltraFeedback-relabeled-binarized","creator_name":"Juyoung Suk","creator_url":"https://huggingface.co/juyoungml","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"wikisource_preferences_ru","keyword":"dpo","description":"\n\t\n\t\t\n\t\tWikisource Preferences [Russian]\n\t\n\n–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. chosen —Ç–µ–∫—Å—Ç—ã –±—Ä–∞–ª–∏—Å—å –∏–∑ kristaller486/wikisource-creative-ru, –∞ rejected –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ LLM –ø–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–∞–º.\n–®–∞–±–ª–æ–Ω –¥–ª—è DPO: axolotl chat_template.default\n–ú–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ rejected —Å–µ–º–ø–ª–æ–≤:\n\ngoogle/gemma-3-27b-it\ngpt-4.1-mini\ngpt-4.1-nano\ngpt-4.1\ngemini-2.0-flash\nQwen/Qwen3-14B-FP8 (without reasoning)\nMoraliane/SAINEMO-reMIX (fp6-llm quantization)\ndeepseek-v3-0324 (api)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kristaller486/wikisource_preferences_ru.","url":"https://huggingface.co/datasets/kristaller486/wikisource_preferences_ru","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"DeepMath-103K","keyword":"rl","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\tüî• News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zwhe99/DeepMath-103K.","url":"https://huggingface.co/datasets/zwhe99/DeepMath-103K","creator_name":"Zhiwei He","creator_url":"https://huggingface.co/zwhe99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"AIME-NoB","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tData Card\n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\n\nFor what purpose was the dataset created?\n\nThe dataset is created for the experiment section of our paper \"Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models\" to pretrained world models and do imitation learning. We release the datasets for the community as a common test bench for similar problems. \nCode available at https://github.com/IcarusWizard/AIME-NoB.\n\nWho created the dataset (e.g., which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IcarusWizard/AIME-NoB.","url":"https://huggingface.co/datasets/IcarusWizard/AIME-NoB","creator_name":"Xingyuan Zhang","creator_url":"https://huggingface.co/IcarusWizard","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","arxiv:2404.18896","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"AIME-NoB","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tData Card\n\t\n\n\n\t\n\t\t\n\t\tMotivation\n\t\n\n\nFor what purpose was the dataset created?\n\nThe dataset is created for the experiment section of our paper \"Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models\" to pretrained world models and do imitation learning. We release the datasets for the community as a common test bench for similar problems. \nCode available at https://github.com/IcarusWizard/AIME-NoB.\n\nWho created the dataset (e.g., which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IcarusWizard/AIME-NoB.","url":"https://huggingface.co/datasets/IcarusWizard/AIME-NoB","creator_name":"Xingyuan Zhang","creator_url":"https://huggingface.co/IcarusWizard","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","English","mit","arxiv:2404.18896","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Time-Bench","keyword":"reinforcement-learning","description":"\n     \n\n\n\nü§ó Model  |  üöÄ Code  |  üìñ Paper\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTime-Bench Dataset\n\t\n\nThis directory contains the Time-Bench dataset, used for training and evaluating the Time-R1 model. The dataset is organized to support the different stages of the Time-R1 training curriculum.\n\n\t\n\t\n\t\n\t\tDataset Files\n\t\n\nBelow is a list of the key dataset files and their corresponding usage in the Time-R1 framework:\n\n\t\t\n\t\tStage 1: Temporal Comprehension\n\t\n\nThese files are used for training and validating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ulab-ai/Time-Bench.","url":"https://huggingface.co/datasets/ulab-ai/Time-Bench","creator_name":"ulab","creator_url":"https://huggingface.co/ulab-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\n\t\n\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\n\n\t\n\t\n\t\n\t\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned.","url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Reve-AI-Halfmoon_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Reve AI Halfmoon Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 51k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Reve AI Halfmoon across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"Synthetic_Unanswerable_Math","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Synthetic Unanswerable Math (SUM)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSynthetic Unanswerable Math (SUM) is a dataset of high-quality, implicitly unanswerable math problems constructed to probe and improve the refusal behavior of large language models (LLMs). The goal is to teach models to identify when a problem cannot be answered due to incomplete, ambiguous, or contradictory information, and respond with epistemic humility (e.g., \\boxed{I don't know}).\nEach entry in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lime-nlp/Synthetic_Unanswerable_Math.","url":"https://huggingface.co/datasets/lime-nlp/Synthetic_Unanswerable_Math","creator_name":"Language, Intelligence, and Model Evaluation Lab","creator_url":"https://huggingface.co/lime-nlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"LunarLander-v2","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tLunarLander-v2 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a PPO policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 500.\nEach entry consists of:\nobs (list): observation with length 8.\naction (int): action (0, 1, 2 and 3).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/LunarLander-v2.","url":"https://huggingface.co/datasets/NathanGavenski/LunarLander-v2","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","100K - 1M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"augmented_codealpaca-20k-using-together-ai-deepseek-v1","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset, named CodeAlpaca-20k, consists of examples that blend coding instructions with outputs and reasoning. Each entry includes structured fields like output, instruction, input, and cot (Chain of Thought). It is particularly designed to train and evaluate AI models that generate code and explanations based on simple programming tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tData Collection and Preparation\n\t\n\nData entries are augmented using the augment_answer function that makes API‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1.","url":"https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","text-to-speech","English","mit"],"keywords_longer_than_N":true},
	{"name":"HalfCheetah-v4","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tHalfCheetah-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 9809.9417.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4.","url":"https://huggingface.co/datasets/NathanGavenski/HalfCheetah-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"VL-PRM300K-train","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for VL-PRM300K-train\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nVL-PRM300K-train is a postprocessed version of VL-PRM300K, ready to be used with training pipelines using HuggingFace Trainer and TRL.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n# pip install -q datasets\nfrom datasets import load_dataset\nds = load_dataset(\"ob11/VL-PRM300K-train\")[\"train\"]\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe data fields are:\n\nmessages: Reasoning trace formatted for supervised finetuning with HuggingFace‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ob11/VL-PRM300K-train.","url":"https://huggingface.co/datasets/ob11/VL-PRM300K-train","creator_name":"Brandon Ong","creator_url":"https://huggingface.co/ob11","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","reinforcement-learning","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Minecraft-Skill-Data","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Minecraft Expert Skill Data\n\t\n\nThis dataset consists of expert demonstration trajectories from a Minecraft simulation environment. Each trajectory includes ground-truth skill segmentation annotations, enabling research into action segmentation, skill discovery, imitation learning, and reinforcement learning with temporally-structured data.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Minecraft Skill Segmentation Dataset contains gameplay trajectories‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dami2106/Minecraft-Skill-Data.","url":"https://huggingface.co/datasets/dami2106/Minecraft-Skill-Data","creator_name":"Dami","creator_url":"https://huggingface.co/dami2106","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","other","code","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"beyond_dpo_vi","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset is from unknown, formatted as dialogues for speed and ease of use. Many thanks to author for releasing it.\nImportantly, this format is easy to use via the default chat template of transformers, meaning you can use huggingface/alignment-handbook immediately, unsloth.\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nView online through viewer.\n\n\t\n\t\t\n\t\n\t\n\t\tNote\n\t\n\nWe advise you to reconsider before use, thank you. If you find it useful, please like and follow this account.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lamhieu/beyond_dpo_vi.","url":"https://huggingface.co/datasets/lamhieu/beyond_dpo_vi","creator_name":"Hieu Lam","creator_url":"https://huggingface.co/lamhieu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","Vietnamese","mit"],"keywords_longer_than_N":true},
	{"name":"synthetic_vc_financial_decisions_reasoning_dataset","keyword":"reinforcement-learning","description":"\n \n\n\nBest Curator Use Case in the Reasoning Datasets Competition: https://www.linkedin.com/feed/update/urn:li:activity:7330998995990781952/\n\n\t\n\t\t\n\t\n\t\n\t\tSynthetic VC Financial Decisions Reasoning Dataset\n\t\n\n\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe Synthetic VC Financial Decisions Reasoning Dataset is a large-scale collection designed to train, evaluate, and fine-tune language models on subjective, abstract financial reasoning tasks.  It simulates venture capital (VC) workflows by capturing multiple‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset.","url":"https://huggingface.co/datasets/ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset","creator_name":"Kenneth Hamilton","creator_url":"https://huggingface.co/ZennyKenny","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","summarization","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"mind2web-subset-human","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMind2Web Subset - Human Demonstrations\n\t\n\nA collection of human-demonstrated web navigation tasks with detailed interaction traces. This dataset captures real browser interactions including clicks, typing, scrolling, DOM states, screenshots, and HTTP requests for web agent training and evaluation.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains tasks performed by humans in real web environments, capturing:\n\nGolden trajectories: Step-by-step sequences of actions (clicks, typing, navigation)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/josancamon/mind2web-subset-human.","url":"https://huggingface.co/datasets/josancamon/mind2web-subset-human","creator_name":"Joan","creator_url":"https://huggingface.co/josancamon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Med-REFL-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tNews\n\t\n\n[2025/06/10] We are releasing the Med-REFL dataset, which is split into two subsets: Reasoning Enhancement Data and Reflection Enhancement Data.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the Direct Preference Optimization (DPO) dataset created by the Med-REFL framework, designed to improve the reasoning and reflection capabilities of Large Language Models in the medical field.\nThe dataset is constructed using a low-cost, scalable pipeline that leverages a Tree-of-Thought (ToT) approach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HANI-LAB/Med-REFL-DPO.","url":"https://huggingface.co/datasets/HANI-LAB/Med-REFL-DPO","creator_name":"HANI-LAB","creator_url":"https://huggingface.co/HANI-LAB","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"dpo","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"gutenberg-dpo-v0.1-tr","keyword":"rlhf","description":"duxx/gutenberg-dpo-v0.1-tr dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/duxx/gutenberg-dpo-v0.1-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"MsitralTrix-test-dpo","keyword":"dpo","description":"CultriX/MsitralTrix-test-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/MsitralTrix-test-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","arrow"],"keywords_longer_than_N":true},
	{"name":"Acrobot-v1","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tAcrobot-v1 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of -69.852.\nEach entry consists of:\nobs (list): observation with length 6.\naction (int): action (0, 1 or 2).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Acrobot-v1.","url":"https://huggingface.co/datasets/NathanGavenski/Acrobot-v1","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"mmlu-5-options-rl-ready","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMMLU ‚Äì 5-Options RL-Ready\n\t\n\nA standardized, RL-friendly remix of MMLU with explicit negatives and a unified five-option presentation string for each question. Ideal for DPO and other RL setups while remaining drop-in for classic multiple-choice evaluation.\n\n\t\n\t\t\n\t\tWhat‚Äôs inside\n\t\n\n\nSplits & size: ~97.8k train + 2k test ‚âà 99.8k total.\n\nSchema (core fields):\n\nquestion: str\nchoices: list[str] (canonical options, typically 4 as in original MMLU)\nanswer: int (0-based index)\ntask: str‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready.","url":"https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"mmlu-5-options-rl-ready","keyword":"dpo","description":"\n\t\n\t\t\n\t\tMMLU ‚Äì 5-Options RL-Ready\n\t\n\nA standardized, RL-friendly remix of MMLU with explicit negatives and a unified five-option presentation string for each question. Ideal for DPO and other RL setups while remaining drop-in for classic multiple-choice evaluation.\n\n\t\n\t\t\n\t\tWhat‚Äôs inside\n\t\n\n\nSplits & size: ~97.8k train + 2k test ‚âà 99.8k total.\n\nSchema (core fields):\n\nquestion: str\nchoices: list[str] (canonical options, typically 4 as in original MMLU)\nanswer: int (0-based index)\ntask: str‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready.","url":"https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"mmlu-5-options-rl-ready","keyword":"rl","description":"\n\t\n\t\t\n\t\tMMLU ‚Äì 5-Options RL-Ready\n\t\n\nA standardized, RL-friendly remix of MMLU with explicit negatives and a unified five-option presentation string for each question. Ideal for DPO and other RL setups while remaining drop-in for classic multiple-choice evaluation.\n\n\t\n\t\t\n\t\tWhat‚Äôs inside\n\t\n\n\nSplits & size: ~97.8k train + 2k test ‚âà 99.8k total.\n\nSchema (core fields):\n\nquestion: str\nchoices: list[str] (canonical options, typically 4 as in original MMLU)\nanswer: int (0-based index)\ntask: str‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready.","url":"https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","description":"medmac01/argilla-dpo-mix-7k-arabic dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/medmac01/argilla-dpo-mix-7k-arabic","creator_name":"Mohammed Machrouh","creator_url":"https://huggingface.co/medmac01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"openassistant-deepseek-coder","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - OpenAssistant DeepSeek Coder\n\t\n\nThis dataset allows for fine-tuning chat models using:\nB_INST = '\\n### Instruction:\\n'\nE_INST = '\\n### Response:\\n'\nBOS = '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>'\nEOS = '\\n<|EOT|>\\n'\n\nSample Preparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder.","url":"https://huggingface.co/datasets/Trelis/openassistant-deepseek-coder","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"MountainCar-v0","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tMountainCar-v0 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a DQN policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of -98.817.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_returns (bool): if that state was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/MountainCar-v0.","url":"https://huggingface.co/datasets/NathanGavenski/MountainCar-v0","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k","keyword":"dpo","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\n","url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-ru","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for \"hh-rlhf-ru\"\n\t\n\nThis is translated version of Anthropic/hh-rlhf dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/hh-rlhf-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","Anthropic/hh-rlhf","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"DPO-En-Zh-20k","keyword":"rlhf","description":"This dataset is composed by\n\n4,000 examples of argilla/distilabel-capybara-dpo-7k-binarized with chosen score>=4.\n3,000 examples of argilla/distilabel-intel-orca-dpo-pairs with chosen score>=8.\n3,000 examples of argilla/ultrafeedback-binarized-preferences-cleaned with chosen score>=4.\n10,000 examples of wenbopan/Chinese-dpo-pairs.\n\nYou can use it in LLaMA Factory by specifying dataset: dpo_mix_en,dpo_mix_zh.\n","url":"https://huggingface.co/datasets/llamafactory/DPO-En-Zh-20k","creator_name":"LLaMA Factory","creator_url":"https://huggingface.co/llamafactory","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Agent-RL-Open-Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOpen Agent RL Dataset: High Quality AI Agent | Tool Use & Function Calls | Reinforcement Learning Datasets\n\t\n\nGithub|Huggingface|Pypi | Open Source AI Agent Marketplace DeepNLP|Agent RL Dataset\nDeepNLP website provides high quality, genuine, online users' request of Agent & RL datasets to help LLM foundation/SFT/Post Train to get more capable models at function call, tool use and planning. The datasets are collected and sampled\nfrom users' requests on our various clients (Web/App/Mini‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/Agent-RL-Open-Dataset.","url":"https://huggingface.co/datasets/DeepNLP/Agent-RL-Open-Dataset","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Agent-RL-Open-Dataset","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOpen Agent RL Dataset: High Quality AI Agent | Tool Use & Function Calls | Reinforcement Learning Datasets\n\t\n\nGithub|Huggingface|Pypi | Open Source AI Agent Marketplace DeepNLP|Agent RL Dataset\nDeepNLP website provides high quality, genuine, online users' request of Agent & RL datasets to help LLM foundation/SFT/Post Train to get more capable models at function call, tool use and planning. The datasets are collected and sampled\nfrom users' requests on our various clients (Web/App/Mini‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/Agent-RL-Open-Dataset.","url":"https://huggingface.co/datasets/DeepNLP/Agent-RL-Open-Dataset","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"helpful-anthropic-raw","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for \"helpful-raw-anthropic\"\n\t\n\nThis is a dataset derived from Anthropic's HH-RLHF data of instructions and model-generated demonstrations. We combined training splits from the following two subsets:\n\nhelpful-base\nhelpful-online\n\nTo convert the multi-turn dialogues into (instruction, demonstration) pairs, just the first response from the Assistant was included. This heuristic captures the most obvious answers, but overlooks more complex questions where multiple turns were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw.","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-anthropic-raw","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"helpful-instructions","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for Helpful Instructions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHelpful Instructions is a dataset of (instruction, demonstration) pairs that are derived from public datasets. As the name suggests, it focuses on instructions that are \"helpful\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform. You can load the dataset as follows:\nfrom datasets import load_dataset\n\n# Load all subsets\nhelpful_instructions =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions.","url":"https://huggingface.co/datasets/HuggingFaceH4/helpful-instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"openai-tldr-filtered","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tFiltered TL;DR Dataset\n\t\n\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\n","url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","crowdsourced","crowdsourced","monolingual","extended"],"keywords_longer_than_N":true},
	{"name":"rlhf-reward-datasets-ru","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"rlhf-reward-datasets-ru\"\n\t\n\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","yitingxie/rlhf-reward-datasets","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"apps_rlaif","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAPPS Dataset for Reinforcement Learning with AI Feedback\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nAPPS_RLAIF is an extended work from APPS [1] \nto use Chat LLMs to create multiple variances for each solution for defined problems. \nIn each solution, we use LLama 34B [2] to transform the original solutions into variances and rank them by score.\nThe generated flow is demonstrated as below; each variance is created based on the previous version of it in the chat. \nWe iterated each solutions n=3 times‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nmd2k/apps_rlaif.","url":"https://huggingface.co/datasets/nmd2k/apps_rlaif","creator_name":"Nguyen Manh Dung","creator_url":"https://huggingface.co/nmd2k","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"rlhf-reward-datasets-ru","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for \"rlhf-reward-datasets-ru\"\n\t\n\nThis is translated version of yitingxie/rlhf-reward-datasets dataset into Russian.\n","url":"https://huggingface.co/datasets/d0rj/rlhf-reward-datasets-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translated","monolingual","yitingxie/rlhf-reward-datasets","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"openassistant-guanaco-EOS","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tChat Fine-tuning Dataset - Guanaco Style\n\t\n\nThis dataset allows for fine-tuning chat models using \"### Human:\" AND \"### Assistant\" as the beginning and end of sequence tokens.\nPreparation:\n\nThe dataset is cloned from TimDettmers, which itself is a subset of the Open Assistant dataset, which you can find here. This subset of the data only contains the highest-rated paths in the conversation tree, with a total of 9,846 samples.\nThe dataset was then slightly adjusted to:\n\n\nif a row of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS.","url":"https://huggingface.co/datasets/Trelis/openassistant-guanaco-EOS","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Spanish","Russian","German","Polish"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-kto","keyword":"rlhf","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for KTO\n\t\n\n\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto.","url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-kto","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for KTO\n\t\n\n\nA KTO signal transformed version of the highly loved distilabel Orca Pairs for DPO.\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto.","url":"https://huggingface.co/datasets/argilla/distilabel-intel-orca-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"intel_orca_dpo_pairs_de","keyword":"dpo","description":"German translation of Intel/orca_dpo_pairs\nUsing azureml for translation and hermeo-7b for rejected answers.\n","url":"https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de","creator_name":"Mayflower GmbH","creator_url":"https://huggingface.co/mayflowergmbh","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","German","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Capybara-Preferences","keyword":"dpo","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Capybara-Preferences\n\t\n\nThis dataset has been created with distilabel.\n\n    \n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be reused‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences.","url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tLongReward-10k\n\t\n\n\n  üíª [Github Repo] ‚Ä¢ üìÉ [LongReward Paper] \n\n\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \nThe dpo_glm4_9b and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongReward-10k.","url":"https://huggingface.co/datasets/zai-org/LongReward-10k","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"agentgym-sft-trajectories","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAgentGym SFT Trajectories\n\t\n\nA comprehensive dataset of 101,926 training examples from 5 AgentGym environments, formatted for supervised fine-tuning (SFT) of language models on interactive tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nTotal Samples: 101,926 training examples\nEnvironments: 5 AgentGym environments with balanced representation\nFormat: ChatML-style messages with environment labels\nFile Size: 459MB\n\n\n\t\n\t\t\n\t\tEnvironments Included\n\t\n\n\nAlfWorld (household tasks)\nBabyAI (grid navigation)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bunnybhaiya/agentgym-sft-trajectories.","url":"https://huggingface.co/datasets/bunnybhaiya/agentgym-sft-trajectories","creator_name":"Shubhanshu Khatana","creator_url":"https://huggingface.co/bunnybhaiya","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Beacon","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tBeacon Dataset for Sycophancy Evaluation\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Beacon dataset is designed to measure sycophantic bias in Large Language Models (LLMs) through a novel single-turn forced-choice evaluation paradigm. It consists of 420 carefully curated prompts, each paired with a principled response and a sycophantic alternative. Expert annotations rate responses on dimensions of Critical Thinking and Fluency, enabling fine-grained behavioral analysis.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sanskxr02/Beacon.","url":"https://huggingface.co/datasets/sanskxr02/Beacon","creator_name":"pandey","creator_url":"https://huggingface.co/sanskxr02","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"bloxorz","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tBloxorz-Style Levels with Guaranteed Shortest Paths\n\t\n\nBloxorz-style grid levels procedurally generated with a guaranteed shortest-path solution (found via BFS).Each example is a small ASCII map with a start (S), goal (G), walkable tiles (#), and void/holes ( ).The dataset includes both the shortest move sequence and its length.\n\nSplits: train (900), test (100)  \nFile format: newline-delimited JSON (.jsonl)  \nMoves: U, D, L, R\n\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset is designed for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mattismegevand/bloxorz.","url":"https://huggingface.co/datasets/mattismegevand/bloxorz","creator_name":"Mattis Megevand","creator_url":"https://huggingface.co/mattismegevand","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","code","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"argilla-dpo-mix-7k-arabic","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"argilla-dpo-mix-7k-arabic\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/2A2I/argilla-dpo-mix-7k-arabic","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ShareGPT52K","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for ShareGPT52K90K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\nThis repository now contains the new 90K conversations version. The previous 52K may\nbe found in the old/ directory.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\ntext-generation\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K.","url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"summarization-preferences","keyword":"dpo","description":"\n\t\n\t\t\n\t\tSummarization Preferences Dataset\n\t\n\nThis is a processed subset of the openai/summarize_from_feedback comparisons subset, including training and validation splits.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe original dataset consists of paired human comparisons between summary candidates for given source texts. This processed version aggregates all comparisons per unique text to determine the overall best (chosen) and worst (rejected) summaries using the Bradley-Terry model.\n\n\t\n\t\t\n\t\tFields\n\t\n\n\ntext:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/summarization-preferences.","url":"https://huggingface.co/datasets/agentlans/summarization-preferences","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-ranking","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"dpo","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"medconceptsqa-sample_medarc_15k","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for MedConceptsQA 15K Sample for MedARC\n\t\n\nThis is a hierarchically stratified sample of the ICD10-CM coding system from\nMedConceptsQA.\nThis dataset was sampled such that maximum coverage across all levels \nof the ICD-10 hierarchy to be as representative as possible while constraining\nthe dataset to a reasonable size for evaluation and potential reinforcement\nlearning.\nUsers can generate their own sampling of MedConceptsQA via \nthis generator script.\n\nOriginal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sameedkhan/medconceptsqa-sample_medarc_15k.","url":"https://huggingface.co/datasets/sameedkhan/medconceptsqa-sample_medarc_15k","creator_name":"Sameed Khan","creator_url":"https://huggingface.co/sameedkhan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","machine-generated","ofir408/MedConceptsQA","English"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlhf","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-intel-orca-dpo-pairs-tr","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tdistilabel Orca Pairs for DPO\n\t\n\nThe dataset is a \"distilabeled\" version of the widely used dataset: Intel/orca_dpo_pairs. The original dataset has been used by 100s of open-source practitioners and models. We knew from fixing UltraFeedback (and before that, Alpacas and Dollys) that this dataset could be highly improved.\nContinuing with our mission to build the best alignment datasets for open-source LLMs and the community, we spent a few hours improving it with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr.","url":"https://huggingface.co/datasets/duxx/distilabel-intel-orca-dpo-pairs-tr","creator_name":"batu","creator_url":"https://huggingface.co/duxx","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Turkish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"hh-rlhf-th","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a üáπüá≠ Thai-translated dataset based on Anthropic/hh-rlhf using Google Cloud Translation. \nThis repository provides access to:\n\n161K Train dataset Anthropic/hh-rlhf (Thai-translated)\n(Soon) 8K Test dataset Anthropic/hh-rlhf (Thai-translated)\n\nDisclaimer: The data contain content that may be offensive or upsetting. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Thaweewat/hh-rlhf-th.","url":"https://huggingface.co/datasets/Thaweewat/hh-rlhf-th","creator_name":"Thaweewat","creator_url":"https://huggingface.co/Thaweewat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Thai","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Open_Assistant_Chains_German_Translation","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\n\n\nThis dataset is derived from OpenAssistant Conversation Chains, which is a reformatting of OpenAssistant Conversations (OASST1), which is itself\n\na human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation.","url":"https://huggingface.co/datasets/m-ric/Open_Assistant_Chains_German_Translation","creator_name":"Aymeric Roucher","creator_url":"https://huggingface.co/m-ric","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","German","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lerobotlekiwi","keyword":"reinforcement-learning","description":"This dataset was created using LeRobot.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmeta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"lekiwi_client\",\n    \"total_episodes\": 54,\n    \"total_frames\": 13500,\n    \"total_tasks\": 5,\n    \"total_videos\": 108,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 10,\n    \"splits\": {\n        \"train\": \"0:54\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Baptiste-le-Beaudry/lerobotlekiwi.","url":"https://huggingface.co/datasets/Baptiste-le-Beaudry/lerobotlekiwi","creator_name":"Beaudry","creator_url":"https://huggingface.co/Baptiste-le-Beaudry","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","apache-2.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"terminal-bench","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tTerminal-Bench Dataset\n\t\n\nThis dataset contains tasks from Terminal-Bench, a benchmark for evaluating AI agents in real terminal environments. Each task is packaged as a complete, self-contained archive that preserves the exact directory structure, binary files, Docker configurations, and test scripts needed for faithful reproduction.\nThe archive column contains a gzipped tarball of the entire task directory.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nTerminal-Bench evaluates AI agents on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ia03/terminal-bench.","url":"https://huggingface.co/datasets/ia03/terminal-bench","creator_name":"Ibrahim Ahmed","creator_url":"https://huggingface.co/ia03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"terminal-bench","keyword":"rl","description":"\n\t\n\t\t\n\t\tTerminal-Bench Dataset\n\t\n\nThis dataset contains tasks from Terminal-Bench, a benchmark for evaluating AI agents in real terminal environments. Each task is packaged as a complete, self-contained archive that preserves the exact directory structure, binary files, Docker configurations, and test scripts needed for faithful reproduction.\nThe archive column contains a gzipped tarball of the entire task directory.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nTerminal-Bench evaluates AI agents on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ia03/terminal-bench.","url":"https://huggingface.co/datasets/ia03/terminal-bench","creator_name":"Ibrahim Ahmed","creator_url":"https://huggingface.co/ia03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"haiku_dpo","keyword":"dpo","description":"\nüå∏ Haiku DPO üå∏\n\n    \n\n\n\nIn data, words flow,\nTeaching AI the art of\nHaiku, line by line.\n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Haiku DPO\n\t\n\n\n\n\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n The data consists of a few different components that are described in more detail below but the key components are:\n\na column of synthetically generated user prompts requesting a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo.","url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"haiku_dpo","keyword":"reinforcement-learning","description":"\nüå∏ Haiku DPO üå∏\n\n    \n\n\n\nIn data, words flow,\nTeaching AI the art of\nHaiku, line by line.\n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Haiku DPO\n\t\n\n\n\n\nThis a synthetic dataset of haikus. The dataset is constructed with the goal of helping to train LLMs to be more 'technically' competent at writing haikus. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n The data consists of a few different components that are described in more detail below but the key components are:\n\na column of synthetically generated user prompts requesting a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/haiku_dpo.","url":"https://huggingface.co/datasets/davanstrien/haiku_dpo","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"routefinder","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRouteFinder Data\n\t\n\nYou may find instructions here: https://github.com/ai4co/routefinder\n","url":"https://huggingface.co/datasets/ai4co/routefinder","creator_name":"AI4CO","creator_url":"https://huggingface.co/ai4co","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["reinforcement-learning","mit","10K<n<100K","üá∫üá∏ Region: US","NCO"],"keywords_longer_than_N":true},
	{"name":"en_vi_DPO","keyword":"reinforcement-learning","description":"gallantVN/en_vi_DPO dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/gallantVN/en_vi_DPO","creator_name":"Nhut Tien","creator_url":"https://huggingface.co/gallantVN","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["translation","reinforcement-learning","English","Vietnamese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Archer-Code-1.5B","keyword":"reinforcement-learning","description":"\n\n\n\t\n\t\t\n\t\t‚ú® ArcherCodeR\n\t\n\n\nüèπÔ∏è  Reinforcement Learning for Enhanced Code Reasoning in LLMs  üéØ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nArcherCodeR-Dataset is a dataset of verifiable, challenging, and diverse coding questions (6.7K). This dataset is used to train the ArcherCodeR model series, which consists of code reasoning models trained using large-scale rule-based reinforcement learning with carefully designed datasets and training recipes.\nWe select, clean, and curate coding problems from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fate-Zero/Archer-Code-1.5B.","url":"https://huggingface.co/datasets/Fate-Zero/Archer-Code-1.5B","creator_name":"Fate","creator_url":"https://huggingface.co/Fate-Zero","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"frontend_dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO JavaScript Dataset\n\t\n\nThis repository contains a modified and expanded version of a closed-source JavaScript dataset. The dataset has been adapted to fit the DPO (Dynamic Programming Object) format, making it compatible with the LLaMA-Factory project. The dataset includes a variety of JavaScript code snippets with optimizations and best practices, generated using closed-source tools and expanded by me.\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the Apache 2.0 License.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/israellaguan/frontend_dpo.","url":"https://huggingface.co/datasets/israellaguan/frontend_dpo","creator_name":"Israel Antonio Rosales Laguan","creator_url":"https://huggingface.co/israellaguan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","dialogue-generation","human-generated","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ARPO-RL-Reasoning-10K","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tAgentic Reinforced Policy Optimization (ARPO) Dataset\n\t\n\nThis repository contains the datasets associated with the paper Agentic Reinforced Policy Optimization.\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nLarge-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dongguanting/ARPO-RL-Reasoning-10K.","url":"https://huggingface.co/datasets/dongguanting/ARPO-RL-Reasoning-10K","creator_name":"KABI","creator_url":"https://huggingface.co/dongguanting","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Ideogram-V2_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Ideogram-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 42k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Ideogram-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"deep-reasoning-kk","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tK&K Dataset\n\t\n\nThis repository contains the K&K dataset used for reproducing results in the paper: Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning.\nCode repo: https://github.com/on1262/deep-reasoning\n\n\t\n\t\t\n\t\tSample Usage\n\t\n\nYou can load the dataset using the Hugging Face datasets library:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"Chen-YT/deep-reasoning-kk\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tCitation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chen-YT/deep-reasoning-kk.","url":"https://huggingface.co/datasets/Chen-YT/deep-reasoning-kk","creator_name":"Yutong Chen","creator_url":"https://huggingface.co/Chen-YT","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","mit","arxiv:2505.17988","üá∫üá∏ Region: US","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"osiris","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tOsiris: A Scalable Dataset Generation Pipeline for Machine Learning in Analog Circuit Design\n\t\n\nOsiris is an end-to-end analog circuits design pipeline capable of producing, validating, and evaluating layouts for generic analog circuits.\nThe Osiris GitHub repository hosts the code that implements the randomized pipeline as well as the reinforcement learning-driven baseline methodology discussed \nin the paper proposed at the NeurIPS 2025 Datasets & Benchmarks Track.\nThe Osiris ü§ó‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hardware-fab/osiris.","url":"https://huggingface.co/datasets/hardware-fab/osiris","creator_name":"Hardware-Fab","creator_url":"https://huggingface.co/hardware-fab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","üá∫üá∏ Region: US","eda"],"keywords_longer_than_N":true},
	{"name":"CoProv2-SD15","keyword":"dpo","description":"This repository contains CoProV2, a synthetically generated dataset of harmful and safe image-text pairs. It was introduced in the paper AlignGuard: Scalable Safety Alignment for Text-to-Image Generation.\nCoProV2 is specifically designed to enable the application of Direct Preference Optimization (DPO) for safety purposes in Text-to-Image (T2I) models. It facilitates the training of \"safety experts\" to guide the generative process away from specific safety-related concepts, enabling scalable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Visualignment/CoProv2-SD15.","url":"https://huggingface.co/datasets/Visualignment/CoProv2-SD15","creator_name":"Visualignment","creator_url":"https://huggingface.co/Visualignment","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"dpo","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tORPO-DPO-mix-40k-flat\n\t\n\n\nThis dataset is designed for ORPO or DPO training.\nSee Uncensor any LLM with Abliteration for more information about how to use it.\nThis is version with raw text instead of lists of dicts as in the original version here.\nIt makes easier to parse in Axolotl, especially for DPO.ORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat.","url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"distilset","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for distilset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/Andresckamilo/distilset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Andresckamilo/distilset.","url":"https://huggingface.co/datasets/Andresckamilo/distilset","creator_name":"Andres Montenegro","creator_url":"https://huggingface.co/Andresckamilo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"respect","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRetrospective Learning from Interactions (Respect) Dataset\n\t\n\nThis repository contains the lil-lab/respect data, based on the ACL paper Retrospective Learning from Interactions. For more resources, please see https://lil-lab.github.io/respect and https://github.com/lil-lab/respect.\n\n\t\n\t\t\n\t\n\t\n\t\tSample Usage\n\t\n\nYou can load the data and associated checkpoints as follows:\nfrom datasets import load_dataset\nfrom transformers import Idefics2ForConditionalGeneration\nfrom peft importPeftModel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lil-lab/respect.","url":"https://huggingface.co/datasets/lil-lab/respect","creator_name":"Cornell LIL Lab","creator_url":"https://huggingface.co/lil-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dpo-pairrm-preferences-llama3","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO Preference Dataset - PairRM\n\t\n\nThis dataset contains preference pairs for Direct Preference Optimization (DPO) training.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nBase Model: Llama-3.2-1B-Instruct\nNumber of Samples: 250\nCreation Method: PairRM\nTask: Preference learning for instruction following\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"pyamy/dpo-pairrm-preferences-llama3\")\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach sample contains:\n\nprompt: The instruction prompt with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pyamy/dpo-pairrm-preferences-llama3.","url":"https://huggingface.co/datasets/pyamy/dpo-pairrm-preferences-llama3","creator_name":"Priyam Choksi","creator_url":"https://huggingface.co/pyamy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lima_dirty_tr","keyword":"dpo","description":"\n\t\n\t\t\n\t\tLima Turkish Translated & Engineered for Alignment\n\t\n\n\n\t\n\t\t\n\t\tGiri≈ü\n\t\n\nBu veri seti, LIMA (Less Is More for Alignment) [^1] √ßalƒ±≈ümasƒ±ndan ilham alƒ±narak olu≈üturulmu≈ü, orijinal LIMA veri setinin T√ºrk√ße'ye √ßevrilmi≈ü ve hizalama (alignment) teknikleri i√ßin √∂zel olarak yapƒ±landƒ±rƒ±lmƒ±≈ü bir versiyonudur. LIMA'nƒ±n temel felsefesi, az sayƒ±da ancak y√ºksek kaliteli √∂rnekle dil modellerini etkili bir ≈üekilde hizalayabilmektir. Bu √ßalƒ±≈üma, bu felsefeyi T√ºrk√ße dil modelleri ekosistemine ta≈üƒ±mayƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emre/lima_dirty_tr.","url":"https://huggingface.co/datasets/emre/lima_dirty_tr","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"dpo-mix-21k","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO Mix 21K Dataset\n\t\n\nSimilar to the argilla/dpo-mix-7k, but with 21k samples:\nOther diferences:\n\nargilla/distilabel-capybara-dpo-7k-binarized changed to argilla/Capybara-Preferences\nargilla/distilabel-intel-orca-dpo-pairs scored filter of >=8 changed to >= 7\n\n\nA small cocktail combining DPO datasets built by Argilla with distilabel. The goal of this dataset is having a small, high-quality DPO dataset by filtering only highly rated chosen responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eduagarcia/dpo-mix-21k.","url":"https://huggingface.co/datasets/eduagarcia/dpo-mix-21k","creator_name":"Eduardo Garcia","creator_url":"https://huggingface.co/eduagarcia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tulu-2.5-preference-data","keyword":"reinforcement-learning","description":"\n\n\n\n\n\t\n\t\t\n\t\tTulu 2.5 Preference Data\n\t\n\nThis dataset contains the preference dataset splits used to train the models described in Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback.\nWe cleaned and formatted all datasets to be in the same format.\nThis means some splits may differ from their original format.\nTo see the code used for creating most splits, see here.\nIf you only wish to download one dataset, each dataset exists in one file under the data/‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/tulu-2.5-preference-data.","url":"https://huggingface.co/datasets/allenai/tulu-2.5-preference-data","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","odc-by","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"luckyvicky-DPO","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tÏõêÏòÅÏ†Å ÏÇ¨Í≥† Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n","url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"Ïù¥Ï∞ΩÏ§Ä","creator_url":"https://huggingface.co/Junnos","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text2text-generation","Korean","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"luckyvicky-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tÏõêÏòÅÏ†Å ÏÇ¨Í≥† Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n","url":"https://huggingface.co/datasets/Junnos/luckyvicky-DPO","creator_name":"Ïù¥Ï∞ΩÏ§Ä","creator_url":"https://huggingface.co/Junnos","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text2text-generation","Korean","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"DeepScaleR_Difficulty","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDifficulty Estimation on DeepScaleR\n\t\n\nWe annotate the entire DeepScaleR dataset with a difficulty score based on the performance of the Qwen 2.5-MATH-7B model. This provides an adaptive signal for curriculum construction and model evaluation.\nDeepScaleR is a curated dataset of 40,000 reasoning-intensive problems used to train and evaluate reinforcement learning-based methods for large language models.\n\n\t\n\t\t\n\t\n\t\n\t\tDifficulty Scoring Method\n\t\n\nDifficulty scores are estimated using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty.","url":"https://huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty","creator_name":"Language, Intelligence, and Model Evaluation Lab","creator_url":"https://huggingface.co/lime-nlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","1M - 10M","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"my-distiset-620d36eb","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-620d36eb\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb.","url":"https://huggingface.co/datasets/kritsanan/my-distiset-620d36eb","creator_name":"namoang","creator_url":"https://huggingface.co/kritsanan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Thai","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"axay-javascript-dataset-pn","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO JavaScript Dataset\n\t\n\nThis repository contains a modified version of the JavaScript dataset originally sourced from axay/javascript-dataset-pn. The dataset has been adapted to fit the DPO (Dynamic Programming Object) format, making it compatible with the LLaMA-Factory project.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the Apache 2.0 License.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThe dataset consists of JavaScript code snippets that have been restructured and enhanced for use in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn.","url":"https://huggingface.co/datasets/israellaguan/axay-javascript-dataset-pn","creator_name":"Israel Antonio Rosales Laguan","creator_url":"https://huggingface.co/israellaguan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","dialogue-generation","machine-generated","machine-generated","axay/javascript-dataset-pn"],"keywords_longer_than_N":true},
	{"name":"openpipe-dpo-scientific-reasoning","keyword":"dpo","description":"\n\t\n\t\t\n\t\tOpenpipe Dpo Scientific Reasoning\n\t\n\nThis dataset contains 100 high-quality examples for Direct Preference Optimization (DPO) training, formatted for OpenPipe fine-tuning, focused on scientific reasoning and analysis.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset was generated using an enhanced DSPy-based pipeline that creates structured reasoning traces for scientific questions. Each example follows the OpenAI chat completion format required by OpenPipe:\n\nOpenAI Chat Format: Standard‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhi26/openpipe-dpo-scientific-reasoning.","url":"https://huggingface.co/datasets/abhi26/openpipe-dpo-scientific-reasoning","creator_name":"ABHISEK GUHA","creator_url":"https://huggingface.co/abhi26","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MMR1-RL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis repository introduces the MMR1 project, focusing on enhancing large multimodal reasoning models. While rapid progress has been made, advancements are constrained by two major limitations:\n\nThe absence of open, large-scale, high-quality long chain-of-thought (CoT) data.\nThe instability of reinforcement learning (RL) algorithms in post-training, where standard Group‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMR1/MMR1-RL.","url":"https://huggingface.co/datasets/MMR1/MMR1-RL","creator_name":"MMR1","creator_url":"https://huggingface.co/MMR1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"qa-ml-dl-jsonl","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tüí° AI Q&A Dataset for ML, DL, RL, TensorFlow, PyTorch\n\t\n\nThis dataset is designed to support training and evaluation of AI systems on question generation, answering, and understanding in the domains of Machine Learning, Deep Learning, Reinforcement Learning, TensorFlow, and PyTorch. It contains a large number of categorized questions along with high-quality answers in two different levels of brevity.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìÅ Dataset Files\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. questions.jsonl\n\t\n\n\nLines: 24,510‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl.","url":"https://huggingface.co/datasets/Koushim/qa-ml-dl-jsonl","creator_name":"K Koushik Reddy","creator_url":"https://huggingface.co/Koushim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","machine-generated","human-verified","English"],"keywords_longer_than_N":true},
	{"name":"HyperThink-X-Nvidia-Opencode-Reasoning-200K","keyword":"reinforcement-learning","description":"\n  \n\n\n\n\t\n\t\t\n\t\tüîÆ HyperThink\n\t\n\nHyperThink is a premium, best-in-class dataset series capturing deep reasoning interactions between users and an advanced Reasoning AI system. Designed for training and evaluating next-gen language models on complex multi-step tasks, the dataset spans a wide range of prompts and guided thinking outputs.\n\n\n\t\n\t\t\n\t\tüöÄ Dataset Tiers\n\t\n\nHyperThink is available in three expertly curated versions, allowing flexible scaling based on compute resources and training goals:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K.","url":"https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K","creator_name":"Nukeverse","creator_url":"https://huggingface.co/NuclearAi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"dpo","description":"\n\t\n\t\t\n\t\tUltraFeedback (Cleaned)\n\t\n\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\nand test split of HuggingFaceH4/ultrafeedback_binarized.\n","url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LongReward-10k","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tLongReward-10k\n\t\n\n\n  üíª [Github Repo] ‚Ä¢ üìÉ [LongReward Paper] \n\n\nLongReward-10k dataset contains 10,000 long-context QA instances (both English and Chinese, up to 64,000 words). \nThe sft split contains SFT data generated by GLM-4-0520, following the self-instruct method in LongAlign. Using this split, we supervised fine-tune two models: LongReward-glm4-9b-SFT and LongReward-llama3.1-8b-SFT, which are based on GLM-4-9B and Meta-Llama-3.1-8B, respectively. \nThe dpo_glm4_9b and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongReward-10k.","url":"https://huggingface.co/datasets/THUDM/LongReward-10k","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NoRobots-AceGPT.13B.Chat-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"NoRobots-AceGPT.13B.Chat-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic No Robots dataset : 2A2I/H4_no_robots which is based on the original No Robots dataset inspired from the InstructGPT paper.\nLanguages: Modern Standard Arabic (MSA)\nLicense: CC BY-NC 4.0\nMaintainers: Ali Elfilali and Marwa El Kamil\n\n\n\t\n\t\n\t\n\t\tPurpose\n\t\n\nNoRobots-AceGPT.13B.Chat-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO.","url":"https://huggingface.co/datasets/2A2I/NoRobots-AceGPT.13B.Chat-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Select-Stack","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack.","url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"NeoRL2","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for NeoRL‚Äë2: Near Real‚ÄëWorld Benchmarks for Offline Reinforcement Learning\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNeoRL-2 is a collection of seven near‚Äìreal-world offline-RL datasets plus their evaluation simulators. This repo we provide the offline-RL dataset, while the simulators are in https://github.com/polixir/NeoRL2.\nEach task injects one or more realistic challenges‚Äîdelays, exogenous disturbances, global safety constraints, traditional rule-based data, and/or severe data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/polixirai/NeoRL2.","url":"https://huggingface.co/datasets/polixirai/NeoRL2","creator_name":"Polixir","creator_url":"https://huggingface.co/polixirai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Preference_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Text-2-Image Alignment dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3.","url":"https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-to-image","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"Imagen4_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 70k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 (imagen-4.0-ultra-generate-exp-05-20) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"GRAM-fine-tuning-65k","keyword":"rlhf","description":"This is the dataset for Fine-tuning GRAM.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach item of the dataset includes following keys:\n\ninstruction: any prompt with corresponding two responses in following template:Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better.\nYour evaluation should consider factors such as the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NiuTrans/GRAM-fine-tuning-65k.","url":"https://huggingface.co/datasets/NiuTrans/GRAM-fine-tuning-65k","creator_name":"NiuTrans","creator_url":"https://huggingface.co/NiuTrans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ball-maze-lerobot","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tBall Maze Environment Dataset\n\t\n\nThis dataset contains episodes of a ball maze environment, converted to the LeRobot format for visualization and training.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nSagar18/ball-maze-lerobot/\n‚îú‚îÄ‚îÄ data/                 # Main dataset files\n‚îú‚îÄ‚îÄ metadata/            # Dataset metadata\n‚îÇ   ‚îî‚îÄ‚îÄ info.json       # Configuration and version info\n‚îú‚îÄ‚îÄ episode_data_index.safetensors  # Episode indexing information\n‚îî‚îÄ‚îÄ stats.safetensors    # Dataset statistics\n\n\n\t\n\t\t\n\t\tFeatures‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sagar18/ball-maze-lerobot.","url":"https://huggingface.co/datasets/Sagar18/ball-maze-lerobot","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","robotics","mit","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"tartanaviation-adsb-19k-clean","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tTartanAviation ADS-B Dataset (19.7K Clean Samples)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n19,714 high-quality ADS-B trajectory datapoints from aircraft operations, rigorously cleaned and validated. Perfect for machine learning research in aviation, reinforcement learning, and trajectory prediction.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n19,714 clean samples (no missing data, no duplicates)  \n17 comprehensive features including aircraft ID, timestamp components, altitude, speed, heading, geolocation, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pathange/tartanaviation-adsb-19k-clean.","url":"https://huggingface.co/datasets/Pathange/tartanaviation-adsb-19k-clean","creator_name":"Pathange  Omkareshwara Rao","creator_url":"https://huggingface.co/Pathange","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["tabular-regression","time-series-forecasting","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"EMMOE-100","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tEMMOE-100 Trainset\n\t\n\n\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nProject\nPaper\nCode\nModel\nDataset\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Feature\n\t\n\n\n\n\n  \n    \n      \n      Task Attributes\n    \n    \n      \n      Task Example\n    \n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEMMOE-100/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ assets/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ train/\n‚îÇ       ‚îú‚îÄ‚îÄ 1/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re1.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re2.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ info_re3.txt\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ keypath.json\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scene.json\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dongping-Li/EMMOE-100.","url":"https://huggingface.co/datasets/Dongping-Li/EMMOE-100","creator_name":"Dongping Li","creator_url":"https://huggingface.co/Dongping-Li","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","reinforcement-learning","robotics","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"ball-maze-images","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for Sagar18/ball-maze-images\n\t\n\nThis dataset contains visual observations of a ball maze environment, derived from the original state-space dataset at notmahi/tutorial-ball-top-20.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"Sagar18/ball-maze-images\")\n\n# Access images (they will be automatically decoded)\nimage = dataset[0][\"image\"]\nstate = dataset[0][\"state\"]\n\n","url":"https://huggingface.co/datasets/Sagar18/ball-maze-images","creator_name":"Basavasagar","creator_url":"https://huggingface.co/Sagar18","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","robotics","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gemini_orpo_dpo_ptbr","keyword":"dpo","description":"celsowm/gemini_orpo_dpo_ptbr dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/celsowm/gemini_orpo_dpo_ptbr","creator_name":"Celso F","creator_url":"https://huggingface.co/celsowm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","Portuguese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"reasoning-1-1k","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tReasoning-1 1K\n\t\n\n\n\t\n\t\t\n\t\tShort about\n\t\n\nThis dataset will help in SFT training of LLM on the Alpaca format.\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\nMade by Fluently Team (@ehristoforu) using distilabel with loveü•∞\n\n\t\n\t\t\n\t\n\t\n\t\tDataset structure\n\t\n\nThis subset can be loaded as:\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k.","url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"Reflective-MAGLLAMA-v0.1.1","keyword":"rlaif","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Reflective-MAGLLAMA-v0.1.1\n\t\n\nThis dataset has been created with distilabel.\nSome changes were made from v1:\n\nadditional cleaning has been done to the generations to ensure proper tag structure\na new reflections column has been added.\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\nThis dataset is tailored specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1.","url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SentimentSynth-ko","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"SentimentSynth\"\n\t\n\nTranslated OEvortex/SentimentSynth using nayohan/llama3-instrucTrans-enko-8b.\n","url":"https://huggingface.co/datasets/nayohan/SentimentSynth-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Hierarchical-Preference-Dataset","keyword":"dpo","description":"\n\t\n\t\t\n\t\tHierarchical Preference Dataset\n\t\n\nThe Hierarchical Preference Dataset is a structured dataset for analyzing and evaluating model reasoning through a hierarchical cognitive decomposition lens. It is derived from the prhegde/preference-data-math-stack-exchange dataset and extends it with annotations that separate model outputs into Refined Query, Meta-Thinking, and Refined Answer components.\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nEach sample in this dataset consists of:\n\nAn instruction or query.\nTwo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Death-Raider/Hierarchical-Preference-Dataset.","url":"https://huggingface.co/datasets/Death-Raider/Hierarchical-Preference-Dataset","creator_name":"Darsh Kachroo","creator_url":"https://huggingface.co/Death-Raider","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"meta-world","keyword":"reinforcement-learning","description":"This repository contains the Meta-World datasets as used in Learning to Modulate Pre-trained Models in RL: \n\nThe 2M folder contains trajectories (2M transitions) for every task are stored as separate .npz.\nThe 2M_separate folder contains the same data, but every trajectory is stored as a separate .hdf5 file and every task is stored as a .tar.gz file.\n\nDownload the dataset using the huggingface-cli:\nhuggingface-cli download ml-jku/meta-world --local-dir=./meta-world --repo-type dataset\n\nFor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ml-jku/meta-world.","url":"https://huggingface.co/datasets/ml-jku/meta-world","creator_name":"Institute for Machine Learning, Johannes Kepler University Linz","creator_url":"https://huggingface.co/ml-jku","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","mit","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"dpo","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VerInstruct","keyword":"rl","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Hao Peng@THUKEG\nLanguage(s) (NLP): English, Chinese\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: https://github.com/THU-KEG/VerIF\nPaper: https://arxiv.org/abs/2506.09942\nSource: This data is sourced from Crab, and we add verification signals for each instance.\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis data is used for RL training for instruction-following.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/VerInstruct.","url":"https://huggingface.co/datasets/THU-KEG/VerInstruct","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"evol-dpo-ita","keyword":"rlhf","description":"\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card\n\t\n\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita.","url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"reward-bench-ko","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for \"reward-bench\"\n\t\n\nTranslated allenai/reward-bench using nayohan/llama3-instrucTrans-enko-8b.\n","url":"https://huggingface.co/datasets/nayohan/reward-bench-ko","creator_name":"Yohan Na","creator_url":"https://huggingface.co/nayohan","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","Korean","odc-by","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"economic_resilience_risk_model","keyword":"reinforcement-learning","description":"Economic Resilience & Risk-Adjusted Growth Dataset (Neutral Benchmark)\nOverview\nThis dataset provides a structured analysis of economic resilience, risk-adjusted capital allocation, and market adaptability across 10,000 simulated agents. The data models two primary capital allocation strategies under varying economic conditions:\nExpansive Growth Agents: Prioritizing market share maximization and high-risk, high-reward expansion.\nSustainable Efficiency Agents: Emphasizing allocative efficiency‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kickmuncher/economic_resilience_risk_model.","url":"https://huggingface.co/datasets/Kickmuncher/economic_resilience_risk_model","creator_name":"Kick Muncher","creator_url":"https://huggingface.co/Kickmuncher","license_name":"Public Domain Dedication & License","license_url":"https://scancode-licensedb.aboutcode.org/pddl-1.0.html","language":"en","first_N":5,"first_N_keywords":["tabular-classification","reinforcement-learning","English","pddl","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"sotopia-rl-reward-annotation","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tSotopia-RL: Reward Design for Social Intelligence Dataset\n\t\n\nThis repository contains the dataset and related resources for the paper Sotopia-RL: Reward Design for Social Intelligence.\nSotopia-RL proposes a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. This enables more effective training of socially intelligent agents through reinforcement learning, particularly addressing challenges like partial observability and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ulab-ai/sotopia-rl-reward-annotation.","url":"https://huggingface.co/datasets/ulab-ai/sotopia-rl-reward-annotation","creator_name":"ulab","creator_url":"https://huggingface.co/ulab-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-scientific-reasoning","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDpo Scientific Reasoning\n\t\n\nThis dataset contains 100 high-quality examples for Direct Preference Optimization (DPO) training, focused on scientific reasoning and analysis.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset was generated using an enhanced DSPy-based pipeline that creates structured reasoning traces for scientific questions. Each example includes:\n\nSeparated content fields: System prompt, user question, and full context as individual columns\nChosen responses: High-quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhi26/dpo-scientific-reasoning.","url":"https://huggingface.co/datasets/abhi26/dpo-scientific-reasoning","creator_name":"ABHISEK GUHA","creator_url":"https://huggingface.co/abhi26","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"local_test","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for local_test\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/elkinsqiu/local_test/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/elkinsqiu/local_test.","url":"https://huggingface.co/datasets/elkinsqiu/local_test","creator_name":"elkins.qiu","creator_url":"https://huggingface.co/elkinsqiu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"reinforcement-learning","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"dpo","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya_dutch_dpo_binarized","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for aya_dutch_dpo\n\t\n\nThis dataset has been created with distilabel.\nThis dataset was created as part of the Data is Better Together project, in particular as part of an ongoing effort to help foster the creation of DPO/ORPO datasets for more languages.\nThe dataset was constructed using the following steps:\n\nstarting with the aya_dataset and filtering for Dutch examples\nusing the Meta-Llama-3-70B-Instruct model to generate new examples for each promptUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized.","url":"https://huggingface.co/datasets/CultriX/aya_dutch_dpo_binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","Dutch","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"dpo","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"my-distiset-7856ab1e","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for my-distiset-7856ab1e\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e.","url":"https://huggingface.co/datasets/Mxytyu/my-distiset-7856ab1e","creator_name":"Max","creator_url":"https://huggingface.co/Mxytyu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"dpo","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-chatml-mix-binarized","keyword":"rlhf","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"chatgpt-prompts-distil-dataset","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for chatgpt-prompts-distil-dataset\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset.","url":"https://huggingface.co/datasets/akattamuri/chatgpt-prompts-distil-dataset","creator_name":"Ashish Kattamuri","creator_url":"https://huggingface.co/akattamuri","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"dpo","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"llama70B-dpo-dataset","keyword":"rlhf","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"bird_train","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tCSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning\n\t\n\nThis repository contains the datasets used and/or generated in the paper CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning.\nCode Repository: https://github.com/CycloneBoy/csc_sql\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cycloneboy/bird_train.","url":"https://huggingface.co/datasets/cycloneboy/bird_train","creator_name":"cycloneboy","creator_url":"https://huggingface.co/cycloneboy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","arxiv:2505.13271","arxiv:2507.22478","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"filtered-high-quality-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tFiltered High-Quality Instruction-Output Dataset\n\t\n\nThis dataset contains high-quality (score = 5) instruction-output pairs generated via a reverse instruction generation pipeline using a fine-tuned backward model and evaluated by LLaMA-2-7B-Chat.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\ninstruction: The generated prompt.\noutput: The original response.\nscore: Quality score assigned ( score = 5 retained).\n\n","url":"https://huggingface.co/datasets/helloTR/filtered-high-quality-dpo","creator_name":"Risa Tori","creator_url":"https://huggingface.co/helloTR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDPO-ORPO-mix-38k\n\t\n\nThis dataset is intended for use with DPO or ORPO training. \nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\nThe dataset integrates high-quality samples from the following DPO datasets:\n\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced.","url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-results","keyword":"human-feedback","description":"\n\t\n\t\t\n\t\tDataset Card for image-preferences-results\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results.","url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1-results","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"french_hh_rlhf","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tDataset Card for french_hh_rlhf\n\t\n\nThis dataset offers a french translation of the famous Anthropic/hh-rlhf dataset in order to improve the allignement work/research in the french NLP community.\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nntnq\n","url":"https://huggingface.co/datasets/AIffl/french_hh_rlhf","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["French","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Reflective-MAGLLAMA-v0.1","keyword":"rlaif","description":"\n\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Reflective-MAGLLAMA-v0.1\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tPlease Use v0.1.1\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nReflective MAGLLAMA is a dataset created using MAGPIE-generated prompts in combination with reflection pattern responses generated by the LLaMa 3.1 70B model.\nThis dataset is tailored specifically to encourage reflection-based analysis through reflection prompting, a technique that enhances deeper thinking and learning.\nIt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1.","url":"https://huggingface.co/datasets/thesven/Reflective-MAGLLAMA-v0.1","creator_name":"Michael Svendsen","creator_url":"https://huggingface.co/thesven","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"procgen","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tProcgen Benchmark\n\t\n\nThis dataset contains expert trajectories generated by a PPO reinforcement learning agent trained on each of the 16 procedurally-generated gym environments from the Procgen Benchmark. The environments were created on distribution_mode=easy and with unlimited levels.\nDisclaimer: This is not an official repository from OpenAI.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Usage\n\t\n\nRegular usage (for environment bigfish):\nfrom datasets import load_dataset\ntrain_dataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EpicPinkPenguin/procgen.","url":"https://huggingface.co/datasets/EpicPinkPenguin/procgen","creator_name":"Marcus Fechner","creator_url":"https://huggingface.co/EpicPinkPenguin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"Imagen-4-ultra-24-7-25_t2i_human_preference","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Ultra 24.7.25 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~83'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 Ultra (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"marathi-hh-rlhf-v02","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset card for Marathi HH-RLHF\n\t\n\nTranslated subset of HH-RLHF version for Marathi language.\n","url":"https://huggingface.co/datasets/amitagh/marathi-hh-rlhf-v02","creator_name":"Amit","creator_url":"https://huggingface.co/amitagh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","Marathi","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"orz_math_57k_collection","keyword":"reinforcement-learning","description":"\n\n\n\t\n\t\t\n\t\tOpen Reasoner Zero\n\t\n\n\n\n\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n\n\n\n\n    \n  \n  \n  \n  \n  Paper Arxiv Link üëÅÔ∏è\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tOverview üåä\n\t\n\nWe introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection.","url":"https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection","creator_name":"Open-Reasoner-Zero","creator_url":"https://huggingface.co/Open-Reasoner-Zero","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"phi3-arena-short-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDPO (Direct Policy Optimization) dataset of normal and short answers generated from lmsys/chatbot_arena_conversations dataset using microsoft/Phi-3-mini-4k-instruct model.\nGenerated using ShortGPT project.\n","url":"https://huggingface.co/datasets/ZSvedic/phi3-arena-short-dpo","creator_name":"Zeljko Svedic","creator_url":"https://huggingface.co/ZSvedic","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"vpt-owamcap","keyword":"reinforcement-learning","description":"This dataset is an OWAMcap conversion from the Video PreTraining (VPT) minecraft dataset. \nIt is compressed with the WebDataset Format, which is essentially a series of compressed tar files. \nEach sample contains:\n\n.mp4 (containing video, from the original VPT dataset)\n.jsonl (containing actions, from the original VPT dataset)\n.mcap (OWAMcap format containing actions, which is converted from jsonl)\n\nThere are 26322 numbers of samples, a total volume of 5.2TB.\nFor reading OWAMcap data, please‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-world-agents/vpt-owamcap.","url":"https://huggingface.co/datasets/open-world-agents/vpt-owamcap","creator_name":"open-world-agents","creator_url":"https://huggingface.co/open-world-agents","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","apache-2.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Multifaceted-Collection-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tDataset Card for Multifaceted Collection DPO\n\t\n\n\n\t\n\t\t\n\t\tLinks for Reference\n\t\n\n\nHomepage: https://lklab.kaist.ac.kr/Janus/ \nRepository: https://github.com/kaistAI/Janus \nPaper: https://arxiv.org/abs/2405.17977 \nPoint of Contact: suehyunpark@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tTL;DR\n\t\n\n\nMultifaceted Collection is a preference dataset for aligning LLMs to diverse human preferences, where system messages are used to represent individual preferences. The instructions are acquired from five existing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO.","url":"https://huggingface.co/datasets/kaist-ai/Multifaceted-Collection-DPO","creator_name":"KAIST AI","creator_url":"https://huggingface.co/kaist-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"saas-sales-conversations","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tsaas-sales-conversations\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a synthetic dataset of sales conversations for SaaS (Software as a Service) companies, designed for training sales conversion prediction models. The dataset was created following the methodology presented in \"SalesRLAgent: A Reinforcement Learning Approach for Real-Time Sales Conversion Prediction and Optimization\" (Nandakishor M, 2025).\nThe dataset contains realistic dialogues between sales representatives and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations.","url":"https://huggingface.co/datasets/DeepMostInnovations/saas-sales-conversations","creator_name":"DeepMost","creator_url":"https://huggingface.co/DeepMostInnovations","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"math_Light-R1-DPOData_preprocess","keyword":"dpo","description":"\n\t\n\t\t\n\t\tdaichira/Light-R1-DPOData_preprocess\n\t\n\nThis dataset is a preprocessed version of qihoo360/Light-R1-DPOData, adapted for use with the verl training pipeline. It is designed for DPO (Direct Preference Optimization) training, containing pairs of chosen and rejected responses for mathematical reasoning problems.\n\n\t\n\t\t\n\t\tOriginal Dataset Overview (from qihoo360/Light-R1-DPOData)\n\t\n\nThe original Light-R1-DPOData is part of the \"Light-R1: Surpassing R1-Distill from Scratch with $1000 through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/math_Light-R1-DPOData_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/math_Light-R1-DPOData_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"bird-rl","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tBIRD-RL\n\t\n\n\n\nThis dataset is a processed dataset of BIRD-SQL for Post-Training. \n","url":"https://huggingface.co/datasets/Rihong/bird-rl","creator_name":"Rihong","creator_url":"https://huggingface.co/Rihong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gastronomia-hispana-dpo","keyword":"dpo","description":"\n\t\n\t\t\n\t\tGastronom√≠a Hispana DPO\n\t\n\n\n\t\n\t\t\n\t\tDescripci√≥n del Dataset\n\t\n\nEste dataset contiene pares de preferencias para el entrenamiento de modelos de lenguaje especializados en gastronom√≠a hispana utilizando la t√©cnica DPO (Direct Preference Optimization). Los datos incluyen conversaciones sobre cocina internacional con un enfoque particular en recetas, ingredientes, t√©cnicas culinarias y tradiciones gastron√≥micas del mundo hispano.\n\n\t\n\t\t\n\t\tEstructura del Dataset\n\t\n\nEl dataset contiene las‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2025/gastronomia-hispana-dpo.","url":"https://huggingface.co/datasets/somosnlp-hackathon-2025/gastronomia-hispana-dpo","creator_name":"Hackathon SomosNLP 2025","creator_url":"https://huggingface.co/somosnlp-hackathon-2025","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Spanish","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"en-fr-debut-kit","keyword":"dpo","description":"\n\t\n\t\t\n\t\tD√©but Kit\n\t\n\n\n  English\nA dataset for training English-French bilingual chatbots\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nD√©but Kit is a comprehensive dataset designed to facilitate the development of English-French bilingual chatbots. It covers three crucial stages of model development:\n\nPretraining\nSupervised Fine-Tuning (SFT)\nDirect Preference Optimization (DPO)\n\nEach stage features a balanced mix of English and French content, ensuring robust bilingual capabilities.\n\n\t\n\t\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/en-fr-debut-kit.","url":"https://huggingface.co/datasets/agentlans/en-fr-debut-kit","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"reinforcement-learning","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"dpo","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"rlhf","description":"\n\n\t\n\t\t\n\t\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nAlignment Lab AI is pleased to introduce our latest research efforts with:\n\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2.","url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"multilevel-legal-reasoning","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tLegal Reasoning Dataset with Multilevel Human and Model-Annotated Explanations\n\t\n\n\nPrepared by Mst Rafia Islam, Umong Sain, Azmine Toushik Wasi\nPrepared as a part of Reasoning Datasets Competition by Bespoke Labs, Hugging Face, and Together.ai.\n\n\n\n\t\n\t\t\n\t\tüß≠ Purpose and Scope\n\t\n\nThe Legal Reasoning Dataset aims to support the evaluation and training of legal reasoning systems, particularly in multilingual or jurisdiction-agnostic contexts. It focuses on international acts and treaties‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning.","url":"https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning","creator_name":"Computational Intelligence and Operations Laboratory (CIOL)","creator_url":"https://huggingface.co/ciol-research","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"dpo","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ethics_qna_preferences","keyword":"rlhf","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\n  subset virtue\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\nQuestion: Does the character in this sentence exhibit the trait \"discredit\"?\\nAnswer: ',\n   'chosen': 'no',\n   'rejected': 'yes'}\n  \n  commonsense\n  {'prompt': 'Post:\\n\"\"\"I went to the principal\\'s office to change my records before going to a different school.\"\"\"\"\\n\\n\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-TidyHouse","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB TidyHouse Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB TidyHouse.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB TidyHouse. Each subtask/object combination (e.g pick 002_master_chef_can) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered for safe robot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-TidyHouse.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-TidyHouse","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"MS-HAB-TidyHouse","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tManiSkill-HAB TidyHouse Dataset\n\t\n\nPaper \n| Website \n| Code \n| Models \n| (Full) Dataset \n| Supplementary\n\n\nWhole-body, low-level control/manipulation demonstration dataset for ManiSkill-HAB TidyHouse.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nDemonstration dataset for ManiSkill-HAB TidyHouse. Each subtask/object combination (e.g pick 002_master_chef_can) has 1000 successful episodes (200 samples/demonstration) gathered using RL policies fitered for safe robot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arth-shukla/MS-HAB-TidyHouse.","url":"https://huggingface.co/datasets/arth-shukla/MS-HAB-TidyHouse","creator_name":"Arth Shukla","creator_url":"https://huggingface.co/arth-shukla","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["robotics","reinforcement-learning","grasping","task-planning","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ru_ifeval-like-data","keyword":"rlaif","description":"\n\t\n\t\t\n\t\tRussian IFEval Like Data\n\t\n\nThis dataset contains instruction-response pairs synthetically generated using Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8 and chatgpt-4o-latest following the style of google/IFEval dataset and verified for correctness with #TODO lm-evaluation-harness.\nThank you for the inspiration ifeval_like_dataset.\n","url":"https://huggingface.co/datasets/mizinovmv/ru_ifeval-like-data","creator_name":"maksim","creator_url":"https://huggingface.co/mizinovmv","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Russian","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Aya-Command.R-DPO","keyword":"dpo","description":"\n\t\n\t\t\n\t\tü§ó Dataset Card for \"Aya-Command.R-DPO\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources & Infos\n\t\n\n\nData Origin: Derived from the Arabic Aya (2A) dataset : 2A2I/Arabic_Aya which is a Curated Subset of the Aya Collection CohereForAI/aya_dataset\nLanguages: Modern Standard Arabic (MSA)\nLicense: Apache-2.0\nMaintainers: Ali Elfilali and Mohammed Machrouh\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nAya-Command.R-DPO is a DPO dataset designed to advance Arabic NLP by comparing human-generated responses, labeled as \"chosen,\" with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO.","url":"https://huggingface.co/datasets/2A2I/Aya-Command.R-DPO","creator_name":"2A2I","creator_url":"https://huggingface.co/2A2I","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Reason-RFT-CoT-Dataset","keyword":"reinforcement-learning","description":"\n\n\n\n\n\t\n\t\t\n\t\tü§ó Reason-RFT CoT Dateset\n\t\n\nThe full dataset used in our project \"Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning\".\n\n    ¬†¬†‚≠êÔ∏è Project¬†¬† ‚îÇ ¬†¬†üåé Github¬†¬† ‚îÇ ¬†¬†üî• Models¬†¬† ‚îÇ ¬†¬†üìë ArXiv¬†¬† ‚îÇ ¬†¬†üí¨ WeChat\n\n\n\n¬†¬†ü§ñ RoboBrain: Aim to Explore ReasonRFT Paradigm to Enhance RoboBrain's Embodied Reasoning Capabilities.\n\n\n\n\t\t\n\t\t‚ô£Ô∏è Quick Start\n\t\n\nPlease refer to Dataset Preparation\n\n\t\n\t\t\n\t\tüî• Overview\n\t\n\nVisual reasoning abilities play a crucial role in understanding complex multimodal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset.","url":"https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset","creator_name":"tanhuajie2001","creator_url":"https://huggingface.co/tanhuajie2001","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","visual-question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dpo-merged","keyword":"dpo","description":"CultriX/dpo-merged dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-merged","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"mypo-4k-rfc-val-phi3test","keyword":"dpo","description":"data: 100 rows of https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc validation split\nbase: https://huggingface.co/edumunozsala/phi3-mini-4k-qlora-python-code-20k\ndpo: https://huggingface.co/joshuasundance/phi3-mini-4k-qlora-python-code-20k-mypo-4k-rfc-pipe\nmade to compare base vs dpo\nbut apparently I clipped the beginning of some of the dpo outputs with sloppy coding\nwill update later\nbase: 16:55, 10.16s/it\nDPO: 15:36,  9.36s/it\n","url":"https://huggingface.co/datasets/joshuasundance/mypo-4k-rfc-val-phi3test","creator_name":"Joshua Sundance Bailey","creator_url":"https://huggingface.co/joshuasundance","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel_test","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for distilabel_test\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/songchunayuan55/distilabel_test/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songchunayuan55/distilabel_test.","url":"https://huggingface.co/datasets/songchunayuan55/distilabel_test","creator_name":"ChuanyuanSong","creator_url":"https://huggingface.co/songchunayuan55","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Hopper-v4","keyword":"expert trajectory","description":"\n\t\n\t\t\n\t\tHopper-v4 - Imitation Learning Datasets\n\t\n\nThis is a dataset created by Imitation Learning Datasets project. \nIt was created by using Stable Baselines weights from a TD3 policy from HuggingFace.\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset consists of 1,000 episodes with an average episodic reward of 3531.1930.\nEach entry consists of:\nobs (list): observation with length 2.\naction (int): action (0 or 1).\nreward (float): reward point for that timestep.\nepisode_starts (bool): if that state was the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NathanGavenski/Hopper-v4.","url":"https://huggingface.co/datasets/NathanGavenski/Hopper-v4","creator_name":"Nathan Schneider Gavenski","creator_url":"https://huggingface.co/NathanGavenski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1M - 10M","json","Tabular","Datasets"],"keywords_longer_than_N":true},
	{"name":"deepscale_r_math_problems","keyword":"rl","description":"Mix of agentica-org/DeepScaleR-Preview-Dataset and inclusionAI/Ring-lite-rl-data only MATH data, deduplicated\nResults in total of 80k samples\n","url":"https://huggingface.co/datasets/thepowerfuldeez/deepscale_r_math_problems","creator_name":"George Grigorev","creator_url":"https://huggingface.co/thepowerfuldeez","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"python-lib-tools-v0.1","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for python-lib-tools-v0.1\n\t\n\nThis dataset has been created with distilabel.\nThe pipeline script was uploaded to easily reproduce the dataset:\npython_tool_synth.py.\nIt can be run directly using the CLI:\ndistilabel pipeline run --script \"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1/raw/main/python_tool_synth.py\"\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDistilabel pipeline\n\t\n\nRequirements:\n# A new virtual environment with python‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1.","url":"https://huggingface.co/datasets/argilla-warehouse/python-lib-tools-v0.1","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"multipref","keyword":"dpo","description":"\n\t\n\t\t\n\t\tMultiPref - a multi-annotated and multi-aspect human preference dataset\n\t\n\n\nPaper link: Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MultiPref dataset (version 1.0) is a rich collection of 10k human preferences. It is:\n\nMulti-annotated: each instance is annotated multiple times‚Äîtwice by normal crowdworkers and twice by domain-experts‚Äî resulting in around 40k annotations.\nMulti-aspect: aside from their Overall preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/multipref.","url":"https://huggingface.co/datasets/allenai/multipref","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"dpo","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"multipref","keyword":"rlhf","description":"\n\t\n\t\t\n\t\tMultiPref - a multi-annotated and multi-aspect human preference dataset\n\t\n\n\nPaper link: Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MultiPref dataset (version 1.0) is a rich collection of 10k human preferences. It is:\n\nMulti-annotated: each instance is annotated multiple times‚Äîtwice by normal crowdworkers and twice by domain-experts‚Äî resulting in around 40k annotations.\nMulti-aspect: aside from their Overall preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/multipref.","url":"https://huggingface.co/datasets/allenai/multipref","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["English","odc-by","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ethics_expression_preferences","keyword":"rlhf","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\nExamples:\nsubset commonsense\n  before 13910 {'label': 1, 'input': \"I went to the principal's office to change my records before going to a different school.\"}\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \"moving my ball when the other player wasn't looking.\"}\n\nsubset utilitarianism\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences.","url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jondurbin_gutenberg-dpo-v0.1","keyword":"dpo","description":"\n\t\n\t\t\n\t\tGutenberg DPO\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a dataset meant to enhance novel writing capabilities of LLMs, by using public domain books from Project Gutenberg\n\n\t\n\t\t\n\t\tProcess\n\t\n\nFirst, the each book is parsed, split into chapters, cleaned up from the original format (remove superfluous newlines, illustration tags, etc.).\nOnce we have chapters, an LLM is prompted with each chapter to create a synthetic prompt that would result in that chapter being written.\nEach chapter has a summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Triangle104/jondurbin_gutenberg-dpo-v0.1.","url":"https://huggingface.co/datasets/Triangle104/jondurbin_gutenberg-dpo-v0.1","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"oasst2_es_instruct_hf","keyword":"human-feedback","description":"This is the Spanish subset from the OpenAssistant/oasst2 dataset.\nThe dataset has been extracted from the 2023-11-05_oasst2_ready.trees.jsonl.gz file to parse all the conversation trees and put it in a huggingface-friendly format so you can use apply_chat_template as explained on the Chat Templating documentation.\n\n\t\n\t\t\n\t\n\t\n\t\tExample\n\t\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nchat = [\n\n  {\"role\": \"user\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf.","url":"https://huggingface.co/datasets/bertin-project/oasst2_es_instruct_hf","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Spanish","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"assettoCorsaGym","keyword":"rl","description":"\n\t\n\t\t\n\t\tDataset Card for Assetto Corsa Gym\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe AssettoCorsaGym dataset comprises 64 million steps, including 2.3 million steps from human drivers and the remaining from Soft Actor-Critic (SAC) policies. Data collection involved 15 drivers completing at least five laps per track and car. Participants included a professional e-sports driver, four experts, five casual drivers, and five beginners.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nAutonomous driving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dasgringuen/assettoCorsaGym.","url":"https://huggingface.co/datasets/dasgringuen/assettoCorsaGym","creator_name":"Adrian R","creator_url":"https://huggingface.co/dasgringuen","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["other","machine-generated","expert-generated","original","English"],"keywords_longer_than_N":true},
	{"name":"RLBench-PutRubbishInBin-euler-relative","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tput_rubbish_in_bin_euler_relative\n\t\n\nGenerated RLBench -> LeRobot v2.1 dataset.\n meta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"franka\",\n    \"total_episodes\": 500,\n    \"total_frames\": 77242,\n    \"total_tasks\":1,\n    \"total_videos\": 2500,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 10,\n    \"splits\": {\n        \"train\": \"0:500\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RonPlusSign/RLBench-PutRubbishInBin-euler-relative.","url":"https://huggingface.co/datasets/RonPlusSign/RLBench-PutRubbishInBin-euler-relative","creator_name":"Andrea Delli","creator_url":"https://huggingface.co/RonPlusSign","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"RLBench-PutRubbishInBin-euler-relative","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tput_rubbish_in_bin_euler_relative\n\t\n\nGenerated RLBench -> LeRobot v2.1 dataset.\n meta/info.json:\n{\n    \"codebase_version\": \"v2.1\",\n    \"robot_type\": \"franka\",\n    \"total_episodes\": 500,\n    \"total_frames\": 77242,\n    \"total_tasks\":1,\n    \"total_videos\": 2500,\n    \"total_chunks\": 1,\n    \"chunks_size\": 1000,\n    \"fps\": 10,\n    \"splits\": {\n        \"train\": \"0:500\"\n    },\n    \"data_path\": \"data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet\",\n    \"video_path\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RonPlusSign/RLBench-PutRubbishInBin-euler-relative.","url":"https://huggingface.co/datasets/RonPlusSign/RLBench-PutRubbishInBin-euler-relative","creator_name":"Andrea Delli","creator_url":"https://huggingface.co/RonPlusSign","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","reinforcement-learning","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TinyStories-MRL","keyword":"reinforcement-learning","description":"\n\t\n\t\t\n\t\tDataset Card for ReactiveAI/TinyStories-MRL\n\t\n\nSynthetic Memory Reinforcement Learning dataset for Proof-of-Concept Reactive Transformer models.\nDataset is divided into subsets, used in different Curriculum Stage of MRL training - each subset have\ndifferent number of follow-up interactions, could use different strategy, and have train and validation\nsplits.\n\nAfter first experiments with MRL, we decided to abandon single step and two steps stages. That's because with single\nstep‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ReactiveAI/TinyStories-MRL.","url":"https://huggingface.co/datasets/ReactiveAI/TinyStories-MRL","creator_name":"Reactive AI","creator_url":"https://huggingface.co/ReactiveAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","text-retrieval","English"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"reinforcement-learning","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"Spider_gpt4o","keyword":"rlaif","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset Card for Spider_gpt4o\n\t\n\nThis dataset has been created with distilabel.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a pipeline.yaml which can be used to reproduce the pipeline that generated it in distilabel using the distilabel CLI:\ndistilabel pipeline run --config \"https://huggingface.co/datasets/mjerome89/Spider_gpt4o/raw/main/pipeline.yaml\"\n\nor explore the configuration:\ndistilabel pipeline info --config‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mjerome89/Spider_gpt4o.","url":"https://huggingface.co/datasets/mjerome89/Spider_gpt4o","creator_name":"Maurice","creator_url":"https://huggingface.co/mjerome89","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true}
]
;
