const data_for_modality_unlearning = 
[
	{"name":"my_dataset_repo","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talmahmud/my_dataset_repo","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","description":"talmahmud/my_dataset_repo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"DUSK","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AI-ISL/DUSK","creator_name":"AI-ISL","creator_url":"https://huggingface.co/AI-ISL","description":"\n\t\n\t\t\n\t\tüåá DUSK: Do Not Unlearn Shared Knowledge\n\t\n\nDUSK is a benchmark dataset designed for evaluating machine unlearning in multi-source settings, where specific data sources must be forgotten while preserving others.\nIn realistic applications, documents often share factual overlap with publicly available content (e.g., Wikipedia, textbooks). DUSK challenges unlearning algorithms to precisely erase only what must be forgotten, while preserving knowledge that remains supported by other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-ISL/DUSK.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","other","machine-generated","original"],"keywords_longer_than_N":true},
	{"name":"custom_tofu","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talmahmud/custom_tofu","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","description":"talmahmud/custom_tofu dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"BLUR","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/forgelab/BLUR","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","description":"\n\t\n\t\t\n\t\tBLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap \n\t\n\nThe BLUR dataset expands on existing unlearning benchmarks by providing harder evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on BLUR, with simple approaches performing better on average than more recent methods.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/BLUR.","first_N":5,"first_N_keywords":["question-answering","text-generation","closed-domain-qa","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"BLUR","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/forgelab/BLUR","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","description":"\n\t\n\t\t\n\t\tBLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap \n\t\n\nThe BLUR dataset expands on existing unlearning benchmarks by providing harder evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on BLUR, with simple approaches performing better on average than more recent methods.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/BLUR.","first_N":5,"first_N_keywords":["question-answering","text-generation","closed-domain-qa","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"tofu_ext1","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talmahmud/tofu_ext1","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","description":"talmahmud/tofu_ext1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"tofu_ext2_rp","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talmahmud/tofu_ext2_rp","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","description":"talmahmud/tofu_ext2_rp dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUEval","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Dornavineeth/TOFUEval","creator_name":"Vineeth Dorna","creator_url":"https://huggingface.co/Dornavineeth","description":"Dornavineeth/TOFUEval dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"instance-level-tofu-unlearning","keyword":"unlearning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning","creator_name":"Fiona Chow","creator_url":"https://huggingface.co/chowfi","description":"\n\t\n\t\t\n\t\tInstance-Level TOFU Benchmark\n\t\n\nThis dataset provides an instance-level adaptation of the TOFU (Maini et al, 2024) dataset for evaluating in-context unlearning in large language models (LLMs). Unlike the original TOFU benchmark, which focuses on entity-level unlearning, this version targets selective memory erasure at the instance level ‚Äî i.e., forgetting specific facts about an entity.\nIt is compatible for evaluation with the locuslab/tofu_ft_llama2-7b model, which was fine-tuned on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"instance-level-tofu-unlearning","keyword":"tofu","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning","creator_name":"Fiona Chow","creator_url":"https://huggingface.co/chowfi","description":"\n\t\n\t\t\n\t\tInstance-Level TOFU Benchmark\n\t\n\nThis dataset provides an instance-level adaptation of the TOFU (Maini et al, 2024) dataset for evaluating in-context unlearning in large language models (LLMs). Unlike the original TOFU benchmark, which focuses on entity-level unlearning, this version targets selective memory erasure at the instance level ‚Äî i.e., forgetting specific facts about an entity.\nIt is compatible for evaluation with the locuslab/tofu_ft_llama2-7b model, which was fine-tuned on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"TOFU","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"RWKU","keyword":"unlearning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jinzhuoran/RWKU","creator_name":"Zhuoran Jin","creator_url":"https://huggingface.co/jinzhuoran","description":"\n\t\n\t\t\n\t\tDataset Card for Real-World Knowledge Unlearning Benchmark (RWKU)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nRWKU is a real-world knowledge unlearning benchmark specifically designed for large language models (LLMs).\nThis benchmark contains 200 real-world unlearning targets and 13,131 multi-level forget probes, including 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial-attack probes.\nRWKU is designed based on the following three key factors: \n\nFor the task setting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinzhuoran/RWKU.","first_N":5,"first_N_keywords":["text-generation","fill-mask","question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"unlearning","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"unlearning","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"code_leak_qa","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"code_leak_qa","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"KnowUnDo","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/KnowUnDo","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"\n\t\n\t\t\n\t\tKnowUnDo\n\t\n\n\n\t\n\t\t\n\t\tüíª Datasets Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"zjunlp/KnowUnDo\", name='copyright', split='unlearn')\n\n\nAvailable configuration names and corresponding splits:\ncopyright: unlearn, retention;\nprivacy: unlearn, retention;\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüéâ Acknowledgement\n\t\n\nWe would like to express our sincere gratitude for the excellent work TOFU, Unlearn Dataset and LLM Unlearning.\n\n\t\t\n\t\tüìñ Citation\n\t\n\nIf finding this work useful for your research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/KnowUnDo.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"copyright_unlearning","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/boyiwei/copyright_unlearning","creator_name":"Boyi Wei","creator_url":"https://huggingface.co/boyiwei","description":"boyiwei/copyright_unlearning dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cf","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cf","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cr","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cr","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCr1","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCr1","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCrP","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCrP","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Shuffle","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Shuffle","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-single","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-single","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cbin","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cbin","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Direct","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Direct","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All.","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"WaterDrum-TOFU","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\n\t\n\t\t\n\t\tWaterDrum: Watermarking for Data-centric Unlearning Metric\n\t\n\nWaterDrum provides an unlearning benchmark for the evaluation of the effectiveness and practicality of unlearning. This repository contains the TOFU corpus of WaterDrum (WaterDrum-TOFU), which contains both unwatermarked and watermarked question-answering datasets based on the original TOFU dataset.\nThe data samples were watermarked with Waterfall.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThe WaterDrum-TOFU dataset contains 6 subsets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"WaterDrum-Ax","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\n\t\n\t\t\n\t\tWaterDrum: Watermarking for Data-centric Unlearning Metric\n\t\n\nWaterDrum provides an unlearning benchmark for the evaluation of the effectiveness and practicality of unlearning. The repository contains the ArXiv corpus of WaterDrum (WaterDrum-Ax), which contains both unwatermarked and watermarked ArXiv paper abstracts across\n20 categories published after the release of the Llama-2 model. Each category contains 400 data samples, aggregating into 8000 samples in the full training set. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Eason_TOFU","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Eason_TOFU","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"UnLOK-VQA","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","description":"\n\t\n\t\t\n\t\tüìä Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\n\t\n\nPaper: Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation\nCode: https://github.com/Vaidehi99/mmmedit\nLink: Dataset Link\nThis dataset contains approximately 500 entries with the following key attributes:\n\n\"id\": Unique Identifier for each entry\n\"src\": The question whose answer is to be deleted ‚ùì\n\"pred\": The answer to the question meant for deletion ‚ùå\n\"loc\": Related neighborhood questions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"SKILL","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WarmIce77/SKILL","creator_name":"Kunwoo Kim","creator_url":"https://huggingface.co/WarmIce77","description":"WarmIce77/SKILL dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","üá∫üá∏ Region: US","privacy"],"keywords_longer_than_N":true},
	{"name":"ELUDe","keyword":"unlearning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/6rightjade/ELUDe","creator_name":"Minseok Choi","creator_url":"https://huggingface.co/6rightjade","description":"\n\t\n\t\t\n\t\tELUDe: Entity-Level Unlearning Dataset\n\t\n\nELUDe (Entity-Level Unlearning Dataset) is a comprehensive machine unlearning dataset focused on the removal of an entire entity from large language models (LLMs).\nThe dataset includes 20 real-world target entities and 144 unique neighboring entities from Wikipedia. All samples were synthesized by GPT-4o, given the Wikipedia documents of the entities.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nPaper: Opt-Out: Investigating Entity-Level Unlearning for Large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6rightjade/ELUDe.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-sa-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TOFU-da","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/miry-itu/TOFU-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","description":"miry-itu/TOFU-da dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-da","keyword":"tofu","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/miry-itu/TOFU-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","description":"miry-itu/TOFU-da dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"ripple-bench","keyword":"unlearning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/royrin/ripple-bench","creator_name":"privacyboy","creator_url":"https://huggingface.co/royrin","description":"\n\t\n\t\t\n\t\tRipple Bench: Measuring Knowledge Ripple Effects in Language Model Unlearning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nRipple Bench is a benchmark for measuring how knowledge changes propagate through related concepts when unlearning specific information from language models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWhen we unlearn specific knowledge from a language model (e.g., information about biological weapons), how does this affect the model's knowledge of related topics? Ripple Bench quantifies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/royrin/ripple-bench.","first_N":5,"first_N_keywords":["question-answering","text-classification","English","mit","100K - 1M"],"keywords_longer_than_N":true}
]
;
