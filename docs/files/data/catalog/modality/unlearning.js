const data_for_modality_unlearning = 
[
	{"name":"TOFU-C","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C.","url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C.","url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cf","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf.","url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cf","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf.","url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCrP","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP.","url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCrP","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP.","url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"copyright_unlearning","keyword":"unlearning","description":"boyiwei/copyright_unlearning dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/boyiwei/copyright_unlearning","creator_name":"Boyi Wei","creator_url":"https://huggingface.co/boyiwei","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All.","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All.","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUEval","keyword":"unlearning","description":"Dornavineeth/TOFUEval dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Dornavineeth/TOFUEval","creator_name":"Vineeth Dorna","creator_url":"https://huggingface.co/Dornavineeth","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Shuffle","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Shuffle","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"code_leak_qa","keyword":"unlearning","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"code_leak_qa","keyword":"tofu","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"custom_tofu","keyword":"unlearning","description":"talmahmud/custom_tofu dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/talmahmud/custom_tofu","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"instance-level-tofu-unlearning","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tInstance-Level TOFU Benchmark\n\t\n\nThis dataset provides an instance-level adaptation of the TOFU (Maini et al, 2024) dataset for evaluating in-context unlearning in large language models (LLMs). Unlike the original TOFU benchmark, which focuses on entity-level unlearning, this version targets selective memory erasure at the instance level ‚Äî i.e., forgetting specific facts about an entity.\nIt is compatible for evaluation with the locuslab/tofu_ft_llama2-7b model, which was fine-tuned on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning.","url":"https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning","creator_name":"Fiona Chow","creator_url":"https://huggingface.co/chowfi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"instance-level-tofu-unlearning","keyword":"tofu","description":"\n\t\n\t\t\n\t\tInstance-Level TOFU Benchmark\n\t\n\nThis dataset provides an instance-level adaptation of the TOFU (Maini et al, 2024) dataset for evaluating in-context unlearning in large language models (LLMs). Unlike the original TOFU benchmark, which focuses on entity-level unlearning, this version targets selective memory erasure at the instance level ‚Äî i.e., forgetting specific facts about an entity.\nIt is compatible for evaluation with the locuslab/tofu_ft_llama2-7b model, which was fine-tuned on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning.","url":"https://huggingface.co/datasets/chowfi/instance-level-tofu-unlearning","creator_name":"Fiona Chow","creator_url":"https://huggingface.co/chowfi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All.","url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"unlearning","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning.","url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-All","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All.","url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"unlearning","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning.","url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cbin","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin.","url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cbin","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin.","url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-da","keyword":"tofu","description":"miry-itu/TOFU-da dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-da","keyword":"unlearning","description":"miry-itu/TOFU-da dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-en-re","keyword":"unlearning","description":"miry-itu/TOFU-en-re dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-en-re","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-en-re","keyword":"tofu","description":"miry-itu/TOFU-en-re dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-en-re","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-single","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single.","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-single","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single.","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"WaterDrum-TOFU","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tWaterDrum: Watermarking for Data-centric Unlearning Metric\n\t\n\nWaterDrum provides an unlearning benchmark for the evaluation of the effectiveness and practicality of unlearning. This repository contains the TOFU corpus of WaterDrum (WaterDrum-TOFU), which contains both unwatermarked and watermarked question-answering datasets based on the original TOFU dataset.\nThe data samples were watermarked with Waterfall.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThe WaterDrum-TOFU dataset contains 6 subsets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU.","url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TOFU-og-da","keyword":"unlearning","description":"miry-itu/TOFU-og-da dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-og-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-og-da","keyword":"tofu","description":"miry-itu/TOFU-og-da dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/miry-itu/TOFU-og-da","creator_name":"Michal Rynowiecki","creator_url":"https://huggingface.co/miry-itu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cr","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr.","url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-Cr","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr.","url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"my_dataset_repo","keyword":"unlearning","description":"talmahmud/my_dataset_repo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/talmahmud/my_dataset_repo","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"BLUR","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tBLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap \n\t\n\nThe BLUR dataset expands on existing unlearning benchmarks by providing harder evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on BLUR, with simple approaches performing better on average than more recent methods.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/BLUR.","url":"https://huggingface.co/datasets/forgelab/BLUR","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","closed-domain-qa","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"BLUR","keyword":"tofu","description":"\n\t\n\t\t\n\t\tBLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap \n\t\n\nThe BLUR dataset expands on existing unlearning benchmarks by providing harder evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on BLUR, with simple approaches performing better on average than more recent methods.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/forgelab/BLUR.","url":"https://huggingface.co/datasets/forgelab/BLUR","creator_name":"Forge Lab","creator_url":"https://huggingface.co/forgelab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","closed-domain-qa","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"WaterDrum-Ax","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tWaterDrum: Watermarking for Data-centric Unlearning Metric\n\t\n\nWaterDrum provides an unlearning benchmark for the evaluation of the effectiveness and practicality of unlearning. The repository contains the ArXiv corpus of WaterDrum (WaterDrum-Ax), which contains both unwatermarked and watermarked ArXiv paper abstracts across\n20 categories published after the release of the Llama-2 model. Each category contains 400 data samples, aggregating into 8000 samples in the full training set. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.","url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"TOFUCr1","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1.","url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFUCr1","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1.","url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"tofu_ext1","keyword":"unlearning","description":"talmahmud/tofu_ext1 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/talmahmud/tofu_ext1","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"RWKU","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tDataset Card for Real-World Knowledge Unlearning Benchmark (RWKU)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nRWKU is a real-world knowledge unlearning benchmark specifically designed for large language models (LLMs).\nThis benchmark contains 200 real-world unlearning targets and 13,131 multi-level forget probes, including 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial-attack probes.\nRWKU is designed based on the following three key factors: \n\nFor the task setting‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinzhuoran/RWKU.","url":"https://huggingface.co/datasets/jinzhuoran/RWKU","creator_name":"Zhuoran Jin","creator_url":"https://huggingface.co/jinzhuoran","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","fill-mask","question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tofu_resplit","keyword":"unlearning","description":"talmahmud/tofu_resplit dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/talmahmud/tofu_resplit","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"KnowUnDo","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tKnowUnDo\n\t\n\n\n\t\n\t\t\n\t\tüíª Datasets Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"zjunlp/KnowUnDo\", name='copyright', split='unlearn')\n\n\nAvailable configuration names and corresponding splits:\ncopyright: unlearn, retention;\nprivacy: unlearn, retention;\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüéâ Acknowledgement\n\t\n\nWe would like to express our sincere gratitude for the excellent work TOFU, Unlearn Dataset and LLM Unlearning.\n\n\t\t\n\t\tüìñ Citation\n\t\n\nIf finding this work useful for your research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/KnowUnDo.","url":"https://huggingface.co/datasets/zjunlp/KnowUnDo","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"UnLOK-VQA","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tüìä Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\n\t\n\nPaper: Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation\nCode: https://github.com/Vaidehi99/mmmedit\nLink: Dataset Link\nThis dataset contains approximately 500 entries with the following key attributes:\n\n\"id\": Unique Identifier for each entry\n\"src\": The question whose answer is to be deleted ‚ùì\n\"pred\": The answer to the question meant for deletion ‚ùå\n\"loc\": Related neighborhood questions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA.","url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"PKU_selected_unsafe_QA","keyword":"unlearning","description":"IMoonKeyBoy/PKU_selected_unsafe_QA dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/IMoonKeyBoy/PKU_selected_unsafe_QA","creator_name":"Heng Xu","creator_url":"https://huggingface.co/IMoonKeyBoy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Eason_TOFU","keyword":"tofu","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Eason_TOFU","keyword":"unlearning","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"SKILL","keyword":"unlearning","description":"WarmIce77/SKILL dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/WarmIce77/SKILL","creator_name":"Kunwoo Kim","creator_url":"https://huggingface.co/WarmIce77","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K<n<10K","üá∫üá∏ Region: US","privacy"],"keywords_longer_than_N":true},
	{"name":"ConceptVectors","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tConceptVectors\n\t\n\nüöÄThe first-ever parametric LLM Unlearning Benchmark!\nWe find current unlearning methods only modify model‚Äôs behavior without truly erasing encoded knowledge in parameters. For this, we present ConceptVectors Benchmark, with each vector strongly tied to a specific concept.\nThe ConceptVectors Benchmark for the paper \"Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces\".\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nPaper: Intrinsic Test of Unlearning Using Parametric Knowledge‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YihuaiHong/ConceptVectors.","url":"https://huggingface.co/datasets/YihuaiHong/ConceptVectors","creator_name":"YihuaiHong","creator_url":"https://huggingface.co/YihuaiHong","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10M<n<100M","arxiv:2406.11614"],"keywords_longer_than_N":true},
	{"name":"tofu_ext2_rp","keyword":"unlearning","description":"talmahmud/tofu_ext2_rp dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/talmahmud/tofu_ext2_rp","creator_name":"Tamim Al Mahmud","creator_url":"https://huggingface.co/talmahmud","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU.","url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU.","url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"DUSK","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tüåá DUSK: Do Not Unlearn Shared Knowledge\n\t\n\nDUSK is a benchmark dataset designed for evaluating machine unlearning in multi-source settings, where specific data sources must be forgotten while preserving others.\nIn realistic applications, documents often share factual overlap with publicly available content (e.g., Wikipedia, textbooks). DUSK challenges unlearning algorithms to precisely erase only what must be forgotten, while preserving knowledge that remains supported by other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-ISL/DUSK.","url":"https://huggingface.co/datasets/AI-ISL/DUSK","creator_name":"AI-ISL","creator_url":"https://huggingface.co/AI-ISL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","other","machine-generated","original"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Direct","keyword":"unlearning","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TOFU-C-Direct","keyword":"tofu","description":"\n\t\n\t\t\n\t\tTOFU: Task of Fictitious Unlearning üç¢\n\t\n\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\n\n\t\n\t\t\n\t\tQuick Links\n\t\n\n\nWebsite: The landing page for TOFU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct.","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","closed-domain-qa","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true}
]
;
