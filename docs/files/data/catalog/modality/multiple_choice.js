const data_for_modality_multiple_choice = 
[
	{"name":"AV_Odyssey_Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench","creator_name":"AV-Odyssey Bench","creator_url":"https://huggingface.co/AV-Odyssey","description":"Official dataset for the paper \"AV-Odyssey: Can Your Multimodal LLMs Really Understand Audio-Visual Information?\".\nüåü For more details, please refer to the project page with data examples: https://av-odyssey.github.io/.\n[üåê Webpage] [üìñ Paper] [ü§ó Huggingface AV-Odyssey Dataset] [ü§ó Huggingface Deaftest Dataset] [üèÜ Leaderboard]\n\n\n\t\n\t\n\t\n\t\tüî• News\n\t\n\n\n2024.11.24 üåü We release AV-Odyssey, the first-ever comprehensive evaluation benchmark to explore whether MLLMs really understand audio-visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2-Pause1","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nüåê Project Page: https://longbench2.github.io\nüíª Github Repo: https://github.com/THUDM/LongBench\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"SITE-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/franky-veteran/SITE-Bench","creator_name":"wenqi wang","creator_url":"https://huggingface.co/franky-veteran","description":"This dataset contains image and video QA test sets for SITE-Bench evaluation.\n","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MedFrameQA","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA","creator_name":"SuhaoYu","creator_url":"https://huggingface.co/SuhaoYu1020","description":"SuhaoYu1020/MedFrameQA dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"melange_visual_bbq","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IDfree/melange_visual_bbq","creator_name":"no_ID","creator_url":"https://huggingface.co/IDfree","description":"\n\t\n\t\t\n\t\tMelange Visual Bias Benchmark\n\t\n\nA visual multiple-choice benchmark for evaluating social bias and reasoning in vision-language models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMelange Visual Bias Benchmark is a multimodal extension of the BBQ (Bias Benchmark for Question Answering) dataset, designed to probe social bias and fairness in VLMs (Vision-Language Models). Instead of relying on textual context, this dataset grounds each multiple-choice question in one or more scene images that depict the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IDfree/melange_visual_bbq.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","visual-question-answering","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"DUSK","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AI-ISL/DUSK","creator_name":"AI-ISL","creator_url":"https://huggingface.co/AI-ISL","description":"\n\t\n\t\t\n\t\tüåá DUSK: Do Not Unlearn Shared Knowledge\n\t\n\nDUSK is a benchmark dataset designed for evaluating machine unlearning in multi-source settings, where specific data sources must be forgotten while preserving others.\nIn realistic applications, documents often share factual overlap with publicly available content (e.g., Wikipedia, textbooks). DUSK challenges unlearning algorithms to precisely erase only what must be forgotten, while preserving knowledge that remains supported by other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-ISL/DUSK.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","other","machine-generated","original"],"keywords_longer_than_N":true},
	{"name":"lme-mc10","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Percena/lme-mc10","creator_name":"Percena","creator_url":"https://huggingface.co/Percena","description":"\n\t\n\t\t\n\t\tLME‚ÄëMC10 ¬∑ LongMemEval(s)¬†Multiple‚ÄëChoice¬†10\n\t\n\nLME‚ÄëMC10 is a 500‚Äëitem multiple‚Äëchoice benchmark derived from LongMemEval(s).Each item probes one of LongMemEval‚Äôs five long‚Äëterm memory abilities, but is reformatted into a 10‚Äëoption MC task for straightforward automated evaluation (plain accuracy, balanced accuracy, etc.). \n\nInformation Extraction¬†(IE)\nMulti-Session Reasoning¬†(MR)\nKnowledge Updates¬†(KU)\nTemporal Reasoning¬†(TR)\nAbstention¬†(ABS)\n\nThe original AI‚Äëjudge rubric is removed;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Percena/lme-mc10.","first_N":5,"first_N_keywords":["question-answering","expert-generated","machine-generated","xiaowu0162/longmemeval","English"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset_support","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_support","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_support.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"Internal_Medicine_questions_binary","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tomshe/Internal_Medicine_questions_binary","creator_name":"Tom s","creator_url":"https://huggingface.co/tomshe","description":"\n\t\n\t\t\n\t\tDataset Card for Internal Medicine MCQ\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of 41 high-quality, two-choice multiple-choice questions (MCQs) focused on core biomedical knowledge and clinical scenarios from internal medicine. These questions were specifically curated for research evaluating medical knowledge, clinical reasoning, and confidence-based interactions among medical trainees and large language models (LLMs).\n\nCurated by: Tom Sheffer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomshe/Internal_Medicine_questions_binary.","first_N":5,"first_N_keywords":["table-question-answering","multiple-choice","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"aqua-rat-mcqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RikoteMaster/aqua-rat-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","description":"\n\t\n\t\t\n\t\tAQUA-RAT MCQA Dataset\n\t\n\nThis dataset contains the AQUA-RAT dataset converted to Multiple Choice Question Answering (MCQA) format with modifications.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAQUA-RAT is a dataset of algebraic word problems with rationales. This version has been processed to:\n\nRemove all questions where the correct answer was option \"E\" (5th choice)\nRemove the \"E\" option from all remaining questions (4 choices: A, B, C, D)\nMerge validation and test splits into a single test split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/aqua-rat-mcqa.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"my_test_01","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/inerrupt/my_test_01","creator_name":"shungang","creator_url":"https://huggingface.co/inerrupt","description":"inerrupt/my_test_01 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"BuddhismEval","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Nethmi14/BuddhismEval","creator_name":"Nethmi Muthugala","creator_url":"https://huggingface.co/Nethmi14","description":"\n\t\n\t\t\n\t\tDataset Card for BuddhismEval\n\t\n\nBuddhismEval is the first bilingual evaluation benchmark designed to assess large language models (LLMs) on Buddhist ethical reasoning and philosophical understanding across Sinhala and English. It includes high-quality, culturally grounded multiple-choice question (MCQ) datasets derived primarily from the Dhammapada, a core TheravƒÅda Buddhist scripture, and other canonical sources and exam materials from Sri Lanka.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nethmi14/BuddhismEval.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Sinhala","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"race_high","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DatologyAI/race_high","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","description":"\n\t\n\t\t\n\t\trace_high Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nOriginal Hugging Face Dataset: race\nSubset: high\nEvaluation Split: test\nTraining Split: train\nTask Type: multiple_choice_with_context\nProcessing Function: process_race\n\n\n\t\n\t\t\n\t\tProcessing Function\n\t\n\nThe following function was used to process the dataset from its original source:\ndef process_race(example: Dict) -> Tuple[str, List[str], int]:\n    \"\"\"Process RACE dataset example.\"\"\"\n    context = example[\"article\"]\n    query =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/race_high.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Hyperphantasia","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shahab7899/Hyperphantasia","creator_name":"Mohammad Shahab Sepehri","creator_url":"https://huggingface.co/shahab7899","description":"\n  \n\n\nA Benchmark for Evaluating the\nMental Visualization Capabilities of Multimodal LLMs\n\n\n\n  Mohammad Shahab Sepehri \n  Berk Tinaz \n  Zalan Fabian \n  Mahdi Soltanolkotabi \n\n\n\n  Github Repository \n\n\n\n  \n\n\nHyperphantasia is a synthetic Visual Question Answering (VQA) benchmark dataset that probes the mental visualization capabilities of Multimodal Large Language Models (MLLMs) from a vision perspective. We reveal that state-of-the-art models struggle with simple tasks that require visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shahab7899/Hyperphantasia.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"UNBench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yueqingliang/UNBench","creator_name":"Yueqing Liang","creator_url":"https://huggingface.co/yueqingliang","description":"\n\t\n\t\t\n\t\tUNBench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project provides tools for analyzing, simulating, and generating content related to United Nations Security Council (UNSC) draft resolutions using language models. The tasks involve coauthor selection, voting simulation, resolution adoption prediction, and statement generation. We released approximately 30 samples for each task, and the full dataset will be made publicly available upon the paper's acceptance.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nTask 1: Coauthor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yueqingliang/UNBench.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","text-generation","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"GoldBench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tianyu-zou/GoldBench","creator_name":"tianyu zou","creator_url":"https://huggingface.co/tianyu-zou","description":"tianyu-zou/GoldBench dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8B‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"TruthfulVQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/TruthfulVQA","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"\n\t\n\t\t\n\t\tTruthfulVQA\n\t\n\nThis dataset is designed to evaluate the truthfulness and honesty of vision-language models.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"PKU-Alignment/TruthfulVQA\", split=\"validation\")\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nTruthfulVQA contains the following categories of truthfulness challenges:\n\n\t\n\t\t\n\t\t1. Information Hiding\n\t\n\n\nVisual Information Distortion\nBlurring / Low-Resolution Processing\nConcealed Features and Information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/TruthfulVQA.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"legalbench_old","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DatologyAI/legalbench_old","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task includes the original data as well as a formatted input column that can be directly fed to models for evaluation.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\nThe tasks are organized into several categories:\n\n\t\n\t\t\n\t\tBasic Legal Datasets\n\t\n\n\ncanada_tax_court_outcomes\njcrew_blocker\nlearned_hands_benefits\ntelemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench_old.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"fire-exam-base64","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taean-yoo/fire-exam-base64","creator_name":"Taean Yoo","creator_url":"https://huggingface.co/taean-yoo","description":"\n\t\n\t\t\n\t\tüî• Fire Exam Dataset with Images\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ ÏÜåÎ∞©Í≥µÎ¨¥Ïõê ÏãúÌóò Î¨∏Ï†úÎ•º Í∏∞Î∞òÏúºÎ°ú Íµ¨ÏÑ±Îêú Î©ÄÌã∞Î™®Îã¨ QA Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.Í∞Å ÏÉòÌîåÏùÄ Î¨∏Ï†ú ÌÖçÏä§Ìä∏, ÏÑ†ÌÉùÏßÄ, Ï†ïÎãµ, Í∑∏Î¶¨Í≥† ÏãúÍ∞Å Ï†ïÎ≥¥Î•º Îã¥ÏùÄ Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°úÎ•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Korean","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"milu-cleaned","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/murthyrudra/milu-cleaned","creator_name":"Rudra Murthy","creator_url":"https://huggingface.co/murthyrudra","description":"\n\t\n\t\t\n\t\tMILU: A Multi-task Indic Language Understanding Benchmark\n\t\n\n\n  \n  \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nMILU (Multi-task Indic Language Understanding Benchmark) is a comprehensive evaluation dataset designed to assess the performance of Large Language Models (LLMs) across 11 Indic languages. It spans 8 domains and 41 subjects, reflecting both general and culturally specific knowledge from India.\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n11 Indian Languages: Bengali, Gujarati, Hindi, Kannada, Malayalam‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/murthyrudra/milu-cleaned.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Bengali","Gujarati","Hindi"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aymanbakiri/MNLP_M3_mcqa_dataset","creator_name":"bakiri","creator_url":"https://huggingface.co/aymanbakiri","description":"\n\t\n\t\t\n\t\tMNLP_M2_mcqa_dataset\n\t\n\nThis dataset merges five MCQ sources (sciqa, openbookqa, mmlu, m1_mcq, ai2_arc) into a single JSONL with 1389 examples.\nEach line of MNLP_M2_mcqa_dataset.jsonl has fields:\n\nid: question identifier  \nquestion: the prompt  \nchoices: list of answer choices  \nanswer: the correct choice index (0‚Äì3)  \njustification: a concise rationale  \ndataset: which source this came from\n\n","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset_2","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset_2","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset 2\n\t\n\nThe MNLP M2 MCQA Dataset 2 is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~25,000 MCQA questions\n7 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset_2.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"FrenchMedMCQA-extended","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/uy-rrodriguez/FrenchMedMCQA-extended","creator_name":"Ricardo R","creator_url":"https://huggingface.co/uy-rrodriguez","description":"\n\t\n\t\t\n\t\tFrenchMedMCQA-extended: A French Multiple-Choice Question Answering Corpus for Medical domain, that supports Comparative Analysis with Human responses\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is based on\nFrenchMedMCQA,\nthe first publicly available Multiple-Choice Question Answering (MCQA) dataset\nin French for medical domain. We have enriched the content with additional\nannotations including student response rates downloaded from MedShake.net (the\noriginal data source), manually‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uy-rrodriguez/FrenchMedMCQA-extended.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","French","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisuRiddles","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yh0075/VisuRiddles","creator_name":"haoyan","creator_url":"https://huggingface.co/yh0075","description":"\n\n  VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning\n\n\n\n    üåê Homepage¬†¬† | ¬†¬†\n    üíª GitHub  | ¬†¬†\n    ü§ó Hugging Face¬†¬† | ¬†¬†\n    üìë Paper¬†¬† \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüëã Introduction\n\t\n\nRecent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in perceiving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yh0075/VisuRiddles.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NicoHelemon/MNLP_M2_mcqa_dataset","creator_name":"Nicolas Gonzalez","creator_url":"https://huggingface.co/NicoHelemon","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset\n\t\n\nA unified multiple-choice question answering (MCQA) benchmark on STEM subjects combining samples from OpenBookQA, SciQ, MMLU-auxiliary, AQUA-Rat, and MedMCQA.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset merges five existing science and knowledge-based MCQA datasets into one standardized format:\n\n\t\n\t\t\nSource\nTrain samples\n\n\n\t\t\nOpenBookQA\n4‚ÄØ900\n\n\nSciQ\n10‚ÄØ000\n\n\nMMLU-aux\n85‚ÄØ100\n\n\nAQUA-Rat\n50‚ÄØ000\n\n\nMedMCQA\n50‚ÄØ000\n\n\nTotal\n200‚ÄØ000\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NicoHelemon/MNLP_M2_mcqa_dataset.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"SolidGeo","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HarryYancy/SolidGeo","creator_name":"HarryYancy","creator_url":"https://huggingface.co/HarryYancy","description":"\n\t\n\t\t\n\t\tSolidGeo: Measuring Multimodal Spatial Math Reasoning in Solid Geometry\n\t\n\n[üåê Homepage] [üíª Github]  [ü§ó Huggingface Dataset] \n[üìä Leaderboard ]  [üîç Visualization]  [üìñ Paper]\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nSolidGeo is the first large-scale benchmark specifically designed to evaluate the performance of MLLMs on mathematical reasoning tasks in solid geometry. SolidGeo consists of 3,113 real-world K‚Äì12 and competition-level problems, each paired with visual context and annotated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HarryYancy/SolidGeo.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-classification","English"],"keywords_longer_than_N":true},
	{"name":"Ava-100","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iesc/Ava-100","creator_name":"Intelligent Edge Sensing and Computing","creator_url":"https://huggingface.co/iesc","description":"\n\t\n\t\t\n\t\tEmpowering Agentic Video Analytics Systems with Video Language Models\n\t\n\n [üñ•Ô∏è Project Code] [üìñ arXiv Paper] [üìä Dataset]\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\n    \n\n\nAVA-100 is an ultra-long video benchmark specially designed to evaluate video analysis capabilities Avas-100 consists of 8 videos, each exceeding 10 hours in length, and includes a total of 120 manually annotated questions. The benchmark covers four typical video analytics scenarios: human daily activities, city walking, wildlife‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iesc/Ava-100.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CCPS","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ledengary/CCPS","creator_name":"Reza Khan Mohammadi","creator_url":"https://huggingface.co/ledengary","description":"\n\t\n\t\t\n\t\tCCPS: Calibrating LLM Confidence by Probing Perturbed Representation Stability\n\t\n\nThis dataset contains structured evaluation sets used to study and benchmark the confidence behavior of large language models (LLMs). The dataset covers both multiple-choice and open-ended formats across diverse domains (e.g., clinical, law), with responses generated by a range of LLMs.\nGitHub Repository: https://github.com/ledengary/ccps\n\n\t\n\t\t\n\t\n\t\n\t\tüìÅ Structure\n\t\n\nThe dataset is organized by task type‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ledengary/CCPS.","first_N":5,"first_N_keywords":["expert-generated","English","mit","10K<n<100K","arxiv:2505.21772"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset\n\t\n\nThe MNLP M2 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~26,000 MCQA questions\n7 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"b-score","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anvo25/b-score","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","description":"\n\t\n\t\t\n\t\tB-score: Detecting Biases in Large Language Models Using Response History\n\t\n\n    \n  by \n    An Vo1,\n    Mohammad Reza Taesiri2, \n    Daeyoung Kim1*,\n    Anh Totti Nguyen3*\n  \n  \n    *Equal advising\n    1KAIST, 2University of Alberta, 3Auburn University\n  \n  \n  \n    International Conference on Machine Learning (ICML 2025)\n  \n\n\n\n\n\n\n\n\n\n\n\nTLDR: When LLMs can see their own previous answers, their biases significantly decrease. We introduce B-score, a novel metric that detects bias by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/b-score.","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_quantized_dataset","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M2_quantized_dataset","creator_name":"Alireza Abdollahopoorrostam","creator_url":"https://huggingface.co/AlirezaAbdollahpoor","description":"\n\t\n\t\t\n\t\tMCQA Test Dataset for Model Evaluation\n\t\n\nThis dataset contains 3254 carefully selected test samples from MetaMathQA and AQuA-RAT datasets, designed for MCQA (Multiple Choice Question Answering) model evaluation and quantization testing.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal Samples: 3254\nMetaMathQA Samples: 3000 (mathematical problems)\nAQuA-RAT Samples: 254 (algebraic word problems)\nQuestion Types: Math, Algebra\nIntended Use: Model evaluation, quantization benchmarking\n\n\n\t\n\t\t\n\t\tSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M2_quantized_dataset.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"bigbench_jsonl","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl","creator_name":"NJUDeepEngine","creator_url":"https://huggingface.co/NJUDeepEngine","description":"BIG-Bench but it doesn't require the hellish dependencies (tensorflow, pypi-bigbench, protobuf) of the official version.\ndataset = load_dataset(\"tasksource/bigbench\",'movie_recommendation')\n\nCode to reproduce:\nhttps://colab.research.google.com/drive/1MKdLdF7oqrSQCeavAcsEnPdI85kD0LzU?usp=sharing\nDatasets are capped to 50k examples to keep things light.\nI also removed the default split when train was available also to save space, as default=train+val.\n@article{srivastava2022beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_quantized_dataset","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oskdabk/MNLP_M2_quantized_dataset","creator_name":"Oskar Dabkowski","creator_url":"https://huggingface.co/oskdabk","description":"\n\t\n\t\t\n\t\tMNLP_M2_mcqa_dataset\n\t\n\nThis dataset merges five MCQ sources (sciqa, openbookqa, mmlu, m1_mcq, ai2_arc) into a single JSONL with 1389 examples.\nEach line of MNLP_M2_mcqa_dataset.jsonl has fields:\n\nid: question identifier  \nquestion: the prompt  \nchoices: list of answer choices  \nanswer: the correct choice index (0‚Äì3)  \njustification: a concise rationale  \ndataset: which source this came from\n\n","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SpineBench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Silversorrow/SpineBench","creator_name":"Zhang","creator_url":"https://huggingface.co/Silversorrow","description":"\n\t\n\t\t\n\t\tDataset Card for SpineBench\n\t\n\n\nBenchmark Details\nPaper Information\nBenchmark Examples\nBenchmark Distribution\nData Format\nData SourceHuman Evaluation of MLLMs Reasoning Performance\nCitation\n\n\n\t\n\t\t\n\t\tBenchmark Details\n\t\n\nSpineBench is a comprehensive Visual Question Answering (VQA) benchmark designed for fine-grained analysis and evaluation of LVLM in the spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images, covering 11 spinal diseases through two critical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Silversorrow/SpineBench.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"medmcqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/openlifescienceai/medmcqa","creator_name":"Open Life Science AI","creator_url":"https://huggingface.co/openlifescienceai","description":"\n\t\n\t\t\n\t\tDataset Card for MedMCQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.\nMedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity.\nEach sample contains a question, correct answer(s), and other options which require‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openlifescienceai/medmcqa.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","multiple-choice-qa","open-domain-qa","no-annotation"],"keywords_longer_than_N":true},
	{"name":"MMMU","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMU/MMMU","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\n\t\n\t\t\n\t\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\n\t\n\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüõ†Ô∏è[2024-05-30]: Fixed duplicate option issues in Materials dataset items (validation_Materials_25; test_Materials_17, 242) and content error in validation_Materials_25.\nüõ†Ô∏è[2024-04-30]: Fixed missing \"-\" or \"^\" signs in Math dataset items (dev_Math_2, validation_Math_11, 12, 16; test_Math_8‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"truthful_qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/truthfulqa/truthful_qa","creator_name":"TruthfulQA","creator_url":"https://huggingface.co/truthfulqa","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/truthfulqa/truthful_qa.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[üíª Github] [üåê Homepage]  [üìä Leaderboard ] [üìä Open Source Leaderboard ] [üîç Visualization] [üìñ Paper]\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tüí• News\n\t\n\n\n[2025.05.16] üí• We now support the official open-source leaderboard! üî•üî•üî• Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. üî•üî•üî•‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"PhyX","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Cloudriver/PhyX","creator_name":"Hui Shen","creator_url":"https://huggingface.co/Cloudriver","description":"\n\t\n\t\t\n\t\tPhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?\n\t\n\nDataset for the paper \"PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?\".\nFor more details, please refer to the project page with dataset exploration and visualization tools: https://phyx-bench.github.io/.\n[üåê Project Page] [üìñ Paper] [üîß Evaluation Code]  [üåê Blog (‰∏≠Êñá)]\n\n\t\n\t\n\t\n\t\tüîî News\n\t\n\n\n[2025.05.27] üéâ PhyX is officially supported by VLMEvalKit for easy evalution.\n[2025.05.23] üöÄ The arXiv paper is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cloudriver/PhyX.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","mit"],"keywords_longer_than_N":true},
	{"name":"xtreme","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\n\t\n\t\t\n\t\tDataset Card for \"xtreme\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","token-classification","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"ScienceQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derek-thomas/ScienceQA","creator_name":"Derek Thomas","creator_url":"https://huggingface.co/derek-thomas","description":"\n\t\n\t\t\n\t\tDataset Card Creation Guide\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMulti-modal Multiple Choice\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nExplore more samples here.\n{'image': Image,\n 'question': 'Which of these states is farthest north?',\n 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'],\n 'answer': 0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","other","visual-question-answering","text-classification"],"keywords_longer_than_N":true},
	{"name":"MathVista","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\t\n\t\t\n\t\tDataset Card for MathVista\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nData Visualization\nData Source\nAutomatic Evaluation\n\n\nLicense\nCitation\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"MathVerse","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVerse","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\t\n\t\t\n\t\tDataset Card for MathVerse\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nCitation\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe capabilities of Multi-modal Large Language Models (MLLMs) in visual math problem-solvingremain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.\n\n     \n\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVerse.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MedMCQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tDataset Card for medmcqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the medmcqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the MedMCQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MRAG-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/uclanlp/MRAG-Bench","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","description":"\n\t\n\t\t\n\t\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\n\t\n\nüåê Homepage | üìñ Paper | üíª Evaluation \n\n\t\n\t\t\n\t\tIntro\n\t\n\nMRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios,  providing a robust and systematic evaluation of Large Vision Language Model (LVLM)‚Äôs vision-centric multimodal retrieval-augmented generation (RAG) abilities.\n\n\n\n\n\t\n\t\t\n\t\tResults\n\t\n\nEvaluated upon 10 open-source and 4 proprietary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/MRAG-Bench.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ChemBench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jablonkagroup/ChemBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","description":"\n\t\n\t\t\n\t\tChemBench\n\t\n\n\n\n\n\n\n\n\n\n\nA manually curated benchmark for evaluating chemistry and materials capabilities of Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/ChemBench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","language-modeling","natural-language-inference","expert-generated"],"keywords_longer_than_N":true},
	{"name":"DrivingVQA","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA","creator_name":"EPFL-DrivingVQA","creator_url":"https://huggingface.co/EPFL-DrivingVQA","description":"\n\t\n\t\t\n\t\tDataset Card for DrivingVQA\n\t\n\nüè† Homepage\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDrivingVQA is a dataset designed to assist candidates preparing for the French driving theory exam, which requires passing both a theoretical and a practical test. The theoretical aspect consists of analyzing 40 multiple-choice questions (MCQs) with real-world images to test the candidates' knowledge of traffic laws, road signs, and safe driving practices. This dataset focuses on visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"MMSI-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RunsenXu/MMSI-Bench","creator_name":"Runsen Xu","creator_url":"https://huggingface.co/RunsenXu","description":"\n\t\n\t\t\n\t\tMMSI-Bench\n\t\n\nThis repo contains evaluation code for the paper \"MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence\" \nüåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n  üî•[2025-06-18]: MMSI-Bench has been supported in the LMMs-Eval repository.\n  ‚ú®[2025-06-11]: MMSI-Bench was used for evaluation in the experiments of VILASR.\n  üî•[2025-06-9]: MMSI-Bench has been supported in the VLMEvalKit repository.\n  üî•[2025-05-30]: We released the ArXiv paper.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RunsenXu/MMSI-Bench.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"qasc","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/qasc","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\n\t\n\t\t\n\t\tDataset Card for \"qasc\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nQASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice\nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\t\n\t\t\n\t\tdefault\n\t\n\n\nSize of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/qasc.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","extractive-qa","multiple-choice-qa","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"psycholinguistic_eval","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KevinZ/psycholinguistic_eval","creator_name":"Kevin Zhao","creator_url":"https://huggingface.co/KevinZ","description":"Psycholinguistic dataset from 'What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models'\nby Allyson Ettinger","first_N":5,"first_N_keywords":["multiple-choice","fill-mask","question-answering","zero-shot-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"movie_recommendation","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/movie_recommendation","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Movie recommendation task based on the Movielens dataset","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","open-domain-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"discourse_marker_qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/discourse_marker_qa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Discourse marker/connective prediction as multiple choice questions based on the Discovery dataset","first_N":5,"first_N_keywords":["question-answering","multiple-choice","open-domain-qa","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"123_test","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JeremyAlain/123_test","creator_name":"J√©r√©my Scheurer","creator_url":"https://huggingface.co/JeremyAlain","description":"The Fewshot Table dataset consists of tables that naturally occur on the web, that are formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. The dataset consists of approximately 413K tables that are extracted from the WDC Web Table Corpora 2015, which is released under the Apache-2.0 license. The WDC Web Table Corpora \"contains vast amounts of HTML tables. [...] The Web Data Commons project extracts relational Web tables from the Common Crawl, the largest and most up-to-date Web corpus that is currently available to the public.\"","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"fig-qa","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nightingal3/fig-qa","creator_name":"Emmy Liu","creator_url":"https://huggingface.co/nightingal3","description":"\n\t\n\t\t\n\t\tDataset Card for Fig-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is the dataset for the paper Testing the Ability of Language Models to Interpret Figurative Language. Fig-QA consists of 10256 examples of human-written creative metaphors that are paired as a Winograd schema. It can be used to evaluate the commonsense reasoning of models. The metaphors themselves can also be used as training data for other tasks, such as metaphor detection or generation. \n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nightingal3/fig-qa.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"unpredictable_full","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_full","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_mmo-champion-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_mmo-champion-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_baseball-fantasysports-yahoo-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_baseball-fantasysports-yahoo-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_phonearena-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_phonearena-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_support-google-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_dividend-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_dividend-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_bulbapedia-bulbagarden-net","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_bulbapedia-bulbagarden-net","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_wkdu-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_wkdu-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_dummies-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_dummies-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_mgoblog-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_mgoblog-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_gamefaqs-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_gamefaqs-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_studystack-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_studystack-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_sittercity-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_sittercity-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_msdn-microsoft-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_msdn-microsoft-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cappex-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cappex-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_en-wikipedia-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_en-wikipedia-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cram-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cram-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_w3-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_w3-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_sporcle-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_sporcle-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_wiki-openmoko-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_wiki-openmoko-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_ensembl-org","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_ensembl-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_5k","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_5k","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_unique","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_unique","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster-noise","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster-noise","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster00","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster00","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster01","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster01","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster10","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster10","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster11","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster11","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster12","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster12","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster13","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster13","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster14","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster14","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster15","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster15","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster16","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster16","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster17","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster17","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster18","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster18","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster19","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster19","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster02","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster02","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster20","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster20","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster21","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster21","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster22","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster22","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster23","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster23","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster24","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster24","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster25","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster25","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster26","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster26","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster27","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster27","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster28","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster28","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster29","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster29","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster03","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster03","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster04","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster04","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster05","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster05","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster06","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster06","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster07","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster07","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster08","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster08","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster09","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster09","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-low","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-low","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-medium","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-medium","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-high","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-high","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"moral_stories","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/demelin/moral_stories","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","description":"Moral Stories is a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented \nsocial reasoning. For detailed information, see https://aclanthology.org/2021.emnlp-main.54.pdf.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","text-classification","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"wikimedqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/wikimedqa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"@inproceedings{sileo-etal-2024-generating-multiple,\n    title = \"Generating Multiple-choice Questions for Medical Question Answering with Distractors and Cue-masking\",\n    author = \"Sileo, Damien  and\n      Uma, Kanimozhi  and\n      Moens, Marie-Francine\",\n    booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sileod/wikimedqa.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"understanding_fables","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/demelin/understanding_fables","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","description":"This task aims to measure the ability of computational models to understand short narratives, by identifying the most \nappropriate moral for a given fable from a set of five alternatives.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","multiple-choice-qa","language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_full","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_full","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_5k","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_5k","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_support-google-com","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_unique","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/unpredictable/unpredictable_unique","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"newyorker_caption_contest","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmhessel/newyorker_caption_contest","creator_name":"Jack Hessel","creator_url":"https://huggingface.co/jmhessel","description":"\n\t\n\t\t\n\t\tDataset Card for New Yorker Caption Contest Benchmarks\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSee capcon.dev for more!\nData from:\nDo Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest\n@inproceedings{hessel2023androids,\n  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''\n         Benchmarks from {The New Yorker Caption Contest}},\n  author={Hessel, Jack and Marasovi{\\'c}, Ana and Hwang, Jena D. and Lee, Lillian\n          and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhessel/newyorker_caption_contest.","first_N":5,"first_N_keywords":["image-to-text","multiple-choice","text-classification","text-generation","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"NeQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/NeQA","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\n\t\n\t\t\n\t\tNeQA: Can Large Language Models Understand Negation in Multi-choice Questions? (Zhengping Zhou and Yuhui Zhang)\n\t\n\n\n\t\n\t\t\n\t\tGeneral description\n\t\n\nThis task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation. The authors find that smaller language models display approximately random performance whereas the performance of larger models become significantly worse than random. \nLanguage models failing to follow‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/NeQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"quote-repetition","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/quote-repetition","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\n\t\n\t\t\n\t\tquote-repetition (Joe Cavanagh, Andrew Gritsevskiy, and Derik Kauffman of Cavendish Labs)\n\t\n\n\n\t\n\t\t\n\t\tGeneral description\n\t\n\nIn this task, the authors ask language models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. Each prompt contains a famous quote with a modified ending to mislead the model into completing the sequence with the famous ending rather than with the ending given in the prompt. The authors find that smaller models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/quote-repetition.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"redefine-math","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/redefine-math","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\n\t\n\t\t\n\t\tredefine-math (Xudong Shen)\n\t\n\n\n\t\n\t\t\n\t\tGeneral description\n\t\n\nIn this task, the author tests whether language models are able to work with common symbols when they are redefined to mean something else. The author finds that larger models are more likely to pick the answer corresponding to the original definition rather than the redefined meaning, relative to smaller models. \nThis task demonstrates that it is difficult for language models to work with new information given at inference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/redefine-math.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"hindsight-neglect-10shot","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/inverse-scaling/hindsight-neglect-10shot","creator_name":"Inverse Scaling Prize","creator_url":"https://huggingface.co/inverse-scaling","description":"\n\t\n\t\t\n\t\tinverse-scaling/hindsight-neglect-10shot (‚ÄòThe Floating Droid‚Äô)\n\t\n\n\n\t\n\t\t\n\t\tGeneral description\n\t\n\nThis task tests whether language models are able to assess whether a bet was worth taking based on its expected value. The author provides few shot examples in which the model predicts whether a bet is worthwhile by correctly answering yes or no when the expected value of the bet is positive (where the model should respond that ‚Äòyes‚Äô, taking the bet is the right decision) or negative (‚Äòno‚Äô‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/inverse-scaling/hindsight-neglect-10shot.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"copa-sse","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anab/copa-sse","creator_name":"Ana Brassard","creator_url":"https://huggingface.co/anab","description":"\n\t\n\t\t\n\t\tDataset Card for COPA-SSE\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nCOPA-SSE contains crowdsourced explanations for the Balanced COPA dataset, a variant of the Choice of Plausible Alternatives (COPA) benchmark. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nCan be used to train a model for explain+predict or predict+explain settings. Suited for both text-based and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anab/copa-sse.","first_N":5,"first_N_keywords":["multiple-choice","explanation-generation","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"probability_words_nli","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Probing neural language models for understanding of words of estimative probability","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"model-written-evals","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Anthropic/model-written-evals","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","description":"\n\t\n\t\t\n\t\tModel-Written Evaluation Datasets\n\t\n\nThis repository includes datasets written by language models, used in our paper on \"Discovering Language Model Behaviors with Model-Written Evaluations.\"\nWe intend the datasets to be useful to:\n\nThose who are interested in understanding the quality and properties of model-generated data\nThose who wish to use our datasets to evaluate other models for the behaviors we examined in our work (e.g., related to model persona, sycophancy, advanced AI risks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/model-written-evals.","first_N":5,"first_N_keywords":["multiple-choice","zero-shot-classification","question-answering","multiple-choice-qa","multiple-choice-coreference-resolution"],"keywords_longer_than_N":true},
	{"name":"cycic_multiplechoice","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/cycic_multiplechoice","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://colab.research.google.com/drive/16nyxZPS7-ZDFwp7tn_q72Jxyv0dzK1MP?usp=sharing\n@article{Kejriwal2020DoFC,\n  title={Do Fine-tuned Commonsense Language Models Really Generalize?},\n  author={Mayank Kejriwal and Ke Shen},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2011.09159}\n}\n\nadded for \n@article{sileo2023tasksource,\n  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},\n  author={Sileo, Damien},\n  url=‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/cycic_multiplechoice.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmlu","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/mmlu","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"MMLU (hendrycks_test on huggingface) without auxiliary train. It is much lighter (7MB vs 162MB) and faster than the original implementation, in which auxiliary train is loaded (+ duplicated!) by default for all the configs in the original version, making it quite heavy.\nWe use this version in tasksource. \nReference to original dataset:\nMeasuring Massive Multitask Language Understanding - https://github.com/hendrycks/test\n@article{hendryckstest2021,\n  title={Measuring Massive Multitask Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/mmlu.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"hhh_alignment","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_mc","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_mc","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","description":"TruthfulQA-MC is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","language-modeling","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_binary","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_binary","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","description":"TruthfulQA-Binary is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","language-modeling","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"geobench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daven3/geobench","creator_name":"Daven","creator_url":"https://huggingface.co/daven3","description":"\n\t\n\t\t\n\t\n\t\n\t\tBenchmark: GeoBenchmark\n\t\n\nIn GeoBenchmark, we collect 183 multiple-choice questions in NPEE, and 1,395 in AP Test, for objective tasks. \nMeanwhile, we gather all 939 subjective questions in NPEE to be the subjective tasks set and use 50 to measure the baselines with human evaluation. \n","first_N":5,"first_N_keywords":["multiple-choice","question-answering","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"LEval","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/L4NLP/LEval","creator_name":"L4NLP","creator_url":"https://huggingface.co/L4NLP","description":"A benchmark to evaluate long document understanding and generation ability of LLM","first_N":5,"first_N_keywords":["summarization","question-answering","multiple-choice","English","gpl-3.0"],"keywords_longer_than_N":true},
	{"name":"JAQKET","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kumapo/JAQKET","creator_name":"kumapo","creator_url":"https://huggingface.co/kumapo","description":"JAQKET: JApanese Questions on Knowledge of EnTitie","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Japanese","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"inverse_scaling_prize-hindsight_neglect","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-hindsight_neglect","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","description":"The hindsight-neglect task from the Inverse Scaling Prize\n","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"inverse_scaling_prize-sig_figs","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-sig_figs","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","description":"jmichaelov/inverse_scaling_prize-sig_figs dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"multiple-choice","license":"Boost Software License 1.0","license_url":"https://choosealicense.com/licenses/bsl-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pengxiang01/test","creator_name":"wang","creator_url":"https://huggingface.co/pengxiang01","description":"aasdfsdf\n","first_N":5,"first_N_keywords":["tabular-to-text","table-to-text","multiple-choice","text-retrieval","time-series-forecasting"],"keywords_longer_than_N":true},
	{"name":"PolyMRC","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigai-nlco/PolyMRC","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","description":"We construct a dataset through entries with multiple meanings and examples from Chinese dictionaries, and set the example as context and explanations as choices, the goal of Polysemy Machine Comprehension (PolyMRC) is to find the correct explanation of the entry in the example.\nthe statistics of the dataset\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nsplit\nsentences\naverage sentence length\n\n\ntrain\n46,119\n38.55\n\n\nvalidation\n5,765\n38.31\n\n\ntest\n5,765\n38.84\n\n\n\t\n\n","first_N":5,"first_N_keywords":["multiple-choice","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"TruthfulQA_de","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeoLM/TruthfulQA_de","creator_name":"LAION LeoLM","creator_url":"https://huggingface.co/LeoLM","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeoLM/TruthfulQA_de.","first_N":5,"first_N_keywords":["multiple-choice","German","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"belebele","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/belebele","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tThe Belebele Benchmark for Massively Multilingual NLU Evaluation\n\t\n\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset. The human annotation procedure was carefully curated to create questions that discriminate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/belebele.","first_N":5,"first_N_keywords":["question-answering","zero-shot-classification","text-classification","multiple-choice","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_context","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/portkey/truthful_qa_context","creator_name":"Portkey AI","creator_url":"https://huggingface.co/portkey","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa_context\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA Context is an extension of the TruthfulQA benchmark, specifically designed to enhance its utility for models that rely on Retrieval-Augmented Generation (RAG). This version includes the original questions and answers from TruthfulQA, along with the added context text directly associated with each question. This additional context aims to provide immediate reference material for models, making it particularly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/portkey/truthful_qa_context.","first_N":5,"first_N_keywords":["text-generation","question-answering","multiple-choice","English","mit"],"keywords_longer_than_N":true},
	{"name":"kaggel-llm-science-exam-2023-RAG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/natnitaract/kaggel-llm-science-exam-2023-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","description":"natnitaract/kaggel-llm-science-exam-2023-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"EusExams","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/EusExams","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\n\t\n\t\t\n\t\tDataset Card for EusExams\n\t\n\nEusExams is a collection of tests designed to prepare individuals for Public Service examinations conducted by several Basque institutions, including the public health system Osakidetza, the Basque Government, the City Councils of Bilbao and Gasteiz, and the University of the Basque Country (UPV/EHU). Within each of these groups, there are different exams for public positions, such as administrative and assistant roles. Each multiple-choice question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/EusExams.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Basque","Spanish","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"CodeFuse-DevOps-Eval","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval","creator_name":"CodeFuse AI","creator_url":"https://huggingface.co/codefuse-ai","description":"DevOps-Eval is a comprehensive chinese evaluation suite specifically designed for foundation models in the DevOps field. It consists of 5977 multi-choice questions spanning 55 diverse categories. Please visit our website and GitHub for more details.\nEach category consists of two splits: dev, and test. The dev set per subject consists of five exemplars with explanations for few-shot evaluation. And the test set is for model evaluation. Labels on the test split are released, users can evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"vi_grade_school_math_mcq","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq","creator_name":"Bui Van Hop","creator_url":"https://huggingface.co/hllj","description":"\n\t\n\t\t\n\t\tDataset Card for Vietnamese Grade School Math Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset includes multiple-choice math exercises for elementary school students from grades 1 to 5 in Vietnam.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe majority of the data is in Vietnamese.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe data includes information about the page paths we crawled and some text that has been post-processed. The structure will be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq.","first_N":5,"first_N_keywords":["text-generation","multiple-choice","Vietnamese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NIFTY","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raeidsaqur/NIFTY","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","description":"\n  \n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \n\n\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\nFor the NIFTY-RL LLM alignment dataset please use nifty-rl.\n\n\t\n\t\t\n\t\n\t\n\t\tüìã Table of Contents\n\t\n\n\nüß© NIFTY Dataset\nüìã Table of Contents\nüìñ Usage\nDownloading the dataset\nDataset structure\n\n\nLarge Language Models \n‚úçÔ∏è Contributing\nüìù Citing\nüôè‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/NIFTY.","first_N":5,"first_N_keywords":["multiple-choice","time-series-forecasting","document-question-answering","topic-classification","semantic-similarity-classification"],"keywords_longer_than_N":true},
	{"name":"E-EVAL","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/E-EVAL/E-EVAL","creator_name":"E-EVAL","creator_url":"https://huggingface.co/E-EVAL","description":"E-EVAL/E-EVAL dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","Chinese","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"MMMU","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aslessor/MMMU","creator_name":"Alex","creator_url":"https://huggingface.co/aslessor","description":"\n\t\n\t\t\n\t\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\n\t\n\nüåê Homepage | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüî•[2023-12-04]: Our evaluation server for test set is now availble on EvalAI. We welcome all submissions and look forward to your participation! üòÜ\n\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aslessor/MMMU.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"open-otter","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/onuralp/open-otter","creator_name":"Onuralp","creator_url":"https://huggingface.co/onuralp","description":"\nDisclaimer: this dataset is curated for NeurIPS 2023 LLM efficiency challange, and currently work in progress. Please use at your own risk.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe curated this dataset to finetune open source base models as part of NeurIPS 2023 LLM Efficiency Challenge (1 LLM + 1 GPU + 1 Day). This challenge requires participants to use open source models and datasets with permissible licenses to encourage wider adoption, use and dissemination of open source contributions in generative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onuralp/open-otter.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"COPAL","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haryoaw/COPAL","creator_name":"HaryoAW","creator_url":"https://huggingface.co/haryoaw","description":"\n\t\n\t\t\n\t\tAbout COPAL-ID\n\t\n\nCOPAL-ID is an Indonesian causal commonsense reasoning dataset that captures local nuances. It provides a more natural portrayal of day-to-day causal reasoning within the Indonesian (especially Jakartan) cultural sphere. Professionally written and validatid from scratch by natives, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID.\nCOPAL-ID is a test set only, intended to be used as a benchmark.\nFor more details, please see our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haryoaw/COPAL.","first_N":5,"first_N_keywords":["multiple-choice","Indonesian","cc-by-sa-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"PCA-Bench-V1","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1","creator_name":"PCA-Bench","creator_url":"https://huggingface.co/PCA-Bench","description":"PCA-Bench\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-Bench is an innovative benchmark for evaluating and locating errors in Multimodal LLMs when conducting embodied decision making tasks, specifically focusing on perception, cognition, and action.\n\n\t\n\t\n\t\n\t\tRelease\n\t\n\n\n[2024.02.15] PCA-Bench-V1 is released. We release the open and closed track data in huggingface. We also set an online leaderboard  accepting users' submission.\n[2023.12.15] PCA-EVAL is accepted to Foundation Model for Decision Making Workshop‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CIDAR-MCQ-100","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/arbml/CIDAR-MCQ-100","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","description":"\n\t\n\t\t\n\t\tDataset Card for \"CIDAR-MCQ-100\"\n\t\n\n\n\t\n\t\t\n\t\tCIDAR-MCQ-100\n\t\n\nCIDAR-MCQ-100 contains 100 multiple-choice questions and answers about the Arabic culture. \n\n\t\n\t\t\n\t\tüìö Datasets Summary\n\t\n\n\n  \nName\nExplanation\n\n\nCIDAR \n10,000 instructions and responses in Arabic\n\n\nCIDAR-EVAL-100 \n100 instructions to evaluate LLMs on cultural relevance\n\n\nCIDAR-MCQ-100 \n100 Multiple choice questions and answers to evaluate LLMs on cultural relevance \n\n\n\n\n\n\n\n\t\n\t\t\nCategory\nCIDAR-EVAL-100\nCIDAR-MCQ-100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR-MCQ-100.","first_N":5,"first_N_keywords":["multiple-choice","Arabic","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"EXAMs","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/EXAMs","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\n\t\n\t\t\n\t\n\t\n\t\tEXAMs\n\t\n\nYou can find details of the dataset in this post:https://arxiv.org/pdf/2308.16149.pdf\n\n\t\n\t\t\n\t\n\t\n\t\tAbout this Arabic dataset\n\t\n\nWe only took the Arabic part of the dataset,which contains 562 data.We then extracted five from each category based on the task domain as a few shot data.\n","first_N":5,"first_N_keywords":["multiple-choice","Arabic","apache-2.0","n<1K","arxiv:2308.16149"],"keywords_longer_than_N":true},
	{"name":"SciBench-TruthfulQA-RAG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/natnitaract/SciBench-TruthfulQA-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","description":"natnitaract/SciBench-TruthfulQA-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"sensory-awareness-benchmark","keyword":"multiple-choice","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark","creator_name":"Nick Doiron","creator_url":"https://huggingface.co/monsoon-nlp","description":"\n\t\n\t\t\n\t\tSensory Awareness Benchmark\n\t\n\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\nControl questions are connected to a specific capability:\n\nCan you receive an image file?\nWould you consider your level to be that of a super-intelligent AI agent?\n\nNatural questions which are possible for the average person, but may require multiple capabilities for a model:\n\nCan you head to the corner and check if my‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark.","first_N":5,"first_N_keywords":["multiple-choice","cc0-1.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Atilla00/truthful_qa_tr","creator_name":"Atilla Karaahmetoƒülu","creator_url":"https://huggingface.co/Atilla00","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\"truthful_qa\" translated to Turkish.\n\n\t\n\t\t\n\t\tUsage\n\t\n\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'generation')\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'multiple_choice')\n\n","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"Curr-ReFT-data","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data","creator_name":"ZTE-AIM","creator_url":"https://huggingface.co/ZTE-AIM","description":"\n\t\n\t\t\n\t\tCurr-ReFT-data\n\t\n\n[üìÇ GitHub][üìù Paper]\n[ü§ó HF Dataset]  [ü§ó HF-Model: Curr-ReFT-3B] \n[ü§ó HF-Model: Curr-ReFT-7B] \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nCurr-ReFT-data contains training data for both stages of the Curr-ReFT methodology. The proposed Curr-ReFT post-training paradigm consists of two consecutive training stages: 1. Curriculum Reinforcement Learning: Gradually increasing task difficulty through reward mechanisms that match task complexity. 2. Rejected Sample based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"PLM-VideoBench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/PLM-VideoBench","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPLM-VideoBench is a collection of human-annotated resources for evaluating Vision Language models, focused on detailed video understanding.\n[üìÉ Tech Report]\n[üìÇ Github]\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nPLM-VideoBench includes evaluation data for the following tasks:\n\n\t\n\t\t\n\t\n\t\n\t\tFGQA\n\t\n\nIn this task, a model must answer a multiple-choice question (MCQ) that probes fine-grained activity understanding. Given a question and multiple options that differ in a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/PLM-VideoBench.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","other","other","English"],"keywords_longer_than_N":true},
	{"name":"VidComposition_Benchmark","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark","creator_name":"JunJiaGuo","creator_url":"https://huggingface.co/JunJiaGuo","description":"\n\t\n\t\t\n\t\tVidComposition Benchmark\n\t\n\nüñ• Project Page | üöÄ Evaluation Space\nThe advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-text-to-text","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"tiny-truthful-qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rahmanidashti/tiny-truthful-qa","creator_name":"Hossein A. (Saeed) Rahmani","creator_url":"https://huggingface.co/rahmanidashti","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 790 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rahmanidashti/tiny-truthful-qa.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"legalbench_basic","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DatologyAI/legalbench_basic","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task preserves its original data and includes an additional input column with a formatted prompt, generated using the LegalBench registry, ready to be fed directly into language models.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\n\nBasic Legal: canada_tax_court_outcomes, jcrew_blocker, learned_hands_benefits, telemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench_basic.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"UHGEvalDataset","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ki-Seki/UHGEvalDataset","creator_name":"Shichao Song","creator_url":"https://huggingface.co/Ki-Seki","description":"The dataset sourced from https://github.com/IAAR-Shanghai/UHGEval\n","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"kor_qasc","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KETI-AIR/kor_qasc","creator_name":"Korea Electronics Technology Institute Artificial Intelligence Research Center","creator_url":"https://huggingface.co/KETI-AIR","description":"\n\t\n\t\t\n\t\tDataset Card for QASC\n\t\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe data is distributed under the CC BY 4.0 license.\n\n\t\n\t\t\n\t\tSource Data Citation INformation\n\t\n\n@article{allenai:qasc,\n      author    = {Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},\n      title     = {QASC: A Dataset for Question Answering via Sentence Composition},\n      journal   = {arXiv:1910.11473v2},\n      year      = {2020},\n}\n\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","extractive-qa","multiple-choice-qa","Korean"],"keywords_longer_than_N":true},
	{"name":"tinyTruthfulQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA","creator_name":"tinyBenchmarks","creator_url":"https://huggingface.co/tinyBenchmarks","description":"\n\t\n\t\t\n\t\ttinyTruthfulQA\n\t\n\nWelcome to tinyTruthfulQA! This dataset serves as a concise version of the truthfulQA dataset, offering a subset of 100 data points selected from the original compilation. \ntinyTruthfulQA is designed to enable users to efficiently estimate the performance of a large language model (LLM) with reduced dataset size, saving computational resources \nwhile maintaining the essence of the truthfulQA evaluation.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nCompact Dataset: With only 100 data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"GroundCocoa","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harsh147/GroundCocoa","creator_name":"Harsh Kohli","creator_url":"https://huggingface.co/harsh147","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nGroundCocoa is a benchmark to evaluate conditional and compositional reasoning in large language models through a flight-booking task presented in multiple-choice format.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe test set consists of 4849 samples consisting of 728 unique user requirements. User requirements may be repeated with varying options. In additon, we also provide a small validation set that may be used for certain parameter tuning. It consists of 52‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harsh147/GroundCocoa.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"openbookqa_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/openbookqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for openbookqa_ca\n\t\n\n\n\nopenbookqa_ca is a question answering dataset in Catalan, professionally translated from the main version of the OpenBookQA dataset in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nopenbookqa_ca (Open Book Question Answering - Catalan) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/openbookqa_ca.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"arc-c-okapi-eval-es","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/arc-c-okapi-eval-es","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\n\t\n\t\t\n\t\tARC-Challenge translated to Spanish\n\t\n\nThis dataset was generated by the Natural Language Processing Group of the University of Oregon, where they used the\noriginal ARC-Challenge dataset in English and translated it into different languages using ChatGPT.\nThis dataset only contains the Spanish translation, but the following languages are also covered within the original\nsubsets posted by the University of Oregon at http://nlp.uoregon.edu/download/okapi-eval/datasets/.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/arc-c-okapi-eval-es.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","open-domain-qa","Spanish"],"keywords_longer_than_N":true},
	{"name":"winograd_th","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pakphum/winograd_th","creator_name":"phakphum artkaew","creator_url":"https://huggingface.co/pakphum","description":"\n\t\n\t\t\n\t\tA collection of Thai Winograd Schemas\n\t\n\nWe present a collection of Winograd Schemas in the Thai language. These schemas are adapted from the original set of English Winograd Schemas proposed by Levesque et al., which was based on Ernest Davis's collection.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Translation\n\t\n\nTwo professional translators, who were native Thai speakers fluent in English and had experience translating from English to Thai, were hired. In a pilot translation phase, one native speaker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pakphum/winograd_th.","first_N":5,"first_N_keywords":["multiple-choice","Thai","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"xstorycloze_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/xstorycloze_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for xstorycloze_ca\n\t\n\n\n\nxstorycloze_ca is a question answering dataset in Catalan, professionally translated from the English StoryCloze dataset (Spring 2016 version), used  to create its multilingual version XStoryCloze.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nxstorycloze_ca (Multilingual Story Cloze Test - Catalan) is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1510 instances in the test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/xstorycloze_ca.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Catalan","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SeaExam","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SeaLLMs/SeaExam","creator_name":"SeaLLMs - Language Models for Southeast Asian Languages","creator_url":"https://huggingface.co/SeaLLMs","description":"\nCheck the üèÜ leaderboard constructed with this dataset and the corresponding üë®üèª‚Äçüíª evaluation code.\n\n\n\t\n\t\t\n\t\tSeaExam dataset\n\t\n\nThe SeaExam dataset aims to evaluate Large Language Models (LLMs) on a diverse set of Southeast Asian (SEA) languages including English, Chinese, Indonesian, Thai, and Vietnamese. \nOur goal is to ensure a fair and consistent comparison across different LLMs on those languages while mitigating the risk of data contamination. \nIt consists of the following two parts:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SeaLLMs/SeaExam.","first_N":5,"first_N_keywords":["multiple-choice","English","Indonesian","Vietnamese","Thai"],"keywords_longer_than_N":true},
	{"name":"truthfull_qa-tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/truthfull_qa-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\n\n\t\n\t\t\n\t\tDataset Card for truthful_qa-tr\n\t\n\nmalhajar/truthful_qa-tr is a translated version of truthful_qa aimed specifically to be used in the OpenLLMTurkishLeaderboard \nDeveloped by: Mohamad Alhajar \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/truthfull_qa-tr.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"mmlu-tr","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/mmlu-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\n\n\t\n\t\t\n\t\tDataset Card for mmlu-tr\n\t\n\nmalhajar/mmlu-tr is a translated version of mmlu aimed specifically to be used in the OpenLLMTurkishLeaderboard \nMMLU (hendrycks_test on huggingface) without auxiliary train. It is much lighter (7MB vs 162MB) and faster than the original implementation, in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu-tr.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"piqa_ca","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/piqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for piqa_ca\n\t\n\n\n\npiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the PIQA  validation set in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\npiqa_ca (Physical Interaction Question Answering - Catalan) is designed to evaluate physical commonsense reasoning using question-answer triplets based on everyday situations. It includes 1838 instances in the validation split. Each instance contains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/piqa_ca.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"siqa_ca","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/siqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for siqa_ca\n\t\n\n\n\nsiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the SIQA \nvalidation set in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nsiqa_ca (Social Interaction Question Answering - Catalan) is designed to evaluate social commonsense intelligence using multiple choice question-answer instances based on reasoning about people‚Äôs actions and their \nsocial implications. It includes‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/siqa_ca.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"enem","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maritaca-ai/enem","creator_name":"Maritaca AI","creator_url":"https://huggingface.co/maritaca-ai","description":"The ENEM 2022, 2023 and 2024 datasets encompass all multiple-choice questions from the last two editions of the Exame Nacional do Ensino M√©dio (ENEM), the main standardized entrance examination adopted by Brazilian universities. The datasets have been created to allow the evaluation of both textual-only and textual-visual language models. To evaluate textual-only models, we incorporated into the datasets the textual descriptions of the images that appear in the questions' statements from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maritaca-ai/enem.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","Portuguese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"cosmos_qa_ptbr","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr","creator_name":"Heloisy Rodrigues","creator_url":"https://huggingface.co/heloisy","description":"\n\t\n\t\t\n\t\tCosmos QA Portugu√™s\n\t\n\nEste dataset √© uma tradu√ß√£o para portugu√™s do Cosmos QA, que originalmente √© na l√≠ngua inglesa. \nA tradu√ß√£o foi feita automaticamente usando o GPT-3.5-turbo, logo pode ter erros que n√£o foram notados numa an√°lise superficial. \nSe atente ao uso.\n\n\t\n\t\t\n\t\tDataset Card for cosmos_qa\n\t\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe data is distributed under the CC BY 4.0 license.\n\n\t\n\t\t\n\t\tSource Data Citation INformation\n\t\n\n@inproceedings{huang-etal-2019-cosmos,\n    title =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr.","first_N":5,"first_N_keywords":["multiple-choice","cosmos_qa","Portuguese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"multiple-choice-questions","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions","creator_name":"Mateus Hamade","creator_url":"https://huggingface.co/mateus-hamade","description":"\n\t\n\t\t\n\t\n\t\n\t\tQuest√µes de M√∫ltipla Escolha - Base de dados (PT-BR)\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tContextualiza√ß√£o\n\t\n\nEste reposit√≥rio cont√©m uma base de dados (data.json) com quest√µes de m√∫ltipla escolha, a qual foi utilizada principalmente no desenvolvimento de modelos de recupera√ß√£o de informa√ß√£o.\n\n\t\n\t\t\n\t\n\t\n\t\tDescri√ß√£o do conjunto de dados\n\t\n\nO conjunto de dados √© composto por quest√µes de m√∫ltipla escolha, abrangendo uma variedade de temas dentro da √°rea da Ci√™ncia da Computa√ß√£o. Cada quest√£o √© estruturada‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions.","first_N":5,"first_N_keywords":["text-classification","question-answering","Portuguese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"FRoG","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GAIR/FRoG","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nFRoG is a fuzzy reasoning benchmark of generalized quantifiers to evaluate the fuzzy reasoning abilities of a model. The questions in FRoG are collected from real-world math word problem benchmarks GSM8K and MathQA and the generalized quantifier that is used to introduce fuzziness come from QuRe.\n\n\t\n\t\t\n\t\n\t\n\t\tSample Data\n\t\n\n{\n\"id\": 1,\n\"question\": \"john and ingrid pay [MASK] and 40 % tax annually , respectively . if john makes $ 60000 and ingrid makes $ 72000 , what is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/FRoG.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Sailcompass_data","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sail/Sailcompass_data","creator_name":"Sea AI Lab","creator_url":"https://huggingface.co/sail","description":"\n\t\n\t\t\n\t\n\t\n\t\tSailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages\n\t\n\nThis repository provides the dataset for evaluation SEA large language model.\n\nProject Website: sailorllm.github.io\nCodebase: https://github.com/sail-sg/sailcompass\n\n\n\t\n\t\t\n\t\n\t\n\t\tAcknowledgment\n\t\n\nThanks to the contributors of the opencompass.\n\n\t\n\t\t\n\t\n\t\n\t\tCiting this work\n\t\n\nIf you use this repository or sailor models, please cite\n@misc{sailcompass,\n      title={SailCompass: Towards Reproducible‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sail/Sailcompass_data.","first_N":5,"first_N_keywords":["text-classification","translation","summarization","table-question-answering","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"MMEvalPro","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MM-Diagnose/MMEvalPro","creator_name":"MM-Diagnose","creator_url":"https://huggingface.co/MM-Diagnose","description":"MMEvalPro\n\n\n  \n    \n  ¬†¬†\n   \n    \n  ¬†¬†\n  \n    \n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MMEvalPro\n\t\n\nWe create MMEvalPro for more accurate and efficent evaluation for Large Multimodal Models. It is designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledgeanchor question through a meticulous annotation process.\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MM-Diagnose/MMEvalPro.","first_N":5,"first_N_keywords":["multiple-choice","English","Chinese","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MUIRBENCH","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH","creator_name":"MUIRBENCH","creator_url":"https://huggingface.co/MUIRBENCH","description":"\n\t\n\t\t\n\t\tMuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding\n\t\n\nüåê Homepage | üìñ Paper | üíª Evaluation \n\n\t\n\t\t\n\t\tIntro\n\t\n\nMuirBench is a benchmark containing 11,264 images and 2,600 multiple-choice questions, providing robust evaluation on 12 multi-image understanding tasks.\n\nMuirBench evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic understanding, diagram understanding, visual retrieval, ..., etc, while prior benchmarks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"EthioEmo","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Tadesse/EthioEmo","creator_name":"Tadesse Destaw Belay","creator_url":"https://huggingface.co/Tadesse","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the following papers:\n@inproceedings{belay-etal-2025-evaluating,\n    title = {Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding},\n    author = {Belay, Tadesse Destaw  and Azime, Israel Abebe  and Ayele, Abinew Ali  and\n      Sidorov, Grigori  and Klakow, Dietrich  and Slusallek, Philip  and Kolesnikova, Olga  and\n      Yimam, Seid Muhie},\n    booktitle = {Proceedings of the 31st International‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tadesse/EthioEmo.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","Amharic","Somali","Tigrinya"],"keywords_longer_than_N":true},
	{"name":"mmlu_tr-v0.2","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","description":"\n\t\n\t\t\n\t\tDataset Card for mmlu_tr-v0.2\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nmalhajar/mmlu_tr-v0.2 is an enhanced version of the original mmlu-tr dataset, specifically developed for use in the OpenLLMTurkishLeaderboard v0.2. This iteration of the dataset has been translated into Turkish using advanced language models like GPT-4, with English text provided for cross-checking to ensure accuracy and reliability. The dataset is tailored to assist in evaluating the performance of Turkish language models (LLMs) and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"MARVEL","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://github.com/1171-jpg/MARVEL_AVR\nPaper [optional]: https://arxiv.org/abs/2404.13591\nDemo [optional]:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"arc_ca","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/arc_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for arc_ca\n\t\n\n\n\narc_ca is a question answering dataset in Catalan, professionally translated from the Easy and Challenge versions of the ARC dataset in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\narc_ca (AI2 Reasoning Challenge - Catalan) is based on multiple-choice science questions at elementary school level. The dataset consists of 2950 instances in the Easy version (570 in the test and 2380 instances in the validation split) and 1469 instances‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/arc_ca.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"belebele_gl","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/belebele_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\n\t\n\t\t\n\t\tDataset Card for belebele_gl\n\t\n\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset. The original dataset includes 122 language variants, with this dataset we include Galician language.\nIt is composed of 900 items translated and adapted to Galician language from the Spanish version. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: Proxecto N√ìS at HuggingFace\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\nIt can be used to evaluate Galician language models. Check the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/belebele_gl.","first_N":5,"first_N_keywords":["text-classification","question-answering","zero-shot-classification","multiple-choice","Galician"],"keywords_longer_than_N":true},
	{"name":"openbookqa_gl","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/openbookqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\n\t\n\t\t\n\t\tDataset Card for OpenBookQA_gl\n\t\n\nopenbookqa_gl is a question answering dataset in Galician, translated from the OpenBookQA dataset in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nopenbookqa_gl is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split. Each instance contains a question stem, four possible choices, and the letter‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/openbookqa_gl.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Galician","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"truthfulqa_gl","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/truthfulqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA_gl\n\t\n\n\n\nTruthfulQA_gl is the Galician version of the TruthfulQA dataset.\nThis dataset is used to measure the truthfulness of a language model when generating answers to questions. It includes questions from different categories that some humans would answer wrongly due to false beliefs or misconceptions.\nNote that this version includes only the generation split.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources\n\t\n\n\nRepository: Proxecto N√ìS at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/truthfulqa_gl.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"USQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Baron-GG/USQA","creator_name":"Anonymity","creator_url":"https://huggingface.co/Baron-GG","description":"Baron-GG/USQA dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"race-qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/klaylouis1932/race-qa","creator_name":"Klay Louis","creator_url":"https://huggingface.co/klaylouis1932","description":"klaylouis1932/race-qa dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PIQA-eu","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/PIQA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\n\t\n\t\t\n\t\tDataset Card for PIQA-eu\n\t\n\n\nPoint of Contact: hitz@ehu.eus\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPIQA-eu is the professional translation to Basque of the PIQA's \n(Bisk et al., 2020) validation partition. \nPIQA is a commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday\nobjects in everyday situations.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\neu-ES\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nPIQA-eu examples look like this:\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/PIQA-eu.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","natural-language-inference","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"XCOPA-eu","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/XCOPA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\n\t\n\t\t\n\t\tDataset Card for XCOPA-eu\n\t\n\n\nPoint of Contact: hitz@ehu.eus\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nXCOPA-eu is the professional translation to Basque of the English COPA dataset (Roemmmele et al., 2011),\nin the spirit of the XCOPA effort (Ponti et al., 2020). \nCOPA is a dataset of causal commmonsense reasoning that focuses on cause-effect relationships between a\npremise and two choices.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\n\neu-ES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/XCOPA-eu.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","natural-language-inference","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"truthful_qa-cs","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\n\t\n\t\t\n\t\tCzech TruthfulQA\n\t\n\nThis is a Czech translation of the original TruthfulQA dataset, created using the WMT 21 En-X model. \nOnly the multiple-choice variant of the dataset is included.\nThe translation was completed for use within the Czech-Bench evaluation framework. \nThe script used for translation can be reviewed here.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nOriginal dataset:\n@misc{lin2021truthfulqa,\n    title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},\n    author={Stephanie Lin and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Czech","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"arc-cs","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/arc-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\n\t\n\t\t\n\t\tCzech AI2 Reasoning Challenge\n\t\n\nThis is a Czech translation of the original ARC dataset, created using the WMT 21 En-X model.\nThe translation was completed for use within the Czech-Bench evaluation framework. \nThe script used for translation can be reviewed here.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nOriginal dataset:\n@article{allenai:arc,\n      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\n                    Ashish Sabharwal and Carissa Schoenick and Oyvind‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/arc-cs.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Czech","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mmlu-cs","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","description":"\n\t\n\t\t\n\t\tCzech MMLU\n\t\n\nThis is a Czech translation of the original MMLU dataset, created using the WMT 21 En-X model. \nThe 'auxiliary_train' subset is not included.\nThe translation was completed for use within the Czech-Bench evaluation framework. \nThe script used for translation can be reviewed here.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nOriginal dataset:\n@article{hendryckstest2021,\n  title={Measuring Massive Multitask Language Understanding},\n  author={Dan Hendrycks and Collin Burns and Steven Basart and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Czech","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"xstorycloze_gl","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/proxectonos/xstorycloze_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","description":"\n\t\n\t\t\n\t\tDataset Card for xstorycloze_gl\n\t\n\n\n\nxstorycloze_gl is a question answering dataset in Galician, translated from the English StoryCloze dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nxstorycloze_gl is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1511 instances in the test split. Each instance contains a story stem, divided in 4 sentences, 2 possible completions, and the number indicating the correct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/xstorycloze_gl.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Galician","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"teleia","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gonzmart/teleia","creator_name":"Gonzalo","creator_url":"https://huggingface.co/gonzmart","description":"\n\t\n\t\t\n\t\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\n\t\n\n\n\t\n\t\t\n\t\tAuthors and Affiliations\n\t\n\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\n\nUniversidad Aut√≥noma de Madrid\nUniversidad Polit√©cnica de Madrid\nNew York University\nUniversidad de Valladolid\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset contains test questions to evaluate LLMs in Spanish\n\nTELEIA_Cervantes_AVE.xlsx: test questions on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gonzmart/teleia.","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"commonsense-embodied-affordance","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ayush8120/commonsense-embodied-affordance","creator_name":"Ayush Agrawal","creator_url":"https://huggingface.co/Ayush8120","description":"\n\t\n\t\t\n\t\n\t\n\t\tPlease refer here for the documentation\n\t\n\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"ECN-QA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/raidium/ECN-QA","creator_name":"Raidium","creator_url":"https://huggingface.co/raidium","description":"\n\t\n\t\t\n\t\tModel Card for Raidium ECN-QA\n\t\n\nThe dataset is introduced in the paper \"Efficient Medical Question Answering with Knowledge-Augmented Question Generation\".\nPaper: https://arxiv.org/abs/2405.14654\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset contains medical questions of different types. It was built from passed ECN exams (french medical examination) and questions created by FreeCN.\nThe questions can be:\n\nIQ (individual question) containing a question and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raidium/ECN-QA.","first_N":5,"first_N_keywords":["multiple-choice","French","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"histoires_morales","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LabHC/histoires_morales","creator_name":"Laboratoire Hubert Curien","creator_url":"https://huggingface.co/LabHC","description":"Together with the Moral Stories dataset, Histoires Morales can be used for:\n\nCommonsense reasoning / social reasoning / moral reasoning The dataset can help evaluate whether pretrained language models can reason about actions that are consistent or inconsistent with social norms, the consequences of actions, and the norms that may motivate those actions. A Mistral model or Mistral-Instruct can be used for this purpose.\n\nText classification This dataset can be used to train models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LabHC/histoires_morales.","first_N":5,"first_N_keywords":["text-classification","multiple-choice","text-generation","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"PubmedQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tDataset Card for pubmedqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the pubmedqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the PubMedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MedQA-Mixtral-CoT","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tDataset Card for medqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the medqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the MedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ReDis-QA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guan-wang/ReDis-QA","creator_name":"Guanchu","creator_url":"https://huggingface.co/guan-wang","description":"\n\t\n\t\t\n\t\tDataset Card for ReDis-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nReDis-QA dataset contains 1360 multi-choice questions focusing on rare disease diagnosis.\nIt consists of 11%, 33%, 13%, 15%, 18% of the questions corresponding to the symptoms, causes, affects, related-disorders, diagnosis of rare diseases, respectively. \nThe remaining 9% of the questions pertain to other properties of the diseases.\n\n\nReDis-QA dataset widely covers 205 types of rare diseases, where the most frequent disease‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guan-wang/ReDis-QA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"famma","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weaverbirdllm/famma","creator_name":"weaverbird_llm","creator_url":"https://huggingface.co/weaverbirdllm","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nFAMMA is a multi-modal financial Q&A benchmark dataset. The questions encompass three heterogeneous image types - tables, charts and text & math screenshots - and span eight subfields in finance, comprehensively covering topics across major asset classes. Additionally, all the questions are categorized by three difficulty levels ‚Äî easy, medium, and hard - and are available in three languages ‚Äî English, Chinese, and French. Furthermore, the questions are divided into two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weaverbirdllm/famma.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","table-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"teleia","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/migonsa/teleia","creator_name":"Miguel Gonz√°lez","creator_url":"https://huggingface.co/migonsa","description":"\n\t\n\t\t\n\t\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\n\t\n\n\n\t\n\t\t\n\t\tAuthors and Affiliations\n\t\n\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\n\nUniversidad Aut√≥noma de Madrid\nUniversidad Polit√©cnica de Madrid\nNew York University\nUniversidad de Valladolid\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset contains test questions to evaluate LLMs in Spanish\n\nTELEIA_Cervantes_AVE: test questions on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/migonsa/teleia.","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CRAFT-BioQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-BioQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated biology question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA.","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"CRAFT-MedQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-MedQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated medicine question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA.","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"CRAFT-CommonSenseQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tCRAFT-CommonSenseQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated commonsense question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA.","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"nifty-rl","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raeidsaqur/nifty-rl","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","description":"\n  \n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \n\n\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\n\n\t\n\t\t\n\t\n\t\n\t\tüìã Table of Contents\n\t\n\n\nüß© NIFTY Dataset\nüìã Table of Contents\nüìñ Usage\nDownloading the dataset\nDataset structure\n\n\nLarge Language Models \n‚úçÔ∏è Contributing\nüìù Citing\nüôè Acknowledgements\n\n\n\n\n\t\n\t\t\n\t\tüìñ Usage\n\t\n\nDownloading and using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/nifty-rl.","first_N":5,"first_N_keywords":["multiple-choice","time-series-forecasting","document-question-answering","topic-classification","semantic-similarity-classification"],"keywords_longer_than_N":true},
	{"name":"MME-RealWorld","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/MME-RealWorld","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n2024.11.14 üåü MME-RealWorld now has a lite version (50 samples per task) for inference acceleration, which is also supported by VLMEvalKit and Lmms-eval.\n2024.10.27 üåü LLaVA-OV currently ranks first on our leaderboard, but its overall accuracy remains below 55%, see our leaderboard for the detail.\n2024.09.03 üåü MME-RealWorld is now supported in the VLMEvalKit and Lmms-eval repository, enabling one-click evaluation‚Äîgive it a try!\" \n2024.08.20 üåü We are very proud to launch MME-RealWorld, which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MME-RealWorld.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"uhura-arc-easy","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masakhane/uhura-arc-easy","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-Arc-Easy\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nUhura-ARC-Easy is a widely recognized scientific question answering benchmark composed of multiple-choice science questions derived from grade-school examinations that test various styles of knowledge and reasoning. \nThe original English version of the benchmark originates from Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark et al., 2018) and is divided into \"Challenge\" and \"Easy\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-arc-easy.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","multilingual","Amharic"],"keywords_longer_than_N":true},
	{"name":"MMMU_Pro","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMU/MMMU_Pro","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\n\t\n\t\t\n\t\tMMMU-Pro (A More Robust Multi-discipline Multimodal Understanding Benchmark)\n\t\n\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüõ†Ô∏èüõ†Ô∏è [2025-03-08] Fixed mismatch between inner image labels and shuffled options in Vision and Standard (10 options) settings. (test_Chemistry_5,94,147,216,314,345,354,461,560,570; test_Materials_450; test_Pharmacy_198; validation_Chemistry_12,26,29; validation_Materials_10,28; validation_Psychology_1)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU_Pro.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"bento","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cindermond/bento","creator_name":"Hongyu Zhao","creator_url":"https://huggingface.co/cindermond","description":"This dataset is based on MMLU, FLAN, Big Bench Hard and AgiEval English.\nThe non-\"reduced\" benchmark is the original benchmark, except for FLAN, which is a sampled version. \nThe \"reduced\" benchmark only contains a few representative tasks in the original ones, such that the performance on the \"reduced\" benchmark can serve as an approximation to the performance on the original ones.\n","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"uhura-truthfulqa","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masakhane/uhura-truthfulqa","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-TruthfulQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a widely recognized safety benchmark designed to measure the truthfulness of language model outputs across 38 categories, including health, law, finance, and politics. The English version of the benchmark originates from TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2022) and consists of 817 questions in both multiple-choice and generation formats, targeting common misconceptions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-truthfulqa.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-generation","multiple-choice-qa","multilingual"],"keywords_longer_than_N":true},
	{"name":"Yue-Benchmark","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BillBao/Yue-Benchmark","creator_name":"Bao","creator_url":"https://huggingface.co/BillBao","description":"\n\t\n\t\t\n\t\tHow Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models\n\t\n\n\nHomepage: https://github.com/jiangjyjy/Yue-Benchmark\nRepository: https://huggingface.co/datasets/BillBao/Yue-Benchmark\nPaper: How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models.\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThe rapid evolution of large language models (LLMs), such as GPT-X and Llama-X, has driven significant advancements in NLP, yet much of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BillBao/Yue-Benchmark.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","translation","Yue Chinese","multilingual"],"keywords_longer_than_N":true},
	{"name":"univ_exams_finnish","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/readd/univ_exams_finnish","creator_name":"Perttu Isotalo","creator_url":"https://huggingface.co/readd","description":"readd/univ_exams_finnish dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Finnish","cc-by-sa-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"BiomixQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kg-rag/BiomixQA","creator_name":"KG-RAG","creator_url":"https://huggingface.co/kg-rag","description":"\n\t\n\t\t\n\t\tBiomixQA Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nBiomixQA is a curated biomedical question-answering dataset comprising two distinct components:\n\nMultiple Choice Questions (MCQ)\nTrue/False Questions\n\nThis dataset has been utilized to validate the Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) framework across different Large Language Models (LLMs). The diverse nature of questions in this dataset, spanning multiple choice and true/false formats, along with its coverage of various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kg-rag/BiomixQA.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"danish-citizenzhip-test-mcq","keyword":"multiple-choice","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq","creator_name":"Tellarin.ai","creator_url":"https://huggingface.co/tellarin-ai","description":"\n\t\n\t\t\n\t\tDataset Card for \"danish-citizen-test-mcq\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset covers Danish tests for both citizenship (\"indf√∏dsretspr√∏ven\") and permanent residence (\"medborgerskabspr√∏ven\"), from 2016-2024.\nData follows the Aya Expedition format for global exams. Only unique questions between exams are kept.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is available in Danish (da).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nAn example from the dataset looks as follows.\n{\n\"language\": \"da\",\n\"country\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Danish","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"openbookqa-es","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BSC-LT/openbookqa-es","creator_name":"Language Technologies Unit @ Barcelona Supercomputing Center","creator_url":"https://huggingface.co/BSC-LT","description":"\n\t\n\t\t\n\t\tDataset Card for openbookqa_es\n\t\n\n\n\nopenbookqa_es is a question answering dataset in Spanish, professionally translated from the main version of the OpenBookQA dataset in English. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nopenbookqa_es (Open Book Question Answering - Spanish) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BSC-LT/openbookqa-es.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Spanish","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SemiEvol","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/SemiEvol","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThe SemiEvol dataset is part of the broader work on semi-supervised fine-tuning for Large Language Models (LLMs). The dataset includes labeled and unlabeled data splits designed to enhance the reasoning capabilities of LLMs through a bi-level knowledge propagation and selection framework, as proposed in the paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/SemiEvol.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"hellasigma","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pszemraj/hellasigma","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","description":"\n\t\n\t\t\n\t\thellasigma\n\t\n\n\n[!IMPORTANT]\nThis is an initial proof of concept and only contains 190 examples. Still, it seems to be able to tease out differences especially in 7b+ models. I've run some initial evals here\n\nMany evaluation datasets focus on a single correct answer to see if the model is \"smart.\" What about when there's no right answer? HellaSigma is an \"eval\" dataset to probe at what your model's personality type may be. Is it a Sigma, or not?\nThis dataset contains generic scenarios‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/hellasigma.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Tin_hoc_mcq_extended","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kamisaiko/Tin_hoc_mcq_extended","creator_name":"NGUYEN VIET TRUNG","creator_url":"https://huggingface.co/kamisaiko","description":"kamisaiko/Tin_hoc_mcq_extended dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["multiple-choice","Vietnamese","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"A newer version of this dataset is available.\nhttps://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1\n\n\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"JMMMU","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JMMMU/JMMMU","creator_name":"JMMMU","creator_url":"https://huggingface.co/JMMMU","description":"\n\t\n\t\t\n\t\tJMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark\n\t\n\nüåê Homepage | ü§ó Dataset | üèÜ HF Leaderboard | üìñ arXiv | üíª Code\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nWe introduce JMMMU (Japanese MMMU), a multimodal benchmark that can truly evaluate LMM performance in Japanese. To create JMMMU, we first carefully analyzed the existing MMMU benchmark and examined its cultural dependencies. Then, for questions in culture-agnostic subjects, we employed native Japanese speakers who‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JMMMU/JMMMU.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"MMMU-Thai","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iapp/MMMU-Thai","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","description":"\n\t\n\t\t\n\t\tMMMU Thai (MMMU Benchmark Translated to Thai)\n\t\n\nMMMU Thai is a dataset for evaluating multimodal models on massive multi-discipline tasks requiring college-level knowledge and deliberate reasoning. This dataset is translated from MMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI) into Thai.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nMMMU Thai consists of 11,500 meticulously collected multimodal questions from college exams, quizzes, and textbooks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/MMMU-Thai.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","Thai","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"include-lite-44","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereLabs/include-lite-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","description":"\n\t\n\t\t\n\t\tINCLUDE-lite (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-lite-44.","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"mmevalpro","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMEVAL/mmevalpro","creator_name":"MMEVAL","creator_url":"https://huggingface.co/MMEVAL","description":"MMEvalPro\n\n\n\n\t\n\t\t\n\t\tDataset Card for MMEvalPro\n\t\n\nWe create MMEvalPro for more accurate and efficent evaluation for Large Multimodal Models. It is designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process.\n\n\t\n\t\t\n\t\n\t\n\t\tData Format\n\t\n\n{\n    \"index\": [int64] The global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMEVAL/mmevalpro.","first_N":5,"first_N_keywords":["multiple-choice","English","Chinese","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"include-base-44","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereLabs/include-base-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","description":"\n\t\n\t\t\n\t\tINCLUDE-base (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-base-44.","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"DynaMath_Sample","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DynaMath/DynaMath_Sample","creator_name":"DynaMath Team","creator_url":"https://huggingface.co/DynaMath","description":"\n\t\n\t\t\n\t\tDataset Card for DynaMath\n\t\n\n\n\n[üíª Github] [üåê Homepage][üìñ Preprint Paper]\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tüîà Notice\n\t\n\nDynaMath is a dynamic benchmark with 501 seed question generators. This dataset is only a sample of 10 variants generated by DynaMath. We encourage you to use the dataset generator on our github site to generate random datasets to test.\n\n\t\n\t\t\n\t\tüåü About DynaMath\n\t\n\nThe rapid advancements in Vision-Language Models (VLMs) have shown significant potential in tackling‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DynaMath/DynaMath_Sample.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-xmmmu","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-xmmmu","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"neulab/PangeaBench-xmmmu dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","Arabic","French"],"keywords_longer_than_N":true},
	{"name":"MedS-Ins","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tHPAI-BSC MedS-Ins\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mc-translation","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/mc-translation","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"This dataset contains professional human translations from OpenAI's MMMLU dataset, repurposed to train translation models that can help translate future evaluation datasets.\n\n\t\n\t\t\n\t\tWhy This Dataset?\n\t\n\nTranslation of evaluation benchmarks is a critical but challenging task. While automated translations may introduce errors or biases, professional human translations are expensive and time-consuming. This dataset leverages existing professional translations (MMMLU) to train specialized‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/mc-translation.","first_N":5,"first_N_keywords":["translation","English","Swahili","Spanish","German"],"keywords_longer_than_N":true},
	{"name":"Cultural-Emo","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-for-emotion/Cultural-Emo","creator_name":"LLM for Emotion","creator_url":"https://huggingface.co/llm-for-emotion","description":"\n\t\n\t\t\n\t\tCulEmo\n\t\n\nCultural Lenses on Emotion (CuLEmo) is the first benchmark to evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. \nIt comprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. It is designed for evaluating LLMs in Sentiment analysis and emotion prediction.\nIf you use this dataset, cite the paper below.\n\n\t\n\t\t\n\t\n\t\n\t\tBibTeX entry and citation info.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-for-emotion/Cultural-Emo.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"LiveXiv","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv (ICLR 2025)\nhttps://arxiv.org/abs/2410.10783\n\n\t\n\t\t\n\t\tLiveXiv Leaderboard\n\t\n\n\n\t\n\t\t\nModel\nV0 VQA\nV0 TQA\nV1 VQA\nV1 TQA\nV2 VQA\nV2 TQA\nV3 VQA\nV3 TQA\nV4 VQA\nV4 TQA\n\n\n\t\t\nClaude-Sonnet\n0.75\n0.81\n0.75\n0.84\n0.78\n0.82\n0.78\n0.84\n0.80\n0.78\n\n\nQwen2-VL-7B\n0.67\n0.58\n0.67\n0.60\n0.68\n0.58\n0.69\n0.67\n0.71\n0.53\n\n\nPixtral\n-\n-\n-\n-\n0.73\n0.59\n0.71\n0.39\n0.73\n0.55\n\n\nInternVL2-8B\n0.62\n0.62\n0.62\n0.62\n0.64\n0.60\n0.65\n0.69\n0.67\n0.57\nGPT4o\n0.51\n0.48\n0.60\n0.55\n0.59\n0.49‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LiveXiv/LiveXiv.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","table-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"multiple-choice","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"big_bench_hard","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Joschka/big_bench_hard","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","description":"All rights and obligations of the dataset are with original authors of the paper/dataset.\nI have merely made this dataset with a MIT licence available on HuggingFace.\n\n\t\n\t\t\n\t\tBIG-Bench Hard Dataset\n\t\n\nThis repository contains a copy of the BIG-Bench Hard dataset.\nSmall edits to the formatting of the dataset are made to integrate it into the Inspect Evals repository, a community contributed LLM\nevaulations for Inspect AI a framework by the UK AI Safety Institute.\nThe BIG-Bench Hard dataset is a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/big_bench_hard.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"MRAG-Bench","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench","creator_name":"mragbenchanonymous","creator_url":"https://huggingface.co/mragbenchanonymous","description":"\n\t\n\t\t\n\t\tICLR 2025 Submision 9148\n\t\n\n\n\t\n\t\t\n\t\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\n\t\n\n\n\t\n\t\t\n\t\tThis is an anonymous repo for openreview https://openreview.net/forum?id=Usklli4gMc\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset contains the following fields:\n\n\t\n\t\t\nField Name\nDescription\n\n\n\t\t\nid\nUnique identifier for the example\n\n\naspect\nAspect type for the example\n\n\nscenario\nThe type of scenario associated with the entry\n\n\nimage\nContains image data in byte‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"legalbench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DatologyAI/legalbench","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task preserves its original data and includes an additional input column with a formatted prompt, generated using the LegalBench registry, ready to be fed directly into language models.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\n\nBasic Legal: canada_tax_court_outcomes, jcrew_blocker, learned_hands_benefits, telemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"microvqa","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmhb/microvqa","creator_name":"James Burgess","creator_url":"https://huggingface.co/jmhb","description":"MicroVQA:  A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research (CVPR 2025)\n\n\n \n  \n üåê Homepage / blog ‚Ä¢\n üìù arXiv ‚Ä¢\n ü§ó HF Dataset ‚Ä¢\nüíª  Code ‚Ä¢\n üèõ CC-BY-SA-4.0\n\n\nMicroVQA is expert-curated benchmark for multimodal reasoning for microscopy-based scientific research, proposed in the paper MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research.\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tPaper abstract\n\t\n\nScientific research demands sophisticated reasoning over multimodal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhb/microvqa.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"msc-memfuse-mc10","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Percena/msc-memfuse-mc10","creator_name":"Percena","creator_url":"https://huggingface.co/Percena","description":"\n\t\n\t\t\n\t\tMSC‚ÄëMemFuse‚ÄëMC10 ¬∑ Multi-Session Chat Memory QA (10-way Multiple Choice)\n\t\n\nMSC‚ÄëMemFuse‚ÄëMC10 is a 500 example benchmark derived from Multi-Session Chat (MSC) and MemGPT‚Äôs MSC-Self-Instruct, modified and extended by the MemFuse team.Each item is a 10-option multiple-choice question probing information embedded within multi-session conversational history. The questions test episodic memory: facts must be inferred from prior dialogue, not static personas.\nThe dataset follows OpenAI's‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Percena/msc-memfuse-mc10.","first_N":5,"first_N_keywords":["question-answering","expert-generated","machine-generated","MemGPT/MSC-Self-Instruct","English"],"keywords_longer_than_N":true},
	{"name":"PM4Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/songjhPKU/PM4Bench","creator_name":"Jiahe Song","creator_url":"https://huggingface.co/songjhPKU","description":"\n\t\n\t\t\n\t\tPM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model\n\t\n\n\n\n\nüåê Homepage | ü§ó Dataset | üìñ Paper \n\n\t\n\t\t\n\t\tüì¢ News\n\t\n\n\nüî•[2025-03-25]: Dataset available on HuggingFace. Paper available on  arXiv.\n\n\n\n\t\n\t\t\n\t\tüßë‚Äçüíª How to Run?\n\t\n\n\n\n\t\n\t\t\n\t\tüè† Set Up\n\t\n\n\n\t\n\t\t\n\t\tDataset Download\n\t\n\nDownload tsv files from HuggingFace and store them in data/tsv/. The directory should be like data/tsv/{DATASET}_{SETTING}_{LANGUAGE}.tsv.\n\n\t\n\t\t\n\t\tEnvironment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songjhPKU/PM4Bench.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"MedQA-USMLE-back-translated","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Detsutut/MedQA-USMLE-back-translated","creator_name":"Tommaso Mario Buonocore","creator_url":"https://huggingface.co/Detsutut","description":"MedQA dataset perturbed using back-translation technique with BAT\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"BIBLE","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MushroomGecko/BIBLE","creator_name":"Devon","creator_url":"https://huggingface.co/MushroomGecko","description":"\n\t\n\t\t\n\t\tBIBLE: Biblically Informed Bot Learning Evaluation\n\t\n\nBIBLE (Biblically Informed Bot Learning Evaluation) is a comprehensive benchmark dataset designed to evaluate AI models on their understanding of the Holy Bible. It covers all 66 books of Scripture and includes additional thematic categories for People of the Bible, Places in the Bible, and Measurements in the Bible.\n\n‚ö†Ô∏è This dataset is not intended for training. It is strictly for evaluation and benchmarking of models on Biblical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushroomGecko/BIBLE.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MedRisk-Bench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jinge13288/MedRisk-Bench","creator_name":"Jinge Wu","creator_url":"https://huggingface.co/jinge13288","description":"MedRisk Benchmark is used for medical risk assessment. \nThe Benchmark is made up with two version with 1232 test samples for each:\nMedRisk-Quantitative: which focuses on the score caculation for medical risk prediction.\nMedRisk-Qualitative: which focuses on the severity for medical condition/disease.\nFull data release can be found at Github\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"CharToM-QA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroXeno/CharToM-QA","creator_name":"Chulun Zhou","creator_url":"https://huggingface.co/ZeroXeno","description":"\n\t\n\t\t\n\t\tDataset Card for CharToM-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nCharToM-QA is a benchmark introduced in the paper The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters. It comprises 1,035 Theory of Mind (ToM) questions based on characters from classic novels. The benchmark is designed to evaluate ToM-related question-answering (QA) capabilities about characters in the context of novels. In CharToM-QA, the task takes the form of ToM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroXeno/CharToM-QA.","first_N":5,"first_N_keywords":["text-generation","text-classification","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"WeatherQA_SFT","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhanxiangHua/WeatherQA_SFT","creator_name":"Zhanxiang Hua","creator_url":"https://huggingface.co/ZhanxiangHua","description":"WeatherQA_SFT is a supervised fine-tuning (SFT) dataset in sharegpt format derived from the WeatherQA dataset for vision/multimodal language models. It focuses on severe weather geo-localization and potential hazard analysis over the Contiguous United States (CONUS).\nThe dataset contains over 7,000 image-text pairs. Including a 'train' set from year 2014-2019 and 'test' set for the year 2020. The images encapsulate complex weather patterns, including:\n\nEnvironmental instability parameters‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhanxiangHua/WeatherQA_SFT.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"dutch-central-exam-mcq","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq","creator_name":"Mike Zhang","creator_url":"https://huggingface.co/jjzha","description":"Multiple Choice Questions of the Dutch Central Exam 1999-2024\n\nImportant Note\n\nPer 01 Apr. 2025: This data is now split accordingly to the benchmark test sets of:\nhttps://huggingface.co/datasets/CohereForAI/include-base-44\nhttps://huggingface.co/datasets/CohereForAI/include-lite-44\nAny question that appears in the development or test set of the INCLUDE benchmark are now in a separate split. \nWe put both the base and lite questions in one test split.\nWe suggest using the INCLUDE benchmark for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq.","first_N":5,"first_N_keywords":["multiple-choice","Dutch","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"CHOICE","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","description":"\n\t\n\t\t\n\t\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\n\t\n\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MME-Unify","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wulin222/MME-Unify","creator_name":"xie","creator_url":"https://huggingface.co/wulin222","description":"\n2024.08.20 üåü We are proud to open-source MME-Unify, a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our Benchmark covers 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\n\nPaper: https://arxiv.org/abs/2504.03641\nCode: https://github.com/MME-Benchmarks/MME-Unify\nProject page: https://mme-unify.github.io/\n\n\n\t\n\t\n\t\n\t\tHow to use?\n\t\n\nYou can download images in this repository and the final structure should look like this:\nMME-Unify‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wulin222/MME-Unify.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"IneqMath","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/IneqMath","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\n  \n\n  \n\n  \n    Solving Inequality Proofs with Large Language Models\n  \n\n  \n\n  \n    \n      üåê Project\n    \n    |\n    \n       arXiv\n    \n    |\n    \n      HF Paper\n    \n    |\n    \n       Github\n    \n    |\n    \n      üèÜ Leaderboard\n    \n    |\n    \n      üîÆ Visualization\n    \n  \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nInequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategically applying theorems. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/IneqMath.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-retrieval","text-classification","English"],"keywords_longer_than_N":true},
	{"name":"Melange_test","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IDfree/Melange_test","creator_name":"no_ID","creator_url":"https://huggingface.co/IDfree","description":"\n\t\n\t\t\n\t\tDataset Name\n\t\n\nShort summary of what this dataset contains.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA longer explanation of the dataset, including its purpose and contents.\nThis dataset consists of:\n\nA .parquet file with metadata and labels\nScene images organized in zipped folders by group\nEach row in the metadata corresponds to a multiple-choice question grounded in one or more scene images.\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be used for:\n\nVisual question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IDfree/Melange_test.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","visual-question-answering","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Synthetic","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\" (COLM 2025).\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Train","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"MCEval8K","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","description":"\n\t\n\t\t\n\t\tMCEval8K\n\t\n\nMCEval8K is a diverse multiple-choice evaluation benchmark for probing language models‚Äô (LMs) understanding of a broad range of language skills using neuron-level analysis. \nIt was introduced in the ACL 2025 paper - \"Neuron Empirical Gradient: Discovering and Quantifying Neurons‚Äô Global Linear Controllability\".\n\n\t\n\t\t\n\t\tüîç Overview\n\t\n\nMCEval8K consists of 22 tasks grouped into six skill genres, covering linguistic analysis, content classification, reasoning, factuality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iszhaoxin/MCEval8K.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","natural-language-inference","acceptability-classification"],"keywords_longer_than_N":true},
	{"name":"mmlu-translated-kk","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kz-transformers/mmlu-translated-kk","creator_name":"Kaz-Transformers","creator_url":"https://huggingface.co/kz-transformers","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite:\n@misc{horde_mmlu_kk2024,\n  author = {Beksultan Sagyndyk, Sanzhar Murzakhmetov, Sanzhar Umbet, Kirill Yakunin},\n  title = {MMLU on kazakh language: Translated MMLU Benchmark},\n  year = {2024},\n  url = {https://huggingface.co/datasets/mmlu-translated-kk},\n  note = {Available on Hugging Face}\n}\n\n","first_N":5,"first_N_keywords":["multiple-choice","Kazakh","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"aya-mm-exams-spanish-nursing","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-nursing","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","description":"Nursing Spanish Exams for the Multimodal Aya Exams Projects.\nQuestions available in file: data.json\nImages stored in: /images\nOriginal data and file available here: link\n","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FinShibainu","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aiqwe/FinShibainu","creator_name":"Jay Lee","creator_url":"https://huggingface.co/aiqwe","description":"\n\t\n\t\t\n\t\tFinShibainu Datset Card\n\t\n\n\ngithub: https://github.com/aiqwe/FinShibainu\nmodel: https://huggingface.co/aiqwe/FinShibainu\n\nKRX LLM Í≤ΩÏßÑÎåÄÌöå Î¶¨ÎçîÎ≥¥ÎìúÏóêÏÑú Ïö∞ÏàòÏÉÅÏùÑ ÏàòÏÉÅÌïú shibainu24 Î™®Îç∏Ïùò Îç∞Ïù¥ÌÑ∞ÏÖã RepositoryÏûÖÎãàÎã§.Î™®Îç∏Ïóê ÎåÄÌïú ÎÇ¥Ïö©ÏùÄ https://huggingface.co/aiqwe/FinShibainuÎ•º Ï∞∏Ï°∞Ìï¥Ï£ºÏÑ∏Ïöî.Îç∞Ïù¥ÌÑ∞ÏÖã ÏàòÏßë Î∞è ÌïôÏäµÏóê Í¥ÄÎ†®Îêú ÏΩîÎìúÎäî https://github.com/aiqwe/FinShibainuÏóê ÏûêÏÑ∏ÌïòÍ≤å Í≥µÍ∞úÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n\t\n\t\t\n\t\n\t\n\t\tDPO\n\t\n\nPreferenceÏùò AÎäî answer_A, BÎäî answer_B Ïª¨ÎüºÏûÖÎãàÎã§.\n\nanswer_A: ReferenceÏôÄ ÏßàÎ¨∏ÏùÑ Ìï®Íªò Ï†úÍ≥µÎ∞õÏùÄ gpt ÎãµÎ≥Ä. ReferenceÏóê ÏùòÏ°¥Ï†ÅÏù¥Í≥† ÏßßÏßÄÎßå Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï®\nanswer_B: ReferenceÏóÜÏù¥ ÏßàÎ¨∏Îßå Ï†úÍ≥µÎ∞õÏùÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aiqwe/FinShibainu.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Cultural-Evaluation-Kalahi","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aisingapore/Cultural-Evaluation-Kalahi","creator_name":"AI Singapore","creator_url":"https://huggingface.co/aisingapore","description":"\n\t\n\t\t\n\t\tKalahi\n\t\n\nKalahi evaluates the ability of LLMs to generate responses relevant to Filipino culture in terms of shared knowledge and ethics. This dataset contains a MCQ-compatible version of the Kalahi dataset that is used in SEA-HELM.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nKalahi is designed for evaluating Filipino cultural representations in instruction-tuned large language models (LLMs). It is part of the SEA-HELM leaderboard from AI Singapore.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nTagalog (tl)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aisingapore/Cultural-Evaluation-Kalahi.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","Tagalog","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"testing_arc_easy_de","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/testing_arc_easy_de","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\n\t\n\t\t\n\t\ttesting_arc_easy_de\n\t\n\nThis is a copy of the dataset openGPT-X/arcx. \n","first_N":5,"first_N_keywords":["multiple-choice","question-answering","German","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"truthful-qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rahmanidashti/truthful-qa","creator_name":"Hossein A. (Saeed) Rahmani","creator_url":"https://huggingface.co/rahmanidashti","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 790 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rahmanidashti/truthful-qa.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Please‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","image-to-text","visual-question-answering","language-modeling"],"keywords_longer_than_N":true},
	{"name":"head_qa_v2","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alesi12/head_qa_v2","creator_name":"Alexis Correa Guillen","creator_url":"https://huggingface.co/alesi12","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHEAD-QA v2 is an updated version of the HEAD-QA dataset, which is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio de Sanidad, Consumo y Bienestar Social, who also provides direct access to the exams of the last 5 years (in Spanish).\nHEAD-QA V2 expands on the original dataset by including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alesi12/head_qa_v2.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","Spanish","English","Galician"],"keywords_longer_than_N":true},
	{"name":"MM-IQ","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","description":"\n\t\n\t\t\n\t\tDataset Card for \"MM-IQ\"\n\t\n\n\nIntroduction\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nAutomatic Evaluation\n\n\nCitation\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"arc_challenge_de_fixed","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TDN007/arc_challenge_de_fixed","creator_name":"Nguyen","creator_url":"https://huggingface.co/TDN007","description":"\n\t\n\t\t\n\t\tARC Challenge DE fixed\n\t\n\nThis is a copy of the dataset LeoLM/ArcChallenge_de which is a German translation of the original allenai/ai2_arc. \nBug fixesWe copied it to fix the numeric labels 1,2,3,4 into A,B,C,D. \nThanksThe translation of the arc_challenge was done by Bjoern: https://github.com/bjoernpl/GermanBenchmark\n","first_N":5,"first_N_keywords":["multiple-choice","question-answering","German","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"24-game","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\n\t\n\t\t\n\t\tMath Twenty Four (24s Game) Dataset\n\t\n\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, √ó, /) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","other","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"CupCase","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ofir408/CupCase","creator_name":"ofir ben shoham","creator_url":"https://huggingface.co/ofir408","description":"\n\t\n\t\t\n\t\tDataset Card for CUPCase\n\t\n\n\n\nCUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset\n(AAAI 2025)\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedical benchmark datasets significantly contribute to devel-\noping Large Language Models (LLMs) for medical knowl-\nedge extraction, diagnosis, summarization, and other uses.\nYet, current benchmarks are mainly derived from exam ques-\ntions given to medical students or cases described in the med-\nical literature, lacking the complexity of real-world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ofir408/CupCase.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"Taiwan-Curlture-MCQ","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aqweteddy/Taiwan-Curlture-MCQ","creator_name":"aqweteddy","creator_url":"https://huggingface.co/aqweteddy","description":"\n\n\t\n\t\t\n\t\tTW-Curlture-MCQ\n\t\n\nTW-Curlture-MCQ ÊòØ‰∏ÄÂÄãË©ïÈáèÂè∞ÁÅ£ÊñáÂåñÁöÑÈÅ∏ÊìáÈ°åË≥áÊñôÈõÜÔºå‰∏ªË¶Å‰æÜËá™‰ª•‰∏ãÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÁØ©ÈÅ∏ËàáÊï¥ÂêàÔºö\n\nTMLU\nTMMLU+\n\n‰ª•‰∫∫Â∑•ÊåëÈÅ∏ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÁöÑÁßëÁõÆÂæåÔºåÂÜçÁî± gpt-4o-mini Âà§Êñ∑ÂïèÈ°åÊòØÂê¶ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÔºåÂÖ± 3828 È°å„ÄÇ\n","first_N":5,"first_N_keywords":["multiple-choice","Chinese","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MapEval-Textual","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-Textual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\n\t\n\t\t\n\t\tMapEval-Textual\n\t\n\nMapEval-Textual is created using MapQaTor.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load dataset\nds = load_dataset(\"MapEval/MapEval-Textual\", name=\"benchmark\")\n\n# Generate better prompts\nfor item in ds[\"test\"]:\n    # Start with a clear task description\n    prompt = (\n        \"You are a highly intelligent assistant. \"\n        \"Based on the given context, answer the multiple-choice question by selecting the correct option.\\n\\n\"\n        \"Context:\\n\" +‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Textual.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","expert-generated","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"aya-mm-exams-spanish-medical","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-medical","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","description":"Medical Spanish Exams for the Multimodal Aya Exams Projects. \n  Questions available in file: data.json \n  Images stored in: /images\nOriginal data and file available here: link\n","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nüåê Project Page: https://longbench2.github.io\nüíª Github Repo: https://github.com/THUDM/LongBench\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"YOLOv8-Multi-Instance-Object-Detection-Dataset","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/duality-robotics/YOLOv8-Multi-Instance-Object-Detection-Dataset","creator_name":"Duality AI","creator_url":"https://huggingface.co/duality-robotics","description":"\n\t\n\t\t\n\t\tMulti Instance Object Detection Dataset Sample\n\t\n\n Duality.ai just released a 1000 image dataset used to train a YOLOv8 model for object detection -- and it's 100% free!\n Just create an EDU account here. \nThis HuggingFace dataset is a 20 image and label sample, but you can get the rest at no cost by creating a FalconCloud account. Once you verify your email, the link will redirect you to the dataset page.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset consists of high-quality images of soup‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duality-robotics/YOLOv8-Multi-Instance-Object-Detection-Dataset.","first_N":5,"first_N_keywords":["object-detection","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PLM-Video-Human","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/PLM-Video-Human","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tDataset Card for PLM-Video Human\n\t\n\nPLM-Video-Human is a collection of human-annotated resources for training Vision Language Models,\nfocused on detailed video understanding. Training tasks include: fine-grained open-ended question answering (FGQA), Region-based Video Captioning (RCap),\nRegion-based Dense Video Captioning (RDCap) and Region-based Temporal Localization (RTLoc).\n[üìÉ Tech Report]\n[üìÇ Github]\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\n\t\n\t\tFine-Grained Question Answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/PLM-Video-Human.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","other","other","English"],"keywords_longer_than_N":true},
	{"name":"ToM-in-AMC","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ShunchiZhang/ToM-in-AMC","creator_name":"Shunchi Zhang","creator_url":"https://huggingface.co/ShunchiZhang","description":"\n\t\n\t\t\n\t\tDataset Card for ToM-in-AMC\n\t\n\nThe dataset consists of ‚àº1,000 parsed movie scripts from IMSDb, each corresponding to a character understanding task.\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nBibTeX:\n@inproceedings{yu2024few,\n  title = {Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind},\n  author = {Yu, Mo and Wang, Qiujing and Zhang, Shunchi and Sang, Yisi and Pu, Kangsheng and Wei, Zekai and Wang, Han and Xu, Liyan and Li, Jing and Yu, Yue and Zhou, Jie}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShunchiZhang/ToM-in-AMC.","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"turkce-atasozleri-lm-evaluation-harness","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrek/turkce-atasozleri-lm-evaluation-harness","creator_name":"abrek er","creator_url":"https://huggingface.co/abrek","description":"\n\t\n\t\t\n\t\tData Source & Processing\n\t\n\nThis dataset is derived from furkanunluturk/turkce-atasozleri. To create a multiple-choice format, we used meta-llama/Llama-3.3-70B-Instruct embeddings to find the top 3 most similar results for each proverb definition, ensuring a similarity threshold of less than 0.92 to prevent duplicate answers. The 0.92 similarity threshold was chosen empirically and may be adjusted in the future.\n","first_N":5,"first_N_keywords":["multiple-choice","Turkish","gpl-3.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_Eval_Real_v1.1","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\" (COLM 2025).\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real_v1.1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Health_Benchmarks","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yesilhealth/Health_Benchmarks","creator_name":"Yesil Health AI","creator_url":"https://huggingface.co/yesilhealth","description":"\n\t\n\t\t\n\t\tLLM Health Benchmarks Dataset by Yesil Science\n\t\n\nThe LLM Health Benchmarks Dataset is a specialized resource for evaluating large language models (LLMs) in different medical specialties. It provides structured question-answer pairs designed to test the performance of AI models in understanding and generating domain-specific knowledge.\n\n\n\t\n\t\t\n\t\tPrimary Purpose\n\t\n\nThis dataset is built to:\n\nBenchmark LLMs in medical specialties and subfields.\nAssess the accuracy and contextual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yesilhealth/Health_Benchmarks.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MapEval-Visual","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-Visual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\n\t\n\t\t\n\t\tMapEval-Visual\n\t\n\nThis dataset was introduced in MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models\n\n\t\n\t\t\n\t\tExample\n\t\n\n\n\n\t\n\t\t\n\t\tQuery\n\t\n\nI am presently visiting Mount Royal Park . Could you please inform me about the nearby historical landmark?\n\n\t\n\t\t\n\t\tOptions\n\t\n\n\nCircle Stone\nSecret pool\nMaison William Caldwell Cottingham\nPoste de cavalerie du Service de police de la Ville de Montreal\n\n\n\t\n\t\t\n\t\tCorrect Option\n\t\n\n\nCircle Stone\n\n\n\t\n\t\t\n\t\tPrerequisite\n\t\n\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Visual.","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MapEval-API","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MapEval/MapEval-API","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","description":"\n\t\n\t\t\n\t\tMapEval-API\n\t\n\nMapEval-API is created using MapQaTor.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load dataset\nds = load_dataset(\"MapEval/MapEval-API\", name=\"benchmark\")\n\n# Generate better prompts\nfor item in ds[\"test\"]:\n    # Start with a clear task description\n    prompt = (\n        \"You are a highly intelligent assistant. \"\n        \"Answer the multiple-choice question by selecting the correct option.\\n\\n\"\n        \"Question:\\n\" + item[\"question\"] + \"\\n\\n\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-API.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"TUNA-Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/TUNA-Bench","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tTUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos (ACL 2025 Main)\n\t\n\n\n\n\n\n\nThis dataset accompanies the paper TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos.\n\n\t\n\t\t\n\t\tPaper abstract\n\t\n\nVideos are unique in their integration of temporal elements, including camera, scene, action, and attribute, along with their dynamic relationships over time. However, existing benchmarks for video understanding often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/TUNA-Bench.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MMLU-Philosophy-Marathi","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shubhamugare/MMLU-Philosophy-Marathi","creator_name":"Shubham Ugare","creator_url":"https://huggingface.co/shubhamugare","description":"\n\t\n\t\t\n\t\tMMLU Philosophy Questions in Marathi\n\t\n\nThis dataset contains philosophy questions from the MMLU (Massive Multitask Language Understanding) benchmark translated into Marathi.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSource: MMLU Philosophy subset from cais/mmlu\nTranslation API: OpenAI GPT-4\nLanguages: English (original) and Marathi (translated)\nTotal Questions: 311\nTask Type: Multiple choice questions with 4 options each\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach row contains:\n\noriginal_question: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shubhamugare/MMLU-Philosophy-Marathi.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Marathi","mit"],"keywords_longer_than_N":true},
	{"name":"MMSI-Bench-test","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sihany/MMSI-Bench-test","creator_name":"sihan yang","creator_url":"https://huggingface.co/sihany","description":"\n\t\n\t\t\n\t\tMMSI-Bench\n\t\n\nThis repo contains evaluation code for the paper \"[MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence]\" \nüåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n  \n\n üî•[2025-05-30]: We released the ArXiv paper.\n\n\t\n\t\t\n\t\n\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\nmmsi_bench = load_dataset(\"RunsenXu/MMSI-Bench\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEvaluation\n\t\n\nPlease refer to the evaluation guidelines of VLMEvalKit\n\n\n\n\t\n\t\n\t\n\t\tüèÜ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sihany/MMSI-Bench-test.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Pediatrics_questions","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tomshe/Pediatrics_questions","creator_name":"Tom s","creator_url":"https://huggingface.co/tomshe","description":"\n\t\n\t\t\n\t\tDataset Card for Pediatrics MCQ\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset comprises high-quality multiple-choice questions (MCQs) covering core biomedical knowledge and clinical scenarios from pediatrics. It includes 50 questions, each with four possible answer choices. These questions were specifically curated for research evaluating pediatric medical knowledge, clinical reasoning, and confidence-based interactions among medical trainees and large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomshe/Pediatrics_questions.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"hellaswag-mcqa","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/RikoteMaster/hellaswag-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","description":"\n\t\n\t\t\n\t\tHellaSwag MCQA Dataset\n\t\n\nThis dataset contains the HellaSwag dataset converted to Multiple Choice Question Answering (MCQA) format.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHellaSwag is a dataset for commonsense inference about physical situations. Given a context describing an activity, the task is to select the most plausible continuation from four choices.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nquestion: The activity label and context combined\nchoices: List of 4 possible‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/hellaswag-mcqa.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"medmcqa-mcqa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RikoteMaster/medmcqa-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","description":"\n\t\n\t\t\n\t\tMedMCQA MCQA Dataset\n\t\n\nThis dataset contains the MedMCQA dataset converted to Multiple Choice Question Answering (MCQA) format.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. It covers various medical subjects and topics, making it ideal for evaluating AI systems on medical knowledge.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nquestion: The medical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/medmcqa-mcqa.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_quantized_dataset","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M3_quantized_dataset","creator_name":"Alireza Abdollahopoorrostam","creator_url":"https://huggingface.co/AlirezaAbdollahpoor","description":"\n\t\n\t\t\n\t\tEnhanced MCQA Test Dataset for Comprehensive Model Evaluation\n\t\n\nThis dataset contains 400 carefully selected test samples from MetaMathQA, AQuA-RAT, OpenBookQA, and SciQ datasets, designed for comprehensive MCQA (Multiple Choice Question Answering) model evaluation and quantization testing across multiple domains.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal Samples: 400\nMetaMathQA Samples: 100 (mathematical problems)\nAQuA-RAT Samples: 100 (algebraic word problems)\nOpenBookQA Samples: 100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M3_quantized_dataset.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"livevqa-benchmark","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fmy666/livevqa-benchmark","creator_name":"fmy666","creator_url":"https://huggingface.co/fmy666","description":"\n\t\n\t\t\n\t\tLiveVQA Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLiveVQA is a comprehensive Visual Question Answering benchmark that evaluates multimodal models across three dynamic domains: News, Academic Papers, and Videos. The dataset features both level1 (basic comprehension) and level2 (advanced reasoning) questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each question\nimage: Path to the associated image\nquestion: The question text\noptions: List‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fmy666/livevqa-benchmark.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"odia_reasoning_benchmark","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Piyushdash94/odia_reasoning_benchmark","creator_name":"Piyush Dash","creator_url":"https://huggingface.co/Piyushdash94","description":"\n\t\n\t\t\n\t\tOdia Reasoning Benchmark\n\t\n\nThis benchmark contains reasoning questions in Odia (logical, Mathematical,Arithmetic, Deductive, Critical Thinking) with answers and optional explanations. Ideal for evaluating Odia QA and reasoning models.\n\n\t\n\t\t\n\t\tDataset structure\n\t\n\n\n\t\n\t\t\nColumn\nDescription\n\n\n\t\t\nQuestion\nReasoning question in Odia\n\n\nAnswer\nCorrect answer (text or number)\n\n\nExplanation\nOptional step-by-step explanation (some blank)\n\n\nType Of Question\nCategory (e.g., Math, Deductive)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Piyushdash94/odia_reasoning_benchmark.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","od","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"StaticEmbodiedBench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaojiahao/StaticEmbodiedBench","creator_name":"Xiao Jiahao","creator_url":"https://huggingface.co/xiaojiahao","description":"\n\t\n\t\t\n\t\tüìò Dataset Description\n\t\n\nStaticEmbodiedBench is a dataset for evaluating vision-language models on embodied intelligence tasks, as featured in the OpenCompass leaderboard.\nIt covers three key capabilities:\n\nMacro Planning: Decomposing a complex task into a sequence of simpler subtasks.\nMicro Perception: Performing concrete simple tasks such as spatial understanding and fine-grained perception.\nStage-wise Reasoning: Deciding the next action based on the agent‚Äôs current state and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xiaojiahao/StaticEmbodiedBench.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","üá∫üá∏ Region: US","embodied-AI"],"keywords_longer_than_N":true},
	{"name":"MathVision_with_difficulty_level","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMathVision with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVision  benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVision_with_difficulty_level\")\nprint(dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"MathVerse_with_difficulty_level","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MathVerse_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMathVerse with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVerse testmini benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVerse_with_difficulty_level\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVerse_with_difficulty_level.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MMMU_with_difficulty_level","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MMMU_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMMMU with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MMMU val benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MMMU_with_difficulty_level\")\nprint(dataset)\n\n\n\t\n\t\n\t\n\t\tüìë‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MMMU_with_difficulty_level.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_2","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_2","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_2.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_5","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_5","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_5.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_6","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_6","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_6.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"mcqa_greek_asep","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ilsp/mcqa_greek_asep","creator_name":"Institute for Language and Speech Processing","creator_url":"https://huggingface.co/ilsp","description":"\n\t\n\t\t\n\t\tDataset Card for Multiple Choice QA Greek ASEP\n\t\n\nThe Multiple Choice QA Greek ASEP dataset is a set of 1200 multiple choice questions in Greek. The questions were extracted and converted from questions available at the website of the Greek Supreme Council for Civil Personnel Selection (ŒëŒΩœéœÑŒ±œÑŒø Œ£œÖŒºŒ≤ŒøœçŒªŒπŒø ŒïœÄŒπŒªŒøŒ≥ŒÆœÇ Œ†œÅŒøœÉœâœÄŒπŒ∫Œøœç, ŒëŒ£ŒïŒ†-ASEP) (1Œì/2025).\nThe dataset includes questions in the following domains:\n\nŒ£œÖŒΩœÑŒ±Œ≥ŒºŒ±œÑŒπŒ∫œå ŒîŒØŒ∫Œ±ŒπŒø (Constitutional Law): 187 questions\n\nŒîŒπŒøŒπŒ∫Œ∑œÑŒπŒ∫œå ŒîŒØŒ∫Œ±ŒπŒø‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilsp/mcqa_greek_asep.","first_N":5,"first_N_keywords":["multiple-choice","monolingual","Greek","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"bbq","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/bbq","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\n\t\n\t\t\n\t\tBBQ Dataset\n\t\n\nThe Bias Benchmark for Question Answering (BBQ) dataset evaluates social biases in language models through question-answering tasks in English.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains questions designed to test for social biases across multiple demographic dimensions. Each question comes in two variants:\n\nAmbiguous (ambig): Questions where the correct answer should be \"unknown\" due to insufficient information\nDisambiguated (disambig): Questions with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/bbq.","first_N":5,"first_N_keywords":["question-answering","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_v1","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_v1","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_v1.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_3","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_3","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_3.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_eval_analysis_4","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_4","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_eval_analysis_4.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset_2","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_2","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_2.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_benchmark","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_benchmark","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP_M3_mcqa_benchmark\n\t\n\nThis benchmark is a filtered subset of the MMLU test set (cais/mmlu) focused on 21 STEM subjects. It is formatted for Multiple Choice Question Answering (MCQA) tasks.\n\n\t\n\t\t\n\t\tDataset Format\n\t\n\nEach entry includes:\n\nquestion: A multiple-choice question in plain text.\nchoices: A list of four possible answers (A, B, C, D).\nanswer: The correct answer, represented by a single letter (A, B, C, or D).\n\n\n\t\n\t\t\n\t\tIncluded Subjects\n\t\n\n\nabstract_algebra‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_benchmark.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"CVC","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Beijing-AISI/CVC","creator_name":"Beijing Institute of AI Safety and Governance","creator_url":"https://huggingface.co/Beijing-AISI","description":"\nThis repository contains all the data associated with the paper \"CVC: A Large-Scale Chinese Value Rule Corpus for Cultural Alignment of Large Language Models\".\n\nWe propose a three-tier value classification framework based on core Chinese values, which includes three dimensions, twelve core values, and fifty derived values. With the assistance of large language models and manual verification, we constructed a large-scale, refined, and high-quality value corpus containing over 250,000 rules. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Beijing-AISI/CVC.","first_N":5,"first_N_keywords":["text-generation","multiple-choice","expert-annotated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"FailureSensorIQ","keyword":"multiple-choice","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cc4718/FailureSensorIQ","creator_name":"Christodoulos Constantinides","creator_url":"https://huggingface.co/cc4718","description":"Dataset introduced in the paper FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes\n\n","first_N":5,"first_N_keywords":["cc-by-4.0","arxiv:2506.03278","üá∫üá∏ Region: US","question-answering","industrial"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset.","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"VisOnlyQA_length_angle","keyword":"multiple-choice","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_length_angle","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\n\t\n\t\t\n\t\tVisOnlyQA\n\t\n\n\nüåê Project Website | üìÑ Paper | ü§ó Dataset | üî• VLMEvalKit\n\n\nThis repository contains the code and data for the paper \"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\".\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_length_angle.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","multiple-choice-qa","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"m1_preference_data_cleaned","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned","creator_name":"Albert Fares","creator_url":"https://huggingface.co/albertfares","description":"\n\t\n\t\t\n\t\tEPFL M1 MCQ Dataset (Cleaned)\n\t\n\nThis dataset contains 645 multiple-choice questions extracted and cleaned from EPFL M1 preference data. Each question has exactly 4 options (A, B, C, D) with balanced sampling when original questions had more options.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Questions: 645\nFormat: Multiple choice questions with exactly 4 options\nDomain: Computer Science and Engineering\nSource: EPFL M1 preference data\nAnswer Distribution: A: 204, B: 145, C: 147, D: 149‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"m1_preference_data_cleaned","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned","creator_name":"Albert Fares","creator_url":"https://huggingface.co/albertfares","description":"\n\t\n\t\t\n\t\tEPFL M1 MCQ Dataset (Cleaned)\n\t\n\nThis dataset contains 645 multiple-choice questions extracted and cleaned from EPFL M1 preference data. Each question has exactly 4 options (A, B, C, D) with balanced sampling when original questions had more options.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Questions: 645\nFormat: Multiple choice questions with exactly 4 options\nDomain: Computer Science and Engineering\nSource: EPFL M1 preference data\nAnswer Distribution: A: 204, B: 145, C: 147, D: 149‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"KokushiMD-10","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/humanalysis-square/KokushiMD-10","creator_name":"Tako AI","creator_url":"https://huggingface.co/humanalysis-square","description":"\n\t\n\t\t\n\t\tKokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nKokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations. This dataset addresses critical gaps in existing medical AI evaluation by providing a linguistically grounded, multimodal, and multi-profession assessment framework for large language models (LLMs) in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humanalysis-square/KokushiMD-10.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","mit"],"keywords_longer_than_N":true},
	{"name":"VEU-Bench","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeeLi4704/VEU-Bench","creator_name":"Bozhen Li","creator_url":"https://huggingface.co/LeeLi4704","description":"\n\t\n\t\t\n\t\tVideo Editing Understanding(VEU) Benchmark\n\t\n\nüñ• Project Page\nWidely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeeLi4704/VEU-Bench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-classification","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"Swordsman","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Junrui1202/Swordsman","creator_name":"Cui Junrui","creator_url":"https://huggingface.co/Junrui1202","description":"Junrui1202/Swordsman dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Chinese","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"EEE-Bench","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/afdsafas/EEE-Bench","creator_name":"Ming Li","creator_url":"https://huggingface.co/afdsafas","description":"\n\t\n\t\t\n\t\tEEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nEEE-Bench is a multimodal benchmark designed to evaluate the practical engineering capabilities of large multimodal models (LMMs), using electrical and electronics engineering (EEE) as the domain focus. It comprises 2,860 carefully curated problems across 10 core subdomains, including analog circuits and control systems, featuring complex visual inputs such as abstract‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/afdsafas/EEE-Bench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"MedMCQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/araag2/MedMCQA","creator_name":"Artur Guimar√£es","creator_url":"https://huggingface.co/araag2","description":"\n\t\n\t\t\n\t\tMedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\nLinks\n\n\n\t\t\nHomepage:\nGithub.io\n\n\nRepository:\nGithub\n\n\nPaper:\narXiv\n\n\nLeaderboard:\nPapers with Code\n\n\nContact (Original Authors):\nAaditya Ura aadityaura@gmail.com), Logesh logesh.umapathi@saama.com\n\n\nContact (Curator):\nArtur Guimar√£es (artur.guimas@gmail.com)\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n`MedMCQA has more than 194k high-quality AIIMS & NEET PG‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/araag2/MedMCQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","cc-by-sa-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"igakuqa-subset-curated","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated","creator_name":"Japan AI","creator_url":"https://huggingface.co/japan-ai-official","description":"\n\t\n\t\t\n\t\tIgakuQA Curated Subset (Text-Only)\n\t\n\nThis dataset contains 66 carefully selected text-only samples from the IgakuQA medical exam dataset, \ncurated using advanced difficulty assessment and model evaluation techniques.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a high-quality subset of the IgakuQA Japanese medical exam dataset, selected based on:\n\nDifficulty Score: Measures how challenging the sample is for AI models\nConsistency Score: Evaluates response consistency across different models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"igakuqa-subset-curated","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated","creator_name":"Japan AI","creator_url":"https://huggingface.co/japan-ai-official","description":"\n\t\n\t\t\n\t\tIgakuQA Curated Subset (Text-Only)\n\t\n\nThis dataset contains 66 carefully selected text-only samples from the IgakuQA medical exam dataset, \ncurated using advanced difficulty assessment and model evaluation techniques.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a high-quality subset of the IgakuQA Japanese medical exam dataset, selected based on:\n\nDifficulty Score: Measures how challenging the sample is for AI models\nConsistency Score: Evaluates response consistency across different models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MedQA","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/araag2/MedQA","creator_name":"Artur Guimar√£es","creator_url":"https://huggingface.co/araag2","description":"\n\t\n\t\t\n\t\tMedQA-USMLE ‚Äî A Large-scale Open Domain Question Answering Dataset from Medical Exams\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\nLinks\n\n\n\t\t\nHomepage:\nGithub.io\n\n\nRepository:\nGithub\n\n\nPaper:\narXiv\n\n\nLeaderboard:\nPapers with Code\n\n\nContact (Original Authors):\nDi Jin (jindi15@mit.edu)\n\n\nContact (Curator):\nArtur Guimar√£es (artur.guimas@gmail.com)\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMedQA is a large-scale multiple-choice question-answering dataset designed to mimic the style of professional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/araag2/MedQA.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anonymous-789/TSQA","creator_name":"Anonymous User","creator_url":"https://huggingface.co/anonymous-789","description":"\n\t\n\t\t\n\t\tTSQA: Time-Sensitive Question Answering Benchmark\n\t\n\nTSQA is a benchmark designed to evaluate a model‚Äôs ability to handle time-aware factual knowledge. Unlike standard static QA datasets, TSQA tests whether models can identify facts whose correct answers change over time.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nName: TSQA (Time-Sensitive Question Answering)\nYears Covered: 2013‚Äì2024\nNumber of Questions: 10,063\nChoices per Question: 4 (one correct, three distractors)\nTemporal Context: Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-789/TSQA.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"truthful_qa","keyword":"multiple-choice","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/onionmonster/truthful_qa","creator_name":"Calvin Ku","creator_url":"https://huggingface.co/onionmonster","description":"\n\t\n\t\t\n\t\tTruthfulQA‚ÄëCFB ¬∑ Measuring How Models Mimic Human Falsehoods (Conversation Fact Benchmark Format)\n\t\n\nTruthfulQA‚ÄëCFB is a 817 example benchmark derived from the original TruthfulQA dataset, transformed and adapted for the Conversation Fact Benchmark framework. Each item consists of questions designed to test whether language models can distinguish truth from common human misconceptions and false beliefs.\nThe dataset focuses on truthfulness evaluation: questions target areas where humans‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onionmonster/truthful_qa.","first_N":5,"first_N_keywords":["question-answering","expert-generated","truthful_qa","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"dream","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/onionmonster/dream","creator_name":"Calvin Ku","creator_url":"https://huggingface.co/onionmonster","description":"\n\t\n\t\t\n\t\tDREAM‚ÄëCFB ¬∑ Dialogue-based Reading Comprehension Examination through Machine Reading (Conversation Fact Benchmark Format)\n\t\n\nDREAM‚ÄëCFB is a 6,444 example dataset derived from the original DREAM dataset, transformed and adapted for the Conversation Fact Benchmark framework. Each item consists of multi-turn dialogues with associated multiple-choice questions that test reading comprehension and conversational understanding.\nThe dataset focuses on dialogue-based reading comprehension:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onionmonster/dream.","first_N":5,"first_N_keywords":["question-answering","expert-generated","dream","English","mit"],"keywords_longer_than_N":true},
	{"name":"shared-imagination","keyword":"multiple-choice","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/shared-imagination","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tDataset Card for Shared Imagination\n\t\n\n\n\nThis dataset contains the problems used in the paper Shared\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains the questions generated for the investigations described in the TMLR paper Shared Imagination: LLMs Hallucinate Alike.\nIf you want to use this dataset to assess new models, please use the default config (i.e., datasets.load_dataset('Salesforce/shared-imagination')). \nThis config contains questions for which the four candidate choices‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/shared-imagination.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MathVista_with_difficulty_level","keyword":"multiple-choice","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JierunChen/MathVista_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","description":"\n\t\n\t\t\n\t\tMathVista with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVista testmini benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper  The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVista_with_difficulty_level\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVista_with_difficulty_level.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true}
]
;
