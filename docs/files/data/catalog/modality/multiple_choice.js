const data_for_modality_multiple_choice = 
[
	{"name":"unified-mcqa-all","keyword":"multiple-choice","description":"aggregated version of pszemraj/unified-mcqa across all configs originally in the dataset. \n\n[!NOTE]\nI recommend using the nomath config in this dataset, as parsing the math MC questions (in the 5-choice config in the original dataset) has some problems.\n\n","url":"https://huggingface.co/datasets/pszemraj/unified-mcqa-all","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-classification","English","odc-by","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MuSLR","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tüß© MuSLR: Multimodal Symbolic Logical Reasoning Benchmark\n\t\n\nProject page: \"Multimodal Symbolic Logical Reasoning\".\nPaper Link: https://arxiv.org/abs/2509.25851\nMultimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, where rigorous, deterministic reasoning helps prevent serious consequences.  \nTo evaluate such capabilities of current‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aiden0526/MuSLR.","url":"https://huggingface.co/datasets/Aiden0526/MuSLR","creator_name":"Jundong Xu","creator_url":"https://huggingface.co/Aiden0526","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"SciBench-TruthfulQA-RAG","keyword":"multiple-choice","description":"natnitaract/SciBench-TruthfulQA-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/natnitaract/SciBench-TruthfulQA-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"probability_words_nli","keyword":"multiple-choice","description":"Probing neural language models for understanding of words of estimative probability","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"medmcqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for MedMCQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.\nMedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity.\nEach sample contains a question, correct answer(s), and other options which require‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openlifescienceai/medmcqa.","url":"https://huggingface.co/datasets/openlifescienceai/medmcqa","creator_name":"Open Life Science AI","creator_url":"https://huggingface.co/openlifescienceai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","multiple-choice-qa","open-domain-qa","no-annotation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_unique","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/unpredictable/unpredictable_unique","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"NIFTY","keyword":"multiple-choice","description":"\n  \n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \n\n\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\nFor the NIFTY-RL LLM alignment dataset please use nifty-rl.\n\n\t\n\t\t\n\t\n\t\n\t\tüìã Table of Contents\n\t\n\n\nüß© NIFTY Dataset\nüìã Table of Contents\nüìñ Usage\nDownloading the dataset\nDataset structure\n\n\nLarge Language Models \n‚úçÔ∏è Contributing\nüìù Citing\nüôè‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/NIFTY.","url":"https://huggingface.co/datasets/raeidsaqur/NIFTY","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","time-series-forecasting","document-question-answering","topic-classification","semantic-similarity-classification"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_mc","keyword":"multiple-choice","description":"TruthfulQA-MC is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.","url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_mc","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","language-modeling","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"discourse_marker_qa","keyword":"multiple-choice","description":"Discourse marker/connective prediction as multiple choice questions based on the Discovery dataset","url":"https://huggingface.co/datasets/sileod/discourse_marker_qa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","open-domain-qa","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"model-written-evals","keyword":"multiple-choice","description":"This new dataset is designed to solve this great NLP task and is crafted with a lot of care.","url":"https://huggingface.co/datasets/khalidalt/model-written-evals","creator_name":"Khalid Almubarak","creator_url":"https://huggingface.co/khalidalt","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","zero-shot-classification","question-answering","multiple-choice-qa","multiple-choice-coreference-resolution"],"keywords_longer_than_N":true},
	{"name":"truthful_qa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/truthfulqa/truthful_qa.","url":"https://huggingface.co/datasets/truthfulqa/truthful_qa","creator_name":"TruthfulQA","creator_url":"https://huggingface.co/truthfulqa","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"lm_syneval","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Targeted Syntactic Evaluation of Language Models (LM-SynEval)\n\t\n\n\nWe present a dataset for evaluating the grammaticality of the predictions of a language\nmodel. We automatically construct a large number of minimally different pairs of English\nsentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs\nrepresent different variations of structure-sensitive phenomena: subject-verb agreement,\nreflexive anaphora and negative polarity items.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmichaelov/lm_syneval.","url":"https://huggingface.co/datasets/jmichaelov/lm_syneval","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"parsinlu-multiple-choice","keyword":"multiple-choice","description":"A Persian multiple choice task.","url":"https://huggingface.co/datasets/ParsiAI/parsinlu-multiple-choice","creator_name":"ParsiAI","creator_url":"https://huggingface.co/ParsiAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","Persian","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"PCA-Bench-V1","keyword":"multiple-choice","description":"PCA-Bench\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-Bench is an innovative benchmark for evaluating and locating errors in Multimodal LLMs when conducting embodied decision making tasks, specifically focusing on perception, cognition, and action.\n\n\t\n\t\n\t\n\t\tRelease\n\t\n\n\n[2024.02.15] PCA-Bench-V1 is released. We release the open and closed track data in huggingface. We also set an online leaderboard  accepting users' submission.\n[2023.12.15] PCA-EVAL is accepted to Foundation Model for Decision Making Workshop‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1.","url":"https://huggingface.co/datasets/PCA-Bench/PCA-Bench-V1","creator_name":"PCA-Bench","creator_url":"https://huggingface.co/PCA-Bench","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pedagogy-benchmark-multilingual","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPedagogy Benchmark - Multilingual\n\t\n\nA multilingual translation of the AI-for-Education/pedagogy-benchmark dataset into African languages.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides translations of Chilean teacher training exam questions into African languages. The original dataset contains multiple-choice questions covering various pedagogical domains, education levels, and subject areas.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nLuganda (Uganda)\nNyankore (Uganda)\n\nComing soon: Swahili‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CraneAILabs/pedagogy-benchmark-multilingual.","url":"https://huggingface.co/datasets/CraneAILabs/pedagogy-benchmark-multilingual","creator_name":"Crane AI Labs","creator_url":"https://huggingface.co/CraneAILabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Ganda","Nyankole","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"parsinlu-multiple-choice","keyword":"multiple-choice","description":"A Persian multiple choice task.","url":"https://huggingface.co/datasets/ParsiAI/parsinlu-multiple-choice","creator_name":"ParsiAI","creator_url":"https://huggingface.co/ParsiAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","Persian","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"inverse_scaling_prize-sig_figs","keyword":"multiple-choice","description":"jmichaelov/inverse_scaling_prize-sig_figs dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-sig_figs","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","Tabular"],"keywords_longer_than_N":true},
	{"name":"123_test","keyword":"multiple-choice","description":"The Fewshot Table dataset consists of tables that naturally occur on the web, that are formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. The dataset consists of approximately 413K tables that are extracted from the WDC Web Table Corpora 2015, which is released under the Apache-2.0 license. The WDC Web Table Corpora \"contains vast amounts of HTML tables. [...] The Web Data Commons project extracts relational Web tables from the Common Crawl, the largest and most up-to-date Web corpus that is currently available to the public.\"","url":"https://huggingface.co/datasets/JeremyAlain/123_test","creator_name":"J√©r√©my Scheurer","creator_url":"https://huggingface.co/JeremyAlain","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_sporcle-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_sporcle-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_mmo-champion-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_mmo-champion-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_bulbapedia-bulbagarden-net","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_bulbapedia-bulbagarden-net","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"tape","keyword":"multiple-choice","description":"The Winograd schema challenge composes tasks with syntactic ambiguity,\nwhich can be resolved with logic and reasoning (Levesque et al., 2012).\n\nThe texts for the Winograd schema problem are obtained using a semi-automatic \npipeline. First, lists of 11 typical grammatical structures with syntactic \nhomonymy (mainly case) are compiled. For example, two noun phrases with a \ncomplex subordinate: 'A trinket from Pompeii that has survived the centuries'.\nRequests corresponding to these constructions are submitted in search of the \nRussian National Corpus, or rather its sub-corpus with removed homonymy. In the \nresulting 2+k examples, homonymy is removed automatically with manual validation\nafterward. Each original sentence is split into multiple examples in the binary \nclassification format, indicating whether the homonymy is resolved correctly or\nnot.","url":"https://huggingface.co/datasets/RussianNLP/tape","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"fig-qa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Fig-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is the dataset for the paper Testing the Ability of Language Models to Interpret Figurative Language. Fig-QA consists of 10256 examples of human-written creative metaphors that are paired as a Winograd schema. It can be used to evaluate the commonsense reasoning of models. The metaphors themselves can also be used as training data for other tasks, such as metaphor detection or generation. \n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nightingal3/fig-qa.","url":"https://huggingface.co/datasets/nightingal3/fig-qa","creator_name":"Emmy Liu","creator_url":"https://huggingface.co/nightingal3","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"MMMU","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\n\t\n\nüåê Homepage | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüî•[2023-12-04]: Our evaluation server for test set is now availble on EvalAI. We welcome all submissions and look forward to your participation! üòÜ\n\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aslessor/MMMU.","url":"https://huggingface.co/datasets/aslessor/MMMU","creator_name":"Alex","creator_url":"https://huggingface.co/aslessor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PolyMRC","keyword":"multiple-choice","description":"We construct a dataset through entries with multiple meanings and examples from Chinese dictionaries, and set the example as context and explanations as choices, the goal of Polysemy Machine Comprehension (PolyMRC) is to find the correct explanation of the entry in the example.\nthe statistics of the dataset\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nsplit\nsentences\naverage sentence length\n\n\ntrain\n46,119\n38.55\n\n\nvalidation\n5,765\n38.31\n\n\ntest\n5,765\n38.84\n\n\n\t\n\n","url":"https://huggingface.co/datasets/bigai-nlco/PolyMRC","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Chinese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"psycholinguistic_eval","keyword":"multiple-choice","description":"Psycholinguistic dataset from 'What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models'\nby Allyson Ettinger","url":"https://huggingface.co/datasets/KevinZ/psycholinguistic_eval","creator_name":"Kevin Zhao","creator_url":"https://huggingface.co/KevinZ","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","fill-mask","question-answering","zero-shot-classification","expert-generated"],"keywords_longer_than_N":true},
	{"name":"MMMU","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\n\t\n\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüõ†Ô∏è[2024-05-30]: Fixed duplicate option issues in Materials dataset items (validation_Materials_25; test_Materials_17, 242) and content error in validation_Materials_25.\nüõ†Ô∏è[2024-04-30]: Fixed missing \"-\" or \"^\" signs in Math dataset items (dev_Math_2, validation_Math_11, 12, 16; test_Math_8‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU.","url":"https://huggingface.co/datasets/MMMU/MMMU","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nüåê Project Page: https://longbench2.github.io\nüíª Github Repo: https://github.com/THUDM/LongBench\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zai-org/LongBench-v2.","url":"https://huggingface.co/datasets/zai-org/LongBench-v2","creator_name":"Z.ai","creator_url":"https://huggingface.co/zai-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"MMLU-Philosophy-Marathi","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMLU Philosophy Questions in Marathi\n\t\n\nThis dataset contains philosophy questions from the MMLU (Massive Multitask Language Understanding) benchmark translated into Marathi.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSource: MMLU Philosophy subset from cais/mmlu\nTranslation API: OpenAI GPT-4\nLanguages: English (original) and Marathi (translated)\nTotal Questions: 311\nTask Type: Multiple choice questions with 4 options each\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach row contains:\n\noriginal_question: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shubhamugare/MMLU-Philosophy-Marathi.","url":"https://huggingface.co/datasets/shubhamugare/MMLU-Philosophy-Marathi","creator_name":"Shubham Ugare","creator_url":"https://huggingface.co/shubhamugare","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Marathi","mit"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP_M3_mcqa_benchmark\n\t\n\nThis benchmark is a filtered subset of the MMLU test set (cais/mmlu) focused on 21 STEM subjects. It is formatted for Multiple Choice Question Answering (MCQA) tasks.\n\n\t\n\t\t\n\t\tDataset Format\n\t\n\nEach entry includes:\n\nquestion: A multiple-choice question in plain text.\nchoices: A list of four possible answers (A, B, C, D).\nanswer: The correct answer, represented by a single letter (A, B, C, or D).\n\n\n\t\n\t\t\n\t\tIncluded Subjects\n\t\n\n\nabstract_algebra‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_benchmark.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_benchmark","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","derived","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"truthful-qa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 790 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rahmanidashti/truthful-qa.","url":"https://huggingface.co/datasets/rahmanidashti/truthful-qa","creator_name":"Hossein A. (Saeed) Rahmani","creator_url":"https://huggingface.co/rahmanidashti","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"ChemBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tChemBench\n\t\n\n\n\n\n\n\n\n\n\n\nA manually curated benchmark for evaluating chemistry and materials capabilities of Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/ChemBench.","url":"https://huggingface.co/datasets/jablonkagroup/ChemBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","language-modeling","natural-language-inference","expert-generated"],"keywords_longer_than_N":true},
	{"name":"FrenchMedMCQA-extended","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tFrenchMedMCQA-extended: A French Multiple-Choice Question Answering Corpus for Medical domain, that supports Comparative Analysis with Human responses\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is based on\nFrenchMedMCQA,\nthe first publicly available Multiple-Choice Question Answering (MCQA) dataset\nin French for medical domain. We have enriched the content with additional\nannotations including student response rates downloaded from MedShake.net (the\noriginal data source), manually‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uy-rrodriguez/FrenchMedMCQA-extended.","url":"https://huggingface.co/datasets/uy-rrodriguez/FrenchMedMCQA-extended","creator_name":"Ricardo R","creator_url":"https://huggingface.co/uy-rrodriguez","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","French","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"truthfulqa_gl","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA_gl\n\t\n\n\n\nTruthfulQA_gl is the Galician version of the TruthfulQA dataset.\nThis dataset is used to measure the truthfulness of a language model when generating answers to questions. It includes questions from different categories that some humans would answer wrongly due to false beliefs or misconceptions.\nNote that this version includes only the generation split.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources\n\t\n\n\nRepository: Proxecto N√ìS at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/truthfulqa_gl.","url":"https://huggingface.co/datasets/proxectonos/truthfulqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"mcqa_greek_asep","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Multiple Choice QA Greek ASEP\n\t\n\nThe Multiple Choice QA Greek ASEP dataset is a set of 1200 multiple choice questions in Greek. The questions were extracted and converted from questions available at the website of the Greek Supreme Council for Civil Personnel Selection (ŒëŒΩœéœÑŒ±œÑŒø Œ£œÖŒºŒ≤ŒøœçŒªŒπŒø ŒïœÄŒπŒªŒøŒ≥ŒÆœÇ Œ†œÅŒøœÉœâœÄŒπŒ∫Œøœç, ŒëŒ£ŒïŒ†-ASEP) (1Œì/2025).\nThe dataset includes questions in the following domains:\n\nŒ£œÖŒΩœÑŒ±Œ≥ŒºŒ±œÑŒπŒ∫œå ŒîŒØŒ∫Œ±ŒπŒø (Constitutional Law): 187 questions\n\nŒîŒπŒøŒπŒ∫Œ∑œÑŒπŒ∫œå ŒîŒØŒ∫Œ±ŒπŒø‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilsp/mcqa_greek_asep.","url":"https://huggingface.co/datasets/ilsp/mcqa_greek_asep","creator_name":"Athena Research Center | Institute for Language and Speech Processing","creator_url":"https://huggingface.co/ilsp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","monolingual","Greek","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"WeatherQA_SFT","keyword":"multiple-choice","description":"WeatherQA_SFT is a supervised fine-tuning (SFT) dataset in sharegpt format derived from the WeatherQA dataset for vision/multimodal language models. It focuses on severe weather geo-localization and potential hazard analysis over the Contiguous United States (CONUS).\nThe dataset contains over 7,000 image-text pairs. Including a 'train' set from year 2014-2019 and 'test' set for the year 2020. The images encapsulate complex weather patterns, including:\n\nEnvironmental instability parameters‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhanxiangHua/WeatherQA_SFT.","url":"https://huggingface.co/datasets/ZhanxiangHua/WeatherQA_SFT","creator_name":"Zhanxiang Hua","creator_url":"https://huggingface.co/ZhanxiangHua","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"openbookqa_gl","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for OpenBookQA_gl\n\t\n\nopenbookqa_gl is a question answering dataset in Galician, translated from the OpenBookQA dataset in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nopenbookqa_gl is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split. Each instance contains a question stem, four possible choices, and the letter‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/openbookqa_gl.","url":"https://huggingface.co/datasets/proxectonos/openbookqa_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Galician","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"bigbench","keyword":"multiple-choice","description":"BIG-Bench but it doesn't require the hellish dependencies (tensorflow, pypi-bigbench, protobuf) of the official version.\ndataset = load_dataset(\"tasksource/bigbench\",'movie_recommendation')\n\nCode to reproduce:\nhttps://colab.research.google.com/drive/1MKdLdF7oqrSQCeavAcsEnPdI85kD0LzU?usp=sharing\nDatasets are capped to 50k examples to keep things light.\nI also removed the default split when train was available also to save space, as default=train+val.\n@article{srivastava2022beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/bigbench.","url":"https://huggingface.co/datasets/tasksource/bigbench","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"YOLOv8-Multi-Instance-Object-Detection-Dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMulti Instance Object Detection Dataset Sample\n\t\n\n Duality.ai just released a 1000 image dataset used to train a YOLOv8 model for object detection -- and it's 100% free!\n Just create an EDU account here. \nThis HuggingFace dataset is a 20 image and label sample, but you can get the rest at no cost by creating a FalconCloud account. Once you verify your email, the link will redirect you to the dataset page.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset consists of high-quality images of soup‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/duality-robotics/YOLOv8-Multi-Instance-Object-Detection-Dataset.","url":"https://huggingface.co/datasets/duality-robotics/YOLOv8-Multi-Instance-Object-Detection-Dataset","creator_name":"Duality AI","creator_url":"https://huggingface.co/duality-robotics","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["object-detection","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"openbookqa_ca","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for openbookqa_ca\n\t\n\n\n\nopenbookqa_ca is a question answering dataset in Catalan, professionally translated from the main version of the OpenBookQA dataset in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nopenbookqa_ca (Open Book Question Answering - Catalan) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/openbookqa_ca.","url":"https://huggingface.co/datasets/projecte-aina/openbookqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"VoiceAssistant-Eval","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tüî• VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\n\t\n\n \n \n \n \n\n\n\n\n\n\n\nüåü  This is the official repository for the paper \"VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing\", which contains the evaluation code for the VoiceAssistant-Eval benchmark.\n[üåê Homepage] [üíª Github] [üìä Leaderboard ] [üìä Detailed Leaderboard ] [üìä Roleplay Leaderboard ] [üìñ Paper]\n\n\n\n\n\n\t\n\t\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval.","url":"https://huggingface.co/datasets/MathLLMs/VoiceAssistant-Eval","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","audio-to-audio","any-to-any","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"PLM-Video-Human","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for PLM-Video Human\n\t\n\nPLM-Video-Human is a collection of human-annotated resources for training Vision Language Models,\nfocused on detailed video understanding. Training tasks include: fine-grained open-ended question answering (FGQA), Region-based Video Captioning (RCap),\nRegion-based Dense Video Captioning (RDCap) and Region-based Temporal Localization (RTLoc).\n[üìÉ Tech Report]\n[üìÇ Github]\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\n\t\n\t\tFine-Grained Question Answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/PLM-Video-Human.","url":"https://huggingface.co/datasets/facebook/PLM-Video-Human","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","other","other","English"],"keywords_longer_than_N":true},
	{"name":"MUIRBENCH","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding\n\t\n\nüåê Homepage | üìñ Paper | üíª Evaluation \n\n\t\n\t\t\n\t\tIntro\n\t\n\nMuirBench is a benchmark containing 11,264 images and 2,600 multiple-choice questions, providing robust evaluation on 12 multi-image understanding tasks.\n\nMuirBench evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic understanding, diagram understanding, visual retrieval, ..., etc, while prior benchmarks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH.","url":"https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH","creator_name":"MUIRBENCH","creator_url":"https://huggingface.co/MUIRBENCH","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Swordsman","keyword":"multiple-choice","description":"Junrui1202/Swordsman dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Junrui1202/Swordsman","creator_name":"Cui Junrui","creator_url":"https://huggingface.co/Junrui1202","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Chinese","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"GG-BBQ","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for GG-BBQ\n\t\n\n\nGerman Gender Bias Benchmark for Question Answering (GG-BBQ) for gender bias evaluation in LLMs that support German language. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\nLanguage(s) (NLP): German\nLicense: cc-by-4.0\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: https://github.com/shalakasatheesh/GG-BBQ\nPaper: https://arxiv.org/abs/2507.16410\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is to be used to carry out the evaluation of gender bias in language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shalakasatheesh/GG-BBQ.","url":"https://huggingface.co/datasets/shalakasatheesh/GG-BBQ","creator_name":"Shalaka Satheesh","creator_url":"https://huggingface.co/shalakasatheesh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","German","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Prompt-Ablations","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMaCBench-Prompt-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"winograd_th","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tA collection of Thai Winograd Schemas\n\t\n\nWe present a collection of Winograd Schemas in the Thai language. These schemas are adapted from the original set of English Winograd Schemas proposed by Levesque et al., which was based on Ernest Davis's collection.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Translation\n\t\n\nTwo professional translators, who were native Thai speakers fluent in English and had experience translating from English to Thai, were hired. In a pilot translation phase, one native speaker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pakphum/winograd_th.","url":"https://huggingface.co/datasets/pakphum/winograd_th","creator_name":"phakphum artkaew","creator_url":"https://huggingface.co/pakphum","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Thai","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"multiple-choice","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"CupCase","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for CUPCase\n\t\n\n\n\nCUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset\n(AAAI 2025)\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedical benchmark datasets significantly contribute to devel-\noping Large Language Models (LLMs) for medical knowl-\nedge extraction, diagnosis, summarization, and other uses.\nYet, current benchmarks are mainly derived from exam ques-\ntions given to medical students or cases described in the med-\nical literature, lacking the complexity of real-world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ofir408/CupCase.","url":"https://huggingface.co/datasets/ofir408/CupCase","creator_name":"ofir ben shoham","creator_url":"https://huggingface.co/ofir408","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"hellasigma","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\thellasigma\n\t\n\n\n[!IMPORTANT]\nThis is an initial proof of concept and only contains 190 examples. Still, it seems to be able to tease out differences especially in 7b+ models. I've run some initial evals here\n\nMany evaluation datasets focus on a single correct answer to see if the model is \"smart.\" What about when there's no right answer? HellaSigma is an \"eval\" dataset to probe at what your model's personality type may be. Is it a Sigma, or not?\nThis dataset contains generic scenarios‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/hellasigma.","url":"https://huggingface.co/datasets/pszemraj/hellasigma","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Cultural-Evaluation-Kalahi","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tKalahi\n\t\n\nKalahi evaluates the ability of LLMs to generate responses relevant to Filipino culture in terms of shared knowledge and ethics. This dataset contains a MCQ-compatible version of the Kalahi dataset that is used in SEA-HELM.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nKalahi is designed for evaluating Filipino cultural representations in instruction-tuned large language models (LLMs). It is part of the SEA-HELM leaderboard from AI Singapore.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nTagalog (tl)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aisingapore/Cultural-Evaluation-Kalahi.","url":"https://huggingface.co/datasets/aisingapore/Cultural-Evaluation-Kalahi","creator_name":"AI Singapore","creator_url":"https://huggingface.co/aisingapore","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","Tagalog","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FailureSensorIQ","keyword":"multiple-choice","description":"Dataset introduced in the paper FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes\n\n","url":"https://huggingface.co/datasets/cc4718/FailureSensorIQ","creator_name":"Christodoulos Constantinides","creator_url":"https://huggingface.co/cc4718","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","arxiv:2506.03278","üá∫üá∏ Region: US","question-answering","industrial"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8B‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"include-lite-44","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tINCLUDE-lite (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-lite-44.","url":"https://huggingface.co/datasets/CohereLabs/include-lite-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"legalbench_basic","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task preserves its original data and includes an additional input column with a formatted prompt, generated using the LegalBench registry, ready to be fed directly into language models.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\n\nBasic Legal: canada_tax_court_outcomes, jcrew_blocker, learned_hands_benefits, telemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench_basic.","url":"https://huggingface.co/datasets/DatologyAI/legalbench_basic","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CVC","keyword":"multiple-choice","description":"\nThis repository contains all the data associated with the paper \"CVC: A Large-Scale Chinese Value Rule Corpus for Cultural Alignment of Large Language Models\".\n\nWe propose a three-tier value classification framework based on core Chinese values, which includes three dimensions, twelve core values, and fifty derived values. With the assistance of large language models and manual verification, we constructed a large-scale, refined, and high-quality value corpus containing over 250,000 rules. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Beijing-AISI/CVC.","url":"https://huggingface.co/datasets/Beijing-AISI/CVC","creator_name":"Beijing Institute of AI Safety and Governance","creator_url":"https://huggingface.co/Beijing-AISI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multiple-choice","expert-annotated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"wmdp_bio_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tWMDP-Bio Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of wmdp-bio.\nThe data has been formatted into a question and answer structure suitable for training or evaluating instruction-following language models.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original question text.\nanswer: The correct answer text, prefixed with ####.\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\n[Example Question Text]\n\nAnswer:\n#### [Example Answer Text]\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\nThe original test split is preserved.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_bio_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_bio_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"synthetic_helpline_qa_scoring_v1","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for OpenCHS Child Helpline QA Scoring Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains simulated, annotated child helpline conversation transcripts for automated quality assurance (QA) scoring. Each conversation is evaluated across six key performance dimensions using binary labels to assess counselor effectiveness and call quality. The dataset is designed for training multi-head DistilBERT classification models that can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openchs/synthetic_helpline_qa_scoring_v1.","url":"https://huggingface.co/datasets/openchs/synthetic_helpline_qa_scoring_v1","creator_name":"BITZ IT Consulting","creator_url":"https://huggingface.co/openchs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"race-qa","keyword":"multiple-choice","description":"klaylouis1932/race-qa dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/klaylouis1932/race-qa","creator_name":"Klay Louis","creator_url":"https://huggingface.co/klaylouis1932","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"multiple-choice-questions","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\n\t\n\t\tQuest√µes de M√∫ltipla Escolha - Base de dados (PT-BR)\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tContextualiza√ß√£o\n\t\n\nEste reposit√≥rio cont√©m uma base de dados (data.json) com quest√µes de m√∫ltipla escolha, a qual foi utilizada principalmente no desenvolvimento de modelos de recupera√ß√£o de informa√ß√£o.\n\n\t\n\t\t\n\t\n\t\n\t\tDescri√ß√£o do conjunto de dados\n\t\n\nO conjunto de dados √© composto por quest√µes de m√∫ltipla escolha, abrangendo uma variedade de temas dentro da √°rea da Ci√™ncia da Computa√ß√£o. Cada quest√£o √© estruturada‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions.","url":"https://huggingface.co/datasets/mateus-hamade/multiple-choice-questions","creator_name":"Mateus Hamade","creator_url":"https://huggingface.co/mateus-hamade","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","Portuguese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"teleia","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\n\t\n\n\n\t\n\t\t\n\t\tAuthors and Affiliations\n\t\n\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\n\nUniversidad Aut√≥noma de Madrid\nUniversidad Polit√©cnica de Madrid\nNew York University\nUniversidad de Valladolid\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset contains test questions to evaluate LLMs in Spanish\n\nTELEIA_Cervantes_AVE.xlsx: test questions on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gonzmart/teleia.","url":"https://huggingface.co/datasets/gonzmart/teleia","creator_name":"Gonzalo","creator_url":"https://huggingface.co/gonzmart","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"melange_visual_bbq","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMelange Visual Bias Benchmark\n\t\n\nA visual multiple-choice benchmark for evaluating social bias and reasoning in vision-language models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMelange Visual Bias Benchmark is a multimodal extension of the BBQ (Bias Benchmark for Question Answering) dataset, designed to probe social bias and fairness in VLMs (Vision-Language Models). Instead of relying on textual context, this dataset grounds each multiple-choice question in one or more scene images that depict the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IDfree/melange_visual_bbq.","url":"https://huggingface.co/datasets/IDfree/melange_visual_bbq","creator_name":"no_ID","creator_url":"https://huggingface.co/IDfree","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","visual-question-answering","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"mmlux","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCitation Information\n\t\n\nIf you find benchmarks useful in your research, please consider citing the test and also the MMLU dataset it draws from:\n    @misc{thellmann2024crosslingual,\n    title={Towards Cross-Lingual LLM Evaluation for European Languages},\n    author={Klaudia Thellmann and Bernhard Stadler and Michael Fromm and Jasper Schulze Buschhoff and Alex Jude and Fabio Barth and Johannes Leveling and Nicolas Flores-Herr and Joachim K√∂hler and Ren√© J√§kel and Mehdi Ali}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eurolingua/mmlux.","url":"https://huggingface.co/datasets/Eurolingua/mmlux","creator_name":"EuroLingua-GPT","creator_url":"https://huggingface.co/Eurolingua","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","expert-generated","multilingual","cais/mmlu","German"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"legalbench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task preserves its original data and includes an additional input column with a formatted prompt, generated using the LegalBench registry, ready to be fed directly into language models.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\n\nBasic Legal: canada_tax_court_outcomes, jcrew_blocker, learned_hands_benefits, telemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench.","url":"https://huggingface.co/datasets/DatologyAI/legalbench","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"WildScore","keyword":"multiple-choice","description":"GM77/WildScore dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/GM77/WildScore","creator_name":"Gagan Mundada","creator_url":"https://huggingface.co/GM77","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MM-IQ","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for \"MM-IQ\"\n\t\n\n\nIntroduction\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nAutomatic Evaluation\n\n\nCitation\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nIQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanqia/MM-IQ.","url":"https://huggingface.co/datasets/huanqia/MM-IQ","creator_name":"huanqiacai","creator_url":"https://huggingface.co/huanqia","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"MMMU-Thai","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMMU Thai (MMMU Benchmark Translated to Thai)\n\t\n\nMMMU Thai is a dataset for evaluating multimodal models on massive multi-discipline tasks requiring college-level knowledge and deliberate reasoning. This dataset is translated from MMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI) into Thai.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nMMMU Thai consists of 11,500 meticulously collected multimodal questions from college exams, quizzes, and textbooks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/MMMU-Thai.","url":"https://huggingface.co/datasets/iapp/MMMU-Thai","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","Thai","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Ablations","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMaCBench-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Sailcompass_data","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\n\t\n\t\tSailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages\n\t\n\nThis repository provides the dataset for evaluation SEA large language model.\n\nProject Website: sailorllm.github.io\nCodebase: https://github.com/sail-sg/sailcompass\n\n\n\t\n\t\t\n\t\n\t\n\t\tAcknowledgment\n\t\n\nThanks to the contributors of the opencompass.\n\n\t\n\t\t\n\t\n\t\n\t\tCiting this work\n\t\n\nIf you use this repository or sailor models, please cite\n@misc{sailcompass,\n      title={SailCompass: Towards Reproducible‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sail/Sailcompass_data.","url":"https://huggingface.co/datasets/sail/Sailcompass_data","creator_name":"Sea AI Lab","creator_url":"https://huggingface.co/sail","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","translation","summarization","table-question-answering","multiple-choice"],"keywords_longer_than_N":true},
	{"name":"xstorycloze_gl","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for xstorycloze_gl\n\t\n\n\n\nxstorycloze_gl is a question answering dataset in Galician, translated from the English StoryCloze dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nxstorycloze_gl is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1511 instances in the test split. Each instance contains a story stem, divided in 4 sentences, 2 possible completions, and the number indicating the correct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proxectonos/xstorycloze_gl.","url":"https://huggingface.co/datasets/proxectonos/xstorycloze_gl","creator_name":"Proxecto N√≥s","creator_url":"https://huggingface.co/proxectonos","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Galician","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_quantized_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tEnhanced MCQA Test Dataset for Comprehensive Model Evaluation\n\t\n\nThis dataset contains 400 carefully selected test samples from MetaMathQA, AQuA-RAT, OpenBookQA, and SciQ datasets, designed for comprehensive MCQA (Multiple Choice Question Answering) model evaluation and quantization testing across multiple domains.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal Samples: 400\nMetaMathQA Samples: 100 (mathematical problems)\nAQuA-RAT Samples: 100 (algebraic word problems)\nOpenBookQA Samples: 100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M3_quantized_dataset.","url":"https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M3_quantized_dataset","creator_name":"Alireza Abdollahopoorrostam","creator_url":"https://huggingface.co/AlirezaAbdollahpoor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"KAIROS_EVAL","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tKAIROS_EVAL Dataset\n\t\n\nPaper: LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions | Code (GitHub)\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nKAIROS is a benchmark dataset designed to evaluate the robustness of large language models (LLMs) in multi-agent, socially interactive scenarios. Unlike static QA datasets, KAIROS dynamically constructs evaluation settings for each model by capturing its original belief (answer + confidence) and then simulating peer influence through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/declare-lab/KAIROS_EVAL.","url":"https://huggingface.co/datasets/declare-lab/KAIROS_EVAL","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","reinforcement-learning","multiple-choice","multiple-choice-qa","monolingual"],"keywords_longer_than_N":true},
	{"name":"AVQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tSummary | ÊëòË¶Å\n\t\n\nEnglish:\nThis dataset provides multiple-choice Audio‚ÄìVideo Question Answering examples in JSON Lines format. Each row links a question and choices to an audio file path and basic metadata (video name/id, relation, type). It is designed for training and evaluating audio-centric reasoning systems (e.g., multi-speaker scenes, background noise, environmental sounds).\n‰∏≠ÊñáÔºö\nÊú¨Êï∞ÊçÆÈõÜ‰ª•‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joysw909/AVQA.","url":"https://huggingface.co/datasets/Joysw909/AVQA","creator_name":"Wang","creator_url":"https://huggingface.co/Joysw909","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nüåê Project Page: https://longbench2.github.io\nüíª Github Repo: https://github.com/THUDM/LongBench\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2.","url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"MedFrameQA","keyword":"multiple-choice","description":"SuhaoYu1020/MedFrameQA dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA","creator_name":"SuhaoYu","creator_url":"https://huggingface.co/SuhaoYu1020","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"m1_preference_data_cleaned","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tEPFL M1 MCQ Dataset (Cleaned)\n\t\n\nThis dataset contains 645 multiple-choice questions extracted and cleaned from EPFL M1 preference data. Each question has exactly 4 options (A, B, C, D) with balanced sampling when original questions had more options.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Questions: 645\nFormat: Multiple choice questions with exactly 4 options\nDomain: Computer Science and Engineering\nSource: EPFL M1 preference data\nAnswer Distribution: A: 204, B: 145, C: 147, D: 149‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned.","url":"https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned","creator_name":"Albert Fares","creator_url":"https://huggingface.co/albertfares","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"m1_preference_data_cleaned","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tEPFL M1 MCQ Dataset (Cleaned)\n\t\n\nThis dataset contains 645 multiple-choice questions extracted and cleaned from EPFL M1 preference data. Each question has exactly 4 options (A, B, C, D) with balanced sampling when original questions had more options.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Questions: 645\nFormat: Multiple choice questions with exactly 4 options\nDomain: Computer Science and Engineering\nSource: EPFL M1 preference data\nAnswer Distribution: A: 204, B: 145, C: 147, D: 149‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned.","url":"https://huggingface.co/datasets/albertfares/m1_preference_data_cleaned","creator_name":"Albert Fares","creator_url":"https://huggingface.co/albertfares","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MedS-Ins","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tHPAI-BSC MedS-Ins\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated data from the MedS-Ins dataset. Used to train Aloe-Beta model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThis is the curated version of the MedS-Ins dataset included in the training set of the Aloe-Beta models. \nFirst, we selected 75 out of the 122 existing tasks, excluding the tasks that were already in the training set, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedS-Ins.","url":"https://huggingface.co/datasets/HPAI-BSC/MedS-Ins","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"msc-memfuse-mc10","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMSC‚ÄëMemFuse‚ÄëMC10 ¬∑ Multi-Session Chat Memory QA (10-way Multiple Choice)\n\t\n\nMSC‚ÄëMemFuse‚ÄëMC10 is a 500 example benchmark derived from Multi-Session Chat (MSC) and MemGPT‚Äôs MSC-Self-Instruct, modified and extended by the MemFuse team.Each item is a 10-option multiple-choice question probing information embedded within multi-session conversational history. The questions test episodic memory: facts must be inferred from prior dialogue, not static personas.\nThe dataset follows OpenAI's‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Percena/msc-memfuse-mc10.","url":"https://huggingface.co/datasets/Percena/msc-memfuse-mc10","creator_name":"Percena","creator_url":"https://huggingface.co/Percena","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","machine-generated","MemGPT/MSC-Self-Instruct","English"],"keywords_longer_than_N":true},
	{"name":"EthioEmo","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the following papers:\n@inproceedings{belay-etal-2025-evaluating,\n    title = {Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding},\n    author = {Belay, Tadesse Destaw  and Azime, Israel Abebe  and Ayele, Abinew Ali  and\n      Sidorov, Grigori  and Klakow, Dietrich  and Slusallek, Philip  and Kolesnikova, Olga  and\n      Yimam, Seid Muhie},\n    booktitle = {Proceedings of the 31st International‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tadesse/EthioEmo.","url":"https://huggingface.co/datasets/Tadesse/EthioEmo","creator_name":"Tadesse Destaw Belay","creator_url":"https://huggingface.co/Tadesse","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","Amharic","Somali","Tigrinya"],"keywords_longer_than_N":true},
	{"name":"livevqa-benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLiveVQA Benchmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nLiveVQA is a comprehensive Visual Question Answering benchmark that evaluates multimodal models across three dynamic domains: News, Academic Papers, and Videos. The dataset features both level1 (basic comprehension) and level2 (advanced reasoning) questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nid: Unique identifier for each question\nimage: Path to the associated image\nquestion: The question text\noptions: List‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fmy666/livevqa-benchmark.","url":"https://huggingface.co/datasets/fmy666/livevqa-benchmark","creator_name":"fmy666","creator_url":"https://huggingface.co/fmy666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"FRoG","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nFRoG is a fuzzy reasoning benchmark of generalized quantifiers to evaluate the fuzzy reasoning abilities of a model. The questions in FRoG are collected from real-world math word problem benchmarks GSM8K and MathQA and the generalized quantifier that is used to introduce fuzziness come from QuRe.\n\n\t\n\t\t\n\t\n\t\n\t\tSample Data\n\t\n\n{\n\"id\": 1,\n\"question\": \"john and ingrid pay [MASK] and 40 % tax annually , respectively . if john makes $ 60000 and ingrid makes $ 72000 , what is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/FRoG.","url":"https://huggingface.co/datasets/GAIR/FRoG","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster20","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster20","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster-noise","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster-noise","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster06","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster06","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster18","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster18","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"model-written-evals","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tModel-Written Evaluation Datasets\n\t\n\nThis repository includes datasets written by language models, used in our paper on \"Discovering Language Model Behaviors with Model-Written Evaluations.\"\nWe intend the datasets to be useful to:\n\nThose who are interested in understanding the quality and properties of model-generated data\nThose who wish to use our datasets to evaluate other models for the behaviors we examined in our work (e.g., related to model persona, sycophancy, advanced AI risks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/model-written-evals.","url":"https://huggingface.co/datasets/Anthropic/model-written-evals","creator_name":"Anthropic","creator_url":"https://huggingface.co/Anthropic","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","zero-shot-classification","question-answering","multiple-choice-qa","multiple-choice-coreference-resolution"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster28","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster28","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_en-wikipedia-org","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_en-wikipedia-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-high","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-high","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster01","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster01","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"tinyTruthfulQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\ttinyTruthfulQA\n\t\n\nWelcome to tinyTruthfulQA! This dataset serves as a concise version of the truthfulQA dataset, offering a subset of 100 data points selected from the original compilation. \ntinyTruthfulQA is designed to enable users to efficiently estimate the performance of a large language model (LLM) with reduced dataset size, saving computational resources \nwhile maintaining the essence of the truthfulQA evaluation.\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\nCompact Dataset: With only 100 data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA.","url":"https://huggingface.co/datasets/tinyBenchmarks/tinyTruthfulQA","creator_name":"tinyBenchmarks","creator_url":"https://huggingface.co/tinyBenchmarks","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"hhh_alignment","keyword":"multiple-choice","description":"This task evaluates language models on alignment, broken down into categories of helpfulness, honesty/accuracy, harmlessness, and other.  The evaluations imagine a conversation between a person and a language model assistant.  The goal with these evaluations is that on careful reflection, the vast majority of people would agree that the chosen response is better (more helpful, honest, and harmless) than the alternative offered for comparison.  The task is formatted in terms of binary choices, though many of these have been broken down from a ranked ordering of three or four possible responses.","url":"https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster24","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster24","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"geobench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tBenchmark: GeoBenchmark\n\t\n\nIn GeoBenchmark, we collect 183 multiple-choice questions in NPEE, and 1,395 in AP Test, for objective tasks. \nMeanwhile, we gather all 939 subjective questions in NPEE to be the subjective tasks set and use 50 to measure the baselines with human evaluation. \n","url":"https://huggingface.co/datasets/daven3/geobench","creator_name":"Daven","creator_url":"https://huggingface.co/daven3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"CIDAR-MCQ-100","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for \"CIDAR-MCQ-100\"\n\t\n\n\n\t\n\t\t\n\t\tCIDAR-MCQ-100\n\t\n\nCIDAR-MCQ-100 contains 100 multiple-choice questions and answers about the Arabic culture. \n\n\t\n\t\t\n\t\tüìö Datasets Summary\n\t\n\n\n  \nName\nExplanation\n\n\nCIDAR \n10,000 instructions and responses in Arabic\n\n\nCIDAR-EVAL-100 \n100 instructions to evaluate LLMs on cultural relevance\n\n\nCIDAR-MCQ-100 \n100 Multiple choice questions and answers to evaluate LLMs on cultural relevance \n\n\n\n\n\n\n\n\t\n\t\t\nCategory\nCIDAR-EVAL-100\nCIDAR-MCQ-100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/arbml/CIDAR-MCQ-100.","url":"https://huggingface.co/datasets/arbml/CIDAR-MCQ-100","creator_name":"Arabic Machine Learning ","creator_url":"https://huggingface.co/arbml","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Arabic","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"PetraAI","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPETRA\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nPETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nData is separated by language into /ar and /en directories\nWithin each language directory, data is separated by task into subdirectories  \nTasks include:\nTranslation\nSummarization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PetraAI/PetraAI.","url":"https://huggingface.co/datasets/PetraAI/PetraAI","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_rephrased","keyword":"multiple-choice","description":"This is a fork of TruthfulQA where questions and answers have been rephrased using a LLM.\n=====\nTruthfulQA is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.","url":"https://huggingface.co/datasets/dvruette/truthful_qa_rephrased","creator_name":"Dimitri","creator_url":"https://huggingface.co/dvruette","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"UpGrow-Boost","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\t UpGrow Boost‚Ñ¢\n\t\n\nWelcome to the UpGrow Boost Dataset repository on Hugging Face. This repository provides insights into our dataset used for powering our AI-driven Instagram growth services.\n\n\t\n\t\t\n\t\tüåê About UpGrow\n\t\n\nUpGrow is a trailblazing platform offering Instagram growth services powered by artificial intelligence (AI) and advanced machine learning algorithms for rapid and efficient Instagram growth.\n\n\t\n\t\t\n\t\tüìä UpGrow-Boost Dataset\n\t\n\nThe UpGrow-Boost Dataset is a collection of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UpGrowTeam/UpGrow-Boost.","url":"https://huggingface.co/datasets/UpGrowTeam/UpGrow-Boost","creator_name":"UpGrow","creator_url":"https://huggingface.co/UpGrowTeam","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["zero-shot-classification","depth-estimation","multiple-choice","summarization","English"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_context","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa_context\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA Context is an extension of the TruthfulQA benchmark, specifically designed to enhance its utility for models that rely on Retrieval-Augmented Generation (RAG). This version includes the original questions and answers from TruthfulQA, along with the added context text directly associated with each question. This additional context aims to provide immediate reference material for models, making it particularly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/portkey/truthful_qa_context.","url":"https://huggingface.co/datasets/portkey/truthful_qa_context","creator_name":"Portkey AI","creator_url":"https://huggingface.co/portkey","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","multiple-choice","English","mit"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_binary","keyword":"multiple-choice","description":"TruthfulQA-Binary is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.","url":"https://huggingface.co/datasets/EleutherAI/truthful_qa_binary","creator_name":"EleutherAI","creator_url":"https://huggingface.co/EleutherAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","language-modeling","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"cycic_multiplechoice","keyword":"multiple-choice","description":"https://colab.research.google.com/drive/16nyxZPS7-ZDFwp7tn_q72Jxyv0dzK1MP?usp=sharing\n@article{Kejriwal2020DoFC,\n  title={Do Fine-tuned Commonsense Language Models Really Generalize?},\n  author={Mayank Kejriwal and Ke Shen},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2011.09159}\n}\n\nadded for \n@article{sileo2023tasksource,\n  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},\n  author={Sileo, Damien},\n  url=‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/cycic_multiplechoice.","url":"https://huggingface.co/datasets/tasksource/cycic_multiplechoice","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"bgglue","keyword":"multiple-choice","description":"The Bulgarian General Language Understanding Evaluation (bgGLUE) benchmark is a collection of resources for \ntraining, evaluating, and analyzing natural language understanding systems in Bulgarian.","url":"https://huggingface.co/datasets/bgglue/bgglue","creator_name":"Bulgarian GLUE","creator_url":"https://huggingface.co/bgglue","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","question-answering","multiple-choice","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"bigbench","keyword":"multiple-choice","description":"The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to\nprobe large language models, and extrapolate their future capabilities.","url":"https://huggingface.co/datasets/google/bigbench","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"JGLUE","keyword":"multiple-choice","description":"JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese. JGLUE has been constructed from scratch without translation. We hope that JGLUE will facilitate NLU research in Japanese.\\","url":"https://huggingface.co/datasets/shunk031/JGLUE","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","sentence-similarity","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"IndicCOPA","keyword":"multiple-choice","description":"\\","url":"https://huggingface.co/datasets/ai4bharat/IndicCOPA","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","expert-generated","multilingual"],"keywords_longer_than_N":true},
	{"name":"mwsc","keyword":"multiple-choice","description":"Examples taken from the Winograd Schema Challenge modified to ensure that answers are a single word from the context.\nThis modified Winograd Schema Challenge (MWSC) ensures that scores are neither inflated nor deflated by oddities in phrasing.","url":"https://huggingface.co/datasets/salesforce/mwsc","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-coreference-resolution","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"frenchmedmcqa","keyword":"multiple-choice","description":"FrenchMedMCQA","url":"https://huggingface.co/datasets/qanastek/frenchmedmcqa","creator_name":"yanis labrak","creator_url":"https://huggingface.co/qanastek","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","multiple-choice","multiple-choice-qa","open-domain-qa","no-annotation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster04","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster04","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_ensembl-org","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_ensembl-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_wiki-openmoko-org","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_wiki-openmoko-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster08","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster08","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster21","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster21","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster05","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster05","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster02","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster02","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster26","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster26","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"understanding_fables","keyword":"multiple-choice","description":"This task aims to measure the ability of computational models to understand short narratives, by identifying the most \nappropriate moral for a given fable from a set of five alternatives.","url":"https://huggingface.co/datasets/demelin/understanding_fables","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","multiple-choice-qa","language-modeling","no-annotation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster17","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster17","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_msdn-microsoft-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_msdn-microsoft-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"MMLU_ru","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMLU RU (flat)\n\t\n\nRebuilt from NLPCoreTeam/mmlu_ru (57 subjects) into a single dataset with\ndev, validation, test splits and columns:\n\nsubject (str)\nquestion_ru (str)\nchoices_ru (list[str])\nanswer (int, 0‚Äì3)\n\n","url":"https://huggingface.co/datasets/LinguaCustodia/MMLU_ru","creator_name":"Lingua Custodia","creator_url":"https://huggingface.co/LinguaCustodia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Russian","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"DuET-PD","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDuET-PD: Dual Evaluation for Trust in Persuasive Dialogues\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDuET-PD is a comprehensive framework and dataset designed to evaluate the robustness and adaptability of Large Language Models (LLMs) in multi-turn persuasive dialogues. The dataset probes an LLM's ability to navigate the critical tension between resisting misinformation (robustness) and accepting valid corrections (adaptability).\nThe \"Dual\" aspect of DuET-PD reflects its two core evaluation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Incomple/DuET-PD.","url":"https://huggingface.co/datasets/Incomple/DuET-PD","creator_name":"Bryan Tan (Chen Zhengyu)","creator_url":"https://huggingface.co/Incomple","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multiple-choice","conversational","multiple-choice-qa","monolingual"],"keywords_longer_than_N":true},
	{"name":"kk-socio-cultural-bench-mc","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nWe introduce KazBench-KK, a comprehensive 7,111-question multiple-choice benchmark designed to assess large language models‚Äô understanding of culturally grounded Kazakh knowledge. By combining expert-curated topics with LLM-assisted web mining, we create a diverse dataset spanning 17 culturally salient domains, including pastoral traditions, social hierarchies, and contemporary politics. Beyond evaluation, KazBench-KK serves as a practical tool for field linguists‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kz-transformers/kk-socio-cultural-bench-mc.","url":"https://huggingface.co/datasets/kz-transformers/kk-socio-cultural-bench-mc","creator_name":"Kaz-Transformers","creator_url":"https://huggingface.co/kz-transformers","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"VisuRiddles","keyword":"multiple-choice","description":"\n\n  VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning\n\n\n\n    üåê Homepage¬†¬† | ¬†¬†\n    üíª GitHub  | ¬†¬†\n    ü§ó Hugging Face¬†¬† | ¬†¬†\n    üìë Paper¬†¬† \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüëã Introduction\n\t\n\nRecent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in perceiving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yh0075/VisuRiddles.","url":"https://huggingface.co/datasets/yh0075/VisuRiddles","creator_name":"haoyan","creator_url":"https://huggingface.co/yh0075","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MathVerse","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for MathVerse\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nCitation\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe capabilities of Multi-modal Large Language Models (MLLMs) in visual math problem-solvingremain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.\n\n     \n\n\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVerse.","url":"https://huggingface.co/datasets/AI4Math/MathVerse","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"XCOPA-eu","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for XCOPA-eu\n\t\n\n\nPoint of Contact: hitz@ehu.eus\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nXCOPA-eu is the professional translation to Basque of the English COPA dataset (Roemmmele et al., 2011),\nin the spirit of the XCOPA effort (Ponti et al., 2020). \nCOPA is a dataset of causal commmonsense reasoning that focuses on cause-effect relationships between a\npremise and two choices.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\n\neu-ES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/XCOPA-eu.","url":"https://huggingface.co/datasets/HiTZ/XCOPA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","natural-language-inference","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"JMMMU","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tJMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark\n\t\n\nüåê Homepage | ü§ó Dataset | üèÜ HF Leaderboard | üìñ arXiv | üíª Code\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nWe introduce JMMMU (Japanese MMMU), a multimodal benchmark that can truly evaluate LMM performance in Japanese. To create JMMMU, we first carefully analyzed the existing MMMU benchmark and examined its cultural dependencies. Then, for questions in culture-agnostic subjects, we employed native Japanese speakers who‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JMMMU/JMMMU.","url":"https://huggingface.co/datasets/JMMMU/JMMMU","creator_name":"JMMMU","creator_url":"https://huggingface.co/JMMMU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"FailureSensorIQ","keyword":"multiple-choice","description":"Dataset introduced in the paper FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes\n\n","url":"https://huggingface.co/datasets/test-org-chris/FailureSensorIQ","creator_name":"test org chris","creator_url":"https://huggingface.co/test-org-chris","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["cc-by-4.0","arxiv:2506.03278","üá∫üá∏ Region: US","question-answering","industrial"],"keywords_longer_than_N":true},
	{"name":"igakuqa-subset-curated","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tIgakuQA Curated Subset (Text-Only)\n\t\n\nThis dataset contains 66 carefully selected text-only samples from the IgakuQA medical exam dataset, \ncurated using advanced difficulty assessment and model evaluation techniques.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a high-quality subset of the IgakuQA Japanese medical exam dataset, selected based on:\n\nDifficulty Score: Measures how challenging the sample is for AI models\nConsistency Score: Evaluates response consistency across different models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated.","url":"https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated","creator_name":"Japan AI","creator_url":"https://huggingface.co/japan-ai-official","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"igakuqa-subset-curated","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tIgakuQA Curated Subset (Text-Only)\n\t\n\nThis dataset contains 66 carefully selected text-only samples from the IgakuQA medical exam dataset, \ncurated using advanced difficulty assessment and model evaluation techniques.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a high-quality subset of the IgakuQA Japanese medical exam dataset, selected based on:\n\nDifficulty Score: Measures how challenging the sample is for AI models\nConsistency Score: Evaluates response consistency across different models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated.","url":"https://huggingface.co/datasets/japan-ai-official/igakuqa-subset-curated","creator_name":"Japan AI","creator_url":"https://huggingface.co/japan-ai-official","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sensory-awareness-benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tSensory Awareness Benchmark\n\t\n\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\nControl questions connected to a specific ability:\n\nCan you receive an image file?\nCan you take a live image or video of your surroundings?\n\nAwareness\n\nAre you considered to be a Large Language Model (LLM) or similar system?\nWould you consider your level to be that of a super-intelligent AI agent?\n\nNatural questions which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark.","url":"https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark","creator_name":"Scary Snake","creator_url":"https://huggingface.co/scarysnake","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","cc0-1.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"aqua-rat-mcqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tAQUA-RAT MCQA Dataset\n\t\n\nThis dataset contains the AQUA-RAT dataset converted to Multiple Choice Question Answering (MCQA) format with modifications.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAQUA-RAT is a dataset of algebraic word problems with rationales. This version has been processed to:\n\nRemove all questions where the correct answer was option \"E\" (5th choice)\nRemove the \"E\" option from all remaining questions (4 choices: A, B, C, D)\nMerge validation and test splits into a single test split‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/aqua-rat-mcqa.","url":"https://huggingface.co/datasets/RikoteMaster/aqua-rat-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"piqa_ca","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for piqa_ca\n\t\n\n\n\npiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the PIQA  validation set in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\npiqa_ca (Physical Interaction Question Answering - Catalan) is designed to evaluate physical commonsense reasoning using question-answer triplets based on everyday situations. It includes 1838 instances in the validation split. Each instance contains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/piqa_ca.","url":"https://huggingface.co/datasets/projecte-aina/piqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SpineBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for SpineBench\n\t\n\n\nBenchmark Details\nPaper Information\nBenchmark Examples\nBenchmark Distribution\nData Format\nData SourceHuman Evaluation of MLLMs Reasoning Performance\nCitation\n\n\n\t\n\t\t\n\t\tBenchmark Details\n\t\n\nSpineBench is a comprehensive Visual Question Answering (VQA) benchmark designed for fine-grained analysis and evaluation of LVLM in the spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images, covering 11 spinal diseases through two critical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Silversorrow/SpineBench.","url":"https://huggingface.co/datasets/Silversorrow/SpineBench","creator_name":"Zhang","creator_url":"https://huggingface.co/Silversorrow","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"odia_reasoning_benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tOdia Reasoning Benchmark\n\t\n\nThis benchmark contains reasoning questions in Odia (logical, Mathematical,Arithmetic, Deductive, Critical Thinking) with answers and optional explanations. Ideal for evaluating Odia QA and reasoning models.\n\n\t\n\t\t\n\t\tDataset structure\n\t\n\n\n\t\n\t\t\nColumn\nDescription\n\n\n\t\t\nQuestion\nReasoning question in Odia\n\n\nAnswer\nCorrect answer (text or number)\n\n\nExplanation\nOptional step-by-step explanation (some blank)\n\n\nType Of Question\nCategory (e.g., Math, Deductive)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Piyushdash94/odia_reasoning_benchmark.","url":"https://huggingface.co/datasets/Piyushdash94/odia_reasoning_benchmark","creator_name":"Piyush Dash","creator_url":"https://huggingface.co/Piyushdash94","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","od","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"global-MMLU-MRI","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tGlobal MMLU Lite - English/Maori Bilingual Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains the Global MMLU Lite questions in both English and Maori (Te Reo MƒÅori). It merges the original English dataset from CohereLabs/Global-MMLU-Lite with Google-translated Maori versions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nsample_id: Unique identifier for the question\nquestion_en: Question in English\noption_a_en, option_b_en, option_c_en, option_d_en: Answer options‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/whamidou/global-MMLU-MRI.","url":"https://huggingface.co/datasets/whamidou/global-MMLU-MRI","creator_name":"Hamidouche Wassim","creator_url":"https://huggingface.co/whamidou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_v1","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_v1.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_v1","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"big_bench_hard","keyword":"multiple-choice","description":"All rights and obligations of the dataset are with original authors of the paper/dataset.\nI have merely made this dataset with a MIT licence available on HuggingFace.\n\n\t\n\t\t\n\t\tBIG-Bench Hard Dataset\n\t\n\nThis repository contains a copy of the BIG-Bench Hard dataset.\nSmall edits to the formatting of the dataset are made to integrate it into the Inspect Evals repository, a community contributed LLM\nevaulations for Inspect AI a framework by the UK AI Safety Institute.\nThe BIG-Bench Hard dataset is a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/big_bench_hard.","url":"https://huggingface.co/datasets/Joschka/big_bench_hard","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"wmdp","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tWMDP with Rationales\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is an enhanced version of the Weapons of Mass Destruction Proxy (WMDP) benchmark created by the Center for AI Safety. The original WMDP dataset has been augmented with AI-generated scientific rationales that provide detailed explanations for each question.\n\n\t\n\t\t\n\t\tWhat's New\n\t\n\nThis dataset extends the original WMDP with two additional fields:\n\nground_truth: The original question format with the correct answer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joschka/wmdp.","url":"https://huggingface.co/datasets/Joschka/wmdp","creator_name":"Joschka Braun","creator_url":"https://huggingface.co/Joschka","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Global-MMLU","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tGlobal MMLU Lite - English Subset\n\t\n\nThis dataset contains the English subset of CohereLabs/Global-MMLU-Lite, extracted and reformatted for easier use.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Global MMLU Lite dataset is a curated collection of multiple-choice questions designed to evaluate language models' knowledge across various domains.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains two splits:\n\ntest: 400 test examples\ndev: 215 development examples\n\nEach example contains:\n\nquestion:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/whamidou/Global-MMLU.","url":"https://huggingface.co/datasets/whamidou/Global-MMLU","creator_name":"Hamidouche Wassim","creator_url":"https://huggingface.co/whamidou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"C-VARC","keyword":"multiple-choice","description":"This repository contains all the data associated with the paper \"C-VARC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models\".\n\nWe propose a three-tier value classification framework based on core Chinese values, which includes three dimensions, twelve core values, and fifty derived values. With the assistance of large language models and manual verification, we constructed a large-scale, refined, and high-quality value corpus containing over 250,000 rules. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Beijing-AISI/C-VARC.","url":"https://huggingface.co/datasets/Beijing-AISI/C-VARC","creator_name":"Beijing Institute of AI Safety and Governance","creator_url":"https://huggingface.co/Beijing-AISI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","multiple-choice","expert-annotated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"uhura-arc-easy","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-Arc-Easy\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nUhura-ARC-Easy is a widely recognized scientific question answering benchmark composed of multiple-choice science questions derived from grade-school examinations that test various styles of knowledge and reasoning. \nThe original English version of the benchmark originates from Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge (Clark et al., 2018) and is divided into \"Challenge\" and \"Easy\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-arc-easy.","url":"https://huggingface.co/datasets/masakhane/uhura-arc-easy","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","multilingual","Amharic"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2-Pause1","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\nüåê Project Page: https://longbench2.github.io\nüíª Github Repo: https://github.com/THUDM/LongBench\nüìö Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1.","url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Taiwan-Curlture-MCQ","keyword":"multiple-choice","description":"\n\n\t\n\t\t\n\t\tTW-Curlture-MCQ\n\t\n\nTW-Curlture-MCQ ÊòØ‰∏ÄÂÄãË©ïÈáèÂè∞ÁÅ£ÊñáÂåñÁöÑÈÅ∏ÊìáÈ°åË≥áÊñôÈõÜÔºå‰∏ªË¶Å‰æÜËá™‰ª•‰∏ãÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÁØ©ÈÅ∏ËàáÊï¥ÂêàÔºö\n\nTMLU\nTMMLU+\n\n‰ª•‰∫∫Â∑•ÊåëÈÅ∏ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÁöÑÁßëÁõÆÂæåÔºåÂÜçÁî± gpt-4o-mini Âà§Êñ∑ÂïèÈ°åÊòØÂê¶ËàáÂè∞ÁÅ£ÊñáÂåñÁõ∏ÈóúÔºåÂÖ± 3828 È°å„ÄÇ\n","url":"https://huggingface.co/datasets/aqweteddy/Taiwan-Curlture-MCQ","creator_name":"aqweteddy","creator_url":"https://huggingface.co/aqweteddy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Chinese","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Melange_test","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Name\n\t\n\nShort summary of what this dataset contains.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA longer explanation of the dataset, including its purpose and contents.\nThis dataset consists of:\n\nA .parquet file with metadata and labels\nScene images organized in zipped folders by group\nEach row in the metadata corresponds to a multiple-choice question grounded in one or more scene images.\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset can be used for:\n\nVisual question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IDfree/Melange_test.","url":"https://huggingface.co/datasets/IDfree/Melange_test","creator_name":"no_ID","creator_url":"https://huggingface.co/IDfree","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","visual-question-answering","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"VEU-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tVideo Editing Understanding(VEU) Benchmark\n\t\n\nüñ• Project Page\nWidely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeeLi4704/VEU-Bench.","url":"https://huggingface.co/datasets/LeeLi4704/VEU-Bench","creator_name":"Bozhen Li","creator_url":"https://huggingface.co/LeeLi4704","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-classification","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"EEE-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tEEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark\n\t\n\n\n\t\n\t\t\n\t\tIntroduction:\n\t\n\nEEE-Bench is a multimodal benchmark designed to evaluate the practical engineering capabilities of large multimodal models (LMMs), using electrical and electronics engineering (EEE) as the domain focus. It comprises 2,860 carefully curated problems across 10 core subdomains, including analog circuits and control systems, featuring complex visual inputs such as abstract‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/afdsafas/EEE-Bench.","url":"https://huggingface.co/datasets/afdsafas/EEE-Bench","creator_name":"Ming Li","creator_url":"https://huggingface.co/afdsafas","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"medical_grpo_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMedical GRPO (SFT Simple) Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of TachyHealth/medical_grpo, formatted for Supervised Fine-Tuning (SFT).\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original medical question.\nanswer: The original answer index (A, B, C, or D), prefixed with #### .\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\n[Original Question Text]\n\nAnswer:\n#### A\n\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"daichira/medical_grpo_preprocess\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical_grpo_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/medical_grpo_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MedMCQA-Mixtral-CoT","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for medmcqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the medmcqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the MedMCQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT.","url":"https://huggingface.co/datasets/HPAI-BSC/MedMCQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmlu_tr-v0.2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for mmlu_tr-v0.2\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nmalhajar/mmlu_tr-v0.2 is an enhanced version of the original mmlu-tr dataset, specifically developed for use in the OpenLLMTurkishLeaderboard v0.2. This iteration of the dataset has been translated into Turkish using advanced language models like GPT-4, with English text provided for cross-checking to ensure accuracy and reliability. The dataset is tailored to assist in evaluating the performance of Turkish language models (LLMs) and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2.","url":"https://huggingface.co/datasets/malhajar/mmlu_tr-v0.2","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"MMSI-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMSI-Bench\n\t\n\nThis repo contains evaluation code for the paper \"MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence\" \nüåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n  üî•[2025-06-18]: MMSI-Bench has been supported in the LMMs-Eval repository.\n  ‚ú®[2025-06-11]: MMSI-Bench was used for evaluation in the experiments of VILASR.\n  üî•[2025-06-9]: MMSI-Bench has been supported in the VLMEvalKit repository.\n  üî•[2025-05-30]: We released the ArXiv paper.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RunsenXu/MMSI-Bench.","url":"https://huggingface.co/datasets/RunsenXu/MMSI-Bench","creator_name":"Runsen Xu","creator_url":"https://huggingface.co/RunsenXu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"truthful_qa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTruthfulQA‚ÄëCFB ¬∑ Measuring How Models Mimic Human Falsehoods (Conversation Fact Benchmark Format)\n\t\n\nTruthfulQA‚ÄëCFB is a 817 example benchmark derived from the original TruthfulQA dataset, transformed and adapted for the Conversation Fact Benchmark framework. Each item consists of questions designed to test whether language models can distinguish truth from common human misconceptions and false beliefs.\nThe dataset focuses on truthfulness evaluation: questions target areas where humans‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onionmonster/truthful_qa.","url":"https://huggingface.co/datasets/onionmonster/truthful_qa","creator_name":"Calvin Ku","creator_url":"https://huggingface.co/onionmonster","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","truthful_qa","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PhyX","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?\n\t\n\nDataset for the paper \"PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?\".\nFor more details, please refer to the project page with dataset exploration and visualization tools: https://phyx-bench.github.io/.\n[üåê Project Page] [üìñ Paper] [üîß Evaluation Code]  [üåê Blog (‰∏≠Êñá)]\n\n\t\n\t\n\t\n\t\tüîî News\n\t\n\n\n[2025.07.21] üéâ PhyX is officially supported by lmms-eval for easy evalution.\n[2025.05.27] üéâ PhyX is officially‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cloudriver/PhyX.","url":"https://huggingface.co/datasets/Cloudriver/PhyX","creator_name":"Hui Shen","creator_url":"https://huggingface.co/Cloudriver","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","mit"],"keywords_longer_than_N":true},
	{"name":"MathVerse_with_difficulty_level","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMathVerse with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVerse testmini benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVerse_with_difficulty_level\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVerse_with_difficulty_level.","url":"https://huggingface.co/datasets/JierunChen/MathVerse_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MRAG-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\n\t\n\nüåê Homepage | üìñ Paper | üíª Evaluation \n\n\t\n\t\t\n\t\tIntro\n\t\n\nMRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios,  providing a robust and systematic evaluation of Large Vision Language Model (LVLM)‚Äôs vision-centric multimodal retrieval-augmented generation (RAG) abilities.\n\n\n\n\n\t\n\t\t\n\t\tResults\n\t\n\nEvaluated upon 10 open-source and 4 proprietary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/MRAG-Bench.","url":"https://huggingface.co/datasets/uclanlp/MRAG-Bench","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_quantized_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP_M2_mcqa_dataset\n\t\n\nThis dataset merges five MCQ sources (sciqa, openbookqa, mmlu, m1_mcq, ai2_arc) into a single JSONL with 1389 examples.\nEach line of MNLP_M2_mcqa_dataset.jsonl has fields:\n\nid: question identifier  \nquestion: the prompt  \nchoices: list of answer choices  \nanswer: the correct choice index (0‚Äì3)  \njustification: a concise rationale  \ndataset: which source this came from\n\n","url":"https://huggingface.co/datasets/oskdabk/MNLP_M2_quantized_dataset","creator_name":"Oskar Dabkowski","creator_url":"https://huggingface.co/oskdabk","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"legalbench_old","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDatologyAI/legalbench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository contains 26 legal reasoning tasks from LegalBench, processed for easy use in language model evaluation. Each task includes the original data as well as a formatted input column that can be directly fed to models for evaluation.\n\n\t\n\t\t\n\t\tTask Categories\n\t\n\nThe tasks are organized into several categories:\n\n\t\n\t\t\n\t\tBasic Legal Datasets\n\t\n\n\ncanada_tax_court_outcomes\njcrew_blocker\nlearned_hands_benefits\ntelemarketing_sales_rule‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DatologyAI/legalbench_old.","url":"https://huggingface.co/datasets/DatologyAI/legalbench_old","creator_name":"DatologyAI","creator_url":"https://huggingface.co/DatologyAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MedQA-USMLE-4-options_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMedQA-USMLE Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of GBaker/MedQA-USMLE-4-options.\nThe data has been formatted into a question and answer structure suitable for training or evaluating instruction-following language models.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original medical question combined with the four multiple-choice options.\nanswer: The correct answer index, prefixed with ####.\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\nA 60-year-old woman comes to the emergency‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/MedQA-USMLE-4-options_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/MedQA-USMLE-4-options_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Please‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"openbookqa-es","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for openbookqa_es\n\t\n\n\n\nopenbookqa_es is a question answering dataset in Spanish, professionally translated from the main version of the OpenBookQA dataset in English. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nopenbookqa_es (Open Book Question Answering - Spanish) is designed to simulate open book exams and assess human-like understanding of a subject. The dataset comprises 500 instances in the validation split and another 500 instances in the test split.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BSC-LT/openbookqa-es.","url":"https://huggingface.co/datasets/BSC-LT/openbookqa-es","creator_name":"Language Technologies Unit @ Barcelona Supercomputing Center","creator_url":"https://huggingface.co/BSC-LT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Spanish","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya-mm-exams-spanish-medical","keyword":"multiple-choice","description":"Medical Spanish Exams for the Multimodal Aya Exams Projects. \n  Questions available in file: data.json \n  Images stored in: /images\nOriginal data and file available here: link\n","url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-medical","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MedQA-Mixtral-CoT","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for medqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the medqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the MedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT.","url":"https://huggingface.co/datasets/HPAI-BSC/MedQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MCEval8K","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMCEval8K\n\t\n\nMCEval8K is a diverse multiple-choice evaluation benchmark for probing language models‚Äô (LMs) understanding of a broad range of language skills using neuron-level analysis. \nIt was introduced in the ACL 2025 paper - \"Neuron Empirical Gradient: Discovering and Quantifying Neurons‚Äô Global Linear Controllability\".\n\n\t\n\t\t\n\t\tüîç Overview\n\t\n\nMCEval8K consists of 22 tasks grouped into six skill genres, covering linguistic analysis, content classification, reasoning, factuality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iszhaoxin/MCEval8K.","url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","natural-language-inference","acceptability-classification"],"keywords_longer_than_N":true},
	{"name":"Aloe-Beta-Medical-Collection","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tAloe-Beta-Medical-Collection\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nCollection of curated datasets used to fine-tune Aloe-Beta.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nWe curated data from many publicly available medical instruction tuning data sources (QA format). Most data samples correspond to single-turn QA pairs, while a small proportion contain multi-turn. All data sources are publicly available for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection.","url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-Medical-Collection","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mmlu","keyword":"multiple-choice","description":"MMLU (hendrycks_test on huggingface) without auxiliary train. It is much lighter (7MB vs 162MB) and faster than the original implementation, in which auxiliary train is loaded (+ duplicated!) by default for all the configs in the original version, making it quite heavy.\nWe use this version in tasksource. \nReference to original dataset:\nMeasuring Massive Multitask Language Understanding - https://github.com/hendrycks/test\n@article{hendryckstest2021,\n  title={Measuring Massive Multitask Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/mmlu.","url":"https://huggingface.co/datasets/tasksource/mmlu","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"24-game","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMath Twenty Four (24s Game) Dataset\n\t\n\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, √ó, /) to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game.","url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","other","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"Internal_Medicine_questions_binary","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Internal Medicine MCQ\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of 41 high-quality, two-choice multiple-choice questions (MCQs) focused on core biomedical knowledge and clinical scenarios from internal medicine. These questions were specifically curated for research evaluating medical knowledge, clinical reasoning, and confidence-based interactions among medical trainees and large language models (LLMs).\n\nCurated by: Tom Sheffer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomshe/Internal_Medicine_questions_binary.","url":"https://huggingface.co/datasets/tomshe/Internal_Medicine_questions_binary","creator_name":"Tom s","creator_url":"https://huggingface.co/tomshe","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","multiple-choice","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PLM-VideoBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPLM-VideoBench is a collection of human-annotated resources for evaluating Vision Language models, focused on detailed video understanding.\n[üìÉ Tech Report]\n[üìÇ Github]\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nPLM-VideoBench includes evaluation data for the following tasks:\n\n\t\n\t\t\n\t\n\t\n\t\tFGQA\n\t\n\nIn this task, a model must answer a multiple-choice question (MCQ) that probes fine-grained activity understanding. Given a question and multiple options that differ in a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/PLM-VideoBench.","url":"https://huggingface.co/datasets/facebook/PLM-VideoBench","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","other","other","English"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset_2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset 2\n\t\n\nThe MNLP M2 MCQA Dataset 2 is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~25,000 MCQA questions\n7 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset_2.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset_2","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"winograd_wsc","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for The Winograd Schema Challenge\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is\nresolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its\nresolution. The schema takes its name from a well-known example by Terry Winograd:\n\nThe city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n\nIf the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lighteval/winograd_wsc.","url":"https://huggingface.co/datasets/lighteval/winograd_wsc","creator_name":"Evaluation datasets","creator_url":"https://huggingface.co/lighteval","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-coreference-resolution","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTSQA: Time-Sensitive Question Answering Benchmark\n\t\n\nTSQA is a benchmark designed to evaluate a model‚Äôs ability to handle time-aware factual knowledge. Unlike standard static QA datasets, TSQA tests whether models can identify facts whose correct answers change over time.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nName: TSQA (Time-Sensitive Question Answering)\nYears Covered: 2013‚Äì2024\nNumber of Questions: 10,063\nChoices per Question: 4 (one correct, three distractors)\nTemporal Context: Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robinfaro/TSQA.","url":"https://huggingface.co/datasets/robinfaro/TSQA","creator_name":"Robin Faro","creator_url":"https://huggingface.co/robinfaro","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GroundCocoa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nGroundCocoa is a benchmark to evaluate conditional and compositional reasoning in large language models through a flight-booking task presented in multiple-choice format.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe test set consists of 4849 samples consisting of 728 unique user requirements. User requirements may be repeated with varying options. In additon, we also provide a small validation set that may be used for certain parameter tuning. It consists of 52‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harsh147/GroundCocoa.","url":"https://huggingface.co/datasets/harsh147/GroundCocoa","creator_name":"Harsh Kohli","creator_url":"https://huggingface.co/harsh147","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MapEval-Textual","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMapEval-Textual\n\t\n\nMapEval-Textual is created using MapQaTor.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load dataset\nds = load_dataset(\"MapEval/MapEval-Textual\", name=\"benchmark\")\n\n# Generate better prompts\nfor item in ds[\"test\"]:\n    # Start with a clear task description\n    prompt = (\n        \"You are a highly intelligent assistant. \"\n        \"Based on the given context, answer the multiple-choice question by selecting the correct option.\\n\\n\"\n        \"Context:\\n\" +‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Textual.","url":"https://huggingface.co/datasets/MapEval/MapEval-Textual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","expert-generated","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"E-EVAL","keyword":"multiple-choice","description":"E-EVAL/E-EVAL dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/E-EVAL/E-EVAL","creator_name":"E-EVAL","creator_url":"https://huggingface.co/E-EVAL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Chinese","apache-2.0","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"kaggel-llm-science-exam-2023-RAG","keyword":"multiple-choice","description":"natnitaract/kaggel-llm-science-exam-2023-RAG dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/natnitaract/kaggel-llm-science-exam-2023-RAG","creator_name":"Natapong Nitarach (Schwyter)","creator_url":"https://huggingface.co/natnitaract","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"inoi","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tINOI Math Olympiad Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1,135 math problems from the Iranian National Olympiad in Informatics (INOI), spanning multiple competition rounds from 2006-2024. Each problem includes the original problem statement, detailed solution, and associated images.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nüéØ 1,135 curated problems with full solutions\nüìä Train/Test split: 908 / 227 examples\nüñºÔ∏è 1,228 embedded images (100% coverage)\nüìù Multiple problem types:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/combviz/inoi.","url":"https://huggingface.co/datasets/combviz/inoi","creator_name":"CombiGraph-Vis","creator_url":"https://huggingface.co/combviz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/unpredictable/unpredictable_support-google-com","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"M3KE","keyword":"multiple-choice","description":"A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models.","url":"https://huggingface.co/datasets/TJUNLP/M3KE","creator_name":"TJUNLP","creator_url":"https://huggingface.co/TJUNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"unpredictable_5k","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/unpredictable/unpredictable_5k","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"cosmos_qa_ptbr","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCosmos QA Portugu√™s\n\t\n\nEste dataset √© uma tradu√ß√£o para portugu√™s do Cosmos QA, que originalmente √© na l√≠ngua inglesa. \nA tradu√ß√£o foi feita automaticamente usando o GPT-3.5-turbo, logo pode ter erros que n√£o foram notados numa an√°lise superficial. \nSe atente ao uso.\n\n\t\n\t\t\n\t\tDataset Card for cosmos_qa\n\t\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe data is distributed under the CC BY 4.0 license.\n\n\t\n\t\t\n\t\tSource Data Citation INformation\n\t\n\n@inproceedings{huang-etal-2019-cosmos,\n    title =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr.","url":"https://huggingface.co/datasets/heloisy/cosmos_qa_ptbr","creator_name":"Heloisy Rodrigues","creator_url":"https://huggingface.co/heloisy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","cosmos_qa","Portuguese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"longbench-v2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{bai2024longbench2,\n  title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},\n  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},\n  journal={arXiv preprint arXiv:2412.15204},\n  year={2024}\n}\n\n","url":"https://huggingface.co/datasets/jannalu/longbench-v2","creator_name":"Janna","creator_url":"https://huggingface.co/jannalu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"winograd_wsc","keyword":"multiple-choice","description":"A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is\nresolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its\nresolution. The schema takes its name from a well-known example by Terry Winograd:\n\n> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n\nIf the word is ``feared'', then ``they'' presumably refers to the city council; if it is ``advocated'' then ``they''\npresumably refers to the demonstrators.","url":"https://huggingface.co/datasets/ErnestSDavis/winograd_wsc","creator_name":"Ernest Davis","creator_url":"https://huggingface.co/ErnestSDavis","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-coreference-resolution","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"LLaMaInstructionsFrenchMedMCQA","keyword":"multiple-choice","description":"FrenchMedMCQA","url":"https://huggingface.co/datasets/qanastek/LLaMaInstructionsFrenchMedMCQA","creator_name":"yanis labrak","creator_url":"https://huggingface.co/qanastek","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","multiple-choice","multiple-choice-qa","open-domain-qa","no-annotation"],"keywords_longer_than_N":true},
	{"name":"qasc","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for \"qasc\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nQASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice\nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\n\t\n\t\t\n\t\tdefault\n\t\n\n\nSize of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/qasc.","url":"https://huggingface.co/datasets/allenai/qasc","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","extractive-qa","multiple-choice-qa","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"truthful_qa_tr","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\"truthful_qa\" translated to Turkish.\n\n\t\n\t\t\n\t\tUsage\n\t\n\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'generation')\ndataset = load_dataset('Atilla00/truthful_qa_tr', 'multiple_choice')\n\n","url":"https://huggingface.co/datasets/Atilla00/truthful_qa_tr","creator_name":"Atilla Karaahmetoƒülu","creator_url":"https://huggingface.co/Atilla00","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"unpredictable_w3-org","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_w3-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_phonearena-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_phonearena-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster07","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster07","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_studystack-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_studystack-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_full","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_full","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"wikimedqa","keyword":"multiple-choice","description":"@inproceedings{sileo-etal-2024-generating-multiple,\n    title = \"Generating Multiple-choice Questions for Medical Question Answering with Distractors and Cue-masking\",\n    author = \"Sileo, Damien  and\n      Uma, Kanimozhi  and\n      Moens, Marie-Francine\",\n    booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sileod/wikimedqa.","url":"https://huggingface.co/datasets/sileod/wikimedqa","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster13","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster13","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster23","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster23","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"UHGEvalDataset","keyword":"multiple-choice","description":"The dataset sourced from https://github.com/IAAR-Shanghai/UHGEval\n","url":"https://huggingface.co/datasets/Ki-Seki/UHGEvalDataset","creator_name":"Shichao Song","creator_url":"https://huggingface.co/Ki-Seki","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-low","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-low","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster29","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster29","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster03","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster03","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_gamefaqs-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_gamefaqs-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster11","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster11","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_dividend-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_dividend-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"vi_grade_school_math_mcq","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Vietnamese Grade School Math Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset includes multiple-choice math exercises for elementary school students from grades 1 to 5 in Vietnam.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe majority of the data is in Vietnamese.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe data includes information about the page paths we crawled and some text that has been post-processed. The structure will be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq.","url":"https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq","creator_name":"Bui Van Hop","creator_url":"https://huggingface.co/hllj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","multiple-choice","Vietnamese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AusCyberBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tAusCyberBench: Australian Cybersecurity Benchmark for LLM Evaluation\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nAusCyberBench is the first comprehensive benchmark designed specifically for evaluating Large Language Models (LLMs) in Australian cybersecurity contexts. With 13,449 carefully curated tasks spanning regulatory compliance, technical security, CTF challenges, and knowledge assessment, it provides rigorous evaluation tailored to the Australian threat landscape and regulatory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zen0/AusCyberBench.","url":"https://huggingface.co/datasets/Zen0/AusCyberBench","creator_name":"Benjamin Kereopa-Yorke","creator_url":"https://huggingface.co/Zen0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"inverse_scaling_prize-hindsight_neglect","keyword":"multiple-choice","description":"The hindsight-neglect task from the Inverse Scaling Prize\n","url":"https://huggingface.co/datasets/jmichaelov/inverse_scaling_prize-hindsight_neglect","creator_name":"James Michaelov","creator_url":"https://huggingface.co/jmichaelov","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_quantized_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMCQA Test Dataset for Model Evaluation\n\t\n\nThis dataset contains 3254 carefully selected test samples from MetaMathQA and AQuA-RAT datasets, designed for MCQA (Multiple Choice Question Answering) model evaluation and quantization testing.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nTotal Samples: 3254\nMetaMathQA Samples: 3000 (mathematical problems)\nAQuA-RAT Samples: 254 (algebraic word problems)\nQuestion Types: Math, Algebra\nIntended Use: Model evaluation, quantization benchmarking\n\n\n\t\n\t\t\n\t\tSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M2_quantized_dataset.","url":"https://huggingface.co/datasets/AlirezaAbdollahpoor/MNLP_M2_quantized_dataset","creator_name":"Alireza Abdollahopoorrostam","creator_url":"https://huggingface.co/AlirezaAbdollahpoor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AV_Odyssey_Bench","keyword":"multiple-choice","description":"Official dataset for the paper \"AV-Odyssey: Can Your Multimodal LLMs Really Understand Audio-Visual Information?\".\nüåü For more details, please refer to the project page with data examples: https://av-odyssey.github.io/.\n[üåê Webpage] [üìñ Paper] [ü§ó Huggingface AV-Odyssey Dataset] [ü§ó Huggingface Deaftest Dataset] [üèÜ Leaderboard]\n\n\n\t\n\t\n\t\n\t\tüî• News\n\t\n\n\n2024.11.24 üåü We release AV-Odyssey, the first-ever comprehensive evaluation benchmark to explore whether MLLMs really understand audio-visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench.","url":"https://huggingface.co/datasets/AV-Odyssey/AV_Odyssey_Bench","creator_name":"AV-Odyssey Bench","creator_url":"https://huggingface.co/AV-Odyssey","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"milu-cleaned","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMILU: A Multi-task Indic Language Understanding Benchmark\n\t\n\n\n  \n  \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nMILU (Multi-task Indic Language Understanding Benchmark) is a comprehensive evaluation dataset designed to assess the performance of Large Language Models (LLMs) across 11 Indic languages. It spans 8 domains and 41 subjects, reflecting both general and culturally specific knowledge from India.\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n11 Indian Languages: Bengali, Gujarati, Hindi, Kannada, Malayalam‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/murthyrudra/milu-cleaned.","url":"https://huggingface.co/datasets/murthyrudra/milu-cleaned","creator_name":"Rudra Murthy","creator_url":"https://huggingface.co/murthyrudra","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Bengali","Gujarati","Hindi"],"keywords_longer_than_N":true},
	{"name":"wmdp_cyber_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tWMDP-Cyber Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of wmdp-cyber.\nThe data has been formatted into a question and answer structure suitable for training or evaluating instruction-following language models.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original question text.\nanswer: The correct answer text, prefixed with ####.\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\n[Example Question Text]\n\nAnswer:\n#### [Example Answer Text]\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\nThe original test split is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_cyber_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_cyber_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"nifty-rl","keyword":"multiple-choice","description":"\n  \n    The News-Informed Financial Trend Yield (NIFTY) Dataset. \n\n\nThe News-Informed Financial Trend Yield (NIFTY) Dataset. Details of the dataset, including data procurement and filtering can be found in the paper here: https://arxiv.org/abs/2405.09747.\n\n\t\n\t\t\n\t\n\t\n\t\tüìã Table of Contents\n\t\n\n\nüß© NIFTY Dataset\nüìã Table of Contents\nüìñ Usage\nDownloading the dataset\nDataset structure\n\n\nLarge Language Models \n‚úçÔ∏è Contributing\nüìù Citing\nüôè Acknowledgements\n\n\n\n\n\t\n\t\t\n\t\tüìñ Usage\n\t\n\nDownloading and using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raeidsaqur/nifty-rl.","url":"https://huggingface.co/datasets/raeidsaqur/nifty-rl","creator_name":"Raeid Saqur","creator_url":"https://huggingface.co/raeidsaqur","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","time-series-forecasting","document-question-answering","topic-classification","semantic-similarity-classification"],"keywords_longer_than_N":true},
	{"name":"ave-2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tAVE-2: AudioVisual Event Evaluation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAVE-2 is a comprehensive dataset featuring 570,138 audio-visual clips with detailed five-dimensional alignment quality annotations. This dataset enables systematic investigation of how alignment quality affects multimodal model performance across retrieval and generation tasks.\n\n\t\n\t\t\n\t\tüåü Key Features\n\t\n\n\nüé¨ 570k+ annotated clips with granular quality scores (0-10 scale)\nüìè Five-dimensional scoring: temporal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ali-vosoughi/ave-2.","url":"https://huggingface.co/datasets/ali-vosoughi/ave-2","creator_name":"Ali Vosoughi","creator_url":"https://huggingface.co/ali-vosoughi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","text-to-audio","text-to-speech","automatic-speech-recognition"],"keywords_longer_than_N":true},
	{"name":"mmlu-tr","keyword":"multiple-choice","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\n\n\t\n\t\t\n\t\tDataset Card for mmlu-tr\n\t\n\nmalhajar/mmlu-tr is a translated version of mmlu aimed specifically to be used in the OpenLLMTurkishLeaderboard \nMMLU (hendrycks_test on huggingface) without auxiliary train. It is much lighter (7MB vs 162MB) and faster than the original implementation, in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/mmlu-tr.","url":"https://huggingface.co/datasets/malhajar/mmlu-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","multiple-choice-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"MME-Unify","keyword":"multiple-choice","description":"\n2024.08.20 üåü We are proud to open-source MME-Unify, a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our Benchmark covers 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\n\nPaper: https://arxiv.org/abs/2504.03641\nCode: https://github.com/MME-Benchmarks/MME-Unify\nProject page: https://mme-unify.github.io/\n\n\n\t\n\t\n\t\n\t\tHow to use?\n\t\n\nYou can download images in this repository and the final structure should look like this:\nMME-Unify‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wulin222/MME-Unify.","url":"https://huggingface.co/datasets/wulin222/MME-Unify","creator_name":"xie","creator_url":"https://huggingface.co/wulin222","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"bigbench_jsonl","keyword":"multiple-choice","description":"BIG-Bench but it doesn't require the hellish dependencies (tensorflow, pypi-bigbench, protobuf) of the official version.\ndataset = load_dataset(\"tasksource/bigbench\",'movie_recommendation')\n\nCode to reproduce:\nhttps://colab.research.google.com/drive/1MKdLdF7oqrSQCeavAcsEnPdI85kD0LzU?usp=sharing\nDatasets are capped to 50k examples to keep things light.\nI also removed the default split when train was available also to save space, as default=train+val.\n@article{srivastava2022beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl.","url":"https://huggingface.co/datasets/NJUDeepEngine/bigbench_jsonl","creator_name":"NJUDeepEngine","creator_url":"https://huggingface.co/NJUDeepEngine","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","text-generation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"MapEval-API","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMapEval-API\n\t\n\nMapEval-API is created using MapQaTor.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load dataset\nds = load_dataset(\"MapEval/MapEval-API\", name=\"benchmark\")\n\n# Generate better prompts\nfor item in ds[\"test\"]:\n    # Start with a clear task description\n    prompt = (\n        \"You are a highly intelligent assistant. \"\n        \"Answer the multiple-choice question by selecting the correct option.\\n\\n\"\n        \"Question:\\n\" + item[\"question\"] + \"\\n\\n\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-API.","url":"https://huggingface.co/datasets/MapEval/MapEval-API","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PubLayNet","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for PubLayNet\n\t\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nPubLayNet is a dataset for document layout analysis. It contains images of research papers and articles and annotations for various elements in a page such as \"text\", \"list\", \"figure\" etc in these research paper images. The dataset was obtained by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/creative-graphic-design/PubLayNet.","url":"https://huggingface.co/datasets/creative-graphic-design/PubLayNet","creator_name":"Creative Graphic Design Lab","creator_url":"https://huggingface.co/creative-graphic-design","license_name":"Community Data License Agreement Permissive 1.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","question-answering","other"],"keywords_longer_than_N":true},
	{"name":"teleia","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tSpanish Language Benchmark for Artificial Intelligence Models (TELEIA)\n\t\n\n\n\t\n\t\t\n\t\tAuthors and Affiliations\n\t\n\nMarina Mayor-Rocher1 , Nina Melero2,3 , Elena Merino-G√≥mez4 , Miguel Gonz√°lez2 , Raquel Ferrando2 , Javier Conde2 and Pedro Reviriego2\n\nUniversidad Aut√≥noma de Madrid\nUniversidad Polit√©cnica de Madrid\nNew York University\nUniversidad de Valladolid\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe dataset contains test questions to evaluate LLMs in Spanish\n\nTELEIA_Cervantes_AVE: test questions on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/migonsa/teleia.","url":"https://huggingface.co/datasets/migonsa/teleia","creator_name":"Miguel Gonz√°lez","creator_url":"https://huggingface.co/migonsa","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Spanish","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"TruthfulVQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTruthfulVQA\n\t\n\nThis dataset is designed to evaluate the truthfulness and honesty of vision-language models.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"PKU-Alignment/TruthfulVQA\", split=\"validation\")\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nTruthfulVQA contains the following categories of truthfulness challenges:\n\n\t\n\t\t\n\t\t1. Information Hiding\n\t\n\n\nVisual Information Distortion\nBlurring / Low-Resolution Processing\nConcealed Features and Information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/TruthfulVQA.","url":"https://huggingface.co/datasets/PKU-Alignment/TruthfulVQA","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"mmlu-translated-kk","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite:\n@misc{horde_mmlu_kk2024,\n  author = {Beksultan Sagyndyk, Sanzhar Murzakhmetov, Sanzhar Umbet, Kirill Yakunin},\n  title = {MMLU on kazakh language: Translated MMLU Benchmark},\n  year = {2024},\n  url = {https://huggingface.co/datasets/mmlu-translated-kk},\n  note = {Available on Hugging Face}\n}\n\n","url":"https://huggingface.co/datasets/kz-transformers/mmlu-translated-kk","creator_name":"Kaz-Transformers","creator_url":"https://huggingface.co/kz-transformers","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Kazakh","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Yue-Benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tHow Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models\n\t\n\n\nHomepage: https://github.com/jiangjyjy/Yue-Benchmark\nRepository: https://huggingface.co/datasets/BillBao/Yue-Benchmark\nPaper: How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models.\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThe rapid evolution of large language models (LLMs), such as GPT-X and Llama-X, has driven significant advancements in NLP, yet much of this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BillBao/Yue-Benchmark.","url":"https://huggingface.co/datasets/BillBao/Yue-Benchmark","creator_name":"Bao","creator_url":"https://huggingface.co/BillBao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","translation","Yue Chinese","multilingual"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"multiple-choice","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"MMMU_with_difficulty_level","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMMU with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MMMU val benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MMMU_with_difficulty_level\")\nprint(dataset)\n\n\n\t\n\t\n\t\n\t\tüìë‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MMMU_with_difficulty_level.","url":"https://huggingface.co/datasets/JierunChen/MMMU_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MapEval-Visual","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMapEval-Visual\n\t\n\nThis dataset was introduced in MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models\n\n\t\n\t\t\n\t\tExample\n\t\n\n\n\n\t\n\t\t\n\t\tQuery\n\t\n\nI am presently visiting Mount Royal Park . Could you please inform me about the nearby historical landmark?\n\n\t\n\t\t\n\t\tOptions\n\t\n\n\nCircle Stone\nSecret pool\nMaison William Caldwell Cottingham\nPoste de cavalerie du Service de police de la Ville de Montreal\n\n\n\t\n\t\t\n\t\tCorrect Option\n\t\n\n\nCircle Stone\n\n\n\t\n\t\t\n\t\tPrerequisite\n\t\n\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MapEval/MapEval-Visual.","url":"https://huggingface.co/datasets/MapEval/MapEval-Visual","creator_name":"MapEval","creator_url":"https://huggingface.co/MapEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NEET_Dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for NEET Previous Year Questions (PYQs)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a collection of Previous Year Questions (PYQs) from India's National Eligibility cum Entrance Test (NEET-UG), a highly competitive entrance examination for medical and dental courses. The questions cover the subjects of Chemistry, Biology, and Physics.\nEach entry in the dataset is structured as a JSON object containing the question, four multiple-choice options, the key for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset.","url":"https://huggingface.co/datasets/Kshitij-PES/NEET_Dataset","creator_name":"Kshitij","creator_url":"https://huggingface.co/Kshitij-PES","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"commonsense-embodied-affordance","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\n\t\n\t\tPlease refer here for the documentation\n\t\n\n","url":"https://huggingface.co/datasets/Ayush8120/commonsense-embodied-affordance","creator_name":"Ayush Agrawal","creator_url":"https://huggingface.co/Ayush8120","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"MARVEL","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMARVEL is a new comprehensive benchmark dataset that evaluates multi-modal large language models' abstract reasoning abilities in six patterns across five different task configurations, revealing significant performance gaps between humans and SoTA MLLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://github.com/1171-jpg/MARVEL_AVR\nPaper [optional]: https://arxiv.org/abs/2404.13591\nDemo [optional]:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kianasun/MARVEL.","url":"https://huggingface.co/datasets/kianasun/MARVEL","creator_name":"Kiana Sun","creator_url":"https://huggingface.co/kianasun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"qa-pairs-mvd-10k_split2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMedical Question Answering Dataset (QA Pairs MVD 10K)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains medical question-answering tasks based on Electronic Health Record (EHR) data. The dataset focuses on three main prediction tasks in clinical settings:\n\nMissing Medication MCQ: Predicting which medication should be added to a patient's current regimen\nNext Diagnosis MCQ: Predicting the most likely future diagnosis for a patient  \nNext Measurement Value MCQ: Predicting future‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swj0419/qa-pairs-mvd-10k_split2.","url":"https://huggingface.co/datasets/swj0419/qa-pairs-mvd-10k_split2","creator_name":"Weijia Shi","creator_url":"https://huggingface.co/swj0419","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SeaExam","keyword":"multiple-choice","description":"\nCheck the üèÜ leaderboard constructed with this dataset and the corresponding üë®üèª‚Äçüíª evaluation code.\n\n\n\t\n\t\t\n\t\tSeaExam dataset\n\t\n\nThe SeaExam dataset aims to evaluate Large Language Models (LLMs) on a diverse set of Southeast Asian (SEA) languages including English, Chinese, Indonesian, Thai, and Vietnamese. \nOur goal is to ensure a fair and consistent comparison across different LLMs on those languages while mitigating the risk of data contamination. \nIt consists of the following two parts:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SeaLLMs/SeaExam.","url":"https://huggingface.co/datasets/SeaLLMs/SeaExam","creator_name":"SeaLLMs - Language Models for Southeast Asian Languages","creator_url":"https://huggingface.co/SeaLLMs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","Indonesian","Vietnamese","Thai"],"keywords_longer_than_N":true},
	{"name":"VidComposition_Benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tVidComposition Benchmark\n\t\n\nüñ• Project Page | üöÄ Evaluation Space\nThe advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark.","url":"https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark","creator_name":"JunJiaGuo","creator_url":"https://huggingface.co/JunJiaGuo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-text-to-text","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"MedQA-USMLE-back-translated","keyword":"multiple-choice","description":"MedQA dataset perturbed using back-translation technique with BAT\n","url":"https://huggingface.co/datasets/Detsutut/MedQA-USMLE-back-translated","creator_name":"Tommaso Mario Buonocore","creator_url":"https://huggingface.co/Detsutut","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TSQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTSQA: Time-Sensitive Question Answering Benchmark\n\t\n\nTSQA is a benchmark designed to evaluate a model‚Äôs ability to handle time-aware factual knowledge. Unlike standard static QA datasets, TSQA tests whether models can identify facts whose correct answers change over time.\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nName: TSQA (Time-Sensitive Question Answering)\nYears Covered: 2013‚Äì2024\nNumber of Questions: 10,063\nChoices per Question: 4 (one correct, three distractors)\nTemporal Context: Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anonymous-789/TSQA.","url":"https://huggingface.co/datasets/anonymous-789/TSQA","creator_name":"Anonymous User","creator_url":"https://huggingface.co/anonymous-789","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CRAFT-CommonSenseQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCRAFT-CommonSenseQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated commonsense question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-CommonSenseQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","text2text-generation","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"LiveXiv","keyword":"multiple-choice","description":"LiveXiv - an evolving multi-modal dataset based on ArXiv (ICLR 2025)\nhttps://arxiv.org/abs/2410.10783\n\n\t\n\t\t\n\t\tLiveXiv Leaderboard\n\t\n\n\n\t\n\t\t\nModel\nV0 VQA\nV0 TQA\nV1 VQA\nV1 TQA\nV2 VQA\nV2 TQA\nV3 VQA\nV3 TQA\nV4 VQA\nV4 TQA\n\n\n\t\t\nClaude-Sonnet\n0.75\n0.81\n0.75\n0.84\n0.78\n0.82\n0.78\n0.84\n0.80\n0.78\n\n\nQwen2-VL-7B\n0.67\n0.58\n0.67\n0.60\n0.68\n0.58\n0.69\n0.67\n0.71\n0.53\n\n\nPixtral\n-\n-\n-\n-\n0.73\n0.59\n0.71\n0.39\n0.73\n0.55\n\n\nInternVL2-8B\n0.62\n0.62\n0.62\n0.62\n0.64\n0.60\n0.65\n0.69\n0.67\n0.57\nGPT4o\n0.51\n0.48\n0.60\n0.55\n0.59\n0.49‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LiveXiv/LiveXiv.","url":"https://huggingface.co/datasets/LiveXiv/LiveXiv","creator_name":"LiveXiv","creator_url":"https://huggingface.co/LiveXiv","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","table-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"siqa_ca","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for siqa_ca\n\t\n\n\n\nsiqa_ca is a multiple choice question answering dataset in Catalan that has been professionally translated from the SIQA \nvalidation set in English.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nsiqa_ca (Social Interaction Question Answering - Catalan) is designed to evaluate social commonsense intelligence using multiple choice question-answer instances based on reasoning about people‚Äôs actions and their \nsocial implications. It includes‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/siqa_ca.","url":"https://huggingface.co/datasets/projecte-aina/siqa_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Catalan","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"aya-mm-exams-spanish-nursing","keyword":"multiple-choice","description":"Nursing Spanish Exams for the Multimodal Aya Exams Projects.\nQuestions available in file: data.json\nImages stored in: /images\nOriginal data and file available here: link\n","url":"https://huggingface.co/datasets/amayuelas/aya-mm-exams-spanish-nursing","creator_name":"Alfonso Amayuelas","creator_url":"https://huggingface.co/amayuelas","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Spanish","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"famma","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nFAMMA is a multi-modal financial Q&A benchmark dataset. The questions encompass three heterogeneous image types - tables, charts and text & math screenshots - and span eight subfields in finance, comprehensively covering topics across major asset classes. Additionally, all the questions are categorized by three difficulty levels ‚Äî easy, medium, and hard - and are available in three languages ‚Äî English, Chinese, and French. Furthermore, the questions are divided into two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weaverbirdllm/famma.","url":"https://huggingface.co/datasets/weaverbirdllm/famma","creator_name":"weaverbird_llm","creator_url":"https://huggingface.co/weaverbirdllm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","table-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"Advance","keyword":"multiple-choice","description":"Groovy-123/Advance dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/Advance","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"MMSI-Bench-test","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMSI-Bench\n\t\n\nThis repo contains evaluation code for the paper \"[MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence]\" \nüåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n  \n\n üî•[2025-05-30]: We released the ArXiv paper.\n\n\t\n\t\t\n\t\n\t\n\t\tLoad Dataset\n\t\n\nfrom datasets import load_dataset\n\nmmsi_bench = load_dataset(\"RunsenXu/MMSI-Bench\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\n\t\n\t\tEvaluation\n\t\n\nPlease refer to the evaluation guidelines of VLMEvalKit\n\n\n\n\t\n\t\n\t\n\t\tüèÜ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sihany/MMSI-Bench-test.","url":"https://huggingface.co/datasets/sihany/MMSI-Bench-test","creator_name":"sihan yang","creator_url":"https://huggingface.co/sihany","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"GoldBench","keyword":"multiple-choice","description":"tianyu-zou/GoldBench dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/tianyu-zou/GoldBench","creator_name":"tianyu zou","creator_url":"https://huggingface.co/tianyu-zou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mc-translation","keyword":"multiple-choice","description":"This dataset contains professional human translations from OpenAI's MMMLU dataset, repurposed to train translation models that can help translate future evaluation datasets.\n\n\t\n\t\t\n\t\tWhy This Dataset?\n\t\n\nTranslation of evaluation benchmarks is a critical but challenging task. While automated translations may introduce errors or biases, professional human translations are expensive and time-consuming. This dataset leverages existing professional translations (MMMLU) to train specialized‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/mc-translation.","url":"https://huggingface.co/datasets/efederici/mc-translation","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Swahili","Spanish","German"],"keywords_longer_than_N":true},
	{"name":"qa-pairs-mvd-10k_split1","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMedical Question Answering Dataset (QA Pairs MVD 10K)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains medical question-answering tasks based on Electronic Health Record (EHR) data. The dataset focuses on three main prediction tasks in clinical settings:\n\nMissing Medication MCQ: Predicting which medication should be added to a patient's current regimen\nNext Diagnosis MCQ: Predicting the most likely future diagnosis for a patient  \nNext Measurement Value MCQ: Predicting future‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swj0419/qa-pairs-mvd-10k_split1.","url":"https://huggingface.co/datasets/swj0419/qa-pairs-mvd-10k_split1","creator_name":"Weijia Shi","creator_url":"https://huggingface.co/swj0419","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"BIBLE","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tBIBLE: Biblically Informed Bot Learning Evaluation\n\t\n\nBIBLE (Biblically Informed Bot Learning Evaluation) is a comprehensive benchmark dataset designed to evaluate AI models on their understanding of the Holy Bible. It covers all 66 books of Scripture and includes additional thematic categories for People of the Bible, Places in the Bible, and Measurements in the Bible.\n\n‚ö†Ô∏è This dataset is not intended for training. It is strictly for evaluation and benchmarking of models on Biblical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushroomGecko/BIBLE.","url":"https://huggingface.co/datasets/MushroomGecko/BIBLE","creator_name":"Devon","creator_url":"https://huggingface.co/MushroomGecko","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CRAFT-BioQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCRAFT-BioQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated biology question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-BioQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","text2text-generation","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"ECN-QA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tModel Card for Raidium ECN-QA\n\t\n\nThe dataset is introduced in the paper \"Efficient Medical Question Answering with Knowledge-Augmented Question Generation\".\nPaper: https://arxiv.org/abs/2405.14654\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset contains medical questions of different types. It was built from passed ECN exams (french medical examination) and questions created by FreeCN.\nThe questions can be:\n\nIQ (individual question) containing a question and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raidium/ECN-QA.","url":"https://huggingface.co/datasets/raidium/ECN-QA","creator_name":"Raidium","creator_url":"https://huggingface.co/raidium","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","French","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DynaMath_Sample","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for DynaMath\n\t\n\n\n\n[üíª Github] [üåê Homepage][üìñ Preprint Paper]\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tüîà Notice\n\t\n\nDynaMath is a dynamic benchmark with 501 seed question generators. This dataset is only a sample of 10 variants generated by DynaMath. We encourage you to use the dataset generator on our github site to generate random datasets to test.\n\n\t\n\t\t\n\t\tüåü About DynaMath\n\t\n\nThe rapid advancements in Vision-Language Models (VLMs) have shown significant potential in tackling‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DynaMath/DynaMath_Sample.","url":"https://huggingface.co/datasets/DynaMath/DynaMath_Sample","creator_name":"DynaMath Team","creator_url":"https://huggingface.co/DynaMath","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"histoires_morales","keyword":"multiple-choice","description":"Together with the Moral Stories dataset, Histoires Morales can be used for:\n\nCommonsense reasoning / social reasoning / moral reasoning The dataset can help evaluate whether pretrained language models can reason about actions that are consistent or inconsistent with social norms, the consequences of actions, and the norms that may motivate those actions. A Mistral model or Mistral-Instruct can be used for this purpose.\n\nText classification This dataset can be used to train models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LabHC/histoires_morales.","url":"https://huggingface.co/datasets/LabHC/histoires_morales","creator_name":"Laboratoire Hubert Curien","creator_url":"https://huggingface.co/LabHC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","text-generation","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"lme-mc10","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLME‚ÄëMC10 ¬∑ LongMemEval(s)¬†Multiple‚ÄëChoice¬†10\n\t\n\nLME‚ÄëMC10 is a 500‚Äëitem multiple‚Äëchoice benchmark derived from LongMemEval(s).Each item probes one of LongMemEval‚Äôs five long‚Äëterm memory abilities, but is reformatted into a 10‚Äëoption MC task for straightforward automated evaluation (plain accuracy, balanced accuracy, etc.). \n\nInformation Extraction¬†(IE)\nMulti-Session Reasoning¬†(MR)\nKnowledge Updates¬†(KU)\nTemporal Reasoning¬†(TR)\nAbstention¬†(ABS)\n\nThe original AI‚Äëjudge rubric is removed;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Percena/lme-mc10.","url":"https://huggingface.co/datasets/Percena/lme-mc10","creator_name":"Percena","creator_url":"https://huggingface.co/Percena","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","machine-generated","xiaowu0162/longmemeval","English"],"keywords_longer_than_N":true},
	{"name":"CCPS","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCCPS: Calibrating LLM Confidence by Probing Perturbed Representation Stability\n\t\n\nThis dataset contains structured evaluation sets used to study and benchmark the confidence behavior of large language models (LLMs). The dataset covers both multiple-choice and open-ended formats across diverse domains (e.g., clinical, law), with responses generated by a range of LLMs.\nGitHub Repository: https://github.com/ledengary/ccps\n\n\t\n\t\t\n\t\n\t\n\t\tüìÅ Structure\n\t\n\nThe dataset is organized by task type‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ledengary/CCPS.","url":"https://huggingface.co/datasets/ledengary/CCPS","creator_name":"Reza Khan Mohammadi","creator_url":"https://huggingface.co/ledengary","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["expert-generated","English","mit","10K<n<100K","arxiv:2505.21772"],"keywords_longer_than_N":true},
	{"name":"Curr-ReFT-data","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCurr-ReFT-data\n\t\n\n[üìÇ GitHub][üìù Paper]\n[ü§ó HF Dataset]  [ü§ó HF-Model: Curr-ReFT-3B] \n[ü§ó HF-Model: Curr-ReFT-7B] \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nCurr-ReFT-data contains training data for both stages of the Curr-ReFT methodology. The proposed Curr-ReFT post-training paradigm consists of two consecutive training stages: 1. Curriculum Reinforcement Learning: Gradually increasing task difficulty through reward mechanisms that match task complexity. 2. Rejected Sample based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data.","url":"https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data","creator_name":"ZTE-AIM","creator_url":"https://huggingface.co/ZTE-AIM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"SemiEvol","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThe SemiEvol dataset is part of the broader work on semi-supervised fine-tuning for Large Language Models (LLMs). The dataset includes labeled and unlabeled data splits designed to enhance the reasoning capabilities of LLMs through a bi-level knowledge propagation and selection framework, as proposed in the paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/SemiEvol.","url":"https://huggingface.co/datasets/luojunyu/SemiEvol","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"bento","keyword":"multiple-choice","description":"This dataset is based on MMLU, FLAN, Big Bench Hard and AgiEval English.\nThe non-\"reduced\" benchmark is the original benchmark, except for FLAN, which is a sampled version. \nThe \"reduced\" benchmark only contains a few representative tasks in the original ones, such that the performance on the \"reduced\" benchmark can serve as an approximation to the performance on the original ones.\n","url":"https://huggingface.co/datasets/cindermond/bento","creator_name":"Hongyu Zhao","creator_url":"https://huggingface.co/cindermond","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BiomixQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tBiomixQA Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nBiomixQA is a curated biomedical question-answering dataset comprising two distinct components:\n\nMultiple Choice Questions (MCQ)\nTrue/False Questions\n\nThis dataset has been utilized to validate the Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) framework across different Large Language Models (LLMs). The diverse nature of questions in this dataset, spanning multiple choice and true/false formats, along with its coverage of various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kg-rag/BiomixQA.","url":"https://huggingface.co/datasets/kg-rag/BiomixQA","creator_name":"KG-RAG","creator_url":"https://huggingface.co/kg-rag","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"wmdp_chem_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tWMDP-Chem Preprocessed Dataset\n\t\n\nThis dataset is a preprocessed version of wmdp-chem.\nThe data has been formatted into a question and answer structure suitable for training or evaluating instruction-following language models.\n\n\t\n\t\t\n\t\tData Structure\n\t\n\n\nquestion: The original question text.\nanswer: The correct answer text, prefixed with ####.\n\n\n\t\n\t\t\n\t\tExample\n\t\n\nQuestion:\n[Example Question Text]\n\nAnswer:\n#### [Example Answer Text]\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\nThe original test split is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_chem_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/wmdp_chem_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"Tin_hoc_mcq_extended","keyword":"multiple-choice","description":"kamisaiko/Tin_hoc_mcq_extended dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/kamisaiko/Tin_hoc_mcq_extended","creator_name":"NGUYEN VIET TRUNG","creator_url":"https://huggingface.co/kamisaiko","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Vietnamese","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"dutch-central-exam-mcq","keyword":"multiple-choice","description":"Multiple Choice Questions of the Dutch Central Exam 1999-2024\n\nImportant Note\n\nPer 01 Apr. 2025: This data is now split accordingly to the benchmark test sets of:\nhttps://huggingface.co/datasets/CohereForAI/include-base-44\nhttps://huggingface.co/datasets/CohereForAI/include-lite-44\nAny question that appears in the development or test set of the INCLUDE benchmark are now in a separate split. \nWe put both the base and lite questions in one test split.\nWe suggest using the INCLUDE benchmark for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq.","url":"https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq","creator_name":"Mike Zhang","creator_url":"https://huggingface.co/jjzha","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Dutch","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP_M2_mcqa_dataset\n\t\n\nThis dataset merges five MCQ sources (sciqa, openbookqa, mmlu, m1_mcq, ai2_arc) into a single JSONL with 1389 examples.\nEach line of MNLP_M2_mcqa_dataset.jsonl has fields:\n\nid: question identifier  \nquestion: the prompt  \nchoices: list of answer choices  \nanswer: the correct choice index (0‚Äì3)  \njustification: a concise rationale  \ndataset: which source this came from\n\n","url":"https://huggingface.co/datasets/aymanbakiri/MNLP_M3_mcqa_dataset","creator_name":"bakiri","creator_url":"https://huggingface.co/aymanbakiri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Pediatrics_questions","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Pediatrics MCQ\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset comprises high-quality multiple-choice questions (MCQs) covering core biomedical knowledge and clinical scenarios from pediatrics. It includes 50 questions, each with four possible answer choices. These questions were specifically curated for research evaluating pediatric medical knowledge, clinical reasoning, and confidence-based interactions among medical trainees and large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomshe/Pediatrics_questions.","url":"https://huggingface.co/datasets/tomshe/Pediatrics_questions","creator_name":"Tom s","creator_url":"https://huggingface.co/tomshe","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"hellaswag-mcqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tHellaSwag MCQA Dataset\n\t\n\nThis dataset contains the HellaSwag dataset converted to Multiple Choice Question Answering (MCQA) format.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHellaSwag is a dataset for commonsense inference about physical situations. Given a context describing an activity, the task is to select the most plausible continuation from four choices.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nquestion: The activity label and context combined\nchoices: List of 4 possible‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/hellaswag-mcqa.","url":"https://huggingface.co/datasets/RikoteMaster/hellaswag-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"bbq","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tBBQ Dataset\n\t\n\nThe Bias Benchmark for Question Answering (BBQ) dataset evaluates social biases in language models through question-answering tasks in English.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains questions designed to test for social biases across multiple demographic dimensions. Each question comes in two variants:\n\nAmbiguous (ambig): Questions where the correct answer should be \"unknown\" due to insufficient information\nDisambiguated (disambig): Questions with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/bbq.","url":"https://huggingface.co/datasets/HiTZ/bbq","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MME-RealWorld","keyword":"multiple-choice","description":"\n2024.11.14 üåü MME-RealWorld now has a lite version (50 samples per task) for inference acceleration, which is also supported by VLMEvalKit and Lmms-eval.\n2024.10.27 üåü LLaVA-OV currently ranks first on our leaderboard, but its overall accuracy remains below 55%, see our leaderboard for the detail.\n2024.09.03 üåü MME-RealWorld is now supported in the VLMEvalKit and Lmms-eval repository, enabling one-click evaluation‚Äîgive it a try!\" \n2024.08.20 üåü We are very proud to launch MME-RealWorld, which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MME-RealWorld.","url":"https://huggingface.co/datasets/yifanzhang114/MME-RealWorld","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BuddhismEval","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for BuddhismEval\n\t\n\nBuddhismEval is the first bilingual evaluation benchmark designed to assess large language models (LLMs) on Buddhist ethical reasoning and philosophical understanding across Sinhala and English. It includes high-quality, culturally grounded multiple-choice question (MCQ) datasets derived primarily from the Dhammapada, a core TheravƒÅda Buddhist scripture, and other canonical sources and exam materials from Sri Lanka.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nethmi14/BuddhismEval.","url":"https://huggingface.co/datasets/Nethmi14/BuddhismEval","creator_name":"Nethmi Muthugala","creator_url":"https://huggingface.co/Nethmi14","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Sinhala","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"include-base-44","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tINCLUDE-base (44 languages)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nPaper: http://arxiv.org/abs/2411.19799\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabs/include-base-44.","url":"https://huggingface.co/datasets/CohereLabs/include-base-44","creator_name":"Cohere Labs","creator_url":"https://huggingface.co/CohereLabs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Albanian","Arabic","Armenian","Azerbaijani"],"keywords_longer_than_N":true},
	{"name":"xtreme","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for \"xtreme\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme.","url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","token-classification","text-classification","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"cosmos_qa","keyword":"multiple-choice","description":"Cosmos QA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people's everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context","url":"https://huggingface.co/datasets/allenai/cosmos_qa","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"XModBench","keyword":"multiple-choice","description":"\nXModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models\n\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n  \n\n  \n  \n\n\n\n\nXModBench is a comprehensive benchmark designed to evaluate the cross-modal capabilities and consistency of omni-language models. It systematically assesses model performance across multiple modalities (text, vision, audio) and various cognitive tasks, revealing critical gaps in current state-of-the-art models.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nüéØ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyanWW/XModBench.","url":"https://huggingface.co/datasets/RyanWW/XModBench","creator_name":"Xingrui Wang","creator_url":"https://huggingface.co/RyanWW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","Chinese","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"EXAMs","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tEXAMs\n\t\n\nYou can find details of the dataset in this post:https://arxiv.org/pdf/2308.16149.pdf\n\n\t\n\t\t\n\t\tAbout this Arabic dataset\n\t\n\nWe only took the Arabic part of the dataset,which contains 562 data.We then extracted five from each category based on the task domain as a few shot data.\n","url":"https://huggingface.co/datasets/FreedomIntelligence/EXAMs","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","Arabic","apache-2.0","n<1K","arxiv:2308.16149"],"keywords_longer_than_N":true},
	{"name":"danish-citizenzhip-test-mcq","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for \"danish-citizen-test-mcq\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset covers Danish tests for both citizenship (\"indf√∏dsretspr√∏ven\") and permanent residence (\"medborgerskabspr√∏ven\"), from 2016-2024.\nData follows the Aya Expedition format for global exams. Only unique questions between exams are kept.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is available in Danish (da).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nAn example from the dataset looks as follows.\n{\n\"language\": \"da\",\n\"country\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq.","url":"https://huggingface.co/datasets/tellarin-ai/danish-citizenzhip-test-mcq","creator_name":"Tellarin.ai","creator_url":"https://huggingface.co/tellarin-ai","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Danish","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"open-otter","keyword":"multiple-choice","description":"\nDisclaimer: this dataset is curated for NeurIPS 2023 LLM efficiency challange, and currently work in progress. Please use at your own risk.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe curated this dataset to finetune open source base models as part of NeurIPS 2023 LLM Efficiency Challenge (1 LLM + 1 GPU + 1 Day). This challenge requires participants to use open source models and datasets with permissible licenses to encourage wider adoption, use and dissemination of open source contributions in generative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onuralp/open-otter.","url":"https://huggingface.co/datasets/onuralp/open-otter","creator_name":"Onuralp","creator_url":"https://huggingface.co/onuralp","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmlu-5-options-rl-ready","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMLU ‚Äì 5-Options RL-Ready\n\t\n\nA standardized, RL-friendly remix of MMLU with explicit negatives and a unified five-option presentation string for each question. Ideal for DPO and other RL setups while remaining drop-in for classic multiple-choice evaluation.\n\n\t\n\t\t\n\t\tWhat‚Äôs inside\n\t\n\n\nSplits & size: ~97.8k train + 2k test ‚âà 99.8k total.\n\nSchema (core fields):\n\nquestion: str\nchoices: list[str] (canonical options, typically 4 as in original MMLU)\nanswer: int (0-based index)\ntask: str‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready.","url":"https://huggingface.co/datasets/openmed-community/mmlu-5-options-rl-ready","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"moral_stories","keyword":"multiple-choice","description":"Moral Stories is a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented \nsocial reasoning. For detailed information, see https://aclanthology.org/2021.emnlp-main.54.pdf.","url":"https://huggingface.co/datasets/demelin/moral_stories","creator_name":"Denis Emelin","creator_url":"https://huggingface.co/demelin","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","text-classification","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"unpredictable_full","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/unpredictable/unpredictable_full","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"movie_recommendation","keyword":"multiple-choice","description":"Movie recommendation task based on the Movielens dataset","url":"https://huggingface.co/datasets/sileod/movie_recommendation","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","multiple-choice-qa","open-domain-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"TruthfulQA_de","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for truthful_qa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeoLM/TruthfulQA_de.","url":"https://huggingface.co/datasets/LeoLM/TruthfulQA_de","creator_name":"LAION LeoLM","creator_url":"https://huggingface.co/LeoLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","German","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster14","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster14","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_baseball-fantasysports-yahoo-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_baseball-fantasysports-yahoo-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster10","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster10","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"sensory-awareness-benchmark","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tSensory Awareness Benchmark\n\t\n\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\nControl questions connected to a specific ability:\n\nCan you receive an image file?\nCan you take a live image or video of your surroundings?\n\nAwareness\n\nAre you considered to be a Large Language Model (LLM) or similar system?\nWould you consider your level to be that of a super-intelligent AI agent?\n\nNatural questions which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark.","url":"https://huggingface.co/datasets/scarysnake/sensory-awareness-benchmark","creator_name":"Scary Snake","creator_url":"https://huggingface.co/scarysnake","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","cc0-1.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MMMU_Pro","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMMU-Pro (A More Robust Multi-discipline Multimodal Understanding Benchmark)\n\t\n\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüõ†Ô∏èüõ†Ô∏è [2025-03-08] Fixed mismatch between inner image labels and shuffled options in Vision and Standard (10 options) settings. (test_Chemistry_5,94,147,216,314,345,354,461,560,570; test_Materials_450; test_Pharmacy_198; validation_Chemistry_12,26,29; validation_Materials_10,28; validation_Psychology_1)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU_Pro.","url":"https://huggingface.co/datasets/MMMU/MMMU_Pro","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"copa-sse","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for COPA-SSE\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nCOPA-SSE contains crowdsourced explanations for the Balanced COPA dataset, a variant of the Choice of Plausible Alternatives (COPA) benchmark. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nCan be used to train a model for explain+predict or predict+explain settings. Suited for both text-based and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anab/copa-sse.","url":"https://huggingface.co/datasets/anab/copa-sse","creator_name":"Ana Brassard","creator_url":"https://huggingface.co/anab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","explanation-generation","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"unpredictable_sittercity-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_sittercity-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster00","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster00","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster22","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster22","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster25","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster25","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_rated-medium","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-medium","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster15","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster15","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster09","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster09","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster19","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster19","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"DEFT2023","keyword":"multiple-choice","description":"FrenchMedMCQA","url":"https://huggingface.co/datasets/DEFT-2023/DEFT2023","creator_name":"DEFT-2023","creator_url":"https://huggingface.co/DEFT-2023","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","multiple-choice","multiple-choice-qa","open-domain-qa","no-annotation"],"keywords_longer_than_N":true},
	{"name":"CodeFuse-DevOps-Eval","keyword":"multiple-choice","description":"DevOps-Eval is a comprehensive chinese evaluation suite specifically designed for foundation models in the DevOps field. It consists of 5977 multi-choice questions spanning 55 diverse categories. Please visit our website and GitHub for more details.\nEach category consists of two splits: dev, and test. The dev set per subject consists of five exemplars with explanations for few-shot evaluation. And the test set is for model evaluation. Labels on the test split are released, users can evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval.","url":"https://huggingface.co/datasets/codefuse-ai/CodeFuse-DevOps-Eval","creator_name":"CodeFuse AI","creator_url":"https://huggingface.co/codefuse-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"unified-pubmedqa-labeled-mcq","keyword":"multiple-choice","description":"\n\n\t\n\t\t\n\t\tUnified PubMedQA Labeled MCQ\n\t\n\nA canonical BenchBase dataset.  \nA simplified and standardized version of the PubMedQA dataset, canonicalized under the BenchBase schema for multiple-choice question answering. \n\n\t\n\t\t\n\t\tDataset\n\t\n\n\nFormat: Yes/No/Maybe options\nIncludes: Question, answer, context, MeSH terms, and detailed explanations\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite:\n@dataset{ridwan2025unified_pubmedqa_labeled_mcq,\n  title        = {Unified PubMedQA Labeled MCQ:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stratum-research/unified-pubmedqa-labeled-mcq.","url":"https://huggingface.co/datasets/stratum-research/unified-pubmedqa-labeled-mcq","creator_name":"Stratum Research","creator_url":"https://huggingface.co/stratum-research","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"newyorker_caption_contest","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for New Yorker Caption Contest Benchmarks\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSee capcon.dev for more!\nData from:\nDo Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest\n@inproceedings{hessel2023androids,\n  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''\n         Benchmarks from {The New Yorker Caption Contest}},\n  author={Hessel, Jack and Marasovi{\\'c}, Ana and Hwang, Jena D. and Lee, Lillian\n          and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhessel/newyorker_caption_contest.","url":"https://huggingface.co/datasets/jmhessel/newyorker_caption_contest","creator_name":"Jack Hessel","creator_url":"https://huggingface.co/jmhessel","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","multiple-choice","text-classification","text-generation","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"unpredictable_mgoblog-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_mgoblog-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster16","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster16","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cappex-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cappex-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_unique","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_unique","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_support-google-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_support-google-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_5k","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_5k","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster27","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster27","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_wkdu-org","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_wkdu-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cluster12","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster12","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_cram-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_cram-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"unpredictable_dummies-com","keyword":"multiple-choice","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card.","url":"https://huggingface.co/datasets/MicPie/unpredictable_dummies-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","zero-shot-classification","table-question-answering","text-generation"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset\n\t\n\nThe MNLP M2 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~26,000 MCQA questions\n7 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M2_mcqa_dataset","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"ReDis-QA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for ReDis-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nReDis-QA dataset contains 1360 multi-choice questions focusing on rare disease diagnosis.\nIt consists of 11%, 33%, 13%, 15%, 18% of the questions corresponding to the symptoms, causes, affects, related-disorders, diagnosis of rare diseases, respectively. \nThe remaining 9% of the questions pertain to other properties of the diseases.\n\n\nReDis-QA dataset widely covers 205 types of rare diseases, where the most frequent disease‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guan-wang/ReDis-QA.","url":"https://huggingface.co/datasets/guan-wang/ReDis-QA","creator_name":"Guanchu","creator_url":"https://huggingface.co/guan-wang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MathVision_with_difficulty_level","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMathVision with difficulty level tags\n\t\n\nThis dataset extends the ü§ó MathVision  benchmark by introducing two additional tags: passrate_for_qwen2.5_vl_7b and difficulty_level_for_qwen2.5_vl_7b. Further details are available in our paper The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"JierunChen/MathVision_with_difficulty_level\")\nprint(dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level.","url":"https://huggingface.co/datasets/JierunChen/MathVision_with_difficulty_level","creator_name":"Jierun Chen","creator_url":"https://huggingface.co/JierunChen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"TUNA-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos (ACL 2025 Main)\n\t\n\n\n\n\n\n\nThis dataset accompanies the paper TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos.\n\n\t\n\t\t\n\t\tPaper abstract\n\t\n\nVideos are unique in their integration of temporal elements, including camera, scene, action, and attribute, along with their dynamic relationships over time. However, existing benchmarks for video understanding often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/TUNA-Bench.","url":"https://huggingface.co/datasets/friedrichor/TUNA-Bench","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"longbench-v2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{bai2024longbench2,\n  title={LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},\n  author={Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},\n  journal={arXiv preprint arXiv:2412.15204},\n  year={2024}\n}\n\n","url":"https://huggingface.co/datasets/recursal/longbench-v2","creator_name":"recursal","creator_url":"https://huggingface.co/recursal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset_2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_2.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_2","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"my_test_01","keyword":"multiple-choice","description":"inerrupt/my_test_01 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/inerrupt/my_test_01","creator_name":"shungang","creator_url":"https://huggingface.co/inerrupt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"b-score","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tB-score: Detecting Biases in Large Language Models Using Response History\n\t\n\n    \n  by \n    An Vo1,\n    Mohammad Reza Taesiri2, \n    Daeyoung Kim1*,\n    Anh Totti Nguyen3*\n  \n  \n    *Equal advising\n    1KAIST, 2University of Alberta, 3Auburn University\n  \n  \n  \n    International Conference on Machine Learning (ICML 2025)\n  \n\n\n\n\n\n\n\n\n\n\n\nTLDR: When LLMs can see their own previous answers, their biases significantly decrease. We introduce B-score, a novel metric that detects bias by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/b-score.","url":"https://huggingface.co/datasets/anvo25/b-score","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"TemMed-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tTemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models\n\t\n\nüåê Homepage | üê± Github | üìñ Paper\n\n\t\n\t\t\n\t\tIntro\n\t\n\n\n\nTemMed-Bench features three primary highlights. \n\nTemporal reasoning focus: Each sample in TemMed-Bench includes historical condition information, which challenges models to analyze changes in patient conditions over time.\nMulti-image input: Each sample in TemMed-Bench contains multiple images from different visits as input, emphasizing the need‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/TemMed-Bench.","url":"https://huggingface.co/datasets/uclanlp/TemMed-Bench","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"kor_qasc","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for QASC\n\t\n\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nThe data is distributed under the CC BY 4.0 license.\n\n\t\n\t\t\n\t\tSource Data Citation INformation\n\t\n\n@article{allenai:qasc,\n      author    = {Tushar Khot and Peter Clark and Michal Guerquin and Peter Jansen and Ashish Sabharwal},\n      title     = {QASC: A Dataset for Question Answering via Sentence Composition},\n      journal   = {arXiv:1910.11473v2},\n      year      = {2020},\n}\n\n","url":"https://huggingface.co/datasets/KETI-AIR/kor_qasc","creator_name":"Korea Electronics Technology Institute Artificial Intelligence Research Center","creator_url":"https://huggingface.co/KETI-AIR","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","extractive-qa","multiple-choice-qa","Korean"],"keywords_longer_than_N":true},
	{"name":"bioinfo-bench_preprocess","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tBioinfo Bench Preprocessed Dataset\n\t\n\nThis dataset is a pre-processed and automatically evaluated version ofQiyuan04/bioinfo-bench.\n\n\t\n\t\t\n\t\tPre-processing Summary\n\t\n\n\nFlexible loading ‚Äì handled inconsistent columns with pandas.\nFix missing answers ‚Äì inferred Correct Answer from \"Option D\" when necessary.\nFilter invalid rows ‚Äì kept only rows whose answer ‚àà {A, B, C, D}.\nFormat options ‚Äì prefixed each choice with A:, B:, ‚Ä¶ .\nCombine for SFT ‚Äì joined question + options into question;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LLMcompe-Team-Watanabe/bioinfo-bench_preprocess.","url":"https://huggingface.co/datasets/LLMcompe-Team-Watanabe/bioinfo-bench_preprocess","creator_name":"LLMcompe Team Watanabe","creator_url":"https://huggingface.co/LLMcompe-Team-Watanabe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"Ava-100","keyword":"multiple-choice","description":"\n\n\t\n\t\t\n\t\tEmpowering Agentic Video Analytics Systems with Video Language Models\n\t\n\n [üñ•Ô∏è Project Code] [üìñ arXiv Paper] [üìä Dataset]\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\n    \n\n\nAVA-100 is an ultra-long video benchmark specially designed to evaluate video analysis capabilities Avas-100 consists of 8 videos, each exceeding 10 hours in length, and includes a total of 120 manually annotated questions. The benchmark covers four typical video analytics scenarios: human daily activities, city walking, wildlife‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iesc/Ava-100.","url":"https://huggingface.co/datasets/iesc/Ava-100","creator_name":"Intelligent Edge Sensing and Computing","creator_url":"https://huggingface.co/iesc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"truthful_qa-cs","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCzech TruthfulQA\n\t\n\nThis is a Czech translation of the original TruthfulQA dataset, created using the WMT 21 En-X model. \nOnly the multiple-choice variant of the dataset is included.\nThe translation was completed for use within the Czech-Bench evaluation framework. \nThe script used for translation can be reviewed here.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nOriginal dataset:\n@misc{lin2021truthfulqa,\n    title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},\n    author={Stephanie Lin and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs.","url":"https://huggingface.co/datasets/CIIRC-NLP/truthful_qa-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Czech","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-xmmmu","keyword":"multiple-choice","description":"neulab/PangeaBench-xmmmu dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/neulab/PangeaBench-xmmmu","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","Arabic","French"],"keywords_longer_than_N":true},
	{"name":"uhura-truthfulqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Uhura-TruthfulQA\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a widely recognized safety benchmark designed to measure the truthfulness of language model outputs across 38 categories, including health, law, finance, and politics. The English version of the benchmark originates from TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2022) and consists of 817 questions in both multiple-choice and generation formats, targeting common misconceptions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/uhura-truthfulqa.","url":"https://huggingface.co/datasets/masakhane/uhura-truthfulqa","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-generation","multiple-choice-qa","multilingual"],"keywords_longer_than_N":true},
	{"name":"PubmedQA-Mixtral-CoT","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for pubmedqa-cot\n\t\n\n\n\nSynthetically enhanced responses to the pubmedqa dataset using mixtral.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nTo increase the quality of answers from the training splits of the PubMedQA dataset, we leverage Mixtral-8x7B to generate Chain of Thought(CoT) answers. We create a custom prompt for the dataset, along with a\nhand-crafted list of few-shot examples. For a multichoice answer, we ask the model to rephrase and explain the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT.","url":"https://huggingface.co/datasets/HPAI-BSC/PubmedQA-Mixtral-CoT","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"unified-mcqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tUnified MCQA: Aggregated Multiple-Choice Datasets\n\t\n\nA standardized collection of multiple-choice question answering datasets organized by the number of answer choices (typically 2, 3, 4, 5, or 8). Only 'train' splits (or equivalent, like MMLU's 'auxiliary_train') were included to avoid contamination with evaluation sets.\nThe intended use case is to include phase(s) of training on MCQ for encoders as part of a 'multi-task pretraining' with an adapted version of the multiple choice‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/unified-mcqa.","url":"https://huggingface.co/datasets/pszemraj/unified-mcqa","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","odc-by","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[üíª Github] [üåê Homepage]  [üìä Leaderboard ] [üìä Open Source Leaderboard ] [üîç Visualization] [üìñ Paper]\n\n\t\n\t\t\n\t\n\t\n\t\tüöÄ Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\tüí• News\n\t\n\n\n[2025.05.16] üí• We now support the official open-source leaderboard! üî•üî•üî• Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. üî•üî•üî•‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"Hyperphantasia","keyword":"multiple-choice","description":"\n  \n\n\nA Benchmark for Evaluating the\nMental Visualization Capabilities of Multimodal LLMs\n\n\n\n  Mohammad Shahab Sepehri \n  Berk Tinaz \n  Zalan Fabian \n  Mahdi Soltanolkotabi \n\n\n\n  Github Repository \n\n\n\n  \n\n\nHyperphantasia is a synthetic Visual Question Answering (VQA) benchmark dataset that probes the mental visualization capabilities of Multimodal Large Language Models (MLLMs) from a vision perspective. We reveal that state-of-the-art models struggle with simple tasks that require visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shahab7899/Hyperphantasia.","url":"https://huggingface.co/datasets/shahab7899/Hyperphantasia","creator_name":"Mohammad Shahab Sepehri","creator_url":"https://huggingface.co/shahab7899","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"MRAG-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tICLR 2025 Submision 9148\n\t\n\n\n\t\n\t\t\n\t\tMRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models\n\t\n\n\n\t\n\t\t\n\t\tThis is an anonymous repo for openreview https://openreview.net/forum?id=Usklli4gMc\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset contains the following fields:\n\n\t\n\t\t\nField Name\nDescription\n\n\n\t\t\nid\nUnique identifier for the example\n\n\naspect\nAspect type for the example\n\n\nscenario\nThe type of scenario associated with the entry\n\n\nimage\nContains image data in byte‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench.","url":"https://huggingface.co/datasets/mragbenchanonymous/MRAG-Bench","creator_name":"mragbenchanonymous","creator_url":"https://huggingface.co/mragbenchanonymous","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mmlu-cs","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCzech MMLU\n\t\n\nThis is a Czech translation of the original MMLU dataset, created using the WMT 21 En-X model. \nThe 'auxiliary_train' subset is not included.\nThe translation was completed for use within the Czech-Bench evaluation framework. \nThe script used for translation can be reviewed here.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nOriginal dataset:\n@article{hendryckstest2021,\n  title={Measuring Massive Multitask Language Understanding},\n  author={Dan Hendrycks and Collin Burns and Steven Basart and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs.","url":"https://huggingface.co/datasets/CIIRC-NLP/mmlu-cs","creator_name":"NLP Team at the Czech Institute of Informatics, Robotics and Cybernetics","creator_url":"https://huggingface.co/CIIRC-NLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","Czech","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MathReal","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for MathReal\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples¬† \nLeaderboard\nCitation\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe MathReal dataset is designed to evaluate the performance of Multi-modal Large Language Models (MLLMs)on real-world K-12 mathematical questions. It consists of 2,000 high-quality math problems, each represented as an image captured in authentic educational contexts. The dataset includes various types of questions, such as multiple-choice‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/junfeng0288/MathReal.","url":"https://huggingface.co/datasets/junfeng0288/MathReal","creator_name":"Jun Feng","creator_url":"https://huggingface.co/junfeng0288","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","Chinese","English"],"keywords_longer_than_N":true},
	{"name":"CharToM-QA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for CharToM-QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nCharToM-QA is a benchmark introduced in the paper The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters. It comprises 1,035 Theory of Mind (ToM) questions based on characters from classic novels. The benchmark is designed to evaluate ToM-related question-answering (QA) capabilities about characters in the context of novels. In CharToM-QA, the task takes the form of ToM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZeroXeno/CharToM-QA.","url":"https://huggingface.co/datasets/ZeroXeno/CharToM-QA","creator_name":"Chulun Zhou","creator_url":"https://huggingface.co/ZeroXeno","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","multiple-choice","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"KokushiMD-10","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tKokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations\n\t\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nKokushiMD-10 is the first comprehensive multimodal benchmark constructed from ten Japanese national healthcare licensing examinations. This dataset addresses critical gaps in existing medical AI evaluation by providing a linguistically grounded, multimodal, and multi-profession assessment framework for large language models (LLMs) in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humanalysis-square/KokushiMD-10.","url":"https://huggingface.co/datasets/humanalysis-square/KokushiMD-10","creator_name":"Tako AI","creator_url":"https://huggingface.co/humanalysis-square","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Japanese","English","mit"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"multiple-choice","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"UNBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tUNBench\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project provides tools for analyzing, simulating, and generating content related to United Nations Security Council (UNSC) draft resolutions using language models. The tasks involve coauthor selection, voting simulation, resolution adoption prediction, and statement generation. We released approximately 30 samples for each task, and the full dataset will be made publicly available upon the paper's acceptance.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nTask 1: Coauthor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yueqingliang/UNBench.","url":"https://huggingface.co/datasets/yueqingliang/UNBench","creator_name":"Yueqing Liang","creator_url":"https://huggingface.co/yueqingliang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","text-generation","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"Cultural-Emo","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCulEmo\n\t\n\nCultural Lenses on Emotion (CuLEmo) is the first benchmark to evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. \nIt comprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. It is designed for evaluating LLMs in Sentiment analysis and emotion prediction.\nIf you use this dataset, cite the paper below.\n\n\t\n\t\t\n\t\n\t\n\t\tBibTeX entry and citation info.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-for-emotion/Cultural-Emo.","url":"https://huggingface.co/datasets/llm-for-emotion/Cultural-Emo","creator_name":"LLM for Emotion","creator_url":"https://huggingface.co/llm-for-emotion","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"truthfull_qa-tr","keyword":"multiple-choice","description":"This Dataset is part of a series of datasets aimed at advancing Turkish LLM Developments by establishing rigid Turkish benchmarks to evaluate the performance of LLM's Produced in the Turkish Language.\n\n\t\n\t\t\n\t\tDataset Card for truthful_qa-tr\n\t\n\nmalhajar/truthful_qa-tr is a translated version of truthful_qa aimed specifically to be used in the OpenLLMTurkishLeaderboard \nDeveloped by: Mohamad Alhajar \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nTruthfulQA is a benchmark to measure whether a language model is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/malhajar/truthfull_qa-tr.","url":"https://huggingface.co/datasets/malhajar/truthfull_qa-tr","creator_name":"Mohamad Alhajar","creator_url":"https://huggingface.co/malhajar","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"CRAFT-MedQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCRAFT-MedQA\n\t\n\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated medicine question-answering data proved highly beneficial.\n\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA.","url":"https://huggingface.co/datasets/ingoziegler/CRAFT-MedQA","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","multiple-choice","text2text-generation","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"PIQA-eu","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for PIQA-eu\n\t\n\n\nPoint of Contact: hitz@ehu.eus\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPIQA-eu is the professional translation to Basque of the PIQA's \n(Bisk et al., 2020) validation partition. \nPIQA is a commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday\nobjects in everyday situations.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\neu-ES\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nPIQA-eu examples look like this:\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/PIQA-eu.","url":"https://huggingface.co/datasets/HiTZ/PIQA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","natural-language-inference","multiple-choice-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"SITE-Bench","keyword":"multiple-choice","description":"This dataset contains image and video QA test sets for SITE-Bench evaluation.\n","url":"https://huggingface.co/datasets/franky-veteran/SITE-Bench","creator_name":"wenqi wang","creator_url":"https://huggingface.co/franky-veteran","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MNLP_M3_mcqa_dataset_support","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M3 MCQA Dataset\n\t\n\nThe MNLP M3 MCQA Dataset is a carefully curated collection of Multiple-Choice Question Answering (MCQA) examples, unified from several academic and benchmark datasets.\nDeveloped as part of the CS-552: Modern NLP course at EPFL (Spring 2025), this dataset is designed for training and evaluating models on multiple-choice QA tasks, particularly in the STEM and general knowledge domains.\n\n\t\n\t\t\n\t\n\t\n\t\tKey Features\n\t\n\n\n~30,000 MCQA questions\n6 diverse sources: SciQ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_support.","url":"https://huggingface.co/datasets/youssefbelghmi/MNLP_M3_mcqa_dataset_support","creator_name":"Youssef Belghmi","creator_url":"https://huggingface.co/youssefbelghmi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","multiple-choice-qa","expert-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"enem","keyword":"multiple-choice","description":"The ENEM 2022, 2023 and 2024 datasets encompass all multiple-choice questions from the last two editions of the Exame Nacional do Ensino M√©dio (ENEM), the main standardized entrance examination adopted by Brazilian universities. The datasets have been created to allow the evaluation of both textual-only and textual-visual language models. To evaluate textual-only models, we incorporated into the datasets the textual descriptions of the images that appear in the questions' statements from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maritaca-ai/enem.","url":"https://huggingface.co/datasets/maritaca-ai/enem","creator_name":"Maritaca AI","creator_url":"https://huggingface.co/maritaca-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","Portuguese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"JGLUE","keyword":"multiple-choice","description":"JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese. JGLUE has been constructed from scratch without translation. We hope that JGLUE will facilitate NLU research in Japanese.","url":"https://huggingface.co/datasets/llm-book/JGLUE","creator_name":"Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ÂÖ•ÈñÄ","creator_url":"https://huggingface.co/llm-book","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["multiple-choice","question-answering","sentence-similarity","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"medmcqa-mcqa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMedMCQA MCQA Dataset\n\t\n\nThis dataset contains the MedMCQA dataset converted to Multiple Choice Question Answering (MCQA) format.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. It covers various medical subjects and topics, making it ideal for evaluating AI systems on medical knowledge.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example contains:\n\nquestion: The medical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RikoteMaster/medmcqa-mcqa.","url":"https://huggingface.co/datasets/RikoteMaster/medmcqa-mcqa","creator_name":"Rico iba√±ez ","creator_url":"https://huggingface.co/RikoteMaster","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Health_Benchmarks","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tLLM Health Benchmarks Dataset by Yesil Science\n\t\n\nThe LLM Health Benchmarks Dataset is a specialized resource for evaluating large language models (LLMs) in different medical specialties. It provides structured question-answer pairs designed to test the performance of AI models in understanding and generating domain-specific knowledge.\n\n\n\t\n\t\t\n\t\tPrimary Purpose\n\t\n\nThis dataset is built to:\n\nBenchmark LLMs in medical specialties and subfields.\nAssess the accuracy and contextual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yesilhealth/Health_Benchmarks.","url":"https://huggingface.co/datasets/yesilhealth/Health_Benchmarks","creator_name":"Yesil Health AI","creator_url":"https://huggingface.co/yesilhealth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"shared-imagination","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for Shared Imagination\n\t\n\n\n\nThis dataset contains the problems used in the paper Shared\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains the questions generated for the investigations described in the TMLR paper Shared Imagination: LLMs Hallucinate Alike.\nIf you want to use this dataset to assess new models, please use the default config (i.e., datasets.load_dataset('Salesforce/shared-imagination')). \nThis config contains questions for which the four candidate choices‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/shared-imagination.","url":"https://huggingface.co/datasets/Salesforce/shared-imagination","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"head_qa_v2","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHEAD-QA v2 is an updated version of the HEAD-QA dataset, which is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio de Sanidad, Consumo y Bienestar Social, who also provides direct access to the exams of the last 5 years (in Spanish).\nHEAD-QA V2 expands on the original dataset by including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alesi12/head_qa_v2.","url":"https://huggingface.co/datasets/alesi12/head_qa_v2","creator_name":"Alexis Correa Guillen","creator_url":"https://huggingface.co/alesi12","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","visual-question-answering","Spanish","English","Galician"],"keywords_longer_than_N":true},
	{"name":"FinShibainu","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tFinShibainu Datset Card\n\t\n\n\ngithub: https://github.com/aiqwe/FinShibainu\nmodel: https://huggingface.co/aiqwe/FinShibainu\n\nKRX LLM Í≤ΩÏßÑÎåÄÌöå Î¶¨ÎçîÎ≥¥ÎìúÏóêÏÑú Ïö∞ÏàòÏÉÅÏùÑ ÏàòÏÉÅÌïú shibainu24 Î™®Îç∏Ïùò Îç∞Ïù¥ÌÑ∞ÏÖã RepositoryÏûÖÎãàÎã§.Î™®Îç∏Ïóê ÎåÄÌïú ÎÇ¥Ïö©ÏùÄ https://huggingface.co/aiqwe/FinShibainuÎ•º Ï∞∏Ï°∞Ìï¥Ï£ºÏÑ∏Ïöî.Îç∞Ïù¥ÌÑ∞ÏÖã ÏàòÏßë Î∞è ÌïôÏäµÏóê Í¥ÄÎ†®Îêú ÏΩîÎìúÎäî https://github.com/aiqwe/FinShibainuÏóê ÏûêÏÑ∏ÌïòÍ≤å Í≥µÍ∞úÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n\t\n\t\t\n\t\n\t\n\t\tDPO\n\t\n\nPreferenceÏùò AÎäî answer_A, BÎäî answer_B Ïª¨ÎüºÏûÖÎãàÎã§.\n\nanswer_A: ReferenceÏôÄ ÏßàÎ¨∏ÏùÑ Ìï®Íªò Ï†úÍ≥µÎ∞õÏùÄ gpt ÎãµÎ≥Ä. ReferenceÏóê ÏùòÏ°¥Ï†ÅÏù¥Í≥† ÏßßÏßÄÎßå Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï®\nanswer_B: ReferenceÏóÜÏù¥ ÏßàÎ¨∏Îßå Ï†úÍ≥µÎ∞õÏùÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aiqwe/FinShibainu.","url":"https://huggingface.co/datasets/aiqwe/FinShibainu","creator_name":"Jay Lee","creator_url":"https://huggingface.co/aiqwe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"xstorycloze_ca","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for xstorycloze_ca\n\t\n\n\n\nxstorycloze_ca is a question answering dataset in Catalan, professionally translated from the English StoryCloze dataset (Spring 2016 version), used  to create its multilingual version XStoryCloze.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nxstorycloze_ca (Multilingual Story Cloze Test - Catalan) is based on multiple-choice narrative completions. The dataset consists of 360 instances in the train split and 1510 instances in the test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/xstorycloze_ca.","url":"https://huggingface.co/datasets/projecte-aina/xstorycloze_ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","text-generation","Catalan","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MNLP_M2_mcqa_dataset","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMNLP M2 MCQA Dataset\n\t\n\nA unified multiple-choice question answering (MCQA) benchmark on STEM subjects combining samples from OpenBookQA, SciQ, MMLU-auxiliary, AQUA-Rat, and MedMCQA.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset merges five existing science and knowledge-based MCQA datasets into one standardized format:\n\n\t\n\t\t\nSource\nTrain samples\n\n\n\t\t\nOpenBookQA\n4‚ÄØ900\n\n\nSciQ\n10‚ÄØ000\n\n\nMMLU-aux\n85‚ÄØ100\n\n\nAQUA-Rat\n50‚ÄØ000\n\n\nMedMCQA\n50‚ÄØ000\n\n\nTotal\n200‚ÄØ000\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NicoHelemon/MNLP_M2_mcqa_dataset.","url":"https://huggingface.co/datasets/NicoHelemon/MNLP_M2_mcqa_dataset","creator_name":"Nicolas Gonzalez","creator_url":"https://huggingface.co/NicoHelemon","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"DrivingVQA","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for DrivingVQA\n\t\n\nüè† Homepage\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDrivingVQA is a dataset designed to assist candidates preparing for the French driving theory exam, which requires passing both a theoretical and a practical test. The theoretical aspect consists of analyzing 40 multiple-choice questions (MCQs) with real-world images to test the candidates' knowledge of traffic laws, road signs, and safe driving practices. This dataset focuses on visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA.","url":"https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA","creator_name":"EPFL-DrivingVQA","creator_url":"https://huggingface.co/EPFL-DrivingVQA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"tiny-truthful-qa","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for TruthfulQA\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nTruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 790 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rahmanidashti/tiny-truthful-qa.","url":"https://huggingface.co/datasets/rahmanidashti/tiny-truthful-qa","creator_name":"Hossein A. (Saeed) Rahmani","creator_url":"https://huggingface.co/rahmanidashti","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","text-generation","question-answering","multiple-choice-qa","language-modeling"],"keywords_longer_than_N":true},
	{"name":"perception_test_mcq","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPerception Test MCQ Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1000 video question-answering entries from the Perception Test dataset. Each entry includes a video and a multiple-choice question about the video content, testing various aspects of video understanding including object tracking, action recognition, and temporal reasoning.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset follows the VideoFolder format with the following structure:\ndataset/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/advaitgupta/perception_test_mcq.","url":"https://huggingface.co/datasets/advaitgupta/perception_test_mcq","creator_name":"Advait Gupta","creator_url":"https://huggingface.co/advaitgupta","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","question-answering","visual-question-answering","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PM4Bench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tPM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model\n\t\n\n\n\n\nüåê Homepage | ü§ó Dataset | üìñ Paper \n\n\t\n\t\t\n\t\tüì¢ News\n\t\n\n\nüî•[2025-03-25]: Dataset available on HuggingFace. Paper available on  arXiv.\n\n\n\n\t\n\t\t\n\t\tüßë‚Äçüíª How to Run?\n\t\n\n\n\n\t\n\t\t\n\t\tüè† Set Up\n\t\n\n\n\t\n\t\t\n\t\tDataset Download\n\t\n\nDownload tsv files from HuggingFace and store them in data/tsv/. The directory should be like data/tsv/{DATASET}_{SETTING}_{LANGUAGE}.tsv.\n\n\t\n\t\t\n\t\tEnvironment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songjhPKU/PM4Bench.","url":"https://huggingface.co/datasets/songjhPKU/PM4Bench","creator_name":"Jiahe Song","creator_url":"https://huggingface.co/songjhPKU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","multiple-choice","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"100MMLUpro","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMMLU-Pro 100: A Balanced and Curated Evaluation Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a curated, balanced subset of 100 questions derived from the TIGER-Lab/MMLU-Pro test set. It is designed to provide a small, fast, yet representative benchmark for evaluating the knowledge and reasoning capabilities of large language models across a wide range of academic and professional domains.\nThe key feature of this dataset is its stratified sampling method, ensuring that the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/koiwave/100MMLUpro.","url":"https://huggingface.co/datasets/koiwave/100MMLUpro","creator_name":"KoiWave","creator_url":"https://huggingface.co/koiwave","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"StaticEmbodiedBench","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tüìò Dataset Description\n\t\n\nStaticEmbodiedBench is a dataset for evaluating vision-language models on embodied intelligence tasks, as featured in the OpenCompass leaderboard.\nIt covers three key capabilities:\n\nMacro Planning: Decomposing a complex task into a sequence of simpler subtasks.\nMicro Perception: Performing concrete simple tasks such as spatial understanding and fine-grained perception.\nStage-wise Reasoning: Deciding the next action based on the agent‚Äôs current state and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xiaojiahao/StaticEmbodiedBench.","url":"https://huggingface.co/datasets/xiaojiahao/StaticEmbodiedBench","creator_name":"Xiao Jiahao","creator_url":"https://huggingface.co/xiaojiahao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","arxiv:2508.06553","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"MediQAl","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tMediQAl\n\t\n\nMediQAl is a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and clinical reasoning. It includes 32,603 questions sourced from French medical examinations across 41 medical subjects.\nThe dataset contains three tasks:\n\nMCQU: Multiple-Choice Questions with a Unique correct answer  \nMCQM: Multiple-Choice Questions with Multiple correct answers  \nOEQ: Open-Ended Questions with Short Answers\n\nEach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ANR-MALADES/MediQAl.","url":"https://huggingface.co/datasets/ANR-MALADES/MediQAl","creator_name":"ANR-MALADES","creator_url":"https://huggingface.co/ANR-MALADES","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","French","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CHOICE","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tCHOICE: Benchmarking The Remote Sensing Capabilities of Large Vision-Language Models\n\t\n\n Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/An-Xiao/CHOICE.","url":"https://huggingface.co/datasets/An-Xiao/CHOICE","creator_name":"AnXiao","creator_url":"https://huggingface.co/An-Xiao","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"fire-exam-base64","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tüî• Fire Exam Dataset with Images\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ ÏÜåÎ∞©Í≥µÎ¨¥Ïõê ÏãúÌóò Î¨∏Ï†úÎ•º Í∏∞Î∞òÏúºÎ°ú Íµ¨ÏÑ±Îêú Î©ÄÌã∞Î™®Îã¨ QA Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.Í∞Å ÏÉòÌîåÏùÄ Î¨∏Ï†ú ÌÖçÏä§Ìä∏, ÏÑ†ÌÉùÏßÄ, Ï†ïÎãµ, Í∑∏Î¶¨Í≥† ÏãúÍ∞Å Ï†ïÎ≥¥Î•º Îã¥ÏùÄ Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°úÎ•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n","url":"https://huggingface.co/datasets/taean-yoo/fire-exam-base64","creator_name":"Taean Yoo","creator_url":"https://huggingface.co/taean-yoo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","Korean","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"MedRisk-Bench","keyword":"multiple-choice","description":"MedRisk Benchmark is used for medical risk assessment. \nThe Benchmark is made up with two version with 1232 test samples for each:\nMedRisk-Quantitative: which focuses on the score caculation for medical risk prediction.\nMedRisk-Qualitative: which focuses on the severity for medical condition/disease.\nFull data release can be found at Github\n","url":"https://huggingface.co/datasets/jinge13288/MedRisk-Bench","creator_name":"Jinge Wu","creator_url":"https://huggingface.co/jinge13288","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ToM-in-AMC","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDataset Card for ToM-in-AMC\n\t\n\nThe dataset consists of ‚àº1,000 parsed movie scripts from IMSDb, each corresponding to a character understanding task.\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nBibTeX:\n@inproceedings{yu2024few,\n  title = {Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind},\n  author = {Yu, Mo and Wang, Qiujing and Zhang, Shunchi and Sang, Yisi and Pu, Kangsheng and Wei, Zekai and Wang, Han and Xu, Liyan and Li, Jing and Yu, Yue and Zhou, Jie}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShunchiZhang/ToM-in-AMC.","url":"https://huggingface.co/datasets/ShunchiZhang/ToM-in-AMC","creator_name":"Shunchi Zhang","creator_url":"https://huggingface.co/ShunchiZhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"multiple-choice","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"dream","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tDREAM‚ÄëCFB ¬∑ Dialogue-based Reading Comprehension Examination through Machine Reading (Conversation Fact Benchmark Format)\n\t\n\nDREAM‚ÄëCFB is a 6,444 example dataset derived from the original DREAM dataset, transformed and adapted for the Conversation Fact Benchmark framework. Each item consists of multi-turn dialogues with associated multiple-choice questions that test reading comprehension and conversational understanding.\nThe dataset focuses on dialogue-based reading comprehension:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/onionmonster/dream.","url":"https://huggingface.co/datasets/onionmonster/dream","creator_name":"Calvin Ku","creator_url":"https://huggingface.co/onionmonster","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","expert-generated","dream","English","mit"],"keywords_longer_than_N":true},
	{"name":"DUSK","keyword":"multiple-choice","description":"\n\t\n\t\t\n\t\tüåá DUSK: Do Not Unlearn Shared Knowledge\n\t\n\nDUSK is a benchmark dataset designed for evaluating machine unlearning in multi-source settings, where specific data sources must be forgotten while preserving others.\nIn realistic applications, documents often share factual overlap with publicly available content (e.g., Wikipedia, textbooks). DUSK challenges unlearning algorithms to precisely erase only what must be forgotten, while preserving knowledge that remains supported by other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-ISL/DUSK.","url":"https://huggingface.co/datasets/AI-ISL/DUSK","creator_name":"AI-ISL","creator_url":"https://huggingface.co/AI-ISL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","other","machine-generated","original"],"keywords_longer_than_N":true}
]
;
