const data_for_modality_to_text = 
[
	{"name":"emova-sft-4m","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-sft-4m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-SFT-4M\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-SFT-4M is a comprehensive dataset curated for omni-modal instruction tuning, including textual, visual, and audio interactions. This dataset is created by gathering open-sourced multi-modal instruction datasets and synthesizing high-quality omni-modal conversation data to enhance user experience. This dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-sft-4m.","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 1-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-11.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v28","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v19","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 4-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response. Argh I forgot to enable this. Using RLE compression.\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v162","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v162","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v162.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v163","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v163","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v163.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v165","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v165","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v165.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v167","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v167","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v167.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v168","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v168","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v168.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v172","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v172","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v172.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v174","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v174","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v174.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v176","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v176","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v176.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v177","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v177","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v177.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v180","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v180","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v180.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v181","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v181","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v181.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Magma-820K","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MagmaAI/Magma-820K","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1‚Ä†¬†\nReuben Tan1‚Ä†¬†\nQianhui Wu1‚Ä†¬†\nRuijie Zheng2‚Ä°¬†\nBaolin Peng1‚Ä°¬†\nYongyuan Liang2‚Ä°\nYu Gu1¬†\nMu Cai3¬†\nSeonghyeon Ye4¬†\nJoel Jang5¬†\nYuquan Deng5¬†\nLars Liden1¬†\nJianfeng Gao1‚ñΩ\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  ‚Ä† First authors  ‚Ä° Second authors  ‚ñΩ Leadership  \n[arXiv Paper] ¬† [Project Page] ¬† [Hugging Face Paper] ¬† [Github Repo]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-820K.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","mit","100K<n<1M","arxiv:2502.13130"],"keywords_longer_than_N":true},
	{"name":"ECG-Grounding","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LANSG/ECG-Grounding","creator_name":"LAN XIANG","creator_url":"https://huggingface.co/LANSG","description":"\n\t\n\t\t\n\t\tECG-Grounding dataset\n\t\n\nECG-Grounding provides more accurate, holistic, and evidence-driven interpretations with diagnoses grounded in measurable ECG features.¬†Currently, it contains 30,000 instruction pairs annotated with heartbeat-level physiological features. This is the first high-granularity ECG grounding dataset, enabling evidence-based diagnosis and improving the trustworthiness of medical AI. We will continue to release more ECG-Grounding data and associated beat-level‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LANSG/ECG-Grounding.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"KVG-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MaxyLee/KVG-Bench","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n \n\n \n \nThis is the official repository of KVG-Bench, a comprehensive benchmark of Knowledge-intensive Visual Grounding (KVG) spanning 10 categories with 1.3K manually curated test cases.\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SightationCompletions","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationCompletions","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationCompletions\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationCompletions.","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"M-Attack_AdvSamples","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI-LLM/M-Attack_AdvSamples","creator_name":"MBZUAI-LLM","creator_url":"https://huggingface.co/MBZUAI-LLM","description":"\n\t\n\t\t\n\t\tM-Attack Adversarial Samples Dataset\n\t\n\nThis dataset contains 100 adversarial samples generated using M-Attack to perturb the images from the NIPS 2017 Adversarial Attacks and Defenses Competition. This dataset is used in the paper A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset consists of total 300 adversarial samples organized in three‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI-LLM/M-Attack_AdvSamples.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"SightationRetrieval","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationRetrieval","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationRetrieval\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationRetrieval.","first_N":5,"first_N_keywords":["image-to-text","mit","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SightationVQA","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationVQA","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationVQA\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationVQA.","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SightationReasoning","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationReasoning","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationReasoning\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationReasoning.","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SightationReasoning","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationReasoning","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationReasoning\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationReasoning.","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"PEBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xuzhaopan/PEBench","creator_name":"xu","creator_url":"https://huggingface.co/xuzhaopan","description":"\n\t\n\t\t\n\t\tPEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models\n\t\n\nPaper\nPEBench, a comprehensive benchmark for evaluating machine unlearning in MLLMs, focusing on both personal entities and event scenes to provide a holistic assessment of unlearning efficacy and scope.\nMore details on loading and using the data are at our github page.\nIf you do find our code helpful or use our benchmark dataset, please citing our paper.\n@article{xu2025pebench‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xuzhaopan/PEBench.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"text_meme","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/text_meme","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\ttext_meme\n\t\n\n–°–æ—Å–∫—Ä–∞–ø–µ–Ω–æ —Å –æ—Ç–ª–∏—á–Ω–æ–≥–æ Telegram –∫–∞–Ω–∞–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ–º—ã.\n","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","monolingual","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"verify-teaser","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jing-bi/verify-teaser","creator_name":"jing bi","creator_url":"https://huggingface.co/jing-bi","description":"\n\t\n\t\t\n\t\tVERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning FidelitY\n\t\n\nVERIFY is the first benchmark explicitly designed to assess the reasoning paths of MLLMs in visual reasoning tasks. \nBy introducing novel evaluation metrics that go beyond mere accuracy, VERIFY highlights critical limitations in current MLLMs and emphasizes the need for a more balanced approach to visual perception and logical reasoning.\nDetails of the benchmark can viewed at the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jing-bi/verify-teaser.","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"GenS-Video-150K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yaolily/GenS-Video-150K","creator_name":"Linli Yao","creator_url":"https://huggingface.co/yaolily","description":"\nüîó Project Page ¬∑ üìñ Paper ¬∑ ‚≠ê GitHub ¬∑ üìä Dataset ¬∑ ü§ó Checkpoints\n\n\n\n\n\t\n\t\t\n\t\tGenS-Video-150K Dataset\n\t\n\nTo enable effective frame sampling, we introduce GenS-Video-150K, a large-scale synthetic dataset specifically designed for training frame sampling models. Annotated by GPT-4o, this dataset features:\n\nDense coverage: Annotates ~20% of all frames with relevance scores.\nFine-grained assessment: Assigns confidence scores (level 1 to 5) to relevant frames.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yaolily/GenS-Video-150K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2503.09146"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-missing-words-train-only","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words-train-only","creator_name":"Alix Chagu√©","creator_url":"https://huggingface.co/alix-tz","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Missing Words in Train Split only\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Missing Words in Train Split only, each variation column is affected by the noise, only when the split is for training. The validation and test splits are not affected by the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words-train-only.","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-xxx-words-train-only","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alix-tz/noisy-gt-xxx-words-train-only","creator_name":"Alix Chagu√©","creator_url":"https://huggingface.co/alix-tz","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Words Replaced with XXX in Train Split only\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Words Replaced with XXX in Train Split only, each variation column is affected by the noise, without considering the split between train, validation and test.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-xxx-words-train-only.","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"tartandrive","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mateoguaman/tartandrive","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","description":"\n\t\n\t\t\n\t\tTartanDrive Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/tartandrive.","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"scand","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mateoguaman/scand","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","description":"\n\t\n\t\t\n\t\tSCAND Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/scand.","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"V1-33K-Old","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K-Old","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nüöÄ  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction üåü\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K-Old.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Video-MMLU","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enxin/Video-MMLU","creator_name":"EnxinSong","creator_url":"https://huggingface.co/Enxin","description":"\n\n\n\t\n\t\t\n\t\tVideo-MMLU Benchmark\n\t\n\n\n  \n    \n    \n    \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: Video-MMLU Benchmark\n\n\n\t\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\tBenchmark Collection and Processing\n\t\n\nVideo-MMLU specifically targets videos that focus on theorem demonstrations and probleming-solving, covering mathematics, physics, and chemistry. The videos deliver dense information through numbers and formulas, pose significant challenges for video LMMs in dynamic OCR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enxin/Video-MMLU.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"pxhere","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/pxhere","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for pxhere Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a large collection of high-quality photographs sourced from pxhere.com, a free stock photo website. The dataset includes approximately 1,100,000 images in full resolution covering a wide range of subjects including nature, people, urban environments, objects, animals, and landscapes. All images are provided under the Creative Commons Zero (CC0) license, making them freely available for personal and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pxhere.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"SMMILE","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VideoGameQA-Bench","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taesiri/VideoGameQA-Bench","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","description":"\n\t\n\t\t\n\t\tVideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance\n\t\n\nby Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer\n\n\t\n\t\t\n\t\tAbstract:\n\t\n\n\nWith video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/VideoGameQA-Bench.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VideoGameQA-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taesiri/VideoGameQA-Bench","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","description":"\n\t\n\t\t\n\t\tVideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance\n\t\n\nby Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer\n\n\t\n\t\t\n\t\tAbstract:\n\t\n\n\nWith video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/VideoGameQA-Bench.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"shamela-waqfeya-library","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tShamela Waqfeya Library\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nShamela Waqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 4,500 PDF books across over 40 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThe dataset includes 12,877 PDF files (spanning 5,138,027 pages) representing 4,661 Islamic books.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"TextVQA_GT_bbox","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox","creator_name":"jiarui zhang","creator_url":"https://huggingface.co/jrzhang","description":"\n\t\n\t\t\n\t\tTextVQA validation set with grounding truth bounding box\n\t\n\nThe dataset used in the paper MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs for studying MLLMs' attention patterns.\nThe dataset is sourced from TextVQA and annotated manually with ground-truth bounding boxes. \nWe consider questions with a single area of interest in the image so that 4370 out of 5000 samples are kept.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find our paper and code useful‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox.","first_N":5,"first_N_keywords":["question-answering","text-generation","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MUSTARD","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/badrivishalk/MUSTARD","creator_name":"Badri Vishal Kasuba","creator_url":"https://huggingface.co/badrivishalk","description":"\n\t\n\t\t\n\t\tDataset Card for MUSTARD\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMUSTARD (Multilingual Scanned and Scene Table Structure Recognition Dataset) is a diverse dataset curated for table structure recognition across multiple languages. The dataset consists of tables extracted from magazines, including printed, scanned, and scene-text tables, labeled with Optimized Table Structure Language (OTSL) sequences. It is designed to facilitate research in multilingual table‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/badrivishalk/MUSTARD.","first_N":5,"first_N_keywords":["image-to-text","English","Hindi","Telugu","Tamil"],"keywords_longer_than_N":true},
	{"name":"Book-Scan-OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MLap/Book-Scan-OCR","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","description":"\n\t\n\t\t\n\t\tBest Usage\n\t\n\n\nSuitable for fine-tuning Vision-Language Models (e.g., PaliGemma).\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was generated using Mistral OCR and Google Lens, followed by manual cleaning for improved accuracy.  \n\n\t\n\t\t\n\t\tImage Source\n\t\n\nImages are sourced from Sarvam.ai.  \n","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SVG","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mortdecai/SVG","creator_name":"Karahan","creator_url":"https://huggingface.co/Mortdecai","description":"\n\t\n\t\t\n\t\tSurprising Visual Genome (SVG)\n\t\n\nThis repository contains the dataset used in the paper \"Complexity in Complexity: Understanding Visual Complexity Through Structure, Color, and Surprise\". The dataset includes visual complexity ratings along with various features that help predict perceived visual complexity.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset includes the following features:\n\n\t\n\t\t\nColumn\nDescription\n\n\n\t\t\nimage_id\nUnique identifier for each image\n\n\ncomplexity\nHuman-rated visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mortdecai/SVG.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ViLBench","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/ViLBench","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"Benchmark Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\narXiv | Project Page\nThere are 600 data collected from 5 existing vision-language tasks\n","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","n<1K","arxiv:2503.20271"],"keywords_longer_than_N":true},
	{"name":"MMMR","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csegirl/MMMR","creator_name":"g","creator_url":"https://huggingface.co/csegirl","description":"This repository contains the data presented in MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks.\nProject page: https://mmmr-benchmark.github.io/\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"function_graph","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alikirec/function_graph","creator_name":"Ali Kirecligol","creator_url":"https://huggingface.co/alikirec","description":"alikirec/function_graph dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"AgentSynth","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sunblaze-ucb/AgentSynth","creator_name":"sunblaze-ucb","creator_url":"https://huggingface.co/sunblaze-ucb","description":"\n\t\n\t\t\n\t\tAgentSynth\n\t\n\n\n\t\n\t\t\n\t\tAgentSynth: Scalable Task Generation for Generalist Computer-Use Agents\n\t\n\nPaper | Project Page | Code\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sunblaze-ucb/AgentSynth.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2506.14205","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"MMIF-23k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChrisDing1105/MMIF-23k","creator_name":"Shengyuan Ding","creator_url":"https://huggingface.co/ChrisDing1105","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nPaper: MM-IFEngine: Towards Multimodal Instruction Following\n\nGithub: SYuan03/MM-IFEngine\n\nProject Page: syuan03.github.io/MM-IFEngine/\n\nMM-IFEval Evaluation: Using VLMEvalKit\n\n\nüòä This is the official repo of MM-IFEngine datasets in MM-IFEngine: Towards Multimodal Instruction Following\nüöÄ We include both the SFT and DPO data in this repo as the v1 dataset (generated mainly by InternVL2.5-78B and Qwen2-VL-7B), which we used to train the model described in our paper.üíñ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChrisDing1105/MMIF-23k.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"deepseek-svg-description","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description","creator_name":"Shahzeb Khoso","creator_url":"https://huggingface.co/ShahzebKhoso","description":"\n\t\n\t\t\n\t\tSVG Reasoning and Generation Dataset\n\t\n\nA rich dataset containing SVG graphics, structured reasoning, and generated descriptions.Built from the base of thesantatitan/deepseek-svg-dataset but enhanced with separated SVG codes and detailed reasoning-based descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription Generation Process\n\t\n\nThe dataset has been enhanced by using the reasoning part from the original completion to generate longer, detailed descriptions. The SVG code part of the completion is ignored‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Magma-AITW-SoM","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1‚Ä†¬†\nReuben Tan1‚Ä†¬†\nQianhui Wu1‚Ä†¬†\nRuijie Zheng2‚Ä°¬†\nBaolin Peng1‚Ä°¬†\nYongyuan Liang2‚Ä°\nYu Gu1¬†\nMu Cai3¬†\nSeonghyeon Ye4¬†\nJoel Jang5¬†\nYuquan Deng5¬†\nLars Liden1¬†\nJianfeng Gao1‚ñΩ\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  ‚Ä† First authors  ‚Ä° Second authors  ‚ñΩ Leadership  \n[arXiv Paper] ¬† [Project Page] ¬† [Hugging Face Paper] ¬† [Github Repo] ¬† [Video]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM.","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"concept_coverage_laion_6m","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayug/concept_coverage_laion_6m","creator_name":"Mayug Maniparambil","creator_url":"https://huggingface.co/mayug","description":"\n\t\n\t\t\n\t\tüì¶ Freeze-Align Dataset\n\t\n\nThe Freeze-Align Dataset (concept_coverage_laion_6m) is a curated collection of high-quality image-text pairs designed to facilitate efficient multimodal alignment using frozen unimodal encoders. This dataset supports the research presented in our CVPR 2025 paper, \"Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment\", enabling models to achieve CLIP-level performance with significantly reduced computational resources.\nThe dataset is curated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mayug/concept_coverage_laion_6m.","first_N":5,"first_N_keywords":["zero-shot-classification","text-to-image","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nüêô GitHub Repo: waltonfuture/MM-UPT\nüìú Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"HANS","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoenRommedahl/HANS","creator_name":"Joen Rommedahl","creator_url":"https://huggingface.co/JoenRommedahl","description":"\n\t\n\t\t\n\t\tDataset Card for HANS\n\t\n\nHANS is under development and not suited for use yet.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Handwriting Archive: Nordic Samples dataset is a small Danish-language dataset, primarily consisting of transscribed, publically available documents from the Danish National Archive.\nThe purpose of the dataset is explore how to properly create, maintain and update a repository of HTR training data.\n\nCurated by: [Joen Rommedahl]\nLanguage(s) (NLP):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JoenRommedahl/HANS.","first_N":5,"first_N_keywords":["image-to-text","Danish","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"videomarathon","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jylins/videomarathon","creator_name":"Jingyang Lin","creator_url":"https://huggingface.co/jylins","description":"\n\t\n\t\t\n\t\tDataset Card for VideoMarathon\n\t\n\nVideoMarathon is a large-scale long video instruction-following dataset with a total duration of approximately 9,700 hours, comprising 3.3 million QA pairs across 22 task categories.\nPaper and more resources: [arXiv] [Project Website] [GitHub] [Model]\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Uses\n\t\n\nThis dataset is used for academic research purposes only.\n\n\t\n\t\t\n\t\n\t\n\t\tTask Taxonomy\n\t\n\nThe dataset contains 22 diverse tasks over six fundamental topics, including temporality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jylins/videomarathon.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tüß† What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/openinterx/UGC-VideoCap","creator_name":"Memories.ai Research","creator_url":"https://huggingface.co/openinterx","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openinterx/UGC-VideoCap.","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","üá∫üá∏ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"cc0-textures","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/cc0-textures","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for CC0 Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 18,785 texture images from cc0-textures.com. It includes textures of wood, metal, concrete, fabric, stone, ceramic, and other materials. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while keeping good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): Texture titles and tags‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cc0-textures.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"thai-license-plate-ocr","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/morsetechlab/thai-license-plate-ocr","creator_name":"Nuttapong Chimwai","creator_url":"https://huggingface.co/morsetechlab","description":"\n\t\n\t\t\n\t\tThai License Plate OCR Dataset üáπüá≠\n\t\n\nüá∫üá∏ English Version\n\nTask: Optical Character Recognition (OCR)\nLanguage: Thai üáπüá≠  \n\nOCR dataset ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PaddleOCR-rec ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞\n‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏õ‡πâ‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î\n\n\n‚ö†Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö PaddleOCR-rec (‡πÑ‡∏°‡πà‡∏°‡∏µ detection / classification)\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nthai-license-ocr-dataset/\n‚îú‚îÄ‚îÄ images/           # ‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n‚îú‚îÄ‚îÄ train.txt         # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô\n‚îú‚îÄ‚îÄ val.txt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/morsetechlab/thai-license-plate-ocr.","first_N":5,"first_N_keywords":["image-to-text","manual","monolingual","Thai","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"LearningPaper24","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vivianchen98/LearningPaper24","creator_name":"Shenghui Chen","creator_url":"https://huggingface.co/vivianchen98","description":"\n\t\n\t\t\n\t\tLearningPaper24 Dataset\n\t\n\nThis dataset contains video recordings and metadata from ICLR and NeurIPS 2024 conference talks. It includes both poster and oral presentations, along with their associated metadata such as titles, abstracts, keywords, and primary areas.\nThe paper list is originally sourced from Paperlists.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nlearningpaper24/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata/\n‚îÇ   ‚îî‚îÄ‚îÄ catalog.json\n‚îî‚îÄ‚îÄ video/\n    ‚îú‚îÄ‚îÄ {openreview_id}_{slideslive_id}.mp4\n    ‚îî‚îÄ‚îÄ ...‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vivianchen98/LearningPaper24.","first_N":5,"first_N_keywords":["summarization","video-text-to-text","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"SCALAR-VG","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiaoMiaoYang/SCALAR-VG","creator_name":"Guluguluha","creator_url":"https://huggingface.co/MiaoMiaoYang","description":"\n\t\n\t\t\n\t\tSCALAR_VG\n\t\n\nWhile the world model continues to advance, existing datasets remain inadequate for supporting large-scale multi-modal training, particularly in comprehensive multi-dimensional scene-aware understanding. Therefore, we have built the SCALAR-VG through the SCALAR, integrating and extending many open-source image datasets to meet this demandImportantly, It contains about 240K images with comprehensive, hierarchical and multi-dimensional annotations. \nCompared with existing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiaoMiaoYang/SCALAR-VG.","first_N":5,"first_N_keywords":["keypoint-detection","image-segmentation","object-detection","visual-question-answering","zero-shot-object-detection"],"keywords_longer_than_N":true},
	{"name":"VideoVista-CulturalLingo","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo","creator_name":"Uni-MoE","creator_url":"https://huggingface.co/Uni-MoE","description":"\n    \n\n\n    \n\n\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tVideoVista-CulturalLingo\n\t\n\nThis repository contains the VideoVista-CulturalLingo, introduced in VideoVista-CulturalLingo: 360¬∞ Horizons-Bridging Cultures, Languages,\nand Domains in Video Comprehension. \n üéâ Our new VideoVista-CulturalLingo bridges cultures (China, North America, and Europe), languages (Chinese and English), and domains (140+)in video comprehension. \n üåç Welcome to join us on this journey of video understanding!\n\n\t\n\t\t\n\t\tFiles\n\t\n\nWe provice the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo.","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Live-WhisperX-526K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","description":"\n\t\n\t\t\n\t\tDataset Card for Live-WhisperX-526K\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for the training of the LiveCC-7B-Instruct model. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nAfter we finished the pre-training of LiveCC-7B-Base model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","100K<n<1M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"shamela-waqfeya-library-compressed","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library-compressed","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tShamela Waqfeya Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nShamela Waqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 4,500 PDF books across over 40 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/shamela-waqfeya-library, with one key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library-compressed.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"waqfeya-library-compressed","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/waqfeya-library-compressed","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tWaqfeya Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nWaqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 10,000 PDF books across over 80 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/waqfeya-library, with one key difference: the contents‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/waqfeya-library-compressed.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"GeoQA-8K-direct-synthesizing","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset supports the unsupervised post-training of multi-modal large language models (MLLMs) as described in the paper Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO. It's designed to enable continual self-improvement without external supervision, using a self-rewarding mechanism based on majority voting over multiple sampled responses. The dataset is used to improve the reasoning ability of MLLMs, as demonstrated by significant improvements on benchmarks like MathVista‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"HueManity","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jayant-Sravan/HueManity","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","description":"\n\t\n\t\t\n\t\tHueManity: A Benchmark for Testing Human-Like Visual Perception in MLLMs\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHueManity is a benchmark dataset featuring 83,850 images designed to test the fine-grained visual perception of Multimodal Large Language Models (MLLMs). Each image presents a two-character alphanumeric string embedded within Ishihara-style dot patterns, challenging models to perform precise pattern recognition in visually cluttered environments.\nThe dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/HueManity.","first_N":5,"first_N_keywords":["question-answering","image-to-text","image-feature-extraction","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"Japanese-photos","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/Japanese-photos","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tÊó•Êú¨„ÅÆÂÜôÁúü„Åü„Å°\n\t\n\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„Åß„ÅØÊó•Êú¨„ÅÆÂÜôÁúü„Åü„Å°„ÇíÂÖ±Êúâ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÊó•Êú¨„Åß„Çà„ÅèË¶ã„Çâ„Çå„ÇãÂÖâÊôØ„ÇíAI„Å´Â≠¶Áøí„Åï„Åõ„Çã„Åì„Å®„Åß\nÊó•Êú¨„Çâ„Åó„ÅÑÂøúÁ≠î„ÅÆ„Åß„Åç„ÇãAI„ÇíÈñãÁô∫„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´„ÄÅ\nCC-0„ÅßÂÜôÁúü800Êûö„Å™„Å©„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÁâπ„Å´ÊúÄËøëÂØøÂè∏„Å®„Åã„ÅåÊó•Êú¨„Çâ„Åó„Åè„Å™„ÅÑ„ÅÆ„ÅåÊ∞ó„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„É™„É≥„ÇØ: images.tar\nËã±Ë™û„ÅÆË™¨ÊòéÊñá : metadata.csv\nÊó•Êú¨Ë™û„ÅÆË™¨ÊòéÊñá : metadata_ja.csv\n\n\t\n\t\t\n\t\n\t\n\t\t„É©„Ç§„Çª„É≥„Çπ„Å´„Å§„ÅÑ„Å¶\n\t\n\nÊíÆÂΩ±ËÄÖ„Åß„ÅÇ„ÇãÁßÅ„ÅØ„Åì„Çå„Çâ„ÅÆÂÜôÁúü„ÅÆËëó‰ΩúÊ®©„ÇíÊîæÊ£Ñ„Åó„Åæ„Åô„ÄÇ\n„ÅÑ„Åã„Çà„ÅÜ„Å´„ÇÇ‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åó„Åã„Åó„ÄÅËÇñÂÉèÊ®©„ÇÑÂïÜÊ®ôÊ®©„ÅØÊÆã„Å£„Å¶„ÅÑ„Åæ„Åô„ÅÆ„Åß„ÄÅ„ÅîÊ≥®ÊÑè„Åè„Å†„Åï„ÅÑ„ÄÇ\nÊó•Êú¨Ë™û„Å®Ëã±Ë™û„ÅÆË™¨ÊòéÊñá„ÅØ„Å®„ÇÇ„Å´Qwen2.5VL„Å´„Çà„Çä‰Ωú„Çâ„Çå„Åæ„Åó„Åü„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅËëó‰ΩúÊ®©„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tÂÜôÁúü„ÅÆÊ¶ÇË¶Å\n\t\n\nÂü∫Êú¨ÁöÑ„Å´ÂêÑÂú∞„ÇíÊóÖË°å„Åó„ÅüÈöõ„ÅÆÊó•Êú¨ÂêÑÂú∞„ÅÆÂÜôÁúü„ÅåÂÜô„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂÆ§Â§ñ„ÇÇ„ÅÇ„Çå„Å∞„ÄÅÂÆ§ÂÜÖ„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇÂÜôÁúü„ÅÆ‰æã„ÇíÂèÇËÄÉ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n\t\n\t\n\t\n\t\tÂÜôÁúü„ÅÆ‰æã\n\t\n\n\nÊó•Êú¨Ë™û„ÅÆË™¨ÊòéÊñá‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/Japanese-photos.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PORTO","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LIACC/PORTO","creator_name":"LIACC","creator_url":"https://huggingface.co/LIACC","description":"\n\t\n\t\t\n\t\tPost-OCR Resources for Text Optimisation\n\t\n\nResource for evaluation and develop OCRs and Post-OCR focused on historical Portuguese.\nHow to load the dataset:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"LIACC/PORTO\")\n\n","first_N":5,"first_N_keywords":["image-to-text","fill-mask","text-generation","Portuguese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"PC-Agent-E","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/henryhe0123/PC-Agent-E","creator_name":"Yanheng He","creator_url":"https://huggingface.co/henryhe0123","description":"This repository contains the dataset used in the paper Efficient Agent Training for Computer Use.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Magma-Video-ToM","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1‚Ä†¬†\nReuben Tan1‚Ä†¬†\nQianhui Wu1‚Ä†¬†\nRuijie Zheng2‚Ä°¬†\nBaolin Peng1‚Ä°¬†\nYongyuan Liang2‚Ä°\nYu Gu1¬†\nMu Cai3¬†\nSeonghyeon Ye4¬†\nJoel Jang5¬†\nYuquan Deng5¬†\nLars Liden1¬†\nJianfeng Gao1‚ñΩ\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  ‚Ä† First authors  ‚Ä° Second authors  ‚ñΩ Leadership  \n[arXiv Paper] ¬† [Project Page] ¬† [Hugging Face Paper] ¬† [Github Repo] ¬† [Video]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM.","first_N":5,"first_N_keywords":["video-text-to-text","robotics","mit","1M - 10M","arrow"],"keywords_longer_than_N":true},
	{"name":"LLaVA-ReCap-676K","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LimeryJorge/LLaVA-ReCap-676K","creator_name":"Limery Jorge","creator_url":"https://huggingface.co/LimeryJorge","description":"This is an integrated version of LLaVA-ReCap, sourced from lmms-lab/LLaVA-ReCap-558K and lmms-lab/LLaVA-ReCap-118K.\nIn this version, the conversations field has been split into two separate fields: prompt and response. Additionally, the <image> special token has been removed to facilitate customization.\nInspired by the original paper, the prompt field has been further expanded with human-crafted variations. Specifically, each prompt is sampled from one of the following 30 instructions:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LimeryJorge/LLaVA-ReCap-676K.","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TAMMs","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IceInPot/TAMMs","creator_name":"ÈîÖ‰∏≠ÂÜ∞","creator_url":"https://huggingface.co/IceInPot","description":"\n\t\n\t\t\n\t\tTAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting\n\t\n\nTAMMs is a large-scale dataset derived from the Functional Map of the World (fMoW) dataset, curated to support multimodal and temporal reasoning tasks such as change detection and future prediction.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 37,003 high-quality temporal sequences, each consisting of at least four distinct satellite images of the same location captured at different‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IceInPot/TAMMs.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"self-alignment","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/self-alignment","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":" \n \n \n \n \n\n\t\n\t\t\n\t\n\t\n\t\tVideo sources\n\t\n\nIn the json files, src indicates the video sources which can be downloaded as follows.\n\nvideo-vqa-webvid_qa: WebVid\nvideo-conversation-videochat2: VideoChat2\nvideo-classification-ssv2: SSv2\nvideo-reasoning-clevrer_qa: CLEVRER\nvideo-vqa-tgif_frame_qa: TGIF\nvideo-reasoning-next_qa: NExTQA\nvideo-conversation-videochat1: VideoChat\nvideo-vqa-tgif_transition_qa: TGIF\nvideo-reasoning-clevrer_mc: CLEVRER\nvideo-vqa-ego_qa: EgoQA\nvideo-classification-k710:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/self-alignment.","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K<n<100K","arxiv:2504.12083","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"objaverse_processed_renders_and_captions","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DamianBoborzi/objaverse_processed_renders_and_captions","creator_name":"Damian Boborzi","creator_url":"https://huggingface.co/DamianBoborzi","description":"Contains rendered views and captions from Objaverse XL objects. the objects are from the alignment and TRELLIS500K (over 1 Millionen processed objects) dataset. We downloaded and rendered 4 views of each object. We added TRELLIS and CAP3D Captions where available. If there were no captions we generated new captions with the large version of Florence 2. This is the base dataset we used to generate MeshFleet which is described in MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DamianBoborzi/objaverse_processed_renders_and_captions.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1M - 10M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"ChemVLM_test_data","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Duke-de-Artois/ChemVLM_test_data","creator_name":"Junxian Li","creator_url":"https://huggingface.co/Duke-de-Artois","description":"arxiv.org/abs/2408.07246Using this dataset, please kindly cite:\n@inproceedings{li2025chemvlm,\n  title={Chemvlm: Exploring the power of multimodal large language models in chemistry area},\n  author={Li, Junxian and Zhang, Di and Wang, Xunzhi and Hao, Zeying and Lei, Jingdi and Tan, Qian and Zhou, Cai and Liu, Wei and Yang, Yaotian and Xiong, Xinrui and others},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={39},\n  number={1},\n  pages={415--423}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Duke-de-Artois/ChemVLM_test_data.","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"Latex-KIE","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Latex-KIE","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tLatex-KIE Dataset\n\t\n\nThe Latex-KIE dataset is a large-scale collection of paired LaTeX formula images and their corresponding LaTeX code. It is specifically designed for training and evaluating models for Image-to-LaTeX, Key Information Extraction (KIE), and Optical Character Recognition (OCR) tasks in scientific domains.\n\n\n\t\n\t\t\n\t\tüìä Dataset Summary\n\t\n\n\nImages: Rendered LaTeX math formulas (black text on white background)\nText: Corresponding raw LaTeX code for each image\nSplit: train‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Latex-KIE.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"keystroke-typing-videos","keyword":"video-text-to-text","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/andrewt28/keystroke-typing-videos","creator_name":"Andrew Tran","creator_url":"https://huggingface.co/andrewt28","description":"\n\t\n\t\t\n\t\tKeystroke Typing Videos of Reuters\n\t\n\nRecordings of typing randomly sampled sentences (<= 150 characters) from nltk Reuters dataset. Keystroke data is provided too.\n","first_N":5,"first_N_keywords":["video-text-to-text","English","afl-3.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"Chinese-Multimodal-Instruct","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mxode/Chinese-Multimodal-Instruct","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"\n  ‰∏≠ÊñáÔºàËßÜËßâÔºâÂ§öÊ®°ÊÄÅÊåá‰ª§Êï∞ÊçÆÈõÜ\n\n\n\n  üíª Github Repo \n\n\nÊú¨È°πÁõÆÊó®Âú®ÊûÑÂª∫‰∏Ä‰∏™È´òË¥®Èáè„ÄÅÂ§ßËßÑÊ®°ÁöÑ‰∏≠ÊñáÔºàËßÜËßâÔºâÂ§öÊ®°ÊÄÅÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÁõÆÂâç‰ªçÂú®ÊñΩÂ∑•‰∏≠ üößüí¶\n\n\n[!Important]\nÊú¨Êï∞ÊçÆÈõÜ‰ªçÂ§Ñ‰∫é WIP (Work in Progress) Áä∂ÊÄÅÔºåÁõÆÂâç Dataset Viewer Â±ïÁ§∫ÁöÑÊòØ 100 Êù°Á§∫‰æã„ÄÇ\nÂàùÊ≠•È¢ÑËÆ°ËßÑÊ®°Â§ßÁ∫¶Âú® 1ÔΩû2MÔºà‰∏çÂåÖÂê´ÂÖ∂‰ªñÊù•Ê∫êÁöÑÊï∞ÊçÆÈõÜÔºâÔºåÂùá‰∏∫Â§öËΩÆÂØπËØùÂΩ¢Âºè„ÄÇ\n\n\n[!Tip]\n[2025/05/05] ÂõæÁâáÂ∑≤Áªè‰∏ä‰º†ÂÆåÊØïÔºåÂêéÁª≠ÊñáÂ≠óÈÉ®ÂàÜÊ≠£Á≠âÂæÖ‰∏ä‰º†„ÄÇ\n\n","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","cc-by-sa-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MicroG-4M","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LEI-QI-233/MicroG-4M","creator_name":"Lei Qi","creator_url":"https://huggingface.co/LEI-QI-233","description":"\n\t\n\t\t\n\t\tMicroG-4M Dataset\n\t\n\nThis repository stores the entire content of the  MicroG-4M dataset itself.\nFor more information and details, including training, evaluation, statistics, and related code, please:\n\nRefer to our paper\n\nVisit our GitHub\n\n\nIn addition to the original dataset format, we provide a Parquet format for automatically generating Croissant files on the Hugging Face platform. Loading via Croissant will fetch these Parquet files directly. For detailed information, please check‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LEI-QI-233/MicroG-4M.","first_N":5,"first_N_keywords":["video-classification","visual-question-answering","video-text-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MultiFinBen-EnglishOCR","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","description":"\n\n\t\n\t\t\n\t\tDataset Card for EnglishOCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe EnglishOCR dataset contains images derived from regulatory documents from SEC EDGAR company filings. This dataset is used for benchmarkingg and evaluating Large Language Models ability on converting unstructured dcuments, such as pdfs and images, into machine readable format, particularly in finance domain, where the conversion task is more complex and valuable.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask: Image-to-Text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR.","first_N":5,"first_N_keywords":["image-to-text","Spanish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MultiFinBen-SpanishOCR","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","description":"\n\n\t\n\t\t\n\t\tDataset Card for SpanishOCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe SpanishOCR dataset contains images derived from regulatory documents from Peru government in pdf format. This dataset is used for benchmarkingg and evaluating Large Language Models ability on converting unstructured dcuments, such as pdfs and images, into machine readable format, particularly in finance domain, where the conversion task is more complex and valuable.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask: Image-to-Text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR.","first_N":5,"first_N_keywords":["image-to-text","Spanish","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"image-wallpapers-dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","description":"\n\t\n\t\t\n\t\tNavanjana/image-wallpapers-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality images paired with descriptive text annotations, designed for computer vision and multimodal machine learning tasks. Each image has been preprocessed to standard dimensions and paired with detailed descriptions extracted from web sources.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Images: [NUMBER] images\nImage Format: JPEG (RGB)\nImage Dimensions: 224√ó224 pixels\nText Descriptions: Natural‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-feature-extraction","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-130k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","description":"This is the SophiaVL-7B-130k dataset of SophiaVL-R1 (https://arxiv.org/abs/2505.17018). The textual data is stored in JSON files (located in ./json/), and the corresponding images are contained in ZIP archives.\nCode: https://github.com/kxfan2002/SophiaVL-R1\nData is in the following format:\n    {\n        \"problem_id\": 1, # id in current class\n        \"problem\": \"Subtract 0 cyan cubes. How many objects are left?\", # textual question\n        \"data_type\": \"image\", # text-only data(\"text\") or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"TON-AITZ-SFT","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kolerk/TON-AITZ-SFT","creator_name":"jiaqi wang","creator_url":"https://huggingface.co/kolerk","description":"This is the dataset trained in the model in the paper: Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","Image","arxiv:2505.16854"],"keywords_longer_than_N":true},
	{"name":"TON-Math-SFT","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kolerk/TON-Math-SFT","creator_name":"jiaqi wang","creator_url":"https://huggingface.co/kolerk","description":"This is the dataset trained for model cited in the paper: Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Sanskrit","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/Sanskrit","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\n\t\n\t\t\n\t\tSanskrit Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a conversion of the Pracalit for Sanskrit and Newar MSS 16th to 19th C., Ground Truth dataset, originally published on Zenodo. The dataset contains pairs of images and corresponding plain text extracted from XML files. This dataset specifically includes the ground truth data from the original repository.\n\n\t\n\t\t\n\t\n\t\n\t\tPotential Use for VLM-based OCR Training\n\t\n\nThe existing ground truth OCR data can be particularly useful for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/Sanskrit.","first_N":5,"first_N_keywords":["image-to-text","Sanskrit","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-10k-256x256","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","description":"\n\t\n\t\t\n\t\tüìÑ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Blank-v2.0-Sample","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0-Sample","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpenpdf-Blank-v2.0-Sample\n\t\n\nOpenpdf-Blank-v2.0-Sample is a sample dataset of blank or near-blank invoice and receipt documents. It contains 255 high-resolution scanned images extracted and cleaned from document PDFs. This dataset is intended to support training and evaluation of OCR, document classification, and layout-based filtering models where blank or structurally minimal pages must be identified and processed.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nFormat: Parquet (auto-converted)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0-Sample.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Openpdf-MultiReceipt-1K","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-MultiReceipt-1K","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpenpdf-MultiReceipt-1K\n\t\n\nOpenpdf-MultiReceipt-1K is a dataset consisting of over 1,000 receipt documents in PDF format. This dataset is designed for use in image-to-text and document understanding tasks, particularly Optical Character Recognition (OCR), receipt parsing, and layout analysis.\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\nNo text annotations or metadata are provided ‚Äî only the raw PDFs.\nIdeal for tasks requiring raw document inputs like PDF-to-Text pipelines.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSize: 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-MultiReceipt-1K.","first_N":5,"first_N_keywords":["image-to-text","English","German","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"UI-Genie-Agent-5k","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HanXiao1999/UI-Genie-Agent-5k","creator_name":"HanXiao","creator_url":"https://huggingface.co/HanXiao1999","description":"This repository contains the dataset from the paper UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\nMobile GUI Agents.\nGithub: https://github.com/Euphoria16/UI-Genie\n","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTest","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTest","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","description":"This repository contains the dataset used in GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nGitHub repository: https://github.com/yueliu1999/GuardReasoner-VL\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ChartGen-200K","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SD122025/ChartGen-200K","creator_name":"Chart","creator_url":"https://huggingface.co/SD122025","description":"SD122025/ChartGen-200K dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"prophet-mosque-library","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tProphet's Mosque Library\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nProphet‚Äôs Mosque Library is one of the primary resources for Islamic books. It hosts more than 48,000 PDF books across over 70 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThe dataset includes 70,884 PDF files (spanning 23,494,042 pages) representing 48,717 Islamic books. Each book is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"Mathematics-Class10-Tnsb","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Mathematics-Class10-Tnsb","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tMathematics-Class10-Tnsb\n\t\n\nThis dataset contains scanned images from a Class 10 Mathematics textbook under the TNSB (Tamil Nadu State Board) curriculum. It is intended for educational machine learning tasks such as image-to-text (OCR), textbook digitization, or educational content understanding.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Tamil Nadu State Board Class 10 Mathematics textbook\nTask: Image-to-Text\nLanguage: English\nSplit: train only\nRows: 352\nFormat: Images only (scanned textbook‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Mathematics-Class10-Tnsb.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Mathematics-Class12-Vol.1.n.2","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Mathematics-Class12-Vol.1.n.2","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tMathematics-Class12-Vol.1.n.2\n\t\n\nThis dataset contains high-quality scanned pages from the Tamil Nadu State Board (TNSB) Class 12 Mathematics Volume 1 & 2 textbook. It is intended for use in machine learning applications involving OCR (optical character recognition), educational AI tools, and digitization of state board curriculum materials.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nSource: Tamil Nadu State Board Class 12 Mathematics ‚Äì Volume 1 & 2\n\nModality: Image\n\nFormat: imagefolder\n\nSplit:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Mathematics-Class12-Vol.1.n.2.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"TableEval","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/katebor/TableEval","creator_name":"Ekaterina Borisova","creator_url":"https://huggingface.co/katebor","description":"\n\t\n\t\t\n\t\tTableEval dataset\n\t\n\n\n\nTableEval is developed to benchmark and compare the performance of (M)LLMs on tables from scientific vs. non-scientific sources, represented as images vs. text. \nIt comprises six data subsets derived from the test sets of existing benchmarks for question answering (QA) and table-to-text (T2T) tasks, containing a total of 3017 tables and 11312 instances. \nThe scienfific subset includes tables from pre-prints and peer-reviewed scholarly publications, while the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/katebor/TableEval.","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Single-Image","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Multi-Images","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-Thinking-156k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","description":"This is the SophiaVL-R1-Thinking-156k dataset for training Thinking Reward Model of SophiaVL-R1 (SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward).\nThe data is constructed in sharegpt format. text_only_part.json is text-only data. multimodal_part.json is image-text data. Images can be found in images.\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"DataSeeds.AI-Sample-Dataset-DSD","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD","creator_name":"Dataseeds AI","creator_url":"https://huggingface.co/Dataseeds","description":"\n\t\n\t\t\n\t\tDataSeeds.AI Sample Dataset (DSD)\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DataSeeds.AI Sample Dataset (DSD) is a high-fidelity, human-curated computer vision-ready dataset comprised of 7,772 peer-ranked, fully annotated photographic images, 350,000+ words of descriptive text, and comprehensive metadata. While the DSD is being released under an open source license, a sister dataset of over 10,000 fully annotated and segmented images is available for immediate commercial licensing, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD.","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"V1-33K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nüöÄ  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction üåü\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-RL-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe RL data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data includes 15,402 training queries with verifierable answers. The key fields include:\n\nquestion, answer, qid\nis_video: a flag to distinguish video and image queries\nimage: a list of image paths.\nFor video-based queries, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MM-Math-Align","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THU-KEG/MM-Math-Align","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","description":"\nHard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models\n\n\n\n| üêô Github Code |\nüìÉ Paper |\n\n\n\n\t\n\t\t\n\t\tDataset description:\n\t\n\nWe release MM-Math-Align, a dataset built upon MM-Math, which is derived from actual geometry questions used in middle school exams. Each sample contains the original geometric diagram(original_image), a Python script's image(positive_image) that approximately reconstructs the diagram, a caption(positive_caption) describing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/MM-Math-Align.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Blank-v2.0","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpenpdf-Blank-v2.0\n\t\n\nOpenpdf-Blank-v2.0 is a small dataset containing blank or near-blank PDF image samples. This dataset is primarily designed to help train and evaluate document processing models, especially in tasks like:\n\nIdentifying and filtering blank or noise-filled documents.\nPreprocessing stages for OCR pipelines.\nReceipt/document classification tasks.\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nModality: Image\nLanguages: English (if applicable)\nSize: Less than 1,000 samples\nLicense:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-50k-256x256","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","description":"\n\t\n\t\t\n\t\tüìÑ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"IndustryEQA","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/IndustryEQA/IndustryEQA","creator_name":"IndustryEQA","creator_url":"https://huggingface.co/IndustryEQA","description":"\n\t\n\t\t\n\t\tIndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios\n\t\n\nAuthors: Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong\nPaper üìù | Code üíª\nThis benchmark dataset accopmanies our paper of the same title. Built upon the NVIDIA Isaac Sim platform,\nIndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets,\ndynamic human agents, and carefully designed hazardous situations inspired by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IndustryEQA/IndustryEQA.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Cosmos-R1-RL-dataset","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mohammadbhat/Cosmos-R1-RL-dataset","creator_name":"Mohammad Qazim Bhat","creator_url":"https://huggingface.co/mohammadbhat","description":"mohammadbhat/Cosmos-R1-RL-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LaTeX_OCR","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linxy/LaTeX_OCR","creator_name":"Lin Xueyuan","creator_url":"https://huggingface.co/linxy","description":"\n\t\n\t\t\n\t\tLaTeX OCR ÁöÑÊï∞ÊçÆ‰ªìÂ∫ì\n\t\n\nÊú¨Êï∞ÊçÆ‰ªìÂ∫ìÊòØ‰∏ì‰∏∫ LaTeX_OCR Âèä LaTeX_OCR_PRO Âà∂‰ΩúÁöÑÊï∞ÊçÆÔºåÊù•Ê∫ê‰∫é https://zenodo.org/record/56198#.V2p0KTXT6eA ‰ª•Âèä https://www.isical.ac.in/~crohme/ ‰ª•ÂèäÊàë‰ª¨Ëá™Â∑±ÊûÑÂª∫„ÄÇ\nÂ¶ÇÊûúËøô‰∏™Êï∞ÊçÆ‰ªìÂ∫ìÊúâÂ∏ÆÂä©Âà∞‰Ω†ÁöÑËØùÔºåËØ∑ÁÇπ‰∫Æ ‚ù§Ô∏èlike ++\nÂêéÁª≠ËøΩÂä†Êñ∞ÁöÑÊï∞ÊçÆ‰πü‰ºöÊîæÂú®Ëøô‰∏™‰ªìÂ∫ì ~~\n\nÂéüÂßãÊï∞ÊçÆ‰ªìÂ∫ìÂú®github LinXueyuanStdio/Data-for-LaTeX_OCR.\n\n\n\t\n\t\t\n\t\n\t\n\t\tÊï∞ÊçÆÈõÜ\n\t\n\nÊú¨‰ªìÂ∫ìÊúâ 5 ‰∏™Êï∞ÊçÆÈõÜ\n\nsmall ÊòØÂ∞èÊï∞ÊçÆÈõÜÔºåÊ†∑Êú¨Êï∞ 110 Êù°ÔºåÁî®‰∫éÊµãËØï\nfull ÊòØÂç∞Âà∑‰ΩìÁ∫¶ 100k ÁöÑÂÆåÊï¥Êï∞ÊçÆÈõÜ„ÄÇÂÆûÈôÖ‰∏äÊ†∑Êú¨Êï∞Áï•Â∞è‰∫é 100kÔºåÂõ†‰∏∫Áî® LaTeX ÁöÑÊäΩË±°ËØ≠Ê≥ïÊ†ëÂâîÈô§‰∫ÜÂæàÂ§ö‰∏çËÉΩÊ∏≤ÊüìÁöÑ LaTeX„ÄÇ\nsynthetic_handwrite ÊòØÊâãÂÜô‰Ωì 100k ÁöÑÂÆåÊï¥Êï∞ÊçÆÈõÜÔºåÂü∫‰∫é full ÁöÑÂÖ¨ÂºèÔºå‰ΩøÁî®ÊâãÂÜôÂ≠ó‰ΩìÂêàÊàêËÄåÊù•ÔºåÂèØ‰ª•ËßÜ‰∏∫‰∫∫Á±ªÂú®Á∫∏‰∏äÁöÑÊâãÂÜô‰Ωì„ÄÇÊ†∑Êú¨Êï∞ÂÆûÈôÖ‰∏äÁï•Â∞è‰∫é 100kÔºåÁêÜÁî±Âêå‰∏ä„ÄÇ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linxy/LaTeX_OCR.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"imagenet-1k-vl-enriched","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched","creator_name":"Visual Layer","creator_url":"https://huggingface.co/visual-layer","description":"\n  \n    Visualize on Visual Layer\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tImagenet-1K-VL-Enriched\n\t\n\nAn enriched version of the ImageNet-1K Dataset with image caption, bounding boxes, and label issues!\nWith this additional information, the ImageNet-1K dataset can be extended to various tasks such as image retrieval or visual question answering.\nThe label issues helps to curate a cleaner and leaner dataset.\n\n\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 6 columns:\n\nimage_id: The original filename of the image from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched.","first_N":5,"first_N_keywords":["object-detection","image-classification","text-to-image","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Cosmos-Reason1-SFT-Dataset","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nThe data format is a pair of video and text annotations. We summarize the data and annotations in Table 4 (SFT), Table 5 (RL), and Table 6 (Benchmark) of the Cosmos-Reason1 paper. ‚Äã‚Äã We release the annotations for embodied reasoning tasks for BridgeDatav2, RoboVQA, Agibot, HoloAssist, AV, and the videos for the RoboVQA and AV datasets. We additionally release the annotations and videos for the RoboFail dataset for benchmarks. By releasing the dataset, NVIDIA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Corvus-OCR-Caption-Mix","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mix","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tCorvus-OCR-Caption-Mix\n\t\n\nCorvus-OCR-Caption-Mix is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. This collection is derived and optimized from the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, with a focus on long-form captions and mixed OCR tasks across a variety of image types.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe dataset spans over 229,000 image-caption pairs and provides a balanced blend of:\n\nOCR-rich documents featuring‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mix.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"FaceCaption-15M","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFacaCaption-15M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\n\n\nFaceCaption-15M, a large-scale, diverse, and high-quality dataset of facial images accompanied by their natural language descriptions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"SARD","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/riotu-lab/SARD","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD.","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"DeepEyes-Datasets-47k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k","creator_name":"ChenShawn","creator_url":"https://huggingface.co/ChenShawn","description":"This repository contains the datasets used in the paper DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning.\nCode: https://github.com/Visual-Agent/DeepEyes\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.14362","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Corvus-OCR-Caption-Mini-Mix","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mini-Mix","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tCorvus-OCR-Caption-Mini-Mix\n\t\n\nCorvus-OCR-Caption-Mini-Mix is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. It is a carefully curated subset of the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, optimized for mixed OCR and long-form captioning tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a balanced mix of:\n\nLong-form natural language captions\nOCR-heavy samples with scientific, mathematical, and document-style‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mini-Mix.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GRIT","keyword":"image-to-text","license":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":"en","dataset_url":"https://huggingface.co/datasets/zzliang/GRIT","creator_name":"zhiliang","creator_url":"https://huggingface.co/zzliang","description":"\n\t\n\t\t\n\t\tGRIT: Large-Scale Training Corpus of Grounded Image-Text Pairs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe introduce GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created based on image-text pairs from COYO-700M and LAION-2B. We construct a pipeline to extract and link text spans (i.e., noun phrases, and referring expressions) in the caption to their corresponding image regions. More details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nDuring the construction, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zzliang/GRIT.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","object-detection","zero-shot-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"Visual-CoT","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deepcs233/Visual-CoT","creator_name":"Hao Shao","creator_url":"https://huggingface.co/deepcs233","description":"\n\t\n\t\t\n\t\tVisCoT Dataset Card\n\t\n\n\n\nThere is a shortage of multimodal datasets for training multi-modal large language models (MLLMs) that require to identify specific regions in an image for additional attention to improve response performance. This type of dataset with grounding bbox annotations could possibly help the MLLM output intermediate interpretable attention area and enhance performance.\nTo fill the gap, we curate a visual CoT dataset. This dataset specifically focuses on identifying‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepcs233/Visual-CoT.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2403.16999","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"ScreenSpot","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rootsautomation/ScreenSpot","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","description":"\n\t\n\t\t\n\t\tDataset Card for ScreenSpot\n\t\n\nGUI Grounding Benchmark: ScreenSpot. \nCreated researchers at Nanjing University and Shanghai AI Laboratory for evaluating large multimodal models (LMMs) on GUI grounding tasks on screens given a text-based instruction.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nScreenSpot is an evaluation benchmark for GUI grounding, comprising over 1200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element types‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/ScreenSpot.","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"german_handwriting","keyword":"image-to-text","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fhswf/german_handwriting","creator_name":"Fachhochschule S√ºdwestfalen","creator_url":"https://huggingface.co/fhswf","description":"\n\t\n\t\t\n\t\tGerman handwriting\n\t\n\nThis dataset contains German handwriting images and corresponding text labels. In total, the dataset contains around 10,000 entries with handwriting from 15 different people.\nThe data was created with the help of transcripts from school and university.\nThe dataset was created as part of a handwriting recognition project at the FH-SWF.\n\n\t\n\t\t\n\t\tHow to use:\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('fhswf/german_handwriting')\n\n","first_N":5,"first_N_keywords":["image-to-text","German","afl-3.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"\n\t\n\t\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"blip3-kale","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-kale","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n  ü•¨ BLIP3-KALE:Knowledge Augmented Large-scale Dense Captions\n\nBLIP3-KALE is an open-source dataset of 218 million image-text pairs, featuring knowledge-augmented dense captions combining web-scale knowledge with detailed image descriptions.\n\nPaper: [To be added]\n\n\t\n\t\t\n\t\tUses\n\t\n\nBLIP3-KALE is designed to facilitate research in multimodal pretraining. The dataset can be used for training large multimodal models that require factually grounded, dense image captions. It has already been an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-kale.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"OmniCorpus-CC","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","description":"\n  üê≥ OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text\n\n\n\n‚≠êÔ∏è NOTE: Several parquet files were marked unsafe (viruses) by official scaning of hf, while they are reported safe by ClamAV and Virustotal. \nWe found many false positive cases of the hf automatic scanning in hf discussions and raise one discussion to ask for a re-scanning.\n\nThis is the repository of OmniCorpus-CC, which contains 988 million image-text interleaved documents collected from Common‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"MobileViews","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mllmTeam/MobileViews","creator_name":"mllm","creator_url":"https://huggingface.co/mllmTeam","description":"\n\t\n\t\t\n\t\n\t\n\t\tüöÄ MobileViews: A Large-Scale Mobile GUI Dataset\n\t\n\nMobileViews is a large-scale dataset designed to support research on mobile agents and mobile user interface (UI) analysis. The first release, MobileViews-600K, includes over 600,000 mobile UI screenshot-view hierarchy (VH) pairs collected from over 20,000 apps on the Google Play Store. This dataset is based on the DroidBot, which we have optimized for large-scale data collection, capturing more comprehensive interaction details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mllmTeam/MobileViews.","first_N":5,"first_N_keywords":["question-answering","image-to-text","task-planning","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"pokemoncards","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tooni/pokemoncards","creator_name":"toni","creator_url":"https://huggingface.co/tooni","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Pokemon Cards TCG\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about Pokemon Trading Card Game (TCG) cards, decks, and sets. It is designed to be used for training machine learning models to classify and analyze Pokemon cards.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nTasks: \nImage Classification\nText Classification\nInformation Retrieval\nImage-to-Text\nImage Feature Extraction\nImage Segmentation\nImage Classification‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tooni/pokemoncards.","first_N":5,"first_N_keywords":["image-classification","image-segmentation","sentence-similarity","image-to-text","image-feature-extraction"],"keywords_longer_than_N":true},
	{"name":"tekno21-brain-stroke-dataset-multi","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-multi","creator_name":"BORAN TOKTAY","creator_url":"https://huggingface.co/BTX24","description":"\n\t\n\t\t\n\t\tDataset Card for BTX24/tekno21-brain-stroke-dataset-multi\n\t\n\n\n\t\n\t\t\n\t\tüîó Dataset Sources\n\t\n\n\nDataset Source: TEKNOFEST-2021 Stroke Dataset\nKaggle: ƒ∞nme Veri Seti (Stroke Dataset)\nSaƒülƒ±k Bakanlƒ±ƒüƒ± A√ßƒ±k Veri Portalƒ±: ƒ∞nme Veri Seti\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: PNG\nTotal Images: 7,369\nCategories:\nhemorajik/ (Hemorrhagic stroke images)\niskemik/ (Ischemic stroke images)\nnormal/ (Non-stroke images)\n\n\nThe dataset is structured in a folder-based format where images are grouped into‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-multi.","first_N":5,"first_N_keywords":["image-classification","feature-extraction","image-to-text","image-feature-extraction","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GAIA","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/azavras/GAIA","creator_name":"Angelos Zavras","creator_url":"https://huggingface.co/azavras","description":"\n\t\n\t\t\n\t\tüåç GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis\n\t\n\n\nThis repository contains the pre-trained model weights, associated code, and complete dataset of the paper GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis.\nGAIA is a large-scale vision-language dataset designed to bridge the gap between remote sensing (RS) imagery and natural language understanding. It provides 205,150 image-text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/azavras/GAIA.","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"showdown-clicks","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/generalagents/showdown-clicks","creator_name":"General Agents","creator_url":"https://huggingface.co/generalagents","description":"\n\t\n\t\t\n\t\tshowdown-clicks\n\t\n\nGeneral Agents\nü§ó Dataset | GitHub\nshowdown is a suite of offline and online benchmarks for computer-use agents.\nshowdown-clicks is a collection of 5,679 left clicks of humans performing various tasks in a macOS desktop environment. It is intended to evaluate instruction-following and low-level control capabilities of computer-use agents.\nAs of March 2025, we are releasing a subset of the full set, showdown-clicks-dev, containing 557 clicks. All examples are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/generalagents/showdown-clicks.","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"OmniSpatial","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qizekun/OmniSpatial","creator_name":"Zekun Qi","creator_url":"https://huggingface.co/qizekun","description":"\n\t\n\t\t\n\t\tOmniSpatial\n\t\n\nThis repository contains the data presented in OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models.\n\n\t\n\t\t\n\t\tTask Schema Documentation\n\t\n\nThis document provides a structured explanation of the task schema for the visual-spatial reasoning benchmark.\n\n\n\t\n\t\t\n\t\tSchema Structure\n\t\n\nThe schema is represented in JSON format, containing the following key components:\n\n\t\n\t\t\nKey\nDescription\n\n\n\t\t\nid\nIdentifier for the question, formatted as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qizekun/OmniSpatial.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VisuLogic/VisuLogic-Train","creator_name":"VisuLogic-Benchmark","creator_url":"https://huggingface.co/VisuLogic","description":"\n\t\n\t\t\n\t\tVisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models\n\t\n\nA Challenging Visual-centric Benchmark for Evaluating Multimodal Reasoning in MLLMs!\nThis is the Benchmark data repo of VisuLogic.\nFor more details, please refer to the project page with dataset exploration and visualization tools: https://visulogic-benchmark.github.io/VisuLogic/.\n\n\t\n\t\t\n\t\n\t\n\t\tVisuLogic Resouces\n\t\n\nüåê Homepage | üèÜ Leaderboard | üìñ Paper | ü§ó Benchmark | ü§ó Train Data \nüíª Eval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VisuLogic/VisuLogic-Train.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"ViRL39K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ViRL39K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\t1. Overview of ViRL39K\n\t\n\nViRL39K (pronounced as \"viral\") provides a curated collection of 38,870 verifiable QAs for Vision-Language RL training. \nIt is built on top of newly collected problems and existing datasets (\nLlava-OneVision, \nR1-OneVision,\nMM-Eureka,\nMM-Math,\nM3CoT,\nDeepScaleR,\nMV-Math)\nthrough cleaning, reformatting, rephrasing and verification.ViRL39K lays the foundation for SoTA Vision-Language Reasoning Model VL-Rethinker. It has the following merits:\n\nhigh-quality and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ViRL39K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"waqfeya-library","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/waqfeya-library","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tWaqfeya Library\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nWaqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 10,000 PDF books across over 80 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThe dataset includes 22,443 PDF files (spanning 8,978,634 pages) representing 10,150 Islamic books. Each book is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/waqfeya-library.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ui-vision","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ServiceNow/ui-vision","creator_name":"ServiceNow","creator_url":"https://huggingface.co/ServiceNow","description":"\n\t\n\t\t\n\t\tUI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction\n\t\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nAutonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/ui-vision.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"SIV-Bench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fancylalala/SIV-Bench","creator_name":"KongFanqi","creator_url":"https://huggingface.co/Fancylalala","description":"This repository contains the dataset for the paper SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning.\nProject page: https://kfq20.github.io/sivbench/\nCode: https://github.com/kfq20/SIV-Bench\n","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","Video"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Code2Logic","creator_url":"https://huggingface.co/Code2Logic","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual game‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ImplicitQA","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ucf-crcv/ImplicitQA","creator_name":"Center for Research in Computer Vision, University of Central Florida","creator_url":"https://huggingface.co/ucf-crcv","description":"\n\t\n\t\t\n\t\tImplicitQA Dataset\n\t\n\nThe ImplicitQA dataset was introduced in the paper ImplicitQA: Going beyond frames towards Implicit Video Reasoning.\nProject page: https://implicitqa.github.io/\nImplicitQA is a novel benchmark specifically designed to test models on implicit reasoning in Video Question Answering (VideoQA). Unlike existing VideoQA benchmarks that primarily focus on questions answerable through explicit visual content (actions, objects, events directly observable within individual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ucf-crcv/ImplicitQA.","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"GameQA-5K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Code2Logic/GameQA-5K","creator_name":"Code2Logic","creator_url":"https://huggingface.co/Code2Logic","description":"In this repository, we specifically provide the 5k training samples from the complete GameQA-140K dataset used in our work for GRPO training of the models.\nRefer to our paper for details. And our code for training and evaluation is at https://github.com/tongjingqi/Code2Logic.\n\n\t\n\t\t\n\t\n\t\n\t\tCode2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning\n\t\n\nThis is the first work, to the best of our knowledge, that leverages game code to synthesize multimodal reasoning data for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-5K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"ShotBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vchitect/ShotBench","creator_name":"Vchitect","creator_url":"https://huggingface.co/Vchitect","description":"\n\t\n\t\t\n\t\tShotBench: Expert-Level Cinematic Understanding in Vision-Language Models\n\t\n\nThis is the official test set of ShotBench, comprising 3,572 question-answer pairs. Each sample is paired with either an image or a video clip. In total, ShotBench includes 3,049 images and 464 videos, primarily sourced from films that received Oscar nominations for Best Cinematography, ensuring high visual quality and strong cinematic style.\n\nPaper: ShotBench: Expert-Level Cinematic Understanding in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vchitect/ShotBench.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"btc-candlestick-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tuankg1028/btc-candlestick-dataset","creator_name":"Tuan","creator_url":"https://huggingface.co/tuankg1028","description":"\n\t\n\t\t\n\t\tCandlestick Chart Dataset - BTCUSDT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains candlestick chart images paired with textual descriptions and trading labels for the BTCUSDT trading pair.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSymbol: BTCUSDT\nInterval: 1h\nWindow Size: 30 candles per chart\nTotal Records: 13081\nImage Format: PNG (candlestick charts with volume)\nText Format: Structured candle data description\nLabels: Trading signal classification\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tuankg1028/btc-candlestick-dataset.","first_N":5,"first_N_keywords":["image-to-text","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"govdocs1-pdf-source","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source","creator_name":"BEEspoke Data","creator_url":"https://huggingface.co/BEE-spoke-data","description":"\n\t\n\t\t\n\t\tgovdocs1: source PDF files\n\t\n\nThis is ~220,000 open-access PDF documents (about 6.6M pages) from the dataset govdocs1. It wants to be OCR'd.\n\nUploaded as tar file pieces of ~10 GiB each due to size/file count limits with an index.csv covering details\n5,000 randomly sampled PDFs are available unarchived in sample/. Hugging Face supports previewing these in-browser, for example this one\n\n\n\t\n\t\n\t\n\t\tRecovering the data\n\t\n\nDownload the data/ directory (with huggingface-cli download or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source.","first_N":5,"first_N_keywords":["image-text-to-text","image-feature-extraction","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Visco-Attack","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/miaozq/Visco-Attack","creator_name":"miaozq","creator_url":"https://huggingface.co/miaozq","description":"\n\t\n\t\t\n\t\tVisCo Attack: Visual Contextual Jailbreak Dataset\n\t\n\nüìÑ arXiv:2507.02844 ¬∑ üíª Code ‚Äì Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\nThis dataset contains the adversarial contexts, prompts, and images from the paper: \"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\".\n\n\t\n\t\t\n\t\n\t\n\t\t‚ö†Ô∏è Content Warning\n\t\n\nThis dataset contains content that is offensive and/or harmful. It was created for research purposes to study the safety‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/miaozq/Visco-Attack.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v2","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v2","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v2\n\t\n\nCaption3o-Opt-v2 is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. Derived from the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, this optimized subset emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: 10,277 image-caption pairs\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v2.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"RotoWire_English-German","keyword":"table-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/RotoWire_English-German","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"Dataset for the WNGT 2019 DGT shared task on \"Document-Level Generation and Translation‚Äù.","first_N":5,"first_N_keywords":["table-to-text","automatically-created","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"RotoWire_English-German","keyword":"data-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/RotoWire_English-German","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"Dataset for the WNGT 2019 DGT shared task on \"Document-Level Generation and Translation‚Äù.","first_N":5,"first_N_keywords":["table-to-text","automatically-created","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"dart","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/dart","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"DART is a large and open-domain structured DAta Record to Text generation corpus\nwith high-quality sentence annotations with each input being a set of\nentity-relation triples following a tree-structured ontology. It consists of\n82191 examples across different domains with each input being a semantic RDF\ntriple set derived from data records in tables and the tree ontology of table\nschema, annotated with sentence description that covers all facts in the triple set.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"dart","keyword":"data-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/dart","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"DART is a large and open-domain structured DAta Record to Text generation corpus\nwith high-quality sentence annotations with each input being a set of\nentity-relation triples following a tree-structured ontology. It consists of\n82191 examples across different domains with each input being a semantic RDF\ntriple set derived from data records in tables and the tree ontology of table\nschema, annotated with sentence description that covers all facts in the triple set.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"e2e_nlg","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/e2e_nlg","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"The E2E dataset is designed for a limited-domain data-to-text task --\ngeneration of restaurant descriptions/recommendations based on up to 8 different\nattributes (name, area, price range etc.).","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"e2e_nlg","keyword":"data-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/e2e_nlg","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"The E2E dataset is designed for a limited-domain data-to-text task --\ngeneration of restaurant descriptions/recommendations based on up to 8 different\nattributes (name, area, price range etc.).","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"sportsett_basketball","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/sportsett_basketball","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"SportSett:Basketball dataset for Data-to-Text Generation contains NBA games stats aligned with their human written summaries.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"sportsett_basketball","keyword":"data-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/sportsett_basketball","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"SportSett:Basketball dataset for Data-to-Text Generation contains NBA games stats aligned with their human written summaries.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"viggo","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/viggo","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"ViGGO was designed for the task of data-to-text generation in chatbots (as opposed to task-oriented dialogue systems), with target responses being more conversational than information-seeking, yet constrained to the information presented in a meaning representation. The dataset, being relatively small and clean, can also serve for demonstrating transfer learning capabilities of neural models.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"viggo","keyword":"data-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/viggo","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"ViGGO was designed for the task of data-to-text generation in chatbots (as opposed to task-oriented dialogue systems), with target responses being more conversational than information-seeking, yet constrained to the information presented in a meaning representation. The dataset, being relatively small and clean, can also serve for demonstrating transfer learning capabilities of neural models.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"wit_base","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/wit_base","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\n\t\n\t\t\n\t\tDataset Card for WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.\nFrom the official blog post:\n\nThe core training data is taken from the Wikipedia Image-Text (WIT) Dataset, a large curated set of more than 37 million image-text associations extracted from Wikipedia articles in 108 languages that was recently released by Google Research.\nThe WIT dataset offers extremely valuable data about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wit_base.","first_N":5,"first_N_keywords":["image-to-text","text-retrieval","image-captioning","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"brill_iconclass","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/biglam/brill_iconclass","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","description":"\n\t\n\t\t\n\t\tDataset Card for Brill Iconclass AI Test Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nA test dataset and challenge to apply machine learning to collections described with the Iconclass classification system.\n\nThis dataset contains 87749 images with Iconclass metadata assigned to the images. The iconclass metadata classification system is intended to provide 'the comprehensive classification system for the content of images.'.\n\nIconclass was developed in the Netherlands as a standard‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biglam/brill_iconclass.","first_N":5,"first_N_keywords":["image-classification","image-to-text","feature-extraction","multi-class-image-classification","multi-label-image-classification"],"keywords_longer_than_N":true},
	{"name":"ShahNegar","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sadrasabouri/ShahNegar","creator_name":"Sadra Sabouri","creator_url":"https://huggingface.co/sadrasabouri","description":"\n\t\n\t\t\n\t\tShahNegar (A Plotted version of The Shahnameh)\n\t\n\nThis dataset is a plotted version of Ferdowsi's Shahnameh (which is a highly-regarded ancient set of Farsi poems) generated using DALL-E mini (aka craiyon). You can use this dataset using the code below: \nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sadrasabouri/ShahNegar\")\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains more than 30K images with their corresponding text from the Shahnameh. For each Shahnameh‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sadrasabouri/ShahNegar.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-captioning","machine-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"coyo-700m","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kakaobrain/coyo-700m","creator_name":"Kakao Brain","creator_url":"https://huggingface.co/kakaobrain","description":"\n\t\n\t\t\n\t\tDataset Card for COYO-700M\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCOYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models. Our dataset follows a similar strategy to previous vision-and-language datasets, collecting many informative pairs of alt-text and its associated image in HTML documents. We expect COYO to be used to train popular large-scale foundation models \ncomplementary to other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kakaobrain/coyo-700m.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"newyorker_caption_contest","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmhessel/newyorker_caption_contest","creator_name":"Jack Hessel","creator_url":"https://huggingface.co/jmhessel","description":"\n\t\n\t\t\n\t\tDataset Card for New Yorker Caption Contest Benchmarks\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSee capcon.dev for more!\nData from:\nDo Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest\n@inproceedings{hessel2023androids,\n  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''\n         Benchmarks from {The New Yorker Caption Contest}},\n  author={Hessel, Jack and Marasovi{\\'c}, Ana and Hwang, Jena D. and Lee, Lillian\n          and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jmhessel/newyorker_caption_contest.","first_N":5,"first_N_keywords":["image-to-text","multiple-choice","text-classification","text-generation","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"TaTA","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/TaTA","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"Dataset loader for TaTA: A Multilingual Table-to-Text Dataset for African Languages","first_N":5,"first_N_keywords":["table-to-text","none","unknown","yes","original"],"keywords_longer_than_N":true},
	{"name":"TaTA","keyword":"data-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/TaTA","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"Dataset loader for TaTA: A Multilingual Table-to-Text Dataset for African Languages","first_N":5,"first_N_keywords":["table-to-text","none","unknown","yes","original"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-turkish-subset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset","creator_name":"Cemil Guney","creator_url":"https://huggingface.co/mcemilg","description":"\n\t\n\t\t\n\t\tDataset Card for laion2B-multi-turkish-subset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLAION-5B is a large scale openly accessible image-text dataset contains text from multiple languages. This is a Turkish subset data of laion/laion2B-multi. It's compatible to be used with image2dataset to fetch the images at scale.\n\n\t\n\t\t\n\t\n\t\n\t\tData Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['SAMPLE_ID', 'URL', 'TEXT', 'HEIGHT', 'WIDTH', 'LICENSE', 'LANGUAGE', 'NSFW', 'similarity']‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"da-wit","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/da-wit","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/da-wit.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"danish-wit","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/danish-wit","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/severo/danish-wit.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"logicnlg","keyword":"data-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kasnerz/logicnlg","creator_name":"Zdenƒõk Kasner","creator_url":"https://huggingface.co/kasnerz","description":"\n\t\n\t\t\n\t\tLogicNLG Dataset\n\t\n\nSee the official wenhuchen/LogicNLG release on GitHub.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"trdg_random_en_zh_text_recognition","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/priyank-m/trdg_random_en_zh_text_recognition","creator_name":"priyank","creator_url":"https://huggingface.co/priyank-m","description":"\n\t\n\t\t\n\t\tDataset Card for \"trdg_random_en_zh_text_recognition\"\n\t\n\nThis synthetic dataset was generated using the TextRecognitionDataGenerator(TRDG) open source repo: \nhttps://github.com/Belval/TextRecognitionDataGenerator\nIt contains images of text with random characters from Engilsh(en) and Chinese(zh) languages.\nReference to the documentation provided by the TRDG repo: \nhttps://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html\n","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"trdg_wikipedia_en_text_recognition","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/priyank-m/trdg_wikipedia_en_text_recognition","creator_name":"priyank","creator_url":"https://huggingface.co/priyank-m","description":"\n\t\n\t\t\n\t\tDataset Card for \"trdg_wikipedia_en_zh_text_recognition\"\n\t\n\nThis synthetic dataset was generated using the TextRecognitionDataGenerator(TRDG) open source repo:\nhttps://github.com/Belval/TextRecognitionDataGenerator\nIt contains synthetic images of text randomly sampled from Engilsh(en) Wikipedia pages.\nReference to the documentation provided by the TRDG repo:\nhttps://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html\n","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ui_refexp_saved","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ivelin/ui_refexp_saved","creator_name":"Ivelin Ivanov","creator_url":"https://huggingface.co/ivelin","description":"\n\t\n\t\t\n\t\tDataset Card for \"ui_refexp_saved_Jan2023\"\n\t\n\nThis is a saved snapshot of the dynamically generated UI Bert dataset. \nMuch faster download time than the dynamic version which pulls and filters large data files from remote sources.\n","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"hl","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michelecafagna26/hl","creator_name":"Michele Cafagna","creator_url":"https://huggingface.co/michelecafagna26","description":"\n\t\n\t\t\n\t\tDataset Card for the High-Level Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe High-Level (HL) dataset aligns object-centric descriptions from COCO \nwith high-level descriptions crowdsourced along 3 axes: scene, action, rationale\nThe HL dataset contains 14997 images from COCO and a total of 134973 crowdsourced captions (3 captions for each axis) aligned with ~749984 object-centric captions from COCO.\nEach axis is collected by asking the following 3 questions:\n\nWhere is the picture‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michelecafagna26/hl.","first_N":5,"first_N_keywords":["image-to-text","question-answering","zero-shot-classification","text-scoring","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"bioleaflets-biomedical-ner","keyword":"data-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ruslan/bioleaflets-biomedical-ner","creator_name":"Ruslan Yermak","creator_url":"https://huggingface.co/ruslan","description":"\n\t\n\t\t\n\t\tDataset Card for BioLeaflets Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioLeaflets is a biomedical dataset for Data2Text generation. It is a corpus of 1,336 package leaflets of medicines authorised in Europe, which were obtained by scraping the European Medicines Agency (EMA) website. \nPackage leaflets are included in the packaging of medicinal products and contain information to help patients use the product safely and appropriately. \nThis dataset comprises the large majority (‚àº 90%) of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ruslan/bioleaflets-biomedical-ner.","first_N":5,"first_N_keywords":["text-generation","language-modeling","machine-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"gan_hkr","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nastyboget/gan_hkr","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset generated from HKR train set using ScrabbleGAN\n\t\n\nNumber of images: 300000\nSources:\n\nHKR dataset\nScrabbleGAN code\n\n","first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"GameplayCaptions","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/asgaardlab/GameplayCaptions","creator_name":"Analytics of Software, Games and Repository Data (ASGAARD) Lab","creator_url":"https://huggingface.co/asgaardlab","description":"\n\t\n\t\t\n\t\tDataset Card for \"Gameplay Captions\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"FeTaQA","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DongfuJiang/FeTaQA","creator_name":"Dongfu Jiang","creator_url":"https://huggingface.co/DongfuJiang","description":"This repo is the unofficial FeTA-QA dataset from paper FeTaQA: Free-form Table Question Answering.\nThe original purpose to make it easier for users to download and use dataset. All the data is publicly avaliable on their offical Github site\nIf there is anything wrong, please raise an issue in the community and I will fix it if I am available.\n","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"FrenchCensus-handwritten-texts","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts","creator_name":"Arnault Gombert","creator_url":"https://huggingface.co/agomberto","description":"\n\t\n\t\t\n\t\tSource\n\t\n\nThis repository contains 3 datasets created within the POPP project (Project for the Oceration of the Paris Population Census) for the task of handwriting text recognition. These datasets have been published in Recognition and information extraction in historical handwritten tables: toward understanding early 20th century Paris census at DAS 2022.\nThe 3 datasets are called ‚ÄúGeneric dataset‚Äù, ‚ÄúBelleville‚Äù, and ‚ÄúChauss√©e d‚ÄôAntin‚Äù and contains lines made from the extracted rows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts.","first_N":5,"first_N_keywords":["image-to-text","French","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"G-PlanET","keyword":"table-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yuchenlin/G-PlanET","creator_name":"Bill Yuchen Lin","creator_url":"https://huggingface.co/yuchenlin","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis G-PlanET dataset is built on AI2 ALFRED.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuchenlin/G-PlanET.","first_N":5,"first_N_keywords":["text-generation","table-to-text","table-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"diffusiondb-pixelart","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jainr3/diffusiondb-pixelart","creator_name":"Rahul Jain","creator_url":"https://huggingface.co/jainr3","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"MMC4-130k-chinese-image","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","description":"MMC4-130k-chineseÊòØÂØπMMC4‰∏≠ÔºåÊäΩÊ†∑‰∫Ü130kÂ∑¶Âè≥ simliartyËæÉÈ´òÁöÑÂõæÊñápairÂæóÂà∞ÁöÑÊï∞ÊçÆÈõÜ\nChineseÁâàÊú¨ÊòØÂØπËøôÈáåÊâÄÊúâÁöÑcaptionËøõË°å‰∫ÜÁøªËØë„ÄÇ\nÊàë‰ª¨‰ºöÈôÜÁª≠Â∞ÜÊõ¥Â§öÊï∞ÊçÆÈõÜÂèëÂ∏ÉÂà∞hfÔºåÂåÖÊã¨\n\n Coco CaptionÁöÑ‰∏≠ÊñáÁøªËØë\n CoQAÁöÑ‰∏≠ÊñáÁøªËØë\n CNewSumÁöÑEmbeddingÊï∞ÊçÆ\n Â¢ûÂπøÁöÑÂºÄÊîæQAÊï∞ÊçÆ\n WizardLMÁöÑ‰∏≠ÊñáÁøªËØë\n\nÂ¶ÇÊûú‰Ω†‰πüÂú®ÂÅöËøô‰∫õÊï∞ÊçÆÈõÜÁöÑÁ≠πÂ§áÔºåÊ¨¢ËøéÊù•ËÅîÁ≥ªÊàë‰ª¨ÔºåÈÅøÂÖçÈáçÂ§çËä±Èí±„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tÈ™ÜÈ©º(Luotuo): ÂºÄÊ∫ê‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°Âûã\n\t\n\nhttps://github.com/LC1332/Luotuo-Chinese-LLM\nÈ™ÜÈ©º(Luotuo)È°πÁõÆÊòØÁî±ÂÜ∑Â≠êÊòÇ @ ÂïÜÊ±§ÁßëÊäÄ, ÈôàÂêØÊ∫ê @ Âçé‰∏≠Â∏àËåÉÂ§ßÂ≠¶ ‰ª•Âèä ÊùéÈ≤ÅÈ≤Å @ ÂïÜÊ±§ÁßëÊäÄ ÂèëËµ∑ÁöÑ‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂºÄÊ∫êÈ°πÁõÆÔºåÂåÖÂê´‰∫Ü‰∏ÄÁ≥ªÂàóËØ≠Ë®ÄÊ®°Âûã„ÄÇ\n( Ê≥®ÊÑè: ÈôàÂêØÊ∫ê Ê≠£Âú®ÂØªÊâæ2024Êé®ÂÖçÂØºÂ∏àÔºåÊ¨¢ËøéËÅîÁ≥ª )\nÈ™ÜÈ©ºÈ°πÁõÆ‰∏çÊòØÂïÜÊ±§ÁßëÊäÄÁöÑÂÆòÊñπ‰∫ßÂìÅ„ÄÇ\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease cite the repo if you use the data or code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Chinese","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304-clean","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nÈùûÂÆòÊñπÁöÑÔºåÁà¨ÂèñËá™midjourney v5ÁöÑ2023Âπ¥4ÊúàÁöÑÊï∞ÊçÆÔºå‰∏ÄÂÖ±1701420Êù°„ÄÇ\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜ‰ø°ÊÅØ Dataset Information\n\t\n\nÂéüÂßãÈ°πÁõÆÂú∞ÂùÄÔºöhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\nÊàëÂÅö‰∫Ü‰∏Ä‰∫õÊ∏ÖÊ¥óÔºåÊ∏ÖÁêÜÂá∫‰∫Ü‰∏§‰∏™Êñá‰ª∂Ôºö\n\nori_prompts_df.parquet Ôºà1,255,812ÂØπÔºåmidjourneyÁöÑÂõõÊ†ºÂõæÔºâ\n\nupscaled_prompts_df.parquet Ôºà445,608ÂØπÔºå‰ΩøÁî®‰∫ÜÈ´òÊ∏ÖÊåá‰ª§ÁöÑÂõæÔºåËøôÊÑèÂë≥ÁùÄËøô‰∏™ÂõæÊõ¥ÂèóÊ¨¢Ëøé„ÄÇÔºâ\n\n\nOriginal project address:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"midjourney-kaggle-clean","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/midjourney-kaggle-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nÈùûÂÆòÊñπÁöÑÔºåÂØπKaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] ‰∏äÁöÑÊï∞ÊçÆÈõÜËøõË°å‰∫ÜÊ∏ÖÁêÜÔºå‰∏ÄÂÖ±Êúâ 248,167ÂØπ„ÄÇ\nUnofficially, a cleanup of the dataset on Kaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] yielded 248,167 pairs.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-kaggle-clean.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"captcha-images","keyword":"image-to-text","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/project-sloth/captcha-images","creator_name":"Sloth","creator_url":"https://huggingface.co/project-sloth","description":"Captcha images dataset.","first_N":5,"first_N_keywords":["image-to-text","wtfpl","10K - 100K","Image","Text"],"keywords_longer_than_N":true},
	{"name":"anime-synthetics","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mirav/anime-synthetics","creator_name":"Mira","creator_url":"https://huggingface.co/mirav","description":"Mostly unfiltered anime-style images generated by various text to image models, collected from various sources (some were submitted for inclusion by their creators).\nIncludes a subset of p1atdev/niji-v5, albeit captioned differently than the source. \nContains 2224 image & caption pairs.\nAs it is unfiltered, some adult content may be included.\nCaptions may not be completely accurate.\nIf you wish to submit content, do it as a pull request.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"pose-controlnet","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/merlinyx/pose-controlnet","creator_name":"Yuxuan Mei","creator_url":"https://huggingface.co/merlinyx","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe data is based on DeepFashion; turned into image pairs of the same person in same garment with different poses.\nThis won't preserve the person/garment at all but just want to process the data first and see what kind of controlnet it can train as an exercise for training a controlnet.\nThe controlnet_aux's openpose detector sometimes return black images for occluded human images so there won't be a lot of valid image pairs.\n","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"table-to-text","license":"Boost Software License 1.0","license_url":"https://choosealicense.com/licenses/bsl-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pengxiang01/test","creator_name":"wang","creator_url":"https://huggingface.co/pengxiang01","description":"aasdfsdf\n","first_N":5,"first_N_keywords":["tabular-to-text","table-to-text","multiple-choice","text-retrieval","time-series-forecasting"],"keywords_longer_than_N":true},
	{"name":"atlas-pdf-img-cluster","keyword":"image-to-text","license":"Open Software License 3.0","license_url":"https://choosealicense.com/licenses/osl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AtlasUnified/atlas-pdf-img-cluster","creator_name":"Atlas Unified","creator_url":"https://huggingface.co/AtlasUnified","description":"\n\t\n\t\t\n\t\n\t\n\t\tAtlas PDF Image Cluster Dataset\n\t\n\nDerives from the following Python Pipeline code:\nhttps://github.com/atlasunified/PDF-to-Image-Cluster\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis dataset is a collection of text extracted from PDF files, originating from various online resources. The dataset was generated using a series of Python scripts forming a robust pipeline that automated the tasks of downloading, converting, and managing the data.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nSample JPG‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AtlasUnified/atlas-pdf-img-cluster.","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","English","osl-3.0"],"keywords_longer_than_N":true},
	{"name":"hl-narratives","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michelecafagna26/hl-narratives","creator_name":"Michele Cafagna","creator_url":"https://huggingface.co/michelecafagna26","description":"\n\t\n\t\t\n\t\tDataset Card for the High-Level Narratives Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe High-Level Narratives (HL-Narratives) dataset aligns object-centric descriptions from COCO \nwith synthetic high-level narratives captions automatically generated by merging scene, action, rationale captions from the HL Dataset using T5\nThe HL-Naratives dataset contains 14997 images from COCO and a total of 134973 synthetic captions (3 captions per image) aligned with ~749984 object-centric captions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michelecafagna26/hl-narratives.","first_N":5,"first_N_keywords":["image-to-text","question-answering","zero-shot-classification","text-scoring","machine-generated"],"keywords_longer_than_N":true},
	{"name":"artelingo-dummy","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssef101/artelingo-dummy","creator_name":"mohamed","creator_url":"https://huggingface.co/youssef101","description":"ArtELingo is a benchmark and dataset introduced in a research paper aimed at promoting work on diversity across languages and cultures. It is an extension of ArtEmis, which is a collection of 80,000 artworks from WikiArt with 450,000 emotion labels and English-only captions. ArtELingo expands this dataset by adding 790,000 annotations in Arabic and Chinese. The purpose of these additional annotations is to evaluate the performance of \"cultural-transfer\" in AI systems.\nThe dataset in ArtELingo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssef101/artelingo-dummy.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-classification","text-to-image","text-generation"],"keywords_longer_than_N":true},
	{"name":"cxr_llm","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cheese111/cxr_llm","creator_name":"Bryan","creator_url":"https://huggingface.co/cheese111","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\nCXR for medical multimodal LLMs\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset aims to provide medical conversations with contextual images.  Most of the questions ensure the model understands what anomalies are present within the CXR images. \\ \nThere are also follow-up questions to teach the LLM how to follow up after identifying the anomaly.\nThere is a total of 104892 human-bot conversations with contextual images \\\n50,021 images from chexpert 5,229‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cheese111/cxr_llm.","first_N":5,"first_N_keywords":["question-answering","image-to-text","cc-by-sa-4.0","100K<n<1M","Image"],"keywords_longer_than_N":true},
	{"name":"Cars_I_like","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Basilisk181297/Cars_I_like","creator_name":"Basil Minhaj","creator_url":"https://huggingface.co/Basilisk181297","description":"Basilisk181297/Cars_I_like dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-classification","image-to-text","depth-estimation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304","creator_name":"JohnTeddy3","creator_url":"https://huggingface.co/JohnTeddy3","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nËΩ¨ËΩΩËá™wanng/midjourney-v5-202304-clean\nÈùûÂÆòÊñπÁöÑÔºåÁà¨ÂèñËá™midjourney v5ÁöÑ2023Âπ¥4ÊúàÁöÑÊï∞ÊçÆÔºå‰∏ÄÂÖ±1701420Êù°„ÄÇ\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜ‰ø°ÊÅØ Dataset Information\n\t\n\nÂéüÂßãÈ°πÁõÆÂú∞ÂùÄÔºöhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\nÊàëÂÅö‰∫Ü‰∏Ä‰∫õÊ∏ÖÊ¥óÔºåÊ∏ÖÁêÜÂá∫‰∫Ü‰∏§‰∏™Êñá‰ª∂Ôºö\n\nori_prompts_df.parquet Ôºà1,255,812ÂØπÔºåmidjourneyÁöÑÂõõÊ†ºÂõæÔºâ\n\nupscaled_prompts_df.parquet Ôºà445,608ÂØπÔºå‰ΩøÁî®‰∫ÜÈ´òÊ∏ÖÊåá‰ª§ÁöÑÂõæÔºåËøôÊÑèÂë≥ÁùÄËøô‰∏™ÂõæÊõ¥ÂèóÊ¨¢Ëøé„ÄÇÔºâ\n\n\nOriginal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"license-plate-text-recognition-full","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full","creator_name":"Sonne Technology, Inc.","creator_url":"https://huggingface.co/sonnetechnology","description":"\n\t\n\t\t\n\t\tDataset Card for \"license-plate-text-recognition-full\"\n\t\n\n\n\t\n\t\t\n\t\tBackground Information\n\t\n\nThis dataset is generated from keremberke/license-plate-object-detection dataset. What we have done is:\n\nGet the Bounding Boxes for each plate in an image,\nCrop the image to make the plate only visible,\nRun it through the microsoft/trocr-large-printed model to extract the written information.\n\n\n\t\n\t\t\n\t\tStructure of the Dataset\n\t\n\nIt has the same structure as the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DataComp_large_pool_BLIP2_captions","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thaottn/DataComp_large_pool_BLIP2_captions","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaottn","description":"\n\t\n\t\t\n\t\tDataset Card for DataComp_large_pool_BLIP2_captions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nWe have used this dataset for pre-training CLIP models and found that it rivals or outperforms models trained on raw web captions on average across the 38 evaluation tasks proposed by DataComp.\nRefer to the DataComp leaderboard (https://www.datacomp.ai/leaderboard.html) for the top baselines uncovered in our work.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nPrimarily English.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thaottn/DataComp_large_pool_BLIP2_captions.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-classification","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"DataComp_medium_pool_BLIP2_captions","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thaottn/DataComp_medium_pool_BLIP2_captions","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaottn","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for DataComp_medium_pool_BLIP2_captions\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nWe have used this dataset for pre-training CLIP models and found that it rivals or outperforms models trained on raw web captions on average across the 38 evaluation tasks proposed by DataComp.\nRefer to the DataComp leaderboard (https://www.datacomp.ai/leaderboard.html) for the top baselines uncovered in our work.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nPrimarily‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thaottn/DataComp_medium_pool_BLIP2_captions.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-classification","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"medieval","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATMuS/medieval","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","description":"\n\t\n\t\t\n\t\tDataset Card for CATMuS Medieval\n\t\n\n\nJoin our Discord to ask questions about the dataset: \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, \nenabling researchers and scholars to analyse vast collections efficiently. \nDespite significant technological progress, establishing consistent ground truth across projects for HTR tasks, \nparticularly for complex and heterogeneous‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/medieval.","first_N":5,"first_N_keywords":["image-to-text","French","English","Dutch","Italian"],"keywords_longer_than_N":true},
	{"name":"mpii-human-pose-captions","keyword":"image-to-text","license":"BSD 2-Clause \"Simplified\" License","license_url":"https://choosealicense.com/licenses/bsd-2-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/saifkhichi96/mpii-human-pose-captions","creator_name":"Muhammad Saif Ullah Khan","creator_url":"https://huggingface.co/saifkhichi96","description":"\n\t\n\t\t\n\t\tDataset Card for MPII Human Pose Descriptions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MPII Human Pose Descriptions dataset extends the widely-used MPII Human Pose Dataset with rich textual annotations. These annotations are generated by various state-of-the-art language models (LLMs) and include detailed descriptions of the activities being performed, the count of people present, and their specific poses.\nThe dataset consists of the same image splits as provided in MMPose, with 14644‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/saifkhichi96/mpii-human-pose-captions.","first_N":5,"first_N_keywords":["zero-shot-classification","image-to-text","English","bsd-2-clause","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"nordjylland-news-image-captioning","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/nordjylland-news-image-captioning","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\n\t\n\t\t\n\t\tDataset Card for \"nordjylland-news-image-captioning\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of image-caption pairs from the Danish newspaper TV2 Nord. \n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nImage captioning is the intended task for this dataset. No leaderboard is active at this point.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is available in Danish (da).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nAn example from the dataset looks as follows.\n{\n  \"file_name\": \"1.jpg\",\n  \"caption\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/nordjylland-news-image-captioning.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","Danish"],"keywords_longer_than_N":true},
	{"name":"PixLore-Rich-Captions","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boni98/PixLore-Rich-Captions","creator_name":"Diego Bonilla","creator_url":"https://huggingface.co/Boni98","description":"Rich image captioning dataset used for training PixLore model: https://arxiv.org/abs/2312.05349\n\"image_path\" contains the path to the COCO dataset image (change the path accordingly),\n\"rich_caption\" contains the rich caption created using the technique described in the paper.\nThe rest of the columns are used for debugging or improving the prompt.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"notre-arte","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taesiri/notre-arte","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","description":"\n\t\n\t\t\n\t\tNotre Arte Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset comprises images sourced from the Notre Arte Instagram page and is intended to serve as a challenging and intriguing dataset for testing visual language models and large multimodal language models. The images in this dataset are characterized by their unique artistic style and complexity, which can provide a robust test for the capabilities of modern AI models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThis dataset is intended for research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/notre-arte.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"beninmadrid","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taesiri/beninmadrid","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","description":"\n\t\n\t\t\n\t\tBenin, Madrid Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset comprises images sourced from the beninmadrid Instagram page and is intended to serve as a challenging and intriguing dataset for testing visual language models and large multimodal language models. The images in this dataset are characterized by their unique artistic style and complexity, which can provide a robust test for the capabilities of modern AI models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThis dataset is intended for research‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/beninmadrid.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"histogram-comparisons-small-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/histogram-comparisons-small-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"This is a small subset of the huge histogram-comparisons-v1 dataset with 3M rows.\nThis dataset contains 150000 items in total. There are 3 curriculums each containing 50000 items.\nEach item is a markdown document.\nEach item contains between 2 and 6 image comparisons, with a Summary at the bottom.\nThe images are between 3x3 and 14x14.\nThe markdown document contains a ## Response, that separates the prompt from the answer.\nThe structure of the markdown document with 3 comparisons: A, B, C.\n#‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/histogram-comparisons-small-v1.","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"damaged-media","keyword":"image-to-text","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/danielaivanova/damaged-media","creator_name":"Daniela Ivanova","creator_url":"https://huggingface.co/danielaivanova","description":"\n\t\n\t\t\n\t\tDataset Card for \"ARTeFACT\"\n\t\n\nARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage\n\nHere we provide example code for downloading the data, loading it as a PyTorch dataset, splitting by material and/or content, and visualising examples.\n\n\t\n\t\t\n\t\n\t\n\t\tHousekeeping\n\t\n\n!pip install datasets\n!pip install -qqqU wandb transformers pytorch-lightning==1.9.2 albumentations torchmetrics torchinfo\n!pip install -qqq requests gradio\n\nimport os\nfrom glob import glob\n\nimport cv2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/danielaivanova/damaged-media.","first_N":5,"first_N_keywords":["image-to-image","image-segmentation","image-to-text","image-classification","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"flickr8k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Naveengo/flickr8k","creator_name":"Naveen Golla","creator_url":"https://huggingface.co/Naveengo","description":"\n\t\n\t\t\n\t\tDataset Card for \"flickr8k\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DefectSpectrum","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Andyson/DefectSpectrum","creator_name":"Syang","creator_url":"https://huggingface.co/Andyson","description":"\n\t\n\t\t\n\t\n\t\n\t\tDefect Spectrum Dataset\n\t\n\nWelcome to the Defect Spectrum dataset repository. This comprehensive benchmark is a granular collection of large-scale defect datasets with rich semantics, designed to push the frontier of industrial defect inspection research and applications.\n\n\t\n\t\t\n\t\n\t\n\t\tIMPORTANT\n\t\n\nPLEASE SEE OUR NEW REPO FOR THE FULL DATASET: https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nDefect inspection is a critical component within the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Andyson/DefectSpectrum.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"WeatherGov-dataset","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aazer/WeatherGov-dataset","creator_name":"aaser fawzy","creator_url":"https://huggingface.co/aazer","description":"aazer/WeatherGov-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["table-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"amazon-products","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ckandemir/amazon-products","creator_name":"CK","creator_url":"https://huggingface.co/ckandemir","description":"\n\t\n\t\t\n\t\tDataset Creation and Processing Overview\n\t\n\nThis dataset underwent a comprehensive process of loading, cleaning, processing, and preparing, incorporating a range of data manipulation and NLP techniques to optimize its utility for machine learning models, particularly in natural language processing.\n\n\t\n\t\t\n\t\tData Loading and Initial Cleaning\n\t\n\n\nSource: Loaded from the Hugging Face dataset repository bprateek/amazon_product_description.\nConversion to Pandas DataFrame: For ease of data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ckandemir/amazon-products.","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ThaiIDCardSynt","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Float16-cloud/ThaiIDCardSynt","creator_name":"Float16.cloud","creator_url":"https://huggingface.co/Float16-cloud","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nCurated by: Matichon Maneegard\nShared by [optional]: Matichon Maneegard\nLanguage(s) (NLP): image-to-text\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\nThe dataset was entirely synthetic. It does not contain real information or pertain to any specific person.\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\nUsing for tranning OCR or Multimodal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nThis dataset contains 98 x 6 = 588 samples, and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Float16-cloud/ThaiIDCardSynt.","first_N":5,"first_N_keywords":["image-to-text","Thai","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-Vietnamese-subset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imthanhlv/laion2B-multi-Vietnamese-subset","creator_name":"Th√†nh L√™","creator_url":"https://huggingface.co/imthanhlv","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for LAION-2B-multi Vietnamese subset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nFilter the Vietnamese subset from Laion2B-multi\nTo get the subset of your language, check out this notebook\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Vietnamese","cc-by-4.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"taln-archives_fr_prompt_data_to_text","keyword":"data-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/taln-archives_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\ttaln-archives_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\ntaln-archives_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 35,370 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset taln-archives.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/taln-archives_fr_prompt_data_to_text.","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/taln-archives"],"keywords_longer_than_N":true},
	{"name":"termith-eval_fr_prompt_data_to_text","keyword":"data-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/termith-eval_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\ttermith-eval_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\ntermith-eval_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 11,886 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset termith-eval.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/termith-eval_fr_prompt_data_to_text.","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/termith-eval"],"keywords_longer_than_N":true},
	{"name":"wikinews-fr-100_fr_prompt_data_to_text","keyword":"data-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/wikinews-fr-100_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\twikinews-fr-100_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nwikinews-fr-100_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 3,000 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset wikinews-fr-100.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/wikinews-fr-100_fr_prompt_data_to_text.","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/wikinews-fr-100"],"keywords_longer_than_N":true},
	{"name":"Military-Aircraft-Recognition-dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Alex5666/Military-Aircraft-Recognition-dataset","creator_name":"Gracio","creator_url":"https://huggingface.co/Alex5666","description":"This is a remote sensing image Military Aircraft Recognition dataset that include 3842 images, 20 types, and 22341 instances annotated with horizontal bounding boxes and oriented bounding boxes.\n","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","image-to-image","object-detection"],"keywords_longer_than_N":true},
	{"name":"CMDS_Multimodal_Document","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sitloboi2012/CMDS_Multimodal_Document","creator_name":"Huy Vo","creator_url":"https://huggingface.co/sitloboi2012","description":"\n\t\n\t\t\n\t\tDataset Card for Cyrillic Multimodel Document (CMDS)\n\t\n\nThis is the dataset consists of 3789 pairs of images and text across 31 categories downloaded from the Bulgarian ministry of finance\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nUses this dataset for downstream task like Document Classification, Image Classification or Text Classification‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sitloboi2012/CMDS_Multimodal_Document.","first_N":5,"first_N_keywords":["image-classification","text-classification","image-to-text","Bulgarian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"furry-e621-sfw-7m-hq","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/furry-e621-sfw-7m-hq","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for furry-e621-sfw-7m-hq\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 6.92 M captions of the images from the safe-for-work (SFW) split of e621 (\"e926\"). It extends to January 2023, before the widespread advent of machine learning images. It includes captions created by LLMs and a custom multilabel classifier along with CogVLM. There are 8 LLM (mistralai/Mistral-7B-v0.1) and 1 CogVLM (THUDM/CogVLM) captions per image.\nMost captions are substantially larger than 77 tokens and are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/furry-e621-sfw-7m-hq.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"train_video_and_instruction","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction","creator_name":"ShareGPTVideo","creator_url":"https://huggingface.co/ShareGPTVideo","description":"\n\t\n\t\t\n\t\n\t\n\t\tShareGPTVideo Training Data\n\t\n\nAll dataset and models can be found at ShareGPTVideo.\n\n\t\n\t\t\n\t\n\t\n\t\tContents:\n\t\n\n\nTrain 300k video frames: contains video frames used for SFT and DPO model, which is a subset of total 900k.\nActivityNet 50k + vidal 150k + webvid 100k.\n\nTrain 600k video frames: contains the rest 600k frames, the total 900k frames are used for pre-training stage. If you just do finetuning using our video QA, you can just download the 300k above.\n900k composition is 400k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"dm-codes","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shortery/dm-codes","creator_name":"-","creator_url":"https://huggingface.co/shortery","description":"\n\t\n\t\t\n\t\tDM codes dataset\n\t\n\nThe dataset contains photos of Data Matrix (DM) codes and their annotations. The photos were taken on an iPhone and annotated manually by a human.\nThe annotations contain text, which is encoded in the DM code and the pixel coordinates of the DM code vertices.\nThe vertices are: tl = top left, tr = top right, br = bottom right, bl = bottom left.\nAttribute is_clean specifies whether the DM code on the image is expected to be easily readable. For every DM code, there is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shortery/dm-codes.","first_N":5,"first_N_keywords":["image-to-text","object-detection","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a given‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"ocr-synthetic-images-of-words-danish-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1","creator_name":"Dennis Iversen","creator_url":"https://huggingface.co/diversen","description":"This repo contains synthetic images with Danish word created for generating an OCR model.\nThere is about 22.9GB of train-data and 4.58GB of validation-data.\nIn order to clone the repo you will need to use the git lfs extension. \nClone:\ngit clone https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1\ncd ocr-synthetic-images-of-words-danish-v1\n\nExtract (this may take some time):\ntar xvfz validation-data.tar.gz\ntar xvfz train-data.tar.gz \n\nInside both train-data and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1.","first_N":5,"first_N_keywords":["image-to-text","mit","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"webcode2m","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xcodemind/webcode2m","creator_name":"xcodemind","creator_url":"https://huggingface.co/xcodemind","description":"WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs with Layouts\n(This dataset is also called Vision2UI.)\n\nAutomatically generating webpage code from webpage designscan significantly reduce the workload of front-end developers, andrecent Multimodal Large Language Models (MLLMs) have shownpromising potential in this area. However, our investigation revealsthat most existing MLLMs are constrained by the absence of highquality, large-scale, real-world datasets, resulting in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xcodemind/webcode2m.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"sllm","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexFromSynlabs/sllm","creator_name":"K","creator_url":"https://huggingface.co/AlexFromSynlabs","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for GEM/viggo\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tLink to Main Data Card\n\t\n\nYou can find the main data card on the GEM Website.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nViGGO is an English data-to-text generation dataset in the video game domain, with target responses being more conversational than information-seeking, yet constrained to the information presented in a meaning representation. The dataset is relatively small with about 5,000 datasets but very clean, and can thus serve for evaluating‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexFromSynlabs/sllm.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"sllm","keyword":"data-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexFromSynlabs/sllm","creator_name":"K","creator_url":"https://huggingface.co/AlexFromSynlabs","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for GEM/viggo\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tLink to Main Data Card\n\t\n\nYou can find the main data card on the GEM Website.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nViGGO is an English data-to-text generation dataset in the video game domain, with target responses being more conversational than information-seeking, yet constrained to the information presented in a meaning representation. The dataset is relatively small with about 5,000 datasets but very clean, and can thus serve for evaluating‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AlexFromSynlabs/sllm.","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"YFCC15M_page_and_download_urls","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vishaal27/YFCC15M_page_and_download_urls","creator_name":"Vishaal Udandarao","creator_url":"https://huggingface.co/vishaal27","description":"\n\t\n\t\t\n\t\tYFCC15M subset used for VLMs\n\t\n\nThis dataset contains the ~15M subset of YFCC100M used for training the models in the paper Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP. The metadata provided in this repo contains both the page-urls and image-download-urls for downloading the dataset.\nThis dataset can be easily downloaded with img2dataset:\nimg2dataset --url_list yfcc15m_final_split_pageandimageurls.csv --input_format \"csv\" --output_format‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vishaal27/YFCC15M_page_and_download_urls.","first_N":5,"first_N_keywords":["zero-shot-classification","image-to-text","English","mit","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"RIMES-2011-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/RIMES-2011-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tRIMES-2011 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe RIMES-2011 database (Recognition and Indexation of handwritten documents and faxes) was created to evaluate automatic recognition and indexing systems for handwritten letters. \nThe database was collected by asking volunteers to write handwritten letters in exchange for gift certificates. Volunteers were given a fictitious identity (same gender as the real one) and up to 5 scenarios. Each scenario was chosen from among 9‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/RIMES-2011-line.","first_N":5,"first_N_keywords":["image-to-text","French","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Esposalles-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/Esposalles-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tEsposalles - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Marriage Licenses ground-truth is compiled from the Marriage Licenses Books conserved at the Archives of the Cathedral of Barcelona.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Catalan.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1244x128 at 0x1A800E8E190‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Esposalles-line.","first_N":5,"first_N_keywords":["image-to-text","Catalan","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"EmbodiedEval","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EmbodiedEval/EmbodiedEval","creator_name":"EmbodiedEval","creator_url":"https://huggingface.co/EmbodiedEval","description":"This repository contains the dataset of the paper EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents.\nGithub repository: https://github.com/thunlp/EmbodiedEval\nProject Page: https://embodiedeval.github.io/\n","first_N":5,"first_N_keywords":["robotics","video-text-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"tab2latex","keyword":"image-to-text","license":"BSD 3-Clause Clear License","license_url":"https://choosealicense.com/licenses/bsd-3-clause-clear/","language":"en","dataset_url":"https://huggingface.co/datasets/lt-asset/tab2latex","creator_name":"Purdue ASSET Research Group","creator_url":"https://huggingface.co/lt-asset","description":"\n\t\n\t\t\n\t\tLATTE: Improving Latex Recognition for Tables and Formulae with Iterative Refinement\n\t\n\nTab2Latex: a Latex table recognition dataset, with 87,513 training, 5,000 validation, and 5,000 test instances. The LaTeX sources are collected from academic papers within these six distinct sub-fields of computer science‚ÄîArtificial Intelligence, Computation and Language, Computer Vision and Pattern Recognition, Cryptography and Security, Programming Languages, and Software Engineering‚Äîfrom the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lt-asset/tab2latex.","first_N":5,"first_N_keywords":["image-to-text","bsd-3-clause-clear","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DualMath-1.1M","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/URSA-MATH/DualMath-1.1M","creator_name":"URSA-MATH","creator_url":"https://huggingface.co/URSA-MATH","description":"\n\t\n\t\t\n\t\tDualMath-1.1M\n\t\n\nImage data can be downloaded from the following address:\n\nMAVIS: https://github.com/ZrrSkywalker/MAVIS, https://drive.google.com/drive/folders/1LGd2JCVHi1Y6IQ7l-5erZ4QRGC4L7Nol.\nMultimath: https://huggingface.co/datasets/pengshuai-rin/multimath-300k.\nGeo170k: https://huggingface.co/datasets/Luckyjhg/Geo170K.\nVarsityTutors: https://huggingface.co/datasets/Math-PUMA/Math-PUMA_Data_Stage2. \nMathV360K: https://huggingface.co/datasets/Zhiqiang007/MathV360K.\n\nThe image data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/URSA-MATH/DualMath-1.1M.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","Chinese","gpl-3.0"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oyasi/test","creator_name":"oyasi zaki ananta","creator_url":"https://huggingface.co/Oyasi","description":"Oyasi/test dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"EndoVis2018LongCap","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liujiquan/EndoVis2018LongCap","creator_name":"Jiquan Liu","creator_url":"https://huggingface.co/liujiquan","description":"This dataset consists of long-text captions of laparoscopic surgical images, extended from https://github.com/XuMengyaAmy/CIDACaptioning.\nDownload the images: https://endovissub2018-roboticscenesegmentation.grand-challenge.org/Data/\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ViDoSeek","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/autumncc/ViDoSeek","creator_name":"QiuchenWang","creator_url":"https://huggingface.co/autumncc","description":"\n\t\n\t\t\n\t\tüöÄOverview\n\t\n\nThis is the Repo for ViDoSeek, a benchmark specifically designed for visually rich document retrieval-reason-answer, fully suited for evaluation of RAG within large document corpus. \n\nThe paper is available at https://arxiv.org/abs/2502.18017.\nViDoRAG Project: https://github.com/Alibaba-NLP/ViDoRAG\n\nViDoSeek sets itself apart with its heightened difficulty level, attributed to the multi-document context and the intricate nature of its content types, particularly the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/autumncc/ViDoSeek.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","visual-document-retrieval","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"synthetic-watch-faces-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/elischwartz/synthetic-watch-faces-dataset","creator_name":"Eli Schwartz","creator_url":"https://huggingface.co/elischwartz","description":"\n\t\n\t\t\n\t\tSynthetic Watch Faces Dataset\n\t\n\nA synthetic dataset of analog watch faces displaying various times for training vision models in time recognition tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of randomly generated analog watch faces showing different times. Each image contains a watch with hour and minute hands positioned to display a specific time. The dataset is designed to help train and evaluate computer vision models and Vision-Language Models (VLMs) for time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/elischwartz/synthetic-watch-faces-dataset.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","image-classification","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"UI-128","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/henryhe0123/UI-128","creator_name":"Yanheng He","creator_url":"https://huggingface.co/henryhe0123","description":"henryhe0123/UI-128 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VidComposition_Benchmark","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark","creator_name":"JunJiaGuo","creator_url":"https://huggingface.co/JunJiaGuo","description":"\n\t\n\t\t\n\t\tVidComposition Benchmark\n\t\n\nüñ• Project Page | üöÄ Evaluation Space\nThe advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-text-to-text","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Himanis-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/Himanis-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tHimanis - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHimanis (HIstorical MANuscript Indexing for user controlled Search) is a corpus of medieval documents.\nThe historical corpus is described in the following publication:\nStutzmann, D., Moufflet, J-F., & Hamel, S. (2017). La recherche en plein texte dans les sources manuscrites m√©di√©vales‚ÄØ: enjeux et perspectives du projet HIMANIS pour l‚Äô√©dition √©lectronique. M√©di√©vales‚ÄØ: Langue, textes, histoire 73 (2017): 67‚Äë96.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Himanis-line.","first_N":5,"first_N_keywords":["image-to-text","Latin","French","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"HOME-Alcar-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/HOME-Alcar-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tHOME-Alcar - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe HOME-Alcar (Aligned and Annotated Cartularies) dataset is a Medieval corpus. The 17 medieval manuscripts in this corpus are cartularies, i.e. books copying charters and legal acts, produced between the 12th and 14th centuries. \nThis dataset comes from the following publication:\nStutzmann, D., Torres Aguilar, S., & Chaffenet, P. (2021). HOME-Alcar: Aligned and Annotated Cartularies [Data set]. Zenodo.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/HOME-Alcar-line.","first_N":5,"first_N_keywords":["image-to-text","Latin","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NewsEye-Austrian-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tNewsEye Austrian - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset comprises Austrian newspaper pages from 19th and early 20th century. The images were provided by the Austrian National Library.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe documents are in Austrian German with the Fraktur font.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line.","first_N":5,"first_N_keywords":["image-to-text","German","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NorHand-v1-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/NorHand-v1-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tNorHand v1 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v1 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian Bokm√•l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'fredag‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v1-line.","first_N":5,"first_N_keywords":["image-to-text","Norwegian Bokm√•l","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NorHand-v2-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/NorHand-v2-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tNorHand v2 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v2 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian Bokm√•l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'og‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v2-line.","first_N":5,"first_N_keywords":["image-to-text","Norwegian Bokm√•l","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"NorHand-v3-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/NorHand-v3-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tNorHand v3 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v3 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian Bokm√•l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'Til‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v3-line.","first_N":5,"first_N_keywords":["image-to-text","Norwegian Bokm√•l","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Belfort-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/Belfort-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tBelfort - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Belfort dataset includes minutes of the municipal council of the French city of Belfort. \nText lines were extracted using an automatic model and may contain segmentation errors. The transcriptions were obtained through a crowdsourcing campaign using the Callico web plateform.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Belfort-line.","first_N":5,"first_N_keywords":["image-to-text","French","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"IAM-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/IAM-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tIAM - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe IAM Handwriting Database contains forms of handwritten English text which can be used to train and test handwritten text recognizers and to perform writer identification and verification experiments.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image':‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/IAM-line.","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CASIA-HWDB2-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tCASIA-HWDB2 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe offline Chinese handwriting database (CASIA-HWDB2) was built by the National Laboratory of Pattern Recognition (NLPR), Institute of Automation of Chinese Academy of Sciences (CASIA). \nThe handwritten samples were produced by 1,020 writers using Anoto pen on papers, such that both online and offline data were obtained.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line.","first_N":5,"first_N_keywords":["image-to-text","Chinese","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"POPP-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/POPP-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tPOPP - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe POPP dataset includes French civil census from Paris from the early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'Joly Ernest 88 Indre M par Employ√© Roblot!18377'\n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/POPP-line.","first_N":5,"first_N_keywords":["image-to-text","French","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"VisualWebBench","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/visualwebbench/VisualWebBench","creator_name":"VisualWebBench","creator_url":"https://huggingface.co/visualwebbench","description":"\n\t\n\t\t\n\t\tVisualWebBench\n\t\n\nDataset for the paper: VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?\nüåê Homepage | üêç GitHub | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nWe introduce VisualWebBench, a multimodal benchmark designed to assess the understanding and grounding capabilities of MLLMs in web scenarios. VisualWebBench consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/visualwebbench/VisualWebBench.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"KOSMOS-VNews","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oztobuzz/KOSMOS-VNews","creator_name":"Tran Ngoc Oanh","creator_url":"https://huggingface.co/Oztobuzz","description":"\n\t\n\t\t\n\t\tDataset Card for KOSMOS-VNews\n\t\n\n\nThis dataset comprises over 170,000 English vision-language samples. The image captions were generated using the KOSMOS-2 model .\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe images are keyframes extracted from Vietnamese news broadcasts (specifically, \"60 Giay\" news from HTV) aired in 2023 and used in the AIChallenge 2023.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nThe dataset consists of keyframes extracted from approximately 600 news videos, with an average of 300 frames‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Oztobuzz/KOSMOS-VNews.","first_N":5,"first_N_keywords":["image-to-text","English","Vietnamese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Perception-Collection","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prometheus-eval/Perception-Collection","creator_name":"prometheus-eval","creator_url":"https://huggingface.co/prometheus-eval","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\nHomepage: https://kaistai.github.io/prometheus-vision/ \nRepository: https://github.com/kaistAI/prometheus-vision \nPaper: https://arxiv.org/abs/2401.06591 \nPoint of Contact: seongyun@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset summary\n\t\n\nPerception Collection is the first multi-modal feedback dataset that could be used to train an evaluator VLM. Perception Collection includes 15K fine-grained criteria that determine the crucial aspect for each instance.\n\n\t\n\t\t\n\t\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/Perception-Collection.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Perception-Bench","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prometheus-eval/Perception-Bench","creator_name":"prometheus-eval","creator_url":"https://huggingface.co/prometheus-eval","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\nHomepage: https://kaistai.github.io/prometheus-vision/ \nRepository: https://github.com/kaistAI/prometheus-vision \nPaper: https://arxiv.org/abs/2401.06591 \nPoint of Contact: seongyun@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset summary\n\t\n\nPerception-Bench is a benchmark for evaluating the long-form response of a VLM (Vision Language Model) across various domains of images, and it is a held-out test\nset of the Perception-Collection\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/Perception-Bench.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"This dataset is generated from TasvirEt[1]. I do not own the copyright of either the images or the captions. Please refer to the paper's webpage for further details.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÃàruÃàntuÃàlerden Otomatik TuÃàrkcÃße AcÃßƒ±klama Olu≈üturma ƒ∞cÃßin Bir Denekta≈üƒ± Veri KuÃàmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal ƒ∞≈üleme ve ƒ∞leti≈üim Uygulamalarƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/atasoglu/flickr8k-turkish.","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"This dataset is generated from TasvirEt[1]. I do not own the copyright of either the images or the captions. Please refer to the paper's webpage for further details.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÃàruÃàntuÃàlerden Otomatik TuÃàrkcÃße AcÃßƒ±klama Olu≈üturma ƒ∞cÃßin Bir Denekta≈üƒ± Veri KuÃàmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal ƒ∞≈üleme ve ƒ∞leti≈üim Uygulamalarƒ±‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/atasoglu/flickr8k-turkish.","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"histogram-comparisons-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/histogram-comparisons-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"If you want a small subset of this dataset, there is histogram-comparisons-small-v1 with 150k rows.\nThis dataset contains 3000000 items in total. There are 3 curriculums each containing 1000000 items.\nEach item is a markdown document.\nEach item contains between 2 and 6 image comparisons, with a Summary at the bottom.\nThe images are between 3x3 and 14x14.\nThe markdown document contains a ## Response, that separates the prompt from the answer.\nThe structure of the markdown document with 3‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/histogram-comparisons-v1.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Defect_Spectrum","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum","creator_name":"DefectSpectrum","creator_url":"https://huggingface.co/DefectSpectrum","description":"\n\t\n\t\t\n\t\tDefect Spectrum Dataset\n\t\n\nWelcome to the Defect Spectrum dataset repository. This comprehensive benchmark is a granular collection of large-scale defect datasets with rich semantics, designed to push the frontier of industrial defect inspection research and applications.\nPaper: https://huggingface.co/papers/2310.17316\nGithub repository: https://github.com/EnVision-Research/Defect_Spectrum\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nDefect inspection is a critical component within the closed-loop‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"minecraft-captioning","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/orzhan/minecraft-captioning","creator_name":"Mikhail","creator_url":"https://huggingface.co/orzhan","description":"orzhan/minecraft-captioning dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ArxivQA","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/ArxivQA","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\tDataset Card for Mutlimodal Arxiv QA\n\t\n\n\n\t\n\t\t\n\t\tDataset Loading Instruction\n\t\n\nEach line of the arxivqa.jsonl file is an example:\n{\"id\": \"cond-mat-2862\",\n\"image\": \"images/0805.4509_1.jpg\",\n\"options\": [\"A) The ordering temperatures for all materials are above the normalized temperature T/Tc T/T_c T/Tc‚Äã of 1.2.\", \"B) The magnetic ordering temperatures decrease for Dy, Tb, and Ho as the normalized temperature T/Tc T/T_c T/Tc‚Äã approaches 1.\", \"C) The magnetic ordering temperatures for all‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivQA.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"StockImages-CC0","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KoalaAI/StockImages-CC0","creator_name":"Koala AI","creator_url":"https://huggingface.co/KoalaAI","description":"\n\t\n\t\t\n\t\tCC0 Stock Images Dataset\n\t\n\nThis dataset contains a collection of stock images that are covered by the Creative Commons Zero (CC0) License, meaning they are free for personal and commercial use with no attribution required. It is designed to support a variety of computer vision tasks such as image tagging, categorization, and machine learning model training.\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nWhile every effort has been made to ensure the reliability and correctness of the data presented, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KoalaAI/StockImages-CC0.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"SynthKhmer-10k","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/seanghay/SynthKhmer-10k","creator_name":"seanghay","creator_url":"https://huggingface.co/seanghay","description":"\n\t\n\t\t\n\t\tSynthKhmer 10k\n\t\n\nImage Size: 896x672\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\nBackground images are from https://picsum.photos/\nProfile Photos are from https://thispersondoesnotexist.com/\nNames are sampled from Khmer Dictionary\n\n","first_N":5,"first_N_keywords":["image-to-text","Khmer","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"laion-coco-aesthetic","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guangyil/laion-coco-aesthetic","creator_name":"Guangyi Liu","creator_url":"https://huggingface.co/guangyil","description":"\n\t\n\t\t\n\t\tLAION COCO with aesthetic score and watermark score\n\t\n\nThis dataset contains 10% samples of the LAION-COCO dataset filtered by some text rules (remove url, special tokens, etc.), and image rules (image size > 384x384, aesthetic score>4.75 and watermark probability<0.5). There are total 8,563,753 data instances in this dataset. And the corresponding aesthetic score and watermark score are also included. \nNoted: watermark score in the table means the probability of the existence of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guangyil/laion-coco-aesthetic.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"stata","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adenhaus/stata","creator_name":"Aden Haussmann","creator_url":"https://huggingface.co/adenhaus","description":"\n\t\n\t\t\n\t\tBackground\n\t\n\nThis dataset contains human evaluations of whether outputs on the TaTA dataset are a) understandable and b) attributable to the source tables. See TaTA: A Multilingual Table-to-Text Dataset for African Languages for more details. \nIt can be used to train a learned metric, called StATA, to evaluate model performance on the TaTA dataset.\nPaper: https://www.arxiv.org/abs/2503.23204\nThe original can be found here.\n","first_N":5,"first_N_keywords":["table-to-text","yes","Arabic","English","French"],"keywords_longer_than_N":true},
	{"name":"stata","keyword":"data-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adenhaus/stata","creator_name":"Aden Haussmann","creator_url":"https://huggingface.co/adenhaus","description":"\n\t\n\t\t\n\t\tBackground\n\t\n\nThis dataset contains human evaluations of whether outputs on the TaTA dataset are a) understandable and b) attributable to the source tables. See TaTA: A Multilingual Table-to-Text Dataset for African Languages for more details. \nIt can be used to train a learned metric, called StATA, to evaluate model performance on the TaTA dataset.\nPaper: https://www.arxiv.org/abs/2503.23204\nThe original can be found here.\n","first_N":5,"first_N_keywords":["table-to-text","yes","Arabic","English","French"],"keywords_longer_than_N":true},
	{"name":"RICO-Screen2Words","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rootsautomation/RICO-Screen2Words","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","description":"\n\t\n\t\t\n\t\tDataset Card for Screen2Words\n\t\n\nScreen2Words is a dataset providing screen summaries (i.e., image captions for mobile screens). \nIt uses the RICO image database. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository:\ngoogle-research-datasets/screen2words\nRICO raw downloads\n\n\nPaper:\nScreen2Words: Automatic Mobile UI Summarization with Multimodal Learning\nRico: A Mobile App Dataset for Building Data-Driven Design Applications\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis dataset is for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-Screen2Words.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"RICO-WidgetCaptioning","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rootsautomation/RICO-WidgetCaptioning","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","description":"\n\t\n\t\t\n\t\tDataset Card for RICO Widget Captioning\n\t\n\nWidget Captioning is a dataset for providing captions for UI elements on mobile screens. \nIt uses the RICO image database. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository:\ngoogle-research-datasets/widget-caption\nRICO raw downloads\n\n\nPaper:\nWidget Captioning: Generating Natural Language Description for Mobile User Interface Elements\nRico: A Mobile App Dataset for Building Data-Driven Design Applications\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-WidgetCaptioning.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SPEC","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wjpoom/SPEC","creator_name":"Wujian Peng","creator_url":"https://huggingface.co/wjpoom","description":"\n\t\n\t\t\n\t\t[CVPR 2024] SPEC Benchmark: Evaluating VLMs in Fine-grained and Compositional Understanding\n\t\n\nintroduced in the CVPR 2024 paper Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding\nCode | ü§ó Paper | üìñ arXiv\nTo evaluate the understanding capability of visual-language models on fine-grained concepts, we propose a new benchmark, SPEC, \nwhich consists of six distinct subsets, distributed across the dimensions of Size, Position, Existence, and Count.\nEach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wjpoom/SPEC.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"StreetView360AtoZ","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/everettshen/StreetView360AtoZ","creator_name":"Everett Shen","creator_url":"https://huggingface.co/everettshen","description":"StreetView 360X is a dataset containing 6342 360 degree equirectangular street view images randomly sampled and downloaded from Google Street View. It is published as part of the paper \"StreetView360X: A Location-Conditioned Latent Diffusion Model for Generating Equirectangular 360 Degree Street Views\" (Princeton COS Senior Independent Work by Everett Shen). Images are labelled with their capture coordinates and panorama IDs. Scripts for extending the dataset (i.e. fetching additional images)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/everettshen/StreetView360AtoZ.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-feature-extraction","mit"],"keywords_longer_than_N":true},
	{"name":"RICO-SCA","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rootsautomation/RICO-SCA","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","description":"\n\t\n\t\t\n\t\tDataset Card for RICO SCA (SeeClick cache)\n\t\n\nThis is the SeeClick cache of a syntehtically generated dataset following RICO SCA's generation procedure. \nIt consists of approximately 170k captions across 70k widgets and 18k screens. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a widget captioning (referring expression comprehension/generation) dataset. \n\nCurated by: Google Research, Nanjing University\nLanguage(s) (NLP): en\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-SCA.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"RICO-ScreenAnnotation-f","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rootsautomation/RICO-ScreenAnnotation-f","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","description":"\n\t\n\t\t\n\t\tDataset Card for RICO Screen Annotations\n\t\n\nThis is a standardization of Google's Screen Annotation dataset on a subset of RICO screens, as described in their ScreenAI paper.\nUnlike the original, this version transforms integer-based bounding boxes into floating-point-based bounding boxes of 2 decimal precision. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is an image-to-text annotation format first proscribed in Google's ScreenAI paper. \nThe idea is to standardize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-ScreenAnnotation-f.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CircuitSketchTextAnnotations","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/edesaras/CircuitSketchTextAnnotations","creator_name":"Aras Ede≈ü","creator_url":"https://huggingface.co/edesaras","description":"edesaras/CircuitSketchTextAnnotations dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","German","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Handwritten-Latex-Datasets","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WindyVerse/Handwritten-Latex-Datasets","creator_name":"WindyVerse","creator_url":"https://huggingface.co/WindyVerse","description":"\n\t\n\t\t\n\t\tDataset\n\t\n\nThis data set includes common handwritten formulas in junior high schools and high schools, and is labeled in Latex format. Can be used to train models that recognize common numbers, fractions, and sets.\n\n\t\n\t\t\n\t\tDataset source\n\t\n\nCollected in various junior high schools and high schools, handwritten by students.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThe label is stored at json folder and scanned hand-writted pictures are stored at pic folder.\nScan the qr code of the picture to get the index and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WindyVerse/Handwritten-Latex-Datasets.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"LaTeX_Image_Pairs","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/henryholloway/LaTeX_Image_Pairs","creator_name":"Henry Holloway","creator_url":"https://huggingface.co/henryholloway","description":"\n\t\n\t\t\n\t\n\t\n\t\tLaTeX Image Pairs Dataset\n\t\n\nThis dataset comprises a unique collection of LaTeX expressions paired with their corresponding images. The LaTeX expressions were meticulously scraped from a variety of open-source textbooks, ensuring a diverse and comprehensive dataset. Sample references from these textbooks will be provided to illustrate the sources of these expressions.\nIn addition to the raw LaTeX expressions, this dataset includes images of the rendered expressions. Each LaTeX‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/henryholloway/LaTeX_Image_Pairs.","first_N":5,"first_N_keywords":["image-classification","text-generation","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"datacomp-small-clip","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fondant-ai/datacomp-small-clip","creator_name":"Fondant","creator_url":"https://huggingface.co/fondant-ai","description":"\n    \n        \n    \n\n\n\n    \n        Production-ready \n        data processing made \n        easy \n        and \n        shareable\n    \n    \n    Explore the Fondant docs ¬ª\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for fondant-ai/datacomp-small-clip\n\t\n\n\n\nThis is a dataset containing image urls and their CLIP embeddings, based on the datacomp_small dataset, and processed with fondant.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nLarge (image) datasets are often unwieldy to use due to their‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fondant-ai/datacomp-small-clip.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","cc-by-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"llava-bench-in-the-wild-ja","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","description":"This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.\nOriginal dataset is liuhaotian/llava-bench-in-the-wild.\n","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Invoice_dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sathyakala/Invoice_dataset","creator_name":"Sathyakala Muthaiyan","creator_url":"https://huggingface.co/Sathyakala","description":"Sathyakala/Invoice_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"flickr30k-fa","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hezarai/flickr30k-fa","creator_name":"Hezar AI","creator_url":"https://huggingface.co/hezarai","description":"The Flickr30K dataset filtered and translated to Persian.\nThis dataset was originally made by Sajjad Ayoubi and uploaded to Kaggle at https://www.kaggle.com/datasets/sajjadayobi360/flickrfa.\nThis repo contains the exact dataset split to train/test using a custom sampling criteria and can be directly loaded using HuggingFace datasets or right from Hezar.\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tHugging Face Datasets\n\t\n\npip install datasets\n\nfrom datasets import load_dataset\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hezarai/flickr30k-fa.","first_N":5,"first_N_keywords":["image-to-text","Persian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GlitchBench","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/glitchbench/GlitchBench","creator_name":"GlitchBench","creator_url":"https://huggingface.co/glitchbench","description":"\n\t\n\t\t\n\t\tGlitchBench\n\t\n\nThis repository contains the dataset for the paper GlitchBench: Can large multimodal models detect video game glitches?\n    \n     by \n        Mohammad Reza Taesiri, \n        Tianjun Feng,\n        Anh Nguyen, and \n        Cor-Paul Bezemer \n    \n    \n    (CVPR 2024)\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nLarge multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/glitchbench/GlitchBench.","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VIP","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryanhe/VIP","creator_name":"Ryan He","creator_url":"https://huggingface.co/ryanhe","description":"\n\t\n\t\t\n\t\tDataset Card for Video Infilling and Prediction (VIP)\n\t\n\nVideo Infilling and Prediction (VIP) is a benchmark dataset for assessing the sequential commonsense reasoning capabilities of vision-language models by generating explanations of videos.\nSee our EMNLP 2023 paper introducing this work\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nCurated by: Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Wang (UC Santa Barbara)\nFunded by: Amazon‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryanhe/VIP.","first_N":5,"first_N_keywords":["video-classification","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"chitralekha","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gksriharsha/chitralekha","creator_name":"Krishna Sriharsha Gundu","creator_url":"https://huggingface.co/gksriharsha","description":"\n\t\n\t\t\n\t\tChitralekha\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Version\n\t\n\nSome of the fonts do not have proper letters/rendering of different telugu letter combinations. Those have been removed as much as I can find them. If there are any other mistakes that you notice, please raise an issue and I will try my best to look into it\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis extensive dataset, hosted on Huggingface, is a comprehensive resource for Optical Character Recognition (OCR) in the Telugu‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gksriharsha/chitralekha.","first_N":5,"first_N_keywords":["image-to-text","Telugu","mit","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"M-BEIR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/M-BEIR","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tUniIR: Training and Benchmarking Universal Multimodal Information Retrievers (ECCV 2024)\n\t\n\nüåê Homepage | ü§ó Model(UniIR Checkpoints) | ü§ó Paper | üìñ arXiv  | GitHub\nHow to download the M-BEIR Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüî•[2023-12-21]: Our M-BEIR Benchmark is now available for use.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nM-BEIR, the Multimodal BEnchmark for Instructed Retrieval, is a comprehensive large-scale retrieval benchmark designed to train and evaluate unified multimodal retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/M-BEIR.","first_N":5,"first_N_keywords":["text-retrieval","text-to-image","image-to-text","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Impressions","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SALT-NLP/Impressions","creator_name":"Social And Language Technology Lab","creator_url":"https://huggingface.co/SALT-NLP","description":"\n\t\n\t\t\n\t\tDataset Card for \"Impressions\"\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Impressions dataset is a multimodal benchmark that consists of 4,100 unique annotations and over 1,375 image-caption pairs from the photography domain. Each annotation explores (1) the aesthetic impactfulness of a photograph, (2) image descriptions in which pragmatic inferences are welcome, (3) emotions/thoughts/beliefs that the photograph may inspire, and (4) the aesthetic elements that elicited the expressed impression.\nEMNLP‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SALT-NLP/Impressions.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ArxivCap","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/ArxivCap","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ArxivCap\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\n\nExample-1 of single (image, caption) pairs\n\n\"......\" stands for omitted parts.\n\n{\n    'src': 'arXiv_src_2112_060/2112.08947', \n    'meta': \n    {\n        'meta_from_kaggle': \n        {\n            'journey': '', \n            'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', \n            'categories': 'cs.ET'\n        }, \n        'meta_from_s2': \n        {\n            'citationCount': 8‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Pexels-400k","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jovianzm/Pexels-400k","creator_name":"Jovian","creator_url":"https://huggingface.co/jovianzm","description":"\n\t\n\t\t\n\t\tPexels 400k\n\t\n\nDataset of 400,476 videos, their thumbnails, viewcounts, explicit classification, and caption.\nNote: The Pexels-320k dataset in the repo is this dataset, with videos <10s removed.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","English"],"keywords_longer_than_N":true},
	{"name":"Khatt-Dataset-Unique-lines-full","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Nada2125/Khatt-Dataset-Unique-lines-full","creator_name":"Abbas","creator_url":"https://huggingface.co/Nada2125","description":"","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","text"],"keywords_longer_than_N":true},
	{"name":"DataComp-12M","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/DataComp-12M","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for DataComp-12M\n\t\n\n\n\nThis dataset contains a 12M subset of DataComp-1B-BestPool.\nWe distribute the image url-text samples and metadata under a standard Creative Common CC-BY-4.0 license. The individual images are under their own copyrights.\nImage-text models trained on DataComp-12M are significantly better than on CC-12M/YFCC-15M as well as DataComp-Small/Medium.\nDataComp-12M was introduced in MobileCLIP paper and along with the reinforced dataset DataCompDR-12M.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/DataComp-12M.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","Image"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-aesthetics","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-anatomy","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-architecture","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-ArXiv","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-typography","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photochat_plus","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/passing2961/photochat_plus","creator_name":"Young-Jun Lee","creator_url":"https://huggingface.co/passing2961","description":"\n\t\n\t\t\n\t\tDataset Card for PhotoChat++\n\t\n\n\nüö® Disclaimer: All models and datasets are intended for research purposes only.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPhotoChat++ is a publicly available multi-modal dialogue dataset, an extended version of PhotoChat. PhotoChat++ contains six intent labels, a triggering sentence, an image description, and salient information (e.g., ‚Äúwords‚Äù or ‚Äúphrases‚Äù) to invoke the image-sharing behavior. The purpose of this dataset is to thoroughly assess the image-sharing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/passing2961/photochat_plus.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","conversational","monolingual","PhotoChat"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-aesthetics","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-aesthetics","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-architecture","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-architecture","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-anatomy","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-anatomy","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-typography","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-typography","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"samaritan_v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/johnlockejrr/samaritan_v1","creator_name":"John Locke","creator_url":"https://huggingface.co/johnlockejrr","description":"\n\t\n\t\t\n\t\tSamaritan v1 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Samaritanv1 dataset comprises Samaritan Biblical manuscripts line images and text from 14th and early 17th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Hebrew, Samaritan Hebrew and Samaritan Aramaic.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/johnlockejrr/samaritan_v1.","first_N":5,"first_N_keywords":["image-to-text","Hebrew","Samaritan Aramaic","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"KHATT_v1.0_dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/johnlockejrr/KHATT_v1.0_dataset","creator_name":"John Locke","creator_url":"https://huggingface.co/johnlockejrr","description":"\n\t\n\t\t\n\t\n\t\n\t\tKHATT_v1.0 - line level\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nKHATT (KFUPM Handwritten Arabic TexT) database is a database of unconstrained handwritten Arabic Text written by 1000 different writers. This research database‚Äôs development was undertaken by a research group from KFUPM, Dhahran, S audi Arabia headed by Professor Sabri Mahmoud in collaboration with Professor Fink from TU-Dortmund, Germany and Dr. M√§rgner from TU-Braunschweig, Germany.\nThe database includes 2000 similar-text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/johnlockejrr/KHATT_v1.0_dataset.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","Image","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"HUVER","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raiselab/HUVER","creator_name":"RAISE Lab","creator_url":"https://huggingface.co/raiselab","description":"\n\t\n\t\t\n\t\tDataset Card for HUVER\n\t\n\n\n\nThe dataset is comprised of a 6,051 unique UAV configurations, where each configuration is described by multiple data for-\nmats, including a grammar string, an RGB image, and an GLB file.\nComplementing these representation modalities, we also provide a configuration-based description, i.e., a text descriptor describing the features of each UAV using natural language\n\nCurated by: Abhiram Karri, Gary Stump, Christopher McComb, Binyang Song\n\nLanguage(s) (NLP):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raiselab/HUVER.","first_N":5,"first_N_keywords":["image-to-text","image-to-3d","image-feature-extraction","text-to-3d","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"NSFW-T2I","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zxbsmk/NSFW-T2I","creator_name":"Jun","creator_url":"https://huggingface.co/zxbsmk","description":"\n\t\n\t\t\n\t\n\t\n\t\tIntroduction (Version 1)\n\t\n\nAbout 38k image-text pairs(10k from LAION and 28k from nsfw_detect), and captions are generated by LLaVA-NeXT with prompt \"Describe the photo in detail (attributes of person)\".\nThe \"txt\" column shown in the dataset viewer is originated from LAION, not the captions yielded by LLaVA-NeXT.\n\n\t\n\t\t\n\t\n\t\n\t\tCaption Codes\n\t\n\npretrained = \"lmms-lab/llama3-llava-next-8b\"\nmodel_name = \"llava_llama3\"\ndevice = \"cuda:2\"\ndevice_map = \"auto\"\ntokenizer, model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zxbsmk/NSFW-T2I.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"chess_checkmate_images_big_bench","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DylanASHillier/chess_checkmate_images_big_bench","creator_name":"Dylan Hillier","creator_url":"https://huggingface.co/DylanASHillier","description":"\n\t\n\t\t\n\t\tDataset Card for BIG-Bench Checkmate In One Move (Images)\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is an adapted version of the BIG-Bench Checkmate in One Move task\nas originally made by Nitish Keskar (nkeskar@salesforce.com).\nCopying the original task description:\n\nThe goal of this task is to probe the ability of language models to play chess in standard algebraic notation (SAN). The input to the model is a sequence of moves such that a next possible move is a checkmate. We curate 3,500 games‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DylanASHillier/chess_checkmate_images_big_bench.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DataDepictQA","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhiyuanyou/DataDepictQA","creator_name":"zhiyuanyou","creator_url":"https://huggingface.co/zhiyuanyou","description":"\n\t\n\t\t\n\t\tDataDepictQA\n\t\n\nDatasets of the papers in DepictQA project:\n\nDepictQA-Wild (DepictQA-v2): paper / project page / code.\nZhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, Tianfan Xue, \"Descriptive Image Quality Assessment in the Wild,\" arXiv preprint arXiv:2405.18842, 2024.\n\nDepictQA-v1: paper / project page / code.\nZhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, Chao Dong, \"Depicting beyond scores: Advancing image quality assessment through multi-modal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zhiyuanyou/DataDepictQA.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","arxiv:2405.18842"],"keywords_longer_than_N":true},
	{"name":"yogera_runyankore_ailab","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Shawal777/yogera_runyankore_ailab","creator_name":"Shawal Mbalire","creator_url":"https://huggingface.co/Shawal777","description":"Shawal777/yogera_runyankore_ailab dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["automatic-speech-recognition","image-to-text","Nyankole","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"midjourney-niji-1m-llavanext","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/midjourney-niji-1m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for midjourney-niji-1m-llavanext\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis is a dataset of 2,079,886 synthetic captions for 1,039,943 images from midjourney-v6-520k-raw and nijijourney-v6-520k-raw. The captions were produced using https://huggingface.co/lmms-lab/llama3-llava-next-8b inferenced in float16 after tags were generated with wd-swinv2-tagger-v3, followed by cleanup and shortening with Meta-Llama-3-8B.\nAll images with metadata are available as MozJPEG encoded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/midjourney-niji-1m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"modern","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATMuS/modern","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","description":"\n\n\t\n\t\t\n\t\tDataset Card for CATMuS Modern and Contemporary (McCATMuS)\n\t\n\nJoin our Discord to ask questions about the dataset: \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, enabling researchers and scholars to analyze vast collections efficiently. Despite significant technological progress, establishing consistent ground truth across projects for HTR tasks, particularly for complex and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/modern.","first_N":5,"first_N_keywords":["image-to-text","French","German","English","Italian"],"keywords_longer_than_N":true},
	{"name":"danbooru2023-florence2-caption","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption","creator_name":"Shih-Ying Yeh","creator_url":"https://huggingface.co/KBlueLeaf","description":"\n\t\n\t\t\n\t\tDanbooru2023 - Florence2 Caption dataset\n\t\n\nThis dataset contains captions of danbooru2023 images generated by microsoft/Florence-2-large \nI use original one with  task token\n\n\t\n\t\t\n\t\tFormat\n\t\n\nparquet:\n\nkey: the danbooru id of the image\nparsed: parsed florence 2 output of the image\n\n\n\t\n\t\t\n\t\tStat\n\t\n\n\n\t\n\t\t\n\t\tMORE_DETAILED_CAPTION\n\t\n\n\nEntries: 7,438,449\nOutput Tokens (Min/Max/Mean/Median):\nFlan T5 Tokenizer: 19/736/120/114\nDFN CLIP Tokenizer: 19/826/108.7/103\nQwen2 Tokenizer:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"DocGenome","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/U4R/DocGenome","creator_name":"Alpha-Innovator Lab","creator_url":"https://huggingface.co/U4R","description":"\n\t\n\t\t\n\t\n\t\n\t\tDocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models\n\t\n\npaper link: DocGenome\nWe present DocGenome, a structured document dataset constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline DocParser. DocGenome features four characteristics:\n\n\nCompleteness: It is the first dataset to structure data from all modalities including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/U4R/DocGenome.","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"svg-stack-labeled","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/svg-stack-labeled","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tSvg Stack - Labeled\n\t\n\nThis dataset consists of the central storage for all datasets related to the SVG Stack dataset. I found it to be lovely, detailed, and of decent to extremely good quality upon observing many different icons and logos during the labeling process.\nThis is the central dataset, and is currently UNDER CONSTRUCTION.  Use with caution, and be aware that the format HAS NOT been frozen. I will make a post announcing when I freeze this dataset, as that will also be the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-labeled.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"neoneye/simon-arc-rle-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wikiart_recaption","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AterMors/wikiart_recaption","creator_name":"Andrea","creator_url":"https://huggingface.co/AterMors","description":"WikiArt Dataset captioned using vikhyatk/moondream2 model with prompt : Generate a short, simple and only visually descriptive caption for this image.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"svg-stack-tmp-alpha-chunk","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tSvg Stack Labeled - Temporary Split Alpha ( Chunk )\n\t\n\nThis dataset is a chunk of SVG Stack Labeled, and was uploaded solely because I lacked reliable high-volume cloud storage at the time, and was going to make the dataset available on HuggingFace in any case.\nHowever, while I will be deleting the now defunct and unused chunks, this one received a few users, and I truly appreciate your usage of my dataets. Thus, this dataset will remain, even as the others perish.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-10","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-50","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-40","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-23","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-14","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-06","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type.\nThe LLM learned some of the types fine. However histograms are causing problems.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. Since this is what my LLM is struggling with.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"housey-home","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/housey-home","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tHousey Home v1 ( DEFUNCT )\n\t\n\nThe data has a flaw, it occurred during the initial synthesis. The erroneous fields have been removed, and the data is currently being selectively re-synthesized.\nAs of 07-15-2024, this dataset is now defunct. It will no longer receive stability, content, or null row fixes. However, as all images are sourced from generative AI models, open source ones at that, I have decided to make this MIT, and will perform tasks upon request if needed. Just ask.\n-<3\n","first_N":5,"first_N_keywords":["unconditional-image-generation","text-to-image","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"Dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VideoGameBunny/Dataset","creator_name":"VideoGameBunny","creator_url":"https://huggingface.co/VideoGameBunny","description":"\n\t\n\t\t\n\t\n\t\n\t\tVideoGameBunny Instruction Following Dataset\n\t\n\n[Website] \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe present a comprehensive dataset of 185,259 high-resolution images from 413 video games, sourced from YouTube videos. This dataset addresses the lack of game-specific instruction-following data and aims to improve the ability of open-source models to understand and respond to video game content.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Composition\n\t\n\nOur dataset includes various types of instructions generated for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VideoGameBunny/Dataset.","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-18","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-recap","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nPhi-3 Vision„ÅßDense Captioning„Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\n‰∏ª„Ç≠„Éº„ÅØphotoid„Åß„Åô„ÅÆ„Åß„ÄÅCommonCatalog CC-BY„Å®ÁµêÂêà„Åô„Çã„Å™„Çä„Åó„Å¶‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ streaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"filtered-coyo-700M-beta","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\n\t\n\t\t\n\t\tDataset Card for filterred-coyo-700M-beta\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe texts in the COYO-700M dataset consist of English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach instance in COYO-700M represents single image-text pair information with meta-attributes:\n{\n  'id': 841814333321,\n  'url': 'https://blog.dogsof.com/wp-content/uploads/2021/03/Image-from-iOS-5-e1614711641382.jpg',\n  'text': 'A Pomsky dog‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Seed","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the seed dataset we used to conduct Google Search.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"A-Bench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/q-future/A-Bench","creator_name":"Q-Future","creator_url":"https://huggingface.co/q-future","description":"Project Page\n\n\n\t\n\t\t\n\t\tGlance at A-Bench Performance\n\t\n\nFor open-source models, LLaVA-NeXT (Qwen-110B) takes the first place. For closed-source models, GEMINI 1.5 PRO takes the first place.\n\n\n\t\n\t\t\n\t\n\t\n\t\tEvaluate your model on A-Bench\n\t\n\nFirst download the dataset and meta information from Huggingface.\nThe imgs.zip contains all the AI-generated images and Abench.json contains all the meta information including the img_path, questions, answers, and categories. The item of Abench.json is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/q-future/A-Bench.","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"anime-with-caption-cc0","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tAnime with caption CC-0 dataset\n\t\n\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„Ç§„É©„Çπ„Éà„Å´ÂØæ„Åô„ÇãÊó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥„Çí\nÂÄ´ÁêÜÁöÑ„Å´Â≠¶Áøí„Åó„ÇÑ„Åô„Åè„Åô„Çã„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ\n„Åì„Åì„Å´Êé≤Ëºâ„Åï„Çå„Å¶„ÅÑ„Çã„Ç§„É©„Çπ„Éà„ÅØËá™ÂæãÁöÑ„Å´AI„Åå‰ΩúÊàê„Åó„Åü„ÇÇ„ÅÆ„Åß„ÅÇ„Çä„ÄÅ\nËëó‰ΩúÊ®©„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åæ„Åü„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÇÇËá™ÂæãÁöÑ„Å´„Å§„Åë„Çâ„Çå„Åü„ÇÇ„ÅÆ„Å™„ÅÆ„Åß„ÄÅ\nËëó‰ΩúÊ®©„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆËëó‰ΩúÊ®©„ÇíÁßÅ„ÅØÊîæÊ£Ñ„Åó„Åæ„Åô„ÄÇ\nÂãùÊâã„Å´‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n\t\n\t\t\n\t\t„É©„Ç§„Çª„É≥„Çπ\n\t\n\n„Éë„Éñ„É™„ÉÉ„ÇØ„Éâ„É°„Ç§„É≥\n\n\t\n\t\t\n\t\t„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊßãÊàê\n\t\n\n„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ‰ª•‰∏ã„ÅÆÂàó„ÅßÊßãÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nimage: Emi 2„Åß„É©„É≥„ÉÄ„É†„Å´ÁîüÊàê„Åó„ÅüÁîªÂÉè\nprompt: Ë®ÄË™û„É¢„Éá„É´„Åß„É©„É≥„ÉÄ„É†„Å´ÁîüÊàê„Åï„Çå„ÅüÁîªÂÉè„ÅÆ„Éó„É≠„É≥„Éó„Éà(„Åü„Å†„Åó„ÄÅÁîªÂÉè„Å®„ÅÇ„Åæ„Çä‰∏ÄËá¥„Åó„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„ÅÇ„Å¶„Å´„Å™„Çâ„Å™„ÅÑ)\nphi3_caption: Phi-3 Vision„ÅßDense captioning„Åó„ÅüÁµêÊûú\nphi3_caption_ja: phi3_caption„ÇíPhi-3 Medium„ÅßÊó•Êú¨Ë™ûË®≥„Åó„ÅüÁµêÊûú\n\n\n\t\n\t\t\n\t\t„Ç§„É©„Çπ„Éà„ÅÆ‰Ωú„ÇäÊñπ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0.","first_N":5,"first_N_keywords":["image-to-text","English","Japanese","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"kassenzettel-synth","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nimalu/kassenzettel-synth","creator_name":"Niklas","creator_url":"https://huggingface.co/nimalu","description":"\n\t\n\t\t\n\t\tkassenzettel-synth\n\t\n\n\n\nThis dataset contains generate images of receipts.\nIt has been generated using github.com/nimalu/kassenzettel.\nThe current version contains 1000 samples.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nFor each instance, there is the underlying data, an image of the receipt, an image of the masks and both images augmented by adding crinkles.\n \n\n","first_N":5,"first_N_keywords":["image-to-text","image-segmentation","German","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Hindi-Captions","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/damerajee/Hindi-Captions","creator_name":"dame rajee","creator_url":"https://huggingface.co/damerajee","description":"\n\t\n\t\t\n\t\tDataset information\n\t\n\nThis dataset is primarily for training a image captioning model ,this model was filtered from damerajee/Hindi-LLaVA-CC3M-Pretrain-595K-3 to only include hindi captions \n\nThe first column Images contains images which are  224 pixels wide and 224 pixels tall\nThe second column Captions contains captions corresponding to the text\n\nThe Average Length of the captions\n\n","first_N":5,"first_N_keywords":["image-to-text","Hindi","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CorDiCas","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/epuertas94/CorDiCas","creator_name":"Elia Puertas Rib√©s","creator_url":"https://huggingface.co/epuertas94","description":"\n\t\n\t\t\n\t\tCorDiCas\n\t\n\nCorDiCas es un prototipo de corpus diacr√≥nico cuyos documentos proceden de una colecci√≥n de m√°s de 120 documentos in√©ditos de car√°cter semiprivado, cuya tem√°tica gira en torno a la sedentarizaci√≥n e inserci√≥n forzosas de la poblaci√≥n gitana durante el siglo XVIII. \nEn la siguiente tabla se ofrece la informaci√≥n estructurada sobre los periodos que se abordan en la colecci√≥n:\n\n\t\n\t\t\nSignatura\nPeriodo\nN.¬∫ textos\n\n\n\t\t\nAMH_01430\n1745 - 1746\n4 textos\n\n\n\n1748\n14 textos\n\n\n\n1749\nM√°s‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/epuertas94/CorDiCas.","first_N":5,"first_N_keywords":["text-classification","token-classification","image-to-text","Spanish","mit"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ja","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Ja\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nLLaVA-JP„ÇíÊîπËâØ„Åó„Åü„É¢„Éá„É´„Å´„Çà„ÇãÁ∞°Êòì„Å™Êó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥1„Å§\nLLaVA-JP„ÇíÊîπËâØ„Åó„Åü„É¢„Éá„É´„Å´„Çà„Çã„Åß„Åç„Çã„Å†„ÅëË©≥Á¥∞„Å™Êó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥3„Å§ (‰∫àÂÆö)\n\n\n\t\n\t\t\n\t\tSample Code\n\t\n\ndf2=pandas.read_csv(\"cc-by-ja.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nfor i,data in enumerate(tqdm(dataset)):\n    data[\"jpg\"].save(f\"/mnt/my_raid/pixart_jp/InternImgs/{i:09}.jpg\")\n\n    data_info.append({\n        \"height\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","Japanese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"living-room-passes","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Nfiniteai/living-room-passes","creator_name":"Nfinite","creator_url":"https://huggingface.co/Nfiniteai","description":"\n\t\n\t\t\n\t\tnfinite-living-room-passes\n\t\n\nVersion of the release: 1.0.0-alphaRelease date: 2024/06/17\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe nfinite-living-room-passes dataset is a dataset of images from 3D models for objects usually found in the living room space. 500 products are available, across 10500 images.  \nEach product has been rendered photo-realistically from a 3D model and is also available as a series of images depicting its normal map, its depth map, and some other information.Those 3D‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nfiniteai/living-room-passes.","first_N":5,"first_N_keywords":["depth-estimation","image-classification","image-segmentation","text-to-image","image-to-text"],"keywords_longer_than_N":true},
	{"name":"gothenburg-price-tag","keyword":"image-to-text","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/fangsonglong/gothenburg-price-tag","creator_name":"Fangsong Long","creator_url":"https://huggingface.co/fangsonglong","description":"fangsonglong/gothenburg-price-tag dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","Swedish","unlicense","< 1K"],"keywords_longer_than_N":true},
	{"name":"flickr8k-pt-br","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ Flickr8K Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nFlickr8K Portuguese Translation, a multimodal dataset for Portuguese image captioning with 8,000 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"flickr30k-pt-br","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ Flickr30K Translated for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nFlickr30K Portuguese Translated, a multimodal dataset for Portuguese image captioning with 31,014 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\nThe dataset is one of the results of work available at:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br.","first_N":5,"first_N_keywords":["text-generation","image-to-text","text-to-image","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"sensor-ocr-benchmark","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/famousdetectiveadrianmonk/sensor-ocr-benchmark","creator_name":"seafog winters","creator_url":"https://huggingface.co/famousdetectiveadrianmonk","description":"\n\t\n\t\t\n\t\tMy Custom Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe original dataset was modified to inserto fake sensor information in bottom of image.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"famousdetectiveadrianmonk/sensor-ocr-benchmark\")\nexample = dataset['train'][0]\nimg = example['pixel_values']\nsensor_zoomin = img.crop((600, 850, 1250, 1050))\n\n\n\t\n\t\t\n\t\n\t\n\t\tAttribution\n\t\n\nThis dataset is based on the original dataset provided by Segments.ai. The original dataset can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/famousdetectiveadrianmonk/sensor-ocr-benchmark.","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"conceptual-captions-cc12m-llavanext","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/conceptual-captions-cc12m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for conceptual-captions-cc12m-llavanext\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of 21,930,344 synthetic captions for 10,965,172 images from conceptual_12m. In the interest of reproducibility, an archive found here on Huggingface was used (cc12m-wds). The captions were produced using llama3-llava-next-8b inferenced in float16, followed by cleanup and shortening with Meta-Llama-3-8B.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThe captions are in English.\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\nAn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/conceptual-captions-cc12m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"TextOCR-GPT4o","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/TextOCR-GPT4o","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for TextOCR-GPT4o\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTextOCR-GPT4o is Meta's TextOCR dataset dataset captioned with emphasis on text OCR using GPT4o. To get the image, you will need to agree to their terms of service.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThe TextOCR-GPT4o dataset is intended for generating benchmarks for comparison of an VLM to GPT4o.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe caption languages are in English, while various texts in images are in many languages such as Spanish, Japanese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/TextOCR-GPT4o.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"flickr-cc-by","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/images9/flickr-cc-by","creator_name":"images","creator_url":"https://huggingface.co/images9","description":"\n\t\n\t\t\n\t\tFlickr-CC-BY Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset includes URLs of images uploaded to Flickr between December 31, 2003, 15:00, and July 27, 2024, 24:00. These URLs are part of JSON response results containing images under the CC-BY-2.0 license. The rights to each image belong to the respective Flickr users.\n\n\t\n\t\t\n\t\tImportant Notices\n\t\n\n\nPersonal Project: This dataset was created as a personal project by Atsuma, driven by individual interest and concern. This dataset has no‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/images9/flickr-cc-by.","first_N":5,"first_N_keywords":["image-to-text","cc0-1.0","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"coco-captions-pt-br","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ COCO Captions Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nCOCO Captions Portuguese Translation, a multimodal dataset for Portuguese image captioning with 123,287 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v68","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v68","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v68.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/Recap-DataComp-1B","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/Recap-DataComp-1B.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"ChartMimic","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChartMimic/ChartMimic","creator_name":"ChartMimic","creator_url":"https://huggingface.co/ChartMimic","description":"\n\n ChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic.\n\nKind Note: ChartMimic has been integrated into VLMEvalKit. Welcome to use ChartMimic through VLMEvalKit! Special thanks to the VLMEvalKit team.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChartMimic/ChartMimic.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Total-Text-Dataset","keyword":"image-to-text","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Voxel51/Total-Text-Dataset","creator_name":"Voxel51","creator_url":"https://huggingface.co/Voxel51","description":"\n\t\n\t\t\n\t\tDataset Card for Total-Text-Dataset\n\t\n\nThe Total-Text consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved\n\nThis is a FiftyOne dataset with 1555 samples.\n\n\t\n\t\t\n\t\n\t\n\t\tInstallation\n\t\n\nIf you haven't already, install FiftyOne:\npip install -U fiftyone\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport fiftyone as fo\nimport fiftyone.utils.huggingface as fouh\n\n# Load the dataset\n# Note: other available arguments include 'max_samples', etc\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Voxel51/Total-Text-Dataset.","first_N":5,"first_N_keywords":["object-detection","image-to-text","English","bsd-3-clause","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"COCO_Person","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamdy20002/COCO_Person","creator_name":"Abdelrahman Hamdy","creator_url":"https://huggingface.co/Hamdy20002","description":"This Dataset is a subsets of COCO 2017 -train- images using \"Crowd\" & \"person\" Labels With the First Caption of Each one\n\nCOCO Summary:\nThe COCO dataset is a comprehensive collection designed for object detection, segmentation, and captioning tasks.\nIt comprises over 200,000 images, encompassing a diverse array of everyday scenes and objects.\nEach image features multiple objects and scenes across 80 distinct object categories, all of which are annotated with descriptive image captions.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"civitai","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigdata-pw/civitai","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","description":"\n\t\n\t\t\n\t\tCivitai Images\n\t\n\nImages+metadata from Civitai\nStats:\n\n~4.1M\n\nFormats:\n\nWebDataset\n10k per shard, ~2GB\njpg + json\n__key__ is Civitai image id\n\n\n\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\n~464k images with no meta field are excluded, this is ~10% of images collected\nFiles for some entries are actually videos, these will be released separately\nCivitai extract metadata on upload, the exact fields in meta will depend on the UI used, some are common e.g. prompt, others are UI specific\nIncludes reaction data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/civitai.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","apache-2.0","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"merit","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/de-Rodrigo/merit","creator_name":"de Rodrigo","creator_url":"https://huggingface.co/de-Rodrigo","description":"\n  \n\n\n\n\t\n\t\t\n\t\tThe MERIT Dataset üéíüìÉüèÜ\n\t\n\nThe MERIT Dataset is a multimodal dataset (image + text + layout) designed for training and benchmarking Large Language Models (LLMs) on Visually Rich Document Understanding (VrDU) tasks. It is a fully labeled synthetic dataset generated using our opensource pipeline available on GitHub. You can explore more details about the dataset and pipeline reading our paper.\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction ‚ÑπÔ∏è\n\t\n\nAI faces some dynamic and technical issues that push‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/de-Rodrigo/merit.","first_N":5,"first_N_keywords":["token-classification","image-to-text","English","Spanish","mit"],"keywords_longer_than_N":true},
	{"name":"MIRB-hf","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VLLMs/MIRB-hf","creator_name":"Train VLLMs","creator_url":"https://huggingface.co/VLLMs","description":"VLLMs/MIRB-hf dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tomg-group-umd/pixelprose","creator_name":"Tom Goldstein's Lab at University of Maryland, College Park","creator_url":"https://huggingface.co/tomg-group-umd","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[ arXiv paper ]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Details\n\t\n\nTotal number of image-caption pairs: 16,896,214 (16.9M)\n\n6,538,898 (6.5M) pairs in the split of CommonPool\n9,066,455 (9.1M) pairs in the split of CC12M\n1,290,861 (1.3M) pairs in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomg-group-umd/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tmp","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YANG-Cheng/tmp","creator_name":"Cheng Yang","creator_url":"https://huggingface.co/YANG-Cheng","description":"\n\n ChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YANG-Cheng/tmp.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/pixelprose","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayuka Jayawardhana‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/pixelprose","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayuka‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestone-horizon/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"VibeEval","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RekaAI/VibeEval","creator_name":"Reka AI","creator_url":"https://huggingface.co/RekaAI","description":"\n\t\n\t\t\n\t\tVibe-Eval\n\t\n\nA benchmark for evaluating multimodal chat models, including especially challenging examples.\n[Link to paper] [Blogpost] [Github]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\nEach example has the following fields:\n\nexample_id: a unique ID for the example\ncategory: the category that this example belongs to, either difficulty-normal or difficulty-hard\nprompt: the user prompt\nreference: a golden reference answer for the prompt\nimage: an image struct (containing bytes and path keys).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RekaAI/VibeEval.","first_N":5,"first_N_keywords":["image-to-text","image-classification","English","Polish","Chinese"],"keywords_longer_than_N":true},
	{"name":"Finna-HKM-images","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NatLibFi/Finna-HKM-images","creator_name":"National Library of Finland","creator_url":"https://huggingface.co/NatLibFi","description":"\n\t\n\t\t\n\t\tOld photographs from Helsinki City Museum\n\t\n\nThis is an image dataset consisting of 5947 old photographs (until 1917) from the collections of the Helsinki City Museum, obtained from the Finna.fi discovery service.\nThe images are intended to be used for different AI/ML tasks such as generating captions or colorizing them.\nThe images themselves are JPEG files under the directory images.\nThe metadata.jsonl file contains metadata about each image, for example descriptive captions (mostly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NatLibFi/Finna-HKM-images.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","Finnish","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Finna-JOKA-images","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NatLibFi/Finna-JOKA-images","creator_name":"National Library of Finland","creator_url":"https://huggingface.co/NatLibFi","description":"\n\t\n\t\t\n\t\n\t\n\t\tOld photographs from Journalistic Picture Archive JOKA\n\t\n\nThis is an image dataset consisting of 4595 old photographs (until 1940) from the collections of the Journalistic Picture Archive JOKA, obtained from the Finna.fi discovery service.\nThe images are intended to be used for different AI/ML tasks such as generating captions or colorizing them.\nThe images themselves are JPEG files under the directory images.\nThe metadata.jsonl file contains metadata about each image, for example‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NatLibFi/Finna-JOKA-images.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","Finnish","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"ru-filtered-web-captions","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DiTy/ru-filtered-web-captions","creator_name":"Dmitry Tishencko","creator_url":"https://huggingface.co/DiTy","description":"\n\t\n\t\t\n\t\tDiTy/ru-filtered-web-captions\n\t\n\n\n\t\n\t\t\n\t\tDataset summary\n\t\n\nThis is a translated Russian¬†part of the filtered web captions.\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n    'caption': 'gladiator standing in a smoke with torch and sword',\n    'url': 'https://thumb9.shutterstock.com/display_pic_with_logo/78238/155376242/stock-photo-gladiator-standing-in-a-smoke-with-torch-and-sword-155376242.jpg',\n    'translated_caption': '–≥–ª–∞–¥–∏–∞—Ç–æ—Ä, —Å—Ç–æ—è—â–∏–π –≤ –¥—ã–º—É —Å —Ñ–∞–∫–µ–ª–æ–º –∏ –º–µ—á–æ–º'\n}\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ncaption:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DiTy/ru-filtered-web-captions.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","feature-extraction","image-feature-extraction","Russian"],"keywords_longer_than_N":true},
	{"name":"anime-caption-danbooru-2021-sfw-5m-hq","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/anime-caption-danbooru-2021-sfw-5m-hq","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for anime-caption-danbooru-2021-sfw-5m-hq\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 5.71 M captions of 1.43 M images from a safe-for-work (SFW) filtered subset of the Danbooru 2021 dataset. There are 4 captions per image: 1 by CogVLM, 1 by llava-v1.6-34b, 1 llava-v1.6-34b cleaned, and 1 llava-v1.6-34b shortened. See the sections below for how they were generated.\nMost captions are substantially larger than 77 tokens and are unsuitable for discrimination using current‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/anime-caption-danbooru-2021-sfw-5m-hq.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-sa-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"pokemon-gpt4-1k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BUAADreamer/pokemon-gpt4-1k","creator_name":"Zhangchi Feng","creator_url":"https://huggingface.co/BUAADreamer","description":"This dataset was modified from diffusers/pokemon-gpt4-captions and contains 1k Pok√©mon-related image-captioning instruction data points.\nYou can organize content in the dataset_info.json in LLaMA Factory like this:\n\"pokemon_1k\": {\n  \"hf_hub_url\": \"BUAADreamer/pokemon-gpt4-1k\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"messages\",\n    \"images\": \"images\"\n  },\n  \"tags\": {\n    \"role_tag\": \"role\",\n    \"content_tag\": \"content\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BUAADreamer/pokemon-gpt4-1k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"UniMER_Dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanderkid/UniMER_Dataset","creator_name":"Bin Wang","creator_url":"https://huggingface.co/wanderkid","description":"\n\t\n\t\t\n\t\tUniMER Dataset\n\t\n\nFor detailed instructions on using the dataset, please refer to the project homepage: UniMERNet Homepage\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe UniMER dataset is a specialized collection curated to advance the field of Mathematical Expression Recognition (MER). It encompasses the comprehensive UniMER-1M training set, featuring over one million instances that represent a diverse and intricate range of mathematical expressions, coupled with the UniMER Test Set, meticulously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanderkid/UniMER_Dataset.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"High-Quality-Synthetic-Images","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Chan-Y/High-Quality-Synthetic-Images","creator_name":"Cihan Yal√ßƒ±n","creator_url":"https://huggingface.co/Chan-Y","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDataset Name: Goldfish High-Quality AI-Generated Images Dataset\nThe Goldfish High-Quality AI-Generated Images Dataset contains a curated collection of high-resolution images. Each image is paired with an AI-generated prompt, specifically crafted to describe the visual content, rather than using the original prompts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nSource: The images were collected from a single high-quality source specializing in AI-generated art.\nResolution: All‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Chan-Y/High-Quality-Synthetic-Images.","first_N":5,"first_N_keywords":["image-to-image","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"vintage-artworks-60k-captioned","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SilentAntagonist/vintage-artworks-60k-captioned","creator_name":"John Smith","creator_url":"https://huggingface.co/SilentAntagonist","description":"This is a dataset consisting of 60k vintage artworks from the 20th century, consisting of vintage pulp, sci-fi and pinup artworks from that era.\nThe dataset has short and long captions for each image, as well as resolution information. The large captions (large_caption column) were made with florence-2-large-ft, and then shortened with llama 3 8b (see short_caption column).\n","first_N":5,"first_N_keywords":["feature-extraction","image-classification","image-feature-extraction","text-to-image","image-to-text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthetic-cyrillic-large","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pumb-ai/synthetic-cyrillic-large","creator_name":"–ü–µ—Ä—à–∏–π –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –ú—ñ–∂–Ω–∞—Ä–æ–¥–Ω–∏–π –ë–∞–Ω–∫","creator_url":"https://huggingface.co/pumb-ai","description":"pumb-ai/synthetic-cyrillic-large dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Ukrainian","Russian","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v18","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v19","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v20","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v21","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v22","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RSTeller_legacy","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","description":"\n\t\n\t\t\n\t\t‚õî Usage Warning\n\t\n\nThis is the legacy version of the RSTeller dataset and is not the latest version referenced in our paper. We are keeping it available here to provide the community with easy access to additional data.\nFor the details and the usage of the dataset, please refer to our github page.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find the dataset and our paper useful, please consider citing our paper:\n@article{ge2025rsteller,\n  title={RSTeller: Scaling up visual language modeling in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v23","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v24","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v25","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v25.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-HTML","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n  üçÉ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nüçÉ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. üçÉ MINT-1T is designed to facilitate research in multimodal pretraining. üçÉ MINT-1T is created by a team from the University of Washington in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML.","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"STAIR-Captions","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shunk031/STAIR-Captions","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","description":"\n\t\n\t\t\n\t\tDataset Card for STAIR-Captions\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nSTAIR Captions is a large-scale dataset containing 820,310 Japanese captions. This dataset can be used for caption generation, multimodal retrieval, and image generation.\n\n\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\n\n\n\n\t\n\t\n\t\n\t\tLanguages\n\t\n\nThe language data in JDocQA is in Japanese (BCP-47 ja-JP).\n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shunk031/STAIR-Captions.","first_N":5,"first_N_keywords":["image-to-text","image-captioning","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"augmented-recap-datacomp-3m","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"This is an experimental augmentation of about 3 million synthetic captions from Recap-Datacomp-1B. This dataset includes about 2 million multilingual captions. \nIt attempts to balance for gender stereotypes, added occupations, race, union membership, and religion to a subsample. We have also performed hair color and eye color balancing. It also includes some permutations of sentence orders, and modificaitons of the number of items (\"Two\" is changed to \"Three\", \"Four\", etc.)\nWe have also run‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"coyo-hd-11m-llavanext","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/coyo-hd-11m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for coyo-hd-11m-llavanext\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of 22,794,288 synthetic captions for 11,397,144 images from coyo-700m. The \"hd\" in the title refers to two aspects: high density and high definition. While large alt-text image pair datasets have many images, only a very small proportion of these images are in higher resolutions and have substantial concept density. For example, many of these datasets consist of more than 50% thumbnail sized or very‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/coyo-hd-11m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"commoncanvas-cc-by-recap-2","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning 2\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nFlorence-2-large-ft„ÅßDense Captioning (More detailed caption) „Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\nstreaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nfor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"llava-med-zh-instruct-60k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BUAADreamer/llava-med-zh-instruct-60k","creator_name":"Zhangchi Feng","creator_url":"https://huggingface.co/BUAADreamer","description":"This Chinese dataset was translated from llava-med using Qwen1.5-14B-Chat and contains 60k medical visual instruction data points.\nYou can organize content in the dataset_info.json in LLaMA Factory like this:\n\"llava_med_zh_60k\": {\n  \"hf_hub_url\": \"BUAADreamer/llava-med-zh-instruct-60k\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"messages\",\n    \"images\": \"images\"\n  },\n  \"tags\": {\n    \"role_tag\": \"role\",\n    \"content_tag\": \"content\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BUAADreamer/llava-med-zh-instruct-60k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Sujet-Finance-Vision-10k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sujet-ai/Sujet-Finance-Vision-10k","creator_name":"Sujet AI","creator_url":"https://huggingface.co/sujet-ai","description":"\n\t\n\t\t\n\t\tSujet Finance Vision 10k Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Sujet Finance Vision 10k dataset is a comprehensive collection of financial document images along with their associated textual annotations. This dataset is specifically designed to facilitate the training and evaluation of Vision-Language Models (VLMs) in recognizing and describing various types of financial documents.\n\n\t\n\t\t\n\t\tImage Characteristics\n\t\n\nThe dataset consists of 9819 handpicked images of financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sujet-ai/Sujet-Finance-Vision-10k.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"NLVR-JA","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/toshi456/NLVR-JA","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","description":"This dataset was created by machine translating \"nlvr\" into Japanese.\nnlvr\nhttps://github.com/lil-lab/nlvr/tree/master/nlvr\n","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Plot2Code","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/Plot2Code","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"\n\t\n\t\t\n\t\tPlot2Code Benchmark\n\t\n\nPlot2Code benchmark is now open-sourced at huggingface (ARC Lab) and GitHub. More information can be found in our paper. \n\n\t\n\t\t\n\t\tWhy we need Plot2Code?\n\t\n\n\nüßê While MLLMs have demonstrated potential in visual contexts, their capabilities in visual coding tasks have not been thoroughly evaluated. Plot2Code offers a platform for comprehensive assessment of these models.\n\nü§ó To enable individuals to ascertain the proficiency of AI assistants in generating code that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/Plot2Code.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"docci_ja","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/toshi456/docci_ja","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","description":"This data was translated from the \"DOCCI\" into Japanese by DeepL\nDOCCI: https://google.github.io/docci/\nLisence\nCC-BY-4.0\n","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"LLaVA-JP-Instruct-108K","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset details\n\t\n\nDataset type: LLaVA JP Instruct 108K created by changing the data sets in Japanese Visual Genome VQA dataset and train data of docci_ja to the LLaVA-Instruct format.\n\n\t\n\t\t\n\t\n\t\n\t\tAcknowledgement\n\t\n\n\nJapanese Visual Genome VQA dataset\nDOCCI\n\n\n\t\n\t\t\n\t\n\t\n\t\tLisence\n\t\n\nApach lisense 2.0\n","first_N":5,"first_N_keywords":["image-to-text","Japanese","apache-2.0","100K<n<1M","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"moondream2-coyo-5M-captions","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/isidentical/moondream2-coyo-5M-captions","creator_name":"Batuhan","creator_url":"https://huggingface.co/isidentical","description":"\n\t\n\t\t\n\t\tMoondream2 COYO-700M 5M subset captions\n\t\n\nA 5-million image, text pair subset of COYO-700M dataset, captioned with Moondream2 (rev=2024-05-08).  Captioning question is Write a long caption for this image given the alt text: {alt_text}.\n\n\t\n\t\t\n\t\tSampling conditions\n\t\n\nRandomly sampled from 5 million images from COYO-700M images that fit to the following filters:\nfilters = [\n    (\"width\", \">=\", 256),\n    (\"height\", \">=\", 256),\n    (\"aesthetic_score_laion_v2\", \">=\", 5.2)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/isidentical/moondream2-coyo-5M-captions.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","visual-question-answering","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"table-vqa","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"VideoGameBunny-Dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/asgaardlab/VideoGameBunny-Dataset","creator_name":"Analytics of Software, Games and Repository Data (ASGAARD) Lab","creator_url":"https://huggingface.co/asgaardlab","description":"\n\t\n\t\t\n\t\tVideoGameBunny Instruction Following Dataset\n\t\n\nPaper - Website\n\n\t\n\t\t\n\t\tOverview\n\t\n\nWe present a comprehensive dataset of 185,259 high-resolution images from 413 video games, sourced from YouTube videos. This dataset addresses the lack of game-specific instruction-following data and aims to improve the ability of open-source models to understand and respond to video game content.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Composition\n\t\n\nOur dataset includes various types of instructions generated for these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/asgaardlab/VideoGameBunny-Dataset.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v32","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v32.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v33","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v33.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v4-rev3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\nTEST rev3. I'm making yet another‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v34","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v34.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v35","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v35.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v36","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v36.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v37","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v37.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v38","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v38.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v39","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v39.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v40","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v40.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v41","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v41.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v42","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v42.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v43","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v43.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"megalith-10m-florence2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aipicasso/megalith-10m-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","description":"\n\t\n\t\t\n\t\tMegalith-10M with Florence-2 Caption\n\t\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis reposity is the supplymentary of Megalith-10M.\nMegalith-10M is an CC-0 like image dataset. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/megalith-10m-florence2\")\n\n\n\t\n\t\n\t\n\t\tHow to get images\n\t\n\ngit lfs install\ngit clone https://huggingface.co/datasets/drawthingsai/megalith-10m‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/megalith-10m-florence2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"soa-full-florence2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aipicasso/soa-full-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","description":"\n\t\n\t\t\n\t\tSmithsonian Open Access Dataset with Florence-2 Caption\n\t\n\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis dataset is made of soa-full.\nsoa-full is an CC-0 image dataset from Smithsonian Open Access. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/soa-full-florence2\")\n\n\n\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nResearch Vision & Language\nDevelop text-to-image model or image-to-text model.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/soa-full-florence2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v44","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v44.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v45","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v45.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v46","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v46.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v47","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v47.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v48","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v48.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v49","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v49.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v50","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v50.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v51","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v51.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v52","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v52.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v53","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v53.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"data","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zahidpichen/data","creator_name":"ninjagamer","creator_url":"https://huggingface.co/zahidpichen","description":"zahidpichen/data dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v54","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v54.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v55","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v55.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v56","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v56.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v57","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v57.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v58","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v58.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v59","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v59","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v59.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v60","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v60","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v60.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v61","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v61","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v61.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v62","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v62","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v62.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"cat_breed","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zahidpichen/cat_breed","creator_name":"ninjagamer","creator_url":"https://huggingface.co/zahidpichen","description":"zahidpichen/cat_breed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"artbench-pd-256x256","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tDataset Card for ArtBench Public Domain 256x256\n\t\n\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis repository is the subset of ArtBench.\nArtBench is the dataset for historical arts such as Art Nouveau and Ukiyo-e.\nI picked up public domain images from ArtBench. Then, I create new dataset.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nYou can use huggingface datasets to download the dataset.\nYou can also download the tar file.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"alfredplpl/artbench-pd-256x256\")\n\n\n\t\n\t\t\n\t\tIntended Use‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v63","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v63","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v63.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v64","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v64","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v64.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v65","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v65","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v65.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v66","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v66","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v66.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v67","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v67","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v67.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v26","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v26.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v27","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v27.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v28","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v28.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v29","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v29","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v29.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v30","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v30","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v30.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v31","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v31","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v31.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v32","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v32.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v33","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v33.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v34","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v34.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v35","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v35.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v36","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v36.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v37","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v37.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v38","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v38.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v39","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v39.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v40","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v40.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v41","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v41.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v42","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v42.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v43","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v43.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nmax_scale=7.\nimage_size: 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v44","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v44.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v18","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v19","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v45","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v45.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v46","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v46.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v20","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v21","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v22","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PIXELPROSE_HU","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU","creator_name":"Obscure Entropy","creator_url":"https://huggingface.co/Obscure-Entropy","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\nThis dataset is an extension of an existing image captioning dataset, enhanced for PixelProse and augmented with Hungarian translations. It provides a valuable resource for researchers and developers working on image captioning, especially those interested in PixelProse and cross-lingual applications. üåê\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics\n\t\n\nWe report below the number of successfully fetched images and the number of failed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Hungarian","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v47","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v47.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"yogera_runyankore_ailab_4_0_1","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Shawal777/yogera_runyankore_ailab_4_0_1","creator_name":"Shawal Mbalire","creator_url":"https://huggingface.co/Shawal777","description":"Shawal777/yogera_runyankore_ailab_4_0_1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["automatic-speech-recognition","image-to-text","Nyankole","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v48","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v48.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v49","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v49.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ArXiv-tables","keyword":"table-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/staghado/ArXiv-tables","creator_name":"Said Taghadouini","creator_url":"https://huggingface.co/staghado","description":"\n\t\n\t\t\n\t\tArxiv-tables Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Arxiv-tables dataset is a collection of tables extracted from scientific papers published on arXiv, primarily focused on ML papers. It includes both the LaTeX source of the tables and their corresponding rendered images from the PDF versions of the papers.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThis dataset can support several tasks, including but not limited to:\n\nTable structure recognition\nLaTeX to image generation for tables\nImage-to-LaTeX‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/staghado/ArXiv-tables.","first_N":5,"first_N_keywords":["table-to-text","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ArXiv-tables","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/staghado/ArXiv-tables","creator_name":"Said Taghadouini","creator_url":"https://huggingface.co/staghado","description":"\n\t\n\t\t\n\t\tArxiv-tables Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Arxiv-tables dataset is a collection of tables extracted from scientific papers published on arXiv, primarily focused on ML papers. It includes both the LaTeX source of the tables and their corresponding rendered images from the PDF versions of the papers.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThis dataset can support several tasks, including but not limited to:\n\nTable structure recognition\nLaTeX to image generation for tables\nImage-to-LaTeX‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/staghado/ArXiv-tables.","first_N":5,"first_N_keywords":["table-to-text","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v23","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v50","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v50.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v51","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v51.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"license_plates","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PawanKrGunjan/license_plates","creator_name":"Pawan Kumar Gunjan","creator_url":"https://huggingface.co/PawanKrGunjan","description":"PawanKrGunjan/license_plates dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v52","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v52.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v53","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v53.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-20.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v54","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v54.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v55","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v55.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"coda-lm","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KaiChen1998/coda-lm","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","description":"\n\t\n\t\t\n\t\n\t\n\t\tCODA-LM Dataset Card\n\t\n\nCODA-LM is the multi-modal version of the CODA dataset, used in the CODA-LM paper. Both English and Chinese annotations are available. Check detailed usage in our Github repo.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@article{li2024automated,\n  title={Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases},\n  author={Li, Yanze and Zhang, Wenhua and Chen, Kai and Liu, Yanxin and Li, Pengxiang and Gao, Ruiyuan and Hong, Lanqing and Tian, Meng and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KaiChen1998/coda-lm.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"lexica_dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vera365/lexica_dataset","creator_name":"Xinyue Shen","creator_url":"https://huggingface.co/vera365","description":"\n\t\n\t\t\n\t\tLexicaDataset\n\t\n\nLexicaDataset is a large-scale text-to-image prompt dataset shared in [USENIX'24] Prompt Stealing Attacks Against Text-to-Image Generation Models.\nIt contains 61,467 prompt-image pairs collected from Lexica.\nAll prompts are curated by real users and images are generated by Stable Diffusion.\nData collection details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tData Splits\n\t\n\nWe randomly sample 80% of a dataset as the training dataset and the rest 20% as the testing dataset.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vera365/lexica_dataset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"xm3600","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xm3600","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\nIt also includes the image features as PIL Image and has a uniform and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600.","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"xm3600_1k","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xm3600_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600 - 1K Split\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a 1K split of XM3600!\n\t\n\nFor this, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600_1k.","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"xflickrco","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xflickrco","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"floschne/xflickrco dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","German","English","Spanish","Indonesian"],"keywords_longer_than_N":true},
	{"name":"pentomino-easy-vsft","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Koshti10/pentomino-easy-vsft","creator_name":"Koshti","creator_url":"https://huggingface.co/Koshti10","description":"\n\t\n\t\t\n\t\tIndividual Module\n\t\n\nThis dataset is created for instruction tuning llava models based on the dataset created here - llava-instruct-mix-vsft\nThis dataset is based on a Pentomino game - More details -> github\n","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ViQuAE-JA","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/toshi456/ViQuAE-JA","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","description":"This dataset was created by machine translating \"ViQuAE\" into Japanese.\noriginal_answer_ja translated from original_answer. I didn't translate answer.\nViQuAE: https://github.com/PaulLerner/ViQuAE\n","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ext","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Extention\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nPhi-3 Vision„ÅßDense Captioning„Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\nËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÇíPhi-3 Medium„ÅßÊó•Êú¨Ë™ûÂåñ„Åó„ÅüÊó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\n‰∏ª„Ç≠„Éº„ÅØphotoid„Åß„Åô„ÅÆ„Åß„ÄÅCommonCatalog CC-BY„Å®ÁµêÂêà„Åô„Çã„Å™„Çä„Åó„Å¶‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\nstreaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tLicense\n\t\n\nÁîªÂÉè„ÅåCC BY„Å™„Åü„ÇÅ„ÄÅ„Çè„Åã„Çä„ÇÑ„Åô„ÅèCC BY„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3-ja.csv\")\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"image-textualization","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/image-textualization","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tImage-Textualization Dataset\n\t\n\nExciting to announce the open-sourcing of our Image-Text Matching Dataset, which consists of 220K image-text pairs. We also release fine-grained annotations, which may be helpful for many downstream tasks.\nThis dataset is designed to facilitate research and development in the field of large mutimodal language model, particularly for tasks such as image captioning, visual question answering, and multimodal understanding.\nNote that our framework can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/image-textualization.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"Mixed_VQA_GenQA_EvalQA_1.5M","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","description":"This repository contains the data for the paper LOVA3: Learning to Visual Question Answering, Asking and Assessment.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tüéì Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuan Zhao and Pan Zhou and Difei Gao and Zechen Bai and Mike Zheng Shou},\n    booktitle={The Thirty-eighth Annual Conference on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"xflickrco_1k","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/floschne/xflickrco_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","description":"floschne/xflickrco_1k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","German","English","Spanish","Indonesian"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v56","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v56.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v57","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v57.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v58","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v58.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v69","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v69","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v69.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v70","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v70","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v70.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v71","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v71","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v71.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Wikipedia-Vision-JA","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/turing-motors/Wikipedia-Vision-JA","creator_name":"Turing Inc.","creator_url":"https://huggingface.co/turing-motors","description":"\n\t\n\t\t\n\t\tDataset Card for Wikipedia-Vision-JA\n\t\n\n\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe Wikipedia-Vision-JA is a Vision Language Model dataset generated from Japanese Wikipedia, containing 1.6M pairs of images, captions, and descriptions.\nThis dataset itself does not contain raw image data. Instead, an image_url is provided for each item.\n\n\t\n\t\t\n\t\tFormat\n\t\n\nWikipedia_Vision_JA.jsonl contains JSON-formatted rows with the following keys:\n\nkey: Unique JSON ID\ncaption: Short caption for the image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/turing-motors/Wikipedia-Vision-JA.","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-sa-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v72","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v72","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v72.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v73","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v73","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v73.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v74","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v74","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v74.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v75","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v75","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v75.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v76","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v76","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v76.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wit","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/wit","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\n  WITT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve images based on multilingual descriptions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://proceedings.mlr.press/v162/bugliarello22a/bugliarello22a.pdf\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"WITT2IRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/wit.","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v77","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v77","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v77.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 6\n\t\n\nOnly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"test-big-dataset","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huggingface/test-big-dataset","creator_name":"Hugging Face","creator_url":"https://huggingface.co/huggingface","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huggingface/test-big-dataset.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v78","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v78","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v78.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v79","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v79","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v79.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v80","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v80","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v80.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v81","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v81","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v81.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v82","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v82","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v82.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v83","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v83","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v83.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v84","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v84","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v84.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v85","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v85","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v85.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pexels-568k-internvl2","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/pexels-568k-internvl2","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for pexels-568k-internvl2\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 567,573 synthetic captions for the images found in ptx0/photo-concept-bucket. The captions were produced using OpenGVLab/InternVL2-40B-AWQ. The dataset was grounded for captioning using the tags originally listed.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text is in English, but occasionally text in images in other languages is transcribed.\n\n\t\n\t\t\n\t\tIntended Usage\n\t\n\nTraining text-to-image models and other machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/pexels-568k-internvl2.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v86","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v86","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v86.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v87","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v87","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v87.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v88","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v88","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v88.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v89","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v89","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v89.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v90","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v90","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v90.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v91","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v91","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v91.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v92","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v92","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v92.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v93","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v93","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v93.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v94","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v94","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v94.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v95","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v95","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v95.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v96","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v96","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v96.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v97","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v97","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v97.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v98","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v98","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v98.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v99","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v99","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v99.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v100","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v100","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v100.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v101","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v101","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v101.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v102","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v102","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v102.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v103","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v103","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v103.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v104","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v104","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v104.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"image-text","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SkillFi/image-text","creator_name":"Alex","creator_url":"https://huggingface.co/SkillFi","description":"SkillFi/image-text dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Sofie","keyword":"image-to-text","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Sofie","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Samael1976","keyword":"image-to-text","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Samael1976","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Hampsty","keyword":"image-to-text","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Hampsty","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v105","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v105","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v105.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v106","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v106","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v106.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-JohnSlegers","keyword":"image-to-text","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-JohnSlegers","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\n\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v107","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v107","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v107.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v108","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v108","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v108.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v109","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v109","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v109.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PELLET-Casimir-Marius-line","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/PELLET-Casimir-Marius-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\n\t\n\t\tPELLET Casimir Marius - Line level\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe PELLET Casimir Marius dataset includes 100 annotated French letters written between 1914 and 1918.\nAnnotations were done at line-level and all images do not have any text.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\n{\n  'image':‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/PELLET-Casimir-Marius-line.","first_N":5,"first_N_keywords":["image-to-text","French","mit","Image","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"flickr-megalith-10m-internvl2-multi-caption","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for flickr-megalith-10m-internvl2-multi-caption\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is approximately 57.3 million synthetic captions for the images found in madebyollin/megalith-10m. \nIt includes the following captions:\n\nInternVL2 8B long captions (by CaptionEmporium)\nInternVL2 8B short captions (by CaptionEmporium)\nFlorence2 long captions (by aipicasso)\nFlorence2 short captions (by CaptionEmporium)\nShareCaptioner long captions (by drawthingsai)\nShareCaptioner short‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v110","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v110","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v110.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v111","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v111","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v111.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v112","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v112","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v112.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"image-captioning-FACAD-100","keyword":"image-to-text","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Luna288/image-captioning-FACAD-100","creator_name":"Luna","creator_url":"https://huggingface.co/Luna288","description":"Luna288/image-captioning-FACAD-100 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","bsd-3-clause","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"image-captioning-FACAD-100","keyword":"image-to-text","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Luna288/image-captioning-FACAD-100","creator_name":"Luna","creator_url":"https://huggingface.co/Luna288","description":"Luna288/image-captioning-FACAD-100 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","bsd-3-clause","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v113","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v113","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v113.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v114","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v114","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v114.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v115","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v115","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v115.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v116","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v116","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v116.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v117","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v117","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v117.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v118","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v118","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v118.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v119","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v119","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v119.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"image-captioning-FACAD-base","keyword":"image-to-text","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Luna288/image-captioning-FACAD-base","creator_name":"Luna","creator_url":"https://huggingface.co/Luna288","description":"Luna288/image-captioning-FACAD-base dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","bsd-3-clause","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v120","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v120","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v120.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v121","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v121","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v121.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v122","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v122","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v122.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v123","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v123","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v123.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-100K","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nRecap-DataComp-100K is a subset of UCSC-VLAA/Recap-DataComp-1B. \nThis dataset aims to ease the development of vision-language models by providing a readily-available small collection of image-text pairs.\nUse this dataset for sanity checks, developing POCs, or other quick multimodal dev. For serious model training please refer to the original repo linked above.  \n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nAlways cite the original authors . I've copied their citation info here for your‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v124","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v124","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v124.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v125","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v125","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v125.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v126","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v126","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v126.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v127","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v127","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v127.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v128","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v128","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v128.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v129","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v129","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v129.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v130","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v130","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v130.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v131","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v131","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v131.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v132","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v132","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v132.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v133","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v133","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v133.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v134","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v134","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v134.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Facecaption-15M-Embeddings","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFacecaption-15M-Embeddings\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\nWe chose about 5M image-text pairs with the highest resolution from Facecaption-15M, extracted the embeddings of the [CLS]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v135","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v135","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v135.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Video-Detailed-Caption","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/Video-Detailed-Caption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\n\n\t\n\t\t\n\t\tVideo Detailed Caption Benchmark\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmark Collection and Processing\n\t\n\nWe building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. Structured detailed captions construction pipeline. We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/Video-Detailed-Caption.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v136","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v136","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v136.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v137","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v137","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v137.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v138","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v138","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v138.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v139","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v139","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v139.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v140","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v140","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v140.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v141","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v141","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v141.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"image-captioning-FACAD-small","keyword":"image-to-text","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/Luna288/image-captioning-FACAD-small","creator_name":"Luna","creator_url":"https://huggingface.co/Luna288","description":"Luna288/image-captioning-FACAD-small dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","bsd-3-clause","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v142","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v142","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v142.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v143","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v143","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v143.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v18","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v19","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v144","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v144","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v144.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Character‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage isze: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hico_det","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zhimeng/hico_det","creator_name":"Zhimeng Guo","creator_url":"https://huggingface.co/zhimeng","description":"\n\t\n\t\t\n\t\tDataset Card for HICO-DET Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHICO-DET is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zhimeng/hico_det.","first_N":5,"first_N_keywords":["object-detection","image-feature-extraction","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v145","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v145","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v145.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v146","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v146","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v146.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v147","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v147","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v147.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 3-12.\noutput pattern image size: 1-4.\npixel count: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v148","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v148","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v148.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v149","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v149","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v149.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v20","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v150","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v150","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v150.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v151","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v151","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v151.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v21","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"laion_cc_sbu","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iammytoo/laion_cc_sbu","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","description":"\n\t\n\t\t\n\t\tLLaVA-1.5 Test Images Not in LAION2B\n\t\n\nThis dataset contains validation and test images from various vision-language datasets that are commonly used to evaluate LLaVA-1.5 and similar models, but are not present in the LAION2B dataset.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"iammytoo/laion_cc_sbu\")\n\n","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1‚Äì4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict object‚Äìsurface‚Äìscene compatibility mappings.\n\nCurated by: Bodhisatta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"video-SALMONN_2_testset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsinghua-ee/video-SALMONN_2_testset","creator_name":"Electronic Engineering @Tsinghua University","creator_url":"https://huggingface.co/tsinghua-ee","description":"\n\t\n\t\t\n\t\tvideo-SALMONN 2 Benchmark\n\t\n\nGithub Link\nPaper Link\n\nGenerate the caption corresponding to the video and the audio with video_salmonn2_test.json\nOrganize your results in the format like the following example:\n\n[\n    {\n        \"id\": [\"0.mp4\"], \n        \"pred\": \"Generated Caption\"\n    }\n]\n\n\nReplace res_file in eval.py with your result file.\nRun python3 eval.py\n\n","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v156","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v156","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v156.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v152","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v152","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v152.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthchat","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nnethercott/synthchat","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","description":"\n\n\t\n\t\t\n\t\tRendered synthetic chats from llama3.1\n\t\n\nThis dataset contains 2.2k screenshots of multi-turn conversations generated by Llama-3.1-70B-Instruct. Each conversation consists of 3-4 short exchanges between a User and an AI Assistant about a certain topic.\nThe original dataset comprising of pure text exchanges can be found here: HuggingFaceTB/everyday-conversations-llama3.1-2k\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nThis dataset aims to improve the OCR performance of vision-language models in terms of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/synthchat.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v153","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v153","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v153.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v157","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v157","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v157.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v18","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 8-16.\nrectangle size: 3-7.\nnumber of rects: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v158","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v158","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v158.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v159","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v159","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v159.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"cool_images_urban_and_nature","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexandrosChariton/cool_images_urban_and_nature","creator_name":"Alexandros Chariton","creator_url":"https://huggingface.co/AlexandrosChariton","description":"This is an amazing dataset. From pythess with love.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v161","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v161","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v161.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v18","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v160","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v160","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v160.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish-detailed-captions","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish-detailed-captions","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"Detailed captions were genereted by gpt-4o-mini using OpenAI API.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÃàruÃàntuÃàlerden Otomatik TuÃàrkcÃße AcÃßƒ±klama Olu≈üturma ƒ∞cÃßin Bir Denekta≈üƒ± Veri KuÃàmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal ƒ∞≈üleme ve ƒ∞leti≈üim Uygulamalarƒ± Kurultayƒ± (SIU 2016), Zonguldak, Mayis 2016\n\n","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish-detailed-captions","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish-detailed-captions","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"Detailed captions were genereted by gpt-4o-mini using OpenAI API.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÃàruÃàntuÃàlerden Otomatik TuÃàrkcÃße AcÃßƒ±klama Olu≈üturma ƒ∞cÃßin Bir Denekta≈üƒ± Veri KuÃàmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal ƒ∞≈üleme ve ƒ∞leti≈üim Uygulamalarƒ± Kurultayƒ± (SIU 2016), Zonguldak, Mayis 2016\n\n","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"arabic-img2md","keyword":"image-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MohamedRashad/arabic-img2md","creator_name":"Mohamed Rashad","creator_url":"https://huggingface.co/MohamedRashad","description":"\n\t\n\t\t\n\t\tArabic Img2MD\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe arabic-img2md dataset consists of 15,000 examples of PDF pages paired with their Markdown counterparts. The dataset is split into:\n\nTrain: 13,700 examples\nTest: 1,520 examples\n\nThis dataset was created as part of the open-source research project Arabic Nougat to enable OCR and Markdown extraction from Arabic documents. It contains mostly Arabic text but also includes examples with English text.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThe dataset was used to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MohamedRashad/arabic-img2md.","first_N":5,"first_N_keywords":["image-to-text","Arabic","gpl-3.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"laion-pop-llama3.2-11b","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/laion-pop-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for laion-pop-llama3.2-11b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 1,580,595 new synthetic captions for the images found in laion/laion-pop. The dataset was restricted to SFW-only images by filtering out every image with a nsfw_prediction greater than or equal to 0.995. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using meta-llama/Llama-3.1-8B-Instruct The dataset was grounded for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/laion-pop-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"EvalQABench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hhenryz/EvalQABench","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","description":"This repository contains the data for LOVA3: Learning to Visual Question Answering, Asking and Assessment. \nLOVA3 is a framework designed to equip MLLMs with the capabilities to answer, ask, and assess questions in the context of images.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tüéì Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuan‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/EvalQABench.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"webcode2m_purified","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xcodemind/webcode2m_purified","creator_name":"xcodemind","creator_url":"https://huggingface.co/xcodemind","description":"WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs\nFeatures:\n\nimage: the screenshot of the webpage.\nbbox: the layout information, i.e., the bounding boxes (Bbox) of all the elements in the webpage, which contains the size, position, and hierarchy information. \ntext: the webpage code text including HTML/CSS code.\nscale: the scale of the screenshot, in the format [width, height].\nlang: the main language of the text content displayed on the rendered page (excluding HTML/CSS‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xcodemind/webcode2m_purified.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"PENCIL","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kausthubkannan17/PENCIL","creator_name":"Kausthub Kannan","creator_url":"https://huggingface.co/kausthubkannan17","description":"kausthubkannan17/PENCIL dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"znanio-images","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/znanio-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 19,060 educational images from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-images.","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"znanio-videos","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/znanio-videos","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Videos\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 6,653 educational videos from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-videos.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"VideoChat2","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shenxq/VideoChat2","creator_name":"Xiaoqian Shen","creator_url":"https://huggingface.co/shenxq","description":"Video training data of LongVU downloaded from\nhttps://huggingface.co/datasets/OpenGVLab/VideoChat2-IT\n\n\t\n\t\t\n\t\tVideo\n\t\n\nPlease download the original videos from the provided links:\n\nBDD100K: bdd.zip\nShareGPTVideo: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction/tree/main/train_300k\nCLEVRER: clevrer_qa.zip\nDiDeMo: didemo.zip\nEgoQA: https://huggingface.co/datasets/ynhe/videochat2_data/resolve/main/egoqa_split_videos.zipKinetics-710: k400.zip\nMovieChat: moviechat.zip‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shenxq/VideoChat2.","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"KOFFVQA_Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maum-ai/KOFFVQA_Data","creator_name":"maum-ai","creator_url":"https://huggingface.co/maum-ai","description":"\n\t\n\t\t\n\t\tAbout this data\n\t\n\nKOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language\nKOFFVQA is a general-purpose VLM benchmark in the Korean language. For more information, refer to our leaderboard page and the official evaluation code.\nThis contains the data for the benchmark consisting of images, their corresponding questions, and response grading criteria.  The benchmark focuses on free-form visual question answering, evaluating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maum-ai/KOFFVQA_Data.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"temporal-vqa","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fazliimam/temporal-vqa","creator_name":"Fazli Imam","creator_url":"https://huggingface.co/fazliimam","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Temporal-VQA dataset is a challenging benchmark designed to evaluate the temporal reasoning capabilities of Multimodal Large Language Models (MLLMs) in tasks requiring visual temporal understanding. It emphasizes real-world temporal dynamics through two core evaluation tasks:- \n\nTemporal Order Understanding: This task presents MLLMs with temporally consecutive frames from video sequences. The models must analyze and determine the correct sequence of events‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fazliimam/temporal-vqa.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"home_decoration_objects_images","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/home_decoration_objects_images","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 5125 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 5125\nAverage words in long description: 18.1\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/home_decoration_objects_images.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"home_decoration_objects_images","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/home_decoration_objects_images","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 5125 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 5125\nAverage words in long description: 18.1\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/home_decoration_objects_images.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"men_women_children_wearing_clothes","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 6979 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 6979\nAverage words in long description: 17.3\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"men_women_children_wearing_clothes","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 6979 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 6979\nAverage words in long description: 17.3\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"clothes_for_men_women_children","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/clothes_for_men_women_children","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 3082 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 3082\nAverage words in long description: 17.5\nAverage words in short description: 8.8\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/clothes_for_men_women_children.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"clothes_for_men_women_children","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AntZet/clothes_for_men_women_children","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 3082 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 3082\nAverage words in long description: 17.5\nAverage words in short description: 8.8\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/clothes_for_men_women_children.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"textures3","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DeathDaDev/textures3","creator_name":"Death the Developer","creator_url":"https://huggingface.co/DeathDaDev","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the third iteration and official release of a dataset curated to power the Materializer model for Blender. The dataset contains a range of labeled texture images that were sourced from ambientCG under their Creative Commons CC0 1.0 Universal License. These textures are designed to help in the classification of various material maps, which are essential for creating realistic 3D materials in Blender.\n\n\t\n\t\t\n\t\tFuture Plans\n\t\n\nThe dataset is still evolving, and I‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeathDaDev/textures3.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"nocaps-pt-br","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/nocaps-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ nocaps Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nnocaps Portuguese Translation, a multimodal dataset for Portuguese image captioning benchmark, each image accompanied by ten descriptive captions\nthat have been generated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the Dataset\n\t\n\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/nocaps-pt-br.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"furry-e621-safe-llama3.2-11b","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/furry-e621-safe-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tfurry-e621-safe-llama3.2-11b: A new anthropomorphic art dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 2,987,631 synthetic captions for 995,877 images found in e921, which is just e621 filtered to the \"safe\" tag. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using meta-llama/Llama-3.1-8B-Instruct The dataset was grounded for captioning using the ground truth tags on every post categorized and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/furry-e621-safe-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"VL-RewardBench","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MMInstruction/VL-RewardBench","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/VL-RewardBench.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mscoco","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/romrawinjp/mscoco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","description":"\n\t\n\t\t\n\t\tCommon Objects in Context (COCO) Dataset\n\t\n\nThis dataset is English captions of COCO dataset. \nThe splits in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.\n\n\t\n\t\t\n\t\n\t\n\t\tReference\n\t\n\n@misc{lin2015microsoftcococommonobjects‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/mscoco.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"multilingual-coco","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/romrawinjp/multilingual-coco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","description":"\n\t\n\t\t\n\t\tMultilingual Common Objects in Context (COCO) Dataset\n\t\n\nThis dataset is a collection of multiple language open-source captions of COCO dataset. \nThe split in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/multilingual-coco.","first_N":5,"first_N_keywords":["image-to-text","English","Thai","Russian","Japanese"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Opendoc1-Analysis-Recognition","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Opendoc1-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpendoc1-Analysis-Recognition Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Opendoc1-Analysis-Recognition dataset is designed for tasks involving image-to-text, text classification, and image feature extraction. It contains images paired with class labels, making it suitable for vision-language tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModalities: Image\nLanguages: English\nSize: Approximately 1,000 samples (n=1K)\nTags: image, analysis, vision-language\nLicense: Apache 2.0\n\n\n\t\n\t\t\n\t\tTasks\n\t\n\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Opendoc1-Analysis-Recognition.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Opendoc2-Analysis-Recognition","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Opendoc2-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpendoc2-Analysis-Recognition Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Opendoc2-Analysis-Recognition dataset is a collection of data designed for tasks involving image analysis and recognition. It is suitable for various machine learning tasks, including image-to-text conversion, text classification, and image feature extraction.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModalities: Likely includes images and associated labels (specific modalities can be confirmed on the dataset's page).\nLanguages:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Opendoc2-Analysis-Recognition.","first_N":5,"first_N_keywords":["image-to-text","text-retrieval","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TextOCR_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tText OCR\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/textocr/metafile.yml\nName: 'Text OCR'\nPaper:\n  Title: 'TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text'\n  URL: https://openaccess.thecvf.com/content/CVPR2021/papers/Singh_TextOCR_Towards_Large-Scale_End-to-End_Reasoning_for_Arbitrary-Shaped_Scene_Text_CVPR_2021_paper.pdf\n  Venue: CVPR\n  Year: '2021'\n  BibTeX: '@inproceedings{singh2021textocr,\n    title={{TextOCR}:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR.","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"dalle3-llama3.2-11b","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for dalle3-llama3.2-11b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 3,577,716 new synthetic captions for the 1,192,572 images found in ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions. The dataset was filtered for duplicates and then re-encoded with JPEGXL lossless or lossy depending on the source. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CUPUM/mid-space","creator_name":"CUPUM","creator_url":"https://huggingface.co/CUPUM","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CUPUM/mid-space.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LAION-SG","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mengcy/LAION-SG","creator_name":"Chenye Meng","creator_url":"https://huggingface.co/mengcy","description":"\n\t\n\t\t\n\t\tDataset Card for LAION-SG\n\t\n\n\n\n\n LAION-SG is a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\n\n\n\n\nLanguage(s) : All of annotations use English as primary language.\n\nLicense: MIT License.\n\n\nRepository: https://github.com/mengcye/LAION-SG?tab=readme-ov-file\n\nPaper:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mengcy/LAION-SG.","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"HRVideoBench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/HRVideoBench","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tHRVideoBench\n\t\n\nThis repo contains the test data for HRVideoBench, which is released under the paper \"VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation\". VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\nüåê Homepage | üìñ arXiv | üíª GitHub | ü§ó VISTA-400K | ü§ó Models | ü§ó HRVideoBench‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/HRVideoBench.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"biomed-visual-instructions","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AdaptLLM/biomed-visual-instructions","creator_name":"AdaptLLM","creator_url":"https://huggingface.co/AdaptLLM","description":"\n\t\n\t\t\n\t\tAdapting Multimodal Large Language Models to Domains via Post-Training\n\t\n\nThis repos contains the biomedicine visual instructions for post-training MLLMs in our paper: On Domain-Specific Post-Training for Multimodal Large Language Models.\nThe main project page is: Adapt-MLLM-to-Domains\n\n\t\n\t\t\n\t\n\t\n\t\tData Information\n\t\n\nUsing our visual instruction synthesizer, we generate visual instruction tasks based on the image-caption pairs from PubMedVision (referred to as PMC_refined in our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AdaptLLM/biomed-visual-instructions.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ICDAR2015_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/ICDAR2015_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/icdar2015/metafile.yml\nName: 'Incidental Scene Text IC15'\nPaper:\n  Title: ICDAR 2015 Competition on Robust Reading\n  URL: https://rrc.cvc.uab.es/files/short_rrc_2015.pdf\n  Venue: ICDAR\n  Year: '2015'\n  BibTeX: '@inproceedings{karatzas2015icdar,\n  title={ICDAR 2015 competition on robust reading},\n  author={Karatzas, Dimosthenis and Gomez-Bigorda, Lluis and Nicolaou, Anguelos and Ghosh, Suman and Bagdanov, Andrew and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/ICDAR2015_OCR.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"IIIT5K_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/IIIT5K_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/iiit5k/metafile.yml\nName: 'IIIT5K'\nPaper:\n  Title: Scene Text Recognition using Higher Order Language Priors\n  URL: http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/Home/mishraBMVC12.pdf\n  Venue: BMVC\n  Year: '2012'\n  BibTeX: '@InProceedings{MishraBMVC12,\n  author    = \"Mishra, A. and Alahari, K. and Jawahar, C.~V.\",\n  title     = \"Scene Text Recognition using Higher Order Language Priors\",\n  booktitle = \"BMVC\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/IIIT5K_OCR.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"7SEG_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/7SEG_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nTRAIN ONLY Synthetic dataset of seven-segment indicators. Useful for detecting numbers on devices.\n","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"LongVA-TPO-10k","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ruili0/LongVA-TPO-10k","creator_name":"Rui Li","creator_url":"https://huggingface.co/ruili0","description":"\n \n \n\n\n\t\n\t\n\t\n\t\t10kTemporal Preference Optimization Dataset for LongVA\n\t\n\nLongVA-TPO-10k, introduced by paper Temporal Preference Optimization for Long-form Video Understanding\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"vibeeval_greek","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ilsp/vibeeval_greek","creator_name":"Institute for Language and Speech Processing","creator_url":"https://huggingface.co/ilsp","description":"\n\t\n\t\t\n\t\tDataset Card for Vibe-Eval Greek\n\t\n\nThe Vibe-Eval Greek dataset is a benchmark of 269 examples for evaluating multimodal chat models, including especially challenging examples. It has been manually translated into Greek from the VibeEval dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nEach example has the following fields:\n\nmedia_url: a URL where the file is hosted publicly\nexample_id: a unique ID for the example\ncategory: the category that this example belongs to, either difficulty-normal or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilsp/vibeeval_greek.","first_N":5,"first_N_keywords":["image-to-text","image-classification","monolingual","Greek","English"],"keywords_longer_than_N":true},
	{"name":"MSTS","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/felfri/MSTS","creator_name":"Felix Friedrich","creator_url":"https://huggingface.co/felfri","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThe MSTS dataset contains content that may be offensive or upsetting in nature. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matter. \nPlease only engage with the data in accordance with your own personal risk tolerance. The data are intended for research purposes, especially research that can make models less harmful.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/felfri/MSTS.","first_N":5,"first_N_keywords":["image-text-to-text","English","Arabic","French","German"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"ContextualBench","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ToughStone/ContextualBench","creator_name":"Hongxi","creator_url":"https://huggingface.co/ToughStone","description":"\n  Challenging and Enhancing the Reasoning Capacity of Multimodal LLMs in Context-violating Images\n      \n\n    Hongxi¬†Li,¬†\n    Yuyang¬†Chen,¬†\n    Yayun¬†Qi,¬†\n    Xinxiao¬†Wu,¬†\n¬†Beijing Institute of Technology\narXiv 2024\nüåéWebsite (Comming soon) |\nüßë‚ÄçüíªCode |\nüìÑarXiv  (Comming soon) |\nüèÜ Leaderboard\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nContextualBench consists of 6(categories) √ó 12 (instances) = 72 context instances, with each context instances containing 7 context-consistent images and 7‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ToughStone/ContextualBench.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"aircraft-images","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/aircraft-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for High-Resolution Aircraft Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 165,340 high-resolution aircraft images collected from the internet, along with machine-generated captions. The captions were generated using Gemini Flash 1.5 AI model and are stored in separate text files matching the image filenames.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All image captions are in English\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/aircraft-images.","first_N":5,"first_N_keywords":["image-classification","image-to-text","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"SynCap-Flickr8k","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kargwalaryan/SynCap-Flickr8k","creator_name":"Aryan Kargwal","creator_url":"https://huggingface.co/kargwalaryan","description":"\n\t\n\t\t\n\t\tSynCap-Flickr8k\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe SynCap-Flickr8k is a unique collection designed to benchmark Vision Language Models (VLMs) in the image captioning task. This dataset was created using the Flickr8k dataset, which contains 8,000 images, each accompanied by five human-generated captions. By leveraging advanced models such as GPT-4o and LLaMA 3.2, we generated synthetic captions that enhance the understanding of how well VLMs can interpret and describe visual content.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kargwalaryan/SynCap-Flickr8k.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"balinese-carving-dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aegishield/balinese-carving-dataset","creator_name":"Bagus Prasetyo","creator_url":"https://huggingface.co/aegishield","description":"\n\t\n\t\t\n\t\tDataset Card for Balinese Carving Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains images of Balinese carvings along with their classifications, materials, and color descriptions. It is designed for image classification and retrieval tasks related to Balinese art and cultural heritage.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset supports multi-label image classification for Balinese carving styles and image retrieval based on textual descriptions of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aegishield/balinese-carving-dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","multi-label-classification","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-trainset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/AuroraCap-trainset","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\n\n\t\n\t\t\n\t\tAuroraCap Trainset\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nWe use over 20 million high-quality image/video-text pairs to train AuroraCap in three stages. \nPretraining stage. We first align visual features with the word embedding space of LLMs. To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector.\nVision stage. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-trainset.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v14","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v16","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Pexels_Gemini_capitoned","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned","creator_name":"Pixel Dust","creator_url":"https://huggingface.co/Pixel-Dust","description":"This dataset features a collection of high-quality images sourced from Pexels and captioned using the Gemini-1.5-Flash API. This dataset is designed to provide accurate, detailed descriptions of various visual content, suitable for text-to-image tasks, training AI models, and more.\nGemini promt:\n\"Describe this image, for a text-to-image train to be accurate, max 74 tokens. (the common theme between these images is '{theme}'), prefer the use of ',' dont use '.' and there is no need to have a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\nnumber of rects: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHere‚Äôs the PVIT-3M statistics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mila-ai4h/mid-space","creator_name":"Mila AI4H","creator_url":"https://huggingface.co/mila-ai4h","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mila-ai4h/mid-space.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v154","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v154","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v154.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"FlowerEvolver-Dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cristianglezm/FlowerEvolver-Dataset","creator_name":"Cristian Gonzalez","creator_url":"https://huggingface.co/cristianglezm","description":"\n\t\n\t\t\n\t\tFlowerEvolver Dataset\n\t\n\nDataset for flower descriptions for FlowerEvolver-frontend\nformat\n\n{\n    \"annotations\":[\n        {\n            \"ImagePath\":\"flowers/001.png\", \n            \"caption\":\"A flower with 10 petals that are elongated and narrow ovals in shape. The petals are yellow with dark green tones, featuring a striped pattern. The center of the flower has a dense and colorful pattern with red, blue, green, and pink.\"\n        },\n        ...\n    ]\n}\n\n\n\t\n\t\n\t\n\t\tMake more flowers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cristianglezm/FlowerEvolver-Dataset.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-to-image-2M","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jackyhate/text-to-image-2M","creator_name":"kzou","creator_url":"https://huggingface.co/jackyhate","description":"\n\t\n\t\t\n\t\ttext-to-image-2M: A High-Quality, Diverse Text-to-Image Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\ntext-to-image-2M is a curated text-image pair dataset designed for fine-tuning text-to-image models. The dataset consists of approximately 2 million samples, carefully selected and enhanced to meet the high demands of text-to-image model training. The motivation behind creating this dataset stems from the observation that datasets with over 1 million samples tend to produce better‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jackyhate/text-to-image-2M.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LLaVa_textcaps_rus","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/LLaVa_textcaps_rus","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\n\t\n\t\t\n\t\tTextcaps\n\t\n\nImage descriptions in English and Russian.\nRussian descriptions were translated from English.\nOriginal data was task textcaps from https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data\nData Format \n'id': str \n'image': dict[str, bytes]  # {'image': b'image bytes'}\n'conversations': List[Dict[str, str]]  # short dialog for two replics in English with image description\nExample: [{'from': 'human', 'value': 'Provide a one-sentence caption for the provided image.'}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/LLaVa_textcaps_rus.","first_N":5,"first_N_keywords":["image-to-text","Russian","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v155","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v155","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v155.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v22","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"gregg-preanniversary-words","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/grascii/gregg-preanniversary-words","creator_name":"Grascii: Language and Tools for Gregg Shorthand","creator_url":"https://huggingface.co/grascii","description":"\n\t\n\t\t\n\t\tGregg Preanniversary Words\n\t\n\nThis dataset is derived from the 1916 Gregg Shorthand Dictionary1\nand builds on top of the Gregg1916\ndataset by:\n\nCorrecting the labels of 250+ images\nAdding 550+ images for words missing in the original dataset\nRedoing 100+ poorly cropped images or images with stray marks\n\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset contains three columns:\n\nimage: The image of a shorthand form in the dictionary\ngrascii_normalized: The normalized grascii of the shorthand form‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/grascii/gregg-preanniversary-words.","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v24","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-10M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing the Agreement\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v2","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v23","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v24","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v25","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nInstead of making tasks out of input/output images.\nI'm now focusing on preserving the original puzzle, with some transformations applied.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-recaption","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/AuroraCap-recaption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\t\n\t\t\n\t\tAuroraCap-recaption\n\t\n\n\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nVideo recaption data by AuroraCap. Continue updating...\nFor some video source, we could upload the raw videos but for the others we could only provide the url since the well-known reason.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@article{chai2024auroracap,\n  title={AuroraCap: Efficient, Performant Video Detailed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-recaption.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Diffuse_Map_Surfaces","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alastandy/Diffuse_Map_Surfaces","creator_name":"Andrew Smith","creator_url":"https://huggingface.co/alastandy","description":"\n\t\n\t\t\n\t\tDataset Card for Diffuse Map Surface\n\t\n\nDetailed surface textures without shadows or hotspots. (Diffuse Maps)\nAn LORA for Flux.1-Dev using this dataset is aviable at https://civitai.com/models/939491\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Vl-RewardBench","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhihui/Vl-RewardBench","creator_name":"Xie","creator_url":"https://huggingface.co/Zhihui","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhihui/Vl-RewardBench.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"PersReFex","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZinengTang/PersReFex","creator_name":"Zineng Tang","creator_url":"https://huggingface.co/ZinengTang","description":"\n\t\n\t\t\n\t\tDataset Card for Multi-Agent Referential Communication Dataset\n\t\n\n\n\n\nExample scene showing the speaker (left) and listener (right) views.\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains spatial dialogue data for multi-agent referential communication tasks in 3D environments. It includes pairs of images showing speaker and listener views within photorealistic indoor scenes, along with natural language descriptions of target object locations.\nThe key‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZinengTang/PersReFex.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"BHM-Bengali-Hateful-Memes","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes","creator_name":"Eftekhar Hossain","creator_url":"https://huggingface.co/Eftekhar","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBHM is a novel multimodal dataset for Bengali Hateful Memes detection. The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, \ntailored for two tasks: (i) detecting hateful memes and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society).\n\n\t\n\t\t\n\t\tPaper Information\n\t\n\n\nPaper: https://aclanthology.org/2024.acl-long.454/\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes.","first_N":5,"first_N_keywords":["other","image-classification","image-to-text","Bengali","mit"],"keywords_longer_than_N":true},
	{"name":"thai_handwriting_dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iapp/thai_handwriting_dataset","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","description":"\n\t\n\t\t\n\t\tThai Handwriting Dataset\n\t\n\nThis dataset combines two major Thai handwriting datasets:\n\nBEST 2019 Thai Handwriting Recognition dataset (train-0000.parquet)\nThai Handwritten Free Dataset by Wang (train-0001.parquet onwards)\n\n\n\t\n\t\t\n\t\tMaintainer\n\t\n\nkobkrit@iapp.co.th\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tBEST 2019 Dataset\n\t\n\nContains handwritten Thai text images along with their ground truth transcriptions. The images have been processed and standardized for machine learning tasks.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/thai_handwriting_dataset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Thai","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"IncivilityCaps","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhaoYeP/IncivilityCaps","creator_name":"yepyep","creator_url":"https://huggingface.co/ZhaoYeP","description":"ZhaoYeP/IncivilityCaps dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Open-Qwen2VL-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data","creator_name":"Weizhi Wang","creator_url":"https://huggingface.co/weizhiwang","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains the data for Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources.\nProject page: https://victorwz.github.io/Open-Qwen2VL\nCode: https://github.com/Victorwz/Open-Qwen2VL\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\n\nccs_ebdataset: CC3M-CC12M-SBU filtered by CLIP, we directly download the webdataset based on the released of curated subset of BLIP-1\ndatacomp_medium_dfn_webdataset: DataComp-Medium-128M filtered by DFN, we just‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data.","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"historical-danish-handwriting","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aarhus-city-archives/historical-danish-handwriting","creator_name":"aarhus-city-archives","creator_url":"https://huggingface.co/aarhus-city-archives","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Historical Danish handwriting dataset is a Danish-language dataset containing more than 11.000 pages of transcribed and proofread handwritten text.\nThe dataset currently consists of the published minutes from a number of City and Parish Council meetings, all dated between 1841 and 1939.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the text is in Danish. The BCP-47 code for Danish is da.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aarhus-city-archives/historical-danish-handwriting.","first_N":5,"first_N_keywords":["image-to-text","Danish","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v5","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v26","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v12","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v3","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v4","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TCGA-Cancer-Variant-and-Clinical-Data","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/seq-to-pheno/TCGA-Cancer-Variant-and-Clinical-Data","creator_name":"Seq-to-Pheno","creator_url":"https://huggingface.co/seq-to-pheno","description":"\n\t\n\t\t\n\t\tTCGA Cancer Variant and Clinical Data\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset combines genetic variant information at the protein level with clinical data from The Cancer Genome Atlas (TCGA) project, curated by the International Cancer Genome Consortium (ICGC). It provides a comprehensive view of protein-altering mutations and clinical characteristics across various cancer types.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset includes:\n\nProtein sequence data for both mutated and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seq-to-pheno/TCGA-Cancer-Variant-and-Clinical-Data.","first_N":5,"first_N_keywords":["table-to-text","entity-linking-retrieval","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VisualDataset100K","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/REILX/VisualDataset100K","creator_name":"SunForlight","creator_url":"https://huggingface.co/REILX","description":"‰∏≠Êñá\n\n\t\n\t\t\n\t\n\t\n\t\tLocal Deployment of Large Models and Construction of VisualDataset100K Dataset\n\t\n\nDeploy large models locally using vllm and utilize them to construct the VisualDataset100K dataset.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Local Deployment of Large Models (vllm + nginx)\n\t\n\nUses multi GPUs, loads the Qwen/Qwen2-VL-2B-Instruct„ÄÅQwen/Qwen2-VL-7B-Instruct„ÄÅQwen/Qwen2-VL-72B-Instruct-GPTQ-Int4 models through vllm, and uses nginx for load balancing.\n1.1 Launch vllm instances:\nRun a vllm instance on each GPU‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/REILX/VisualDataset100K.","first_N":5,"first_N_keywords":["question-answering","image-to-text","Chinese","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"sft_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Share4oReasoning/sft_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nSFT instruction: Contains GPT-4o distilled chain-of-thought reasoning data covering wide range of tasks. Together with corresponding short-answer prediction data.\n\nImage: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODO‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/sft_data.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"coda-lm-llava-format","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KaiChen1998/coda-lm-llava-format","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","description":"\n\t\n\t\t\n\t\tCODA-LM Dataset Card\n\t\n\nCODA-LM is the multi-modal version of the CODA dataset, used in the CODA-LM paper. Both English and Chinese annotations are available. Check detailed usage in our Github repo.\nThis repo contains the CODA-LM dataset, which has been reorganized in the LLaVA data format. \nYou are also welcome to check the original CODA-LM data which contains more metadata vanilla annotations. \n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# name can be selected from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KaiChen1998/coda-lm-llava-format.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-Video-small-swift","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malterei/LLaVA-Video-small-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-small-swift\n\t\n\nSmall subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CTW1500_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/CTW1500_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tCTW1500\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/ctw1500/metafile.yml\nName: 'CTW1500'\nPaper:\n  Title: Curved scene text detection via transverse and longitudinal sequence connection\n  URL: https://www.sciencedirect.com/science/article/pii/S0031320319300664\n  Venue: PR\n  Year: '2019'\n  BibTeX: '@article{liu2019curved,\n  title={Curved scene text detection via transverse and longitudinal sequence connection},\n  author={Liu, Yuliang and Jin, Lianwen and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/CTW1500_OCR.","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SVHN_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/SVHN_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tSVHN\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttp://ufldl.stanford.edu/housenumbers/\nName: 'SVHN'\nData:\n  Title: The Street View House Numbers (SVHN) Dataset \n  URL: http://ufldl.stanford.edu/housenumbers/\n\n","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"PhysBench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/USC-GVL/PhysBench","creator_name":"USC Geomtry, Vision, and Learning Lab","creator_url":"https://huggingface.co/USC-GVL","description":"\n  PhysBench \n\n\n    üåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üî∫ EvalAI\n\n\nThis repo contains evaluation code for the paper \"PhysBench: Benchmarking and Enhancing VLMs for Physical World Understanding\"\nIf you like our project, please give us a star ‚≠ê on GitHub for latest update.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nUnderstanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/USC-GVL/PhysBench.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2501.16411","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"ICDAR2013_OCR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MiXaiLL76/ICDAR2013_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/icdar2013/metafile.yml\nName: 'Incidental Scene Text IC13'\nPaper:\n  Title: ICDAR 2013 Robust Reading Competition\n  URL: https://www.imlab.jp/publication_data/1352/icdar_competition_report.pdf\n  Venue: ICDAR\n  Year: '2013'\n  BibTeX:'@inproceedings{karatzas2013icdar,\n  title={ICDAR 2013 robust reading competition},\n  author={Karatzas, Dimosthenis and Shafait, Faisal and Uchida, Seiichi and Iwamura, Masakazu and i Bigorda‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/ICDAR2013_OCR.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mm-interp-CompCap-gpt4-data","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/htlou/mm-interp-CompCap-gpt4-data","creator_name":"LHT","creator_url":"https://huggingface.co/htlou","description":"\n\t\n\t\t\n\t\tCompCap-GPT4: A GPT-4 Captioned Version of CompCap-118K\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: CompCap: Improving Multimodal Large Language Models with Composite Captions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\nDirect Download:The repository includes CI_type.zip and CI_type.json. The JSON file follows the Llava format:\n{\n  \"id\": ID,\n  \"image\": IMAGE_PATH,\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": QUESTION},\n    {\"from\": \"gpt\", \"value\": ANSWER}\n  ]\n}\n\n\nUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/htlou/mm-interp-CompCap-gpt4-data.","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Mulberry-SFT","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuanjinYao/Mulberry-SFT","creator_name":"Huanjin Yao","creator_url":"https://huggingface.co/HuanjinYao","description":"Please check our GitHub for more details.: https://github.com/HJYao00/Mulberry\n\n\t\n\t\t\n\t\tTraining\n\t\n\nWe use LLaMA-Factory to fine-tune the Mulberry models. We provide the training instructions and configs here.\nFirst, install LLaMA-Factory according to the official_instruction.\nThen, refer here and update the following customized dataset into dataset_info.json in LLaMA-Factory.\n\"mulberry\": {\n    \"file_name\": \"./mulberry_sft.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n      \"messages\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuanjinYao/Mulberry-SFT.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RSTeller_metadata","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","description":"\n\t\n\t\t\n\t\tMetadata for RSTeller\n\t\n\nThis dataset contains the necessary metadata for the dataset SlytherinGe/RSTeller.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe metadata table provides detailed information for the RSTeller dataset, with the following columns:\n\npatch_id: The primary key of the table, corresponding to the \"__key__\" or the \"patch_id\" field in the JSON of the RSTeller dataset.\n\npatch_lat and patch_lon: The latitude and longitude coordinates of the patch center in WGS84‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"otpensource_dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hateslopacademy/otpensource_dataset","creator_name":"hateslop_academy","creator_url":"https://huggingface.co/hateslopacademy","description":"\n\t\n\t\t\n\t\tDataset Card for otpensource_dataset\n\t\n\nThis dataset contains curated information about fashion items, focusing on various categories, subcategories, genders, seasons, and unique features. It is intended for use in AI applications related to fashion recommendation systems, trend analysis, and image-to-text generation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset includes 8,998 examples of Korean Mushinsa fashion data, each with detailed attributes such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hateslopacademy/otpensource_dataset.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-captioning","sentiment-analysis","Korean"],"keywords_longer_than_N":true},
	{"name":"otpensource_dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hateslopacademy/otpensource_dataset","creator_name":"hateslop_academy","creator_url":"https://huggingface.co/hateslopacademy","description":"\n\t\n\t\t\n\t\tDataset Card for otpensource_dataset\n\t\n\nThis dataset contains curated information about fashion items, focusing on various categories, subcategories, genders, seasons, and unique features. It is intended for use in AI applications related to fashion recommendation systems, trend analysis, and image-to-text generation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset includes 8,998 examples of Korean Mushinsa fashion data, each with detailed attributes such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hateslopacademy/otpensource_dataset.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-captioning","sentiment-analysis","Korean"],"keywords_longer_than_N":true},
	{"name":"publicdomainpictures","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/publicdomainpictures","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Public Domain Pictures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 644,412 public domain images from publicdomainpictures.net, a public domain photo sharing platform. The dataset includes detailed image metadata including titles, descriptions, and keywords.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All metadata including titles, descriptions and keywords\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe metadata for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/publicdomainpictures.","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"R1-VL-10K","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jingyiZ00/R1-VL-10K","creator_name":"Jingyi Zhang","creator_url":"https://huggingface.co/jingyiZ00","description":"Please check our GitHub for more details: https://github.com/jingyi0000/R1-VL.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"GUI-Lasagne-L1","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SenseLLM/GUI-Lasagne-L1","creator_name":"SenseLLM","creator_url":"https://huggingface.co/SenseLLM","description":"This repository contains the GUI-Lasagne dataset used to train SpiritSight Agent, as described in the paper SpiritSight Agent: Advanced GUI Agent with One Look.\nProject Page: https://hzhiyuan.github.io/SpiritSight-Agent\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2503.03196","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Qilin","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUIR/Qilin","creator_name":"THUIR","creator_url":"https://huggingface.co/THUIR","description":"\n\t\n\t\t\n\t\tQilin\n\t\n\nQilin is a large-scale multimodal dataset designed for advancing research in search, recommendation, and Retrieval-Augmented Generation (RAG) systems. This repository contains the official implementation of the dataset paper, baseline models, and evaluation tools.  This dataset was presented in Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions.\nGithub: https://github.com/RED-Search/Qilin\nThe image data can be found at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUIR/Qilin.","first_N":5,"first_N_keywords":["question-answering","text-classification","sentence-similarity","text-retrieval","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"ABC-Pretraining-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tABC Pretraining Data\n\t\n\n\nThis the the pretraining data for ABC. This dataset is derived from Google's Conceptual Captions dataset.\nThe each item in the dataset contain a URL where the corresponding image can be downloaded and mined negatives for each item. Full dataaset is ~300 GB of images. For a detailed description of how we mined the negatives please check out our ppaer ;).Update I have added the images to this repository, for an example of how to use and download this dataset see‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create high‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"paligemma-multitask-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xingqiang/paligemma-multitask-dataset","creator_name":"chen.xingqiang","creator_url":"https://huggingface.co/xingqiang","description":"\n\t\n\t\t\n\t\tPaliGemma Multitask Dataset\n\t\n\nThis dataset is designed for training and evaluating the PaliGemma multitask model for defect detection and analysis. It combines a base set of annotated samples with an extended collection of 874 real-world structural inspection images.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe dataset contains images of structural defects along with their corresponding annotations for:\n\nObject detection (bounding boxes)\nDefect classification\nDescriptive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xingqiang/paligemma-multitask-dataset.","first_N":5,"first_N_keywords":["object-detection","image-to-text","visual-question-answering","image-captioning","English"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"HAIC","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KuaishouHAIC/HAIC","creator_name":"KuaishouHAIC","creator_url":"https://huggingface.co/KuaishouHAIC","description":"\n\t\n\t\t\n\t\tHAIC: Human Action and Interaction Comprehension Dataset\n\t\n\nFrom the paper: \"HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models\"\nRead the Paper\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nHAICBench is a comprehensive video dataset featuring manually annotated, fine-grained human captions that features:\n\nMultiple Human Subjects: Captions detail interactions and activities involving more than one person, capturing the complexity of human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KuaishouHAIC/HAIC.","first_N":5,"first_N_keywords":["video-text-to-text","English","Chinese","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"grustnogram","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/grustnogram","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Grustnogram\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 597,704 posts from Grustnogram.ru, a Russian \"emotional network\" similar to Instagram but with a distinctive black and white filter aesthetic and dark atmosphere. The dataset includes 542,917 image posts with associated metadata and 54,787 anonymous text-only posts.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian (ru).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Splits\n\t\n\nThe dataset is divided into‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/grustnogram.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-classification","multi-label-image-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"reverse-instruct-1.3m","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m","creator_name":"Patrick (Tsung-Han) Wu","creator_url":"https://huggingface.co/tsunghanwu","description":"\n\t\n\t\t\n\t\tREVERSE Visual Instruct 1.3M\n\t\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDataset Type:REVERSE Visual Instruct 1.3M is a GPT-generated instruction-following dataset designed for training hallucination-aware vision-language models (VLMs). It builds on the LLaVA Instruct 665K dataset and includes structured annotations to indicate model confidence. We introduce three special tokens:  \n\n<SPAN>: marks the beginning of a key phrase  \n</CN>: denotes a confident (grounded) phrase  \n</UN>: denotes an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","mit","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"portugues_ocr_dataset_full","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mazafard/portugues_ocr_dataset_full","creator_name":"Mohammadreza Asadollahifard","creator_url":"https://huggingface.co/mazafard","description":"\n\t\n\t\t\n\t\tPortugues OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe portugues_ocr_dataset_full is a dataset designed for Optical Character Recognition (OCR) tasks. It contains images of text from the Portuguese literary work Os Lus√≠adas by Lu√≠s Vaz de Cam√µes, as well as the corresponding ground truth text labels. This dataset can be used for training and evaluating OCR models for Portuguese text recognition.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of:\n\nImages: Each image is a cropped portion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mazafard/portugues_ocr_dataset_full.","first_N":5,"first_N_keywords":["image-to-text","manual","monolingual","Portuguese","English"],"keywords_longer_than_N":true},
	{"name":"MR-Video","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ziqipang/MR-Video","creator_name":"Ziqi Pang","creator_url":"https://huggingface.co/ziqipang","description":"This repository contains the data presented in MR. Video: \"MapReduce\" is the Principle for Long Video Understanding.\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MolLangBench","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"portuguese-ocr-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mazafard/portuguese-ocr-dataset","creator_name":"Mohammadreza Asadollahifard","creator_url":"https://huggingface.co/mazafard","description":"task_categories:\n\nimage-to-text\ntask_ids:\noptical-character-recognition\ntext-recognition\n\n\n\t\n\t\t\n\t\tPortuguese OCR Dataset\n\t\n\nA comprehensive dataset for Portuguese OCR (Optical Character Recognition) generated from classic Portuguese literature with diverse fonts and visual styles.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 20000 text images for OCR training, created from Portuguese books from Project Gutenberg. Each image contains a complete Portuguese sentence with proper‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mazafard/portuguese-ocr-dataset.","first_N":5,"first_N_keywords":["image-to-text","text-generation","Portuguese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"gex_dataset_merged","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MosRat/gex_dataset_merged","creator_name":"MosRat","creator_url":"https://huggingface.co/MosRat","description":"MosRat/gex_dataset_merged dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Chinese","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"CHAOS","keyword":"image-to-text","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/omoured/CHAOS","creator_name":"Omar Moured","creator_url":"https://huggingface.co/omoured","description":"\n\t\n\t\t\n\t\tCHAOS Benchmark: Robustness Testing for Noisy Charts üåÄ\n\t\n\n    \nOmar Moured, Yufan Chen, Jiaming Zheng, Simon Rei√ü, Ruiping Liu, Rainer Stiefelhagen\n\n\t\n\t\t\n\t\n\t\n\t\t[Introduction]\n\t\n\nWhat happens when the input is messy‚Äîblurred labels, typos, occlusions, or color shifts? ü§îCHAOS (CHart Analysis with Outlier Samples) is the first benchmark purposely designed to stress‚Äëtest MLLMs under realistic noise. We:\n\nevaluate 10 visual and 5 textual perturbations, each at three increasing severity‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omoured/CHAOS.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","agpl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Jedi","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xlangai/Jedi","creator_name":"XLang NLP Lab","creator_url":"https://huggingface.co/xlangai","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nNOTE: Before you use this dataset, make sure you understand the logic of absolute coordinates and image processor for Qwen2.5-VL. \nThis dataset is set with the image processor max tokens to be 2700, a.k.a max_pixels=2700x14x14x2x2 , the coordinates were resized to be smaller and you have to resize the image as well within max_pixels=2700x14x14x2x2 via image processor to make them align.\nMake sure you also follow it in your training procedure, otherwise the performance will not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/Jedi.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2502.13923","arxiv:2505.13227","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"beautiVis","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/beautiVis/beautiVis","creator_name":"beautiVis","creator_url":"https://huggingface.co/beautiVis","description":"\n\t\n\t\t\n\t\tbeautiVis\n\t\n\n\n\t\n\t\t\n\t\tAbout the Dataset\n\t\n\nbeautiVis is a richly annotated dataset of 50,000+ static images sourced from Reddit's r/dataisbeautiful subreddit between February 2012 and January 2025. The dataset was built through a three-phase pipeline:\nPhase 1: Data CollectionFirst, we downloaded the complete post history from r/dataisbeautiful using the Arctic-Shift Reddit Download Tool, which provided raw JSON data containing post metadata, titles, and image URLs. During this initial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/beautiVis/beautiVis.","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","cc0-1.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"Magma-Mind2Web-SoM","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1‚Ä†¬†\nReuben Tan1‚Ä†¬†\nQianhui Wu1‚Ä†¬†\nRuijie Zheng2‚Ä°¬†\nBaolin Peng1‚Ä°¬†\nYongyuan Liang2‚Ä°\nYu Gu1¬†\nMu Cai3¬†\nSeonghyeon Ye4¬†\nJoel Jang5¬†\nYuquan Deng5¬†\nLars Liden1¬†\nJianfeng Gao1‚ñΩ\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  ‚Ä† First authors  ‚Ä° Second authors  ‚ñΩ Leadership  \n[arXiv Paper] ¬† [Project Page] ¬† [Hugging Face Paper] ¬† [Github Repo] ¬† [Video]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM.","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"si-wiki-OCR-synth-deduped","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Suchinthana/si-wiki-OCR-synth-deduped","creator_name":"Wijesundara","creator_url":"https://huggingface.co/Suchinthana","description":"Suchinthana/si-wiki-OCR-synth-deduped dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Sinhala","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Marathi_Handwritten","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Process-Venue/Marathi_Handwritten","creator_name":"ProcessVenue","creator_url":"https://huggingface.co/Process-Venue","description":"\n\t\n\t\t\n\t\tDataset Card for Marathi Handwritten OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Marathi Handwritten Text Dataset is a collection of handwritten text images in Marathi (‡§¶‡•á‡§µ‡§®‡§æ‡§ó‡§∞‡•Ä ‡§≤‡§ø‡§™‡•Ä),\naimed at supporting the development of Optical Character Recognition (OCR) systems, handwriting analysis tools,\nand language research.The dataset was curated from native Marathi speakers to ensure a variety of handwriting styles and character variations.\nThe dataset contains 2520 images with two‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Process-Venue/Marathi_Handwritten.","first_N":5,"first_N_keywords":["image-classification","image-to-text","image-feature-extraction","Marathi","mit"],"keywords_longer_than_N":true},
	{"name":"HumanEval-V-Benchmark","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark","creator_name":"HumanEval-V","creator_url":"https://huggingface.co/HumanEval-V","description":"\n\t\n\t\t\n\t\tHumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks\n\t\n\n\n    üìÑ Paper  ‚Ä¢\n    üè† Home Page ‚Ä¢\n    üíª GitHub Repository  ‚Ä¢\n    üèÜ Leaderboard ‚Ä¢\n    ü§ó Dataset Viewer \n\n\nHumanEval-V is a novel benchmark designed to evaluate the diagram understanding and reasoning capabilities of Large Multimodal Models (LMMs) in programming contexts. Unlike existing benchmarks, HumanEval-V focuses on coding tasks that require sophisticated visual reasoning over‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"dataset-kid-fr-lexia","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Marsouuu/dataset-kid-fr-lexia","creator_name":"Martial Roberge","creator_url":"https://huggingface.co/Marsouuu","description":"\n\t\n\t\t\n\t\tDataset KID-FR: Documents d'Informations Cl√©s Financiers en Fran√ßais\n\t\n\nCe dataset contient une collection de Documents d'Informations Cl√©s (KID - Key Information Documents) financiers en fran√ßais. Il est con√ßu pour l'entra√Ænement de mod√®les de vision par ordinateur et de traitement du langage naturel dans le domaine financier.\n\n\t\n\t\t\n\t\tContenu\n\t\n\nLe dataset se compose de documents financiers structur√©s comprenant :\n\nDes images de documents financiers\nLe texte extrait de ces documents‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Marsouuu/dataset-kid-fr-lexia.","first_N":5,"first_N_keywords":["image-to-text","French","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MIRe_ViD2R","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Y-J-Ju/MIRe_ViD2R","creator_name":"Yeong-Joon Ju","creator_url":"https://huggingface.co/Y-J-Ju","description":"\n\t\n\t\t\n\t\tMIRe Pre-training Dataset for Multimodal Query Retrieval\n\t\n\nThis repository contains the pre-training dataset used in our work on MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval. The dataset is designed for training multimodal retrieval systems that integrate both visual and textual cues without fusing text features during the alignment stage.\nNote: This release excludes data from the WiT corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Y-J-Ju/MIRe_ViD2R.","first_N":5,"first_N_keywords":["text-retrieval","image-to-text","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ecom-prod-demo","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ariwala99/ecom-prod-demo","creator_name":"Pratham Ariwala","creator_url":"https://huggingface.co/ariwala99","description":"ariwala99/ecom-prod-demo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"EgoLife","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/EgoLife","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"Data cleaning, stay tuned! Please refer to https://egolife-ai.github.io/ first for general info.\nCheckout the paper EgoLife (https://arxiv.org/abs/2503.03803) for more information.\nCode: https://github.com/egolife-ai/EgoLife\n","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","mit","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"danbooru2023","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/danbooru2023","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\n\t\n\t\t\n\t\t[Mirror]Danbooru2023: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset\n\t\n\nDanbooru2023 is an extension of Danbooru2021, featuring over 6.8 million anime-style images, totaling more than 8.3 TB. \nEach image is accompanied by community-contributed tags that provide detailed descriptions of its content, including characters, \nartists, copyright information, concepts, and attire. \nThis makes it a crucial resource for stylized computer vision tasks and transfer learning.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/danbooru2023.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-to-image","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"journals","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/journals","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Historical Russian Technical Journal Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains images of pages from old Russian technical journals with descriptions generated using Google Gemini 2.0 Flash.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nRussian (ru): All journal pages are in Russian with corresponding Russian descriptions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nThe dataset consists of:\n\nImage files (.jpg format)\nCorresponding description‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/journals.","first_N":5,"first_N_keywords":["image-classification","image-to-text","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"SPATIAL-v1.0","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Youlln/SPATIAL-v1.0","creator_name":"Lalain Youri","creator_url":"https://huggingface.co/Youlln","description":"\n\t\n\t\t\n\t\tüöÄ Aerospace Knowledge Dataset (VLM)\n\t\n\n\n\t\n\t\t\n\t\tüìå Overview\n\t\n\nThe Aerospace Knowledge Dataset is a large-scale, multi-modal dataset designed for training Vision-Language Models (VLMs) in the aerospace domain. It is built from over 26,000 pages of technical documents, research papers, engineering reports, and mission data from leading space organizations such as NASA, ArianeGroup, SpaceX, ESA, and others.  \nThis dataset is structured in a query + image format, allowing AI models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Youlln/SPATIAL-v1.0.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"3d_layout_reasoning","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning","creator_name":"Zhenyu Pan","creator_url":"https://huggingface.co/zhenyupan","description":"Dataset for MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse\nGithub: https://github.com/PzySeere/MetaSpatial\n","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"table-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"GPRadar-Defect-MultiTask","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xingqiang/GPRadar-Defect-MultiTask","creator_name":"chen.xingqiang","creator_url":"https://huggingface.co/xingqiang","description":"\n\t\n\t\t\n\t\tGPRadar-Defect-MultiTask Êï∞ÊçÆÈõÜ\n\t\n\nÊú¨‰ªìÂ∫ìÂåÖÂê´Áî®‰∫éÂæÆË∞ÉPaLI-GEMMAÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂú∞Ë¥®Èõ∑Ëææ(GPR)Áº∫Èô∑Ê£ÄÊµãÊï∞ÊçÆÈõÜ„ÄÇËØ•Êï∞ÊçÆÈõÜ‰∏ìÊ≥®‰∫éÂú∞‰∏ãÁªìÊûÑ‰∏≠ÁöÑÁ©∫Ê¥ûÂíåË£ÇÁºùÊ£ÄÊµã‰∏éÂàÜÊûê„ÄÇ\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÁªìÊûÑ\n\t\n\nÊï∞ÊçÆÈõÜÁªÑÁªáÂ¶Ç‰∏ãÔºö\ndataset/\n‚îú‚îÄ‚îÄ annotations/ - ÂåÖÂê´JSONÂíåJSONLÊ†ºÂºèÁöÑÊ†áÊ≥®Êñá‰ª∂\n‚îÇ   ‚îú‚îÄ‚îÄ _annotations.train.jsonl - ËÆ≠ÁªÉÈõÜÊ†áÊ≥®\n‚îÇ   ‚îú‚îÄ‚îÄ _annotations.valid.jsonl - È™åËØÅÈõÜÊ†áÊ≥®\n‚îÇ   ‚îú‚îÄ‚îÄ _annotations.test.jsonl - ÊµãËØïÈõÜÊ†áÊ≥®\n‚îÇ   ‚îú‚îÄ‚îÄ p-1.v1i.paligemma/ - ‰∏ªÊï∞ÊçÆÈõÜÂÖÉÊï∞ÊçÆ\n‚îÇ   ‚îî‚îÄ‚îÄ p-1.v1i.paligemma-multimodal/ - Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÂÖÉÊï∞ÊçÆ\n‚îú‚îÄ‚îÄ images/ - ÂåÖÂê´ÊâÄÊúâÂõæÂÉèÊñá‰ª∂\n\n\n\t\n\t\t\n\t\tÁâπÁÇπ\n\t\n\n\nÂåÖÂê´874Âº†Â∏¶Ê≥®ÈáäÁöÑÂú∞Ë¥®Èõ∑ËææÊâ´ÊèèÂõæÂÉè\nÂõæÂÉèÈ¢ÑÂ§ÑÁêÜ‰∏∫640x640ÂÉèÁ¥†Â§ßÂ∞è\nÊîØÊåÅÂ§öÁßç‰ªªÂä°Á±ªÂûãÔºöÁº∫Èô∑Ê£ÄÊµã„ÄÅ‰ΩçÁΩÆÂÆö‰ΩçÂíåÊèèËø∞ÁîüÊàê‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xingqiang/GPRadar-Defect-MultiTask.","first_N":5,"first_N_keywords":["object-detection","image-to-text","visual-question-answering","image-captioning","English"],"keywords_longer_than_N":true},
	{"name":"laion-eus","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/laion-eus","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"This dataset contains a subset of laion/relaion2B-multi-research where captions are written in Basque. Although language is already detected for its captions, we used another language detector Mike0307/multilingual-e5-language-detection and a high certainty threshold (p > 0.98) to better filter the instances in Basque.\nApart from the same attributes found in laion/relaion2B-multi-research, we add the following to each instance.\n\n\"lang_laion\": language originally detected by the classifier used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/laion-eus.","first_N":5,"first_N_keywords":["image-to-text","Basque","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"TVC-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Allen8/TVC-Data","creator_name":"Allen Sun","creator_url":"https://huggingface.co/Allen8","description":"\n\t\n\t\t\n\t\tDataset Card for TVC-Data\n\t\n\nThis repository contains the data presented in Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning.\nProject page: https://sun-hailong.github.io/projects/TVC\nCode: https://github.com/sun-hailong/TVC\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nA mixture of 345K multimodal long-chain reasoning data. \nFor more statistics of the dataset, please refer to our paper (coming soon)\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nLLaVA-OneVision:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Allen8/TVC-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","apache-2.0","arxiv:2503.13360"],"keywords_longer_than_N":true},
	{"name":"t2i_tiny_nasa","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaangml/t2i_tiny_nasa","creator_name":"Kaan GML","creator_url":"https://huggingface.co/kaangml","description":"\n\t\n\t\t\n\t\tDataset Card for t2i_tiny_nasa\n\t\n\n\n\t\n\t\t\n\t\tNASA Image Dataset\n\t\n\nThis dataset is created using images obtained from NASA's official image library. The dataset contains a collection of images along with their corresponding textual descriptions (prompts). This dataset can be used for various applications, including image-to-text tasks, text-to-image generation, and other AI-based image analysis studies.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSource: NASA Image Library\nContent: Images and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaangml/t2i_tiny_nasa.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"rrvideo-element-highlights","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights","creator_name":"Earl Potters","creator_url":"https://huggingface.co/Slyracoon23","description":"\n\t\n\t\t\n\t\tDataset Card for rrvideo-element-highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains UI element images extracted from web snapshots, along with captions describing each element.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset contains 61 unique UI elements from 2 different snapshots. Each element is associated with a caption describing its basic properties.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal elements: 61\nUnique snapshots: 2\nAverage element width: 1280.00 pixels\nAverage element‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights.","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NIH-CXR14-BiomedCLIP-Features","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features","creator_name":"Tun√ßer","creator_url":"https://huggingface.co/Yasintuncer","description":"\n\t\n\t\t\n\t\tNIH-CXR14-BiomedCLIP-Features Dataset\n\t\n\nThis dataset is derived from the NIH Chest X-ray Dataset (NIH-CXR14) and processed using the BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 model from Microsoft. It contains image and text features extracted from chest X-ray images and their corresponding textual findings.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe original NIH-CXR14 dataset comprises 112,120 chest X-ray images with disease labels from 30,805 unique patients. This processed dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features.","first_N":5,"first_N_keywords":["image-classification","text-retrieval","text-classification","image-feature-extraction","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"globalrg-grounding-task","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UBC-VL/globalrg-grounding-task","creator_name":"UBCVL","creator_url":"https://huggingface.co/UBC-VL","description":"UBC-VL/globalrg-grounding-task dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","text-to-image","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Mini-YoChameleon-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaoshibe","description":"\n\t\n\t\t\n\t\tMini Yo'Chameleon Data\n\t\n\nThis is a mini-training-data for Yo'Chameleon, with example of personalized subject called <bo> (From Yo'LLaVA).\nWhat you will find:\n\n10/10 positive images for training/ testing\n1000 HARD negative images for training (retrieved from LAION-5B based on similarity with subject)\n1000 random images for training\n\nThe folder structure:\nmini-yochameleon-data\n |_ random_negative_example\n |   |_ [1000 random images example for training recognition abilities]\n |_ test\n |‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"phycics_dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wzmmmm/phycics_dataset","creator_name":"wzm","creator_url":"https://huggingface.co/wzmmmm","description":"wzmmmm/phycics_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Chinese","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"myanmar-ocr-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/chuuhtetnaing/myanmar-ocr-dataset","creator_name":"Chuu Htet Naing","creator_url":"https://huggingface.co/chuuhtetnaing","description":"\n\t\n\t\t\n\t\tMyanmar OCR Dataset\n\t\n\nA synthetic dataset for training and fine-tuning Optical Character Recognition (OCR) models specifically for the Myanmar language.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains synthetically generated OCR images created specifically for Myanmar text recognition tasks. The images were generated using myanmar-ocr-data-generator, a fork of TextRecognitionDataGenerator with fixes for proper Myanmar character splitting.\n\n\t\n\t\t\n\t\n\t\n\t\tDirect Download\n\t\n\nAvailable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chuuhtetnaing/myanmar-ocr-dataset.","first_N":5,"first_N_keywords":["image-to-text","Burmese","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"Video-R1-data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Video-R1/Video-R1-data","creator_name":"Video-R1","creator_url":"https://huggingface.co/Video-R1","description":"This repository contains the data presented in Video-R1: Reinforcing Video Reasoning in MLLMs.\nCode: https://github.com/tulerfeng/Video-R1\nVideo data folder: CLEVRER, LLaVA-Video-178K, NeXT-QA, PerceptionTest, STAR\nImage data folder: Chart, General, Knowledge, Math, OCR, Spatial\nVideo-R1-COT-165k.json is for SFT cold start, and Video-R1-260k.json is for RL training.\nData Format in Video-R1-COT-165k:\n  {\n      \"problem_id\": 2,\n      \"problem\": \"What appears on the screen in Russian during the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Video-R1/Video-R1-data.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Image"],"keywords_longer_than_N":true},
	{"name":"programmerhumor","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/programmerhumor","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for ProgrammerHumor.io Memes\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains programming-related memes and humor content collected from programmerhumor.io, along with associated metadata such as titles, categories, tags, and image captions.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All meme content and descriptions are primarily in English\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nThe dataset consists of:\n\nImage files (stored in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/programmerhumor.","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"SEED-Bench-R1","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/SEED-Bench-R1","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"This repository contains the datasets presented in Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1.\n","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2503.24376","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"KVG","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MaxyLee/KVG","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n\n \n \n \nThis is the official repository of KVG training data for DeepPerception.\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"SightationPreference","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationPreference","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationPreference\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationPreference.","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-missing-words","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words","creator_name":"Alix Chagu√©","creator_url":"https://huggingface.co/alix-tz","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Missing Words\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Missing Words, each variation column is affected by the noise, without considering the split between train, validation and test.\n\n\t\n\t\t\n\t\n\t\n\t\tData structure\n\t\n\nThe dataset is composed of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words.","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"coda","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mateoguaman/coda","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","description":"\n\t\n\t\t\n\t\tCODa Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/coda.","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LSDBench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TainU/LSDBench","creator_name":"QU Tianyuan","creator_url":"https://huggingface.co/TainU","description":"\n\t\n\t\t\n\t\tDataset Card for LSDBench: Long-video Sampling Dilemma Benchmark\n\t\n\nA benchmark that focuses on the sampling dilemma in long-video tasks. Through well-designed tasks, it evaluates the sampling efficiency of long-video VLMs.\nArxiv Paper: üìñ Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?\nGithub : https://github.com/dvlab-research/LSDBench\n(Left) In Q1, identifying a camera wearer's visited locations requires analyzing the entire video. However, key frames‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TainU/LSDBench.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"TinyLLaVA-Video-v1-training-data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data","creator_name":"Zhang Xingjian","creator_url":"https://huggingface.co/Zhang199","description":"TinyLLaVA-Video\n\nThis dataset combines data from multiple sources for pre-training and fine-tuning.\nPretrain Data: Four subsets of LLaVA-Video-178K (0_30_s_academic_v0_1, 30_60_s_academic_v0_1, 0_30_s_youtube_v0_1, 30_60_s_youtube_v0_1), supplemented with filtered Video-LLaVA data (https://huggingface.co/datasets/LanguageBind/Video-LLaVA) and data from Valley (https://github.com/RupertLuo/Valley). The video data can be downloaded from the linked datasets, and cleaned annotations are provided‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data.","first_N":5,"first_N_keywords":["video-text-to-text","apache-2.0","arxiv:2501.15513","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"gregg-preanniversary-phrases","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/grascii/gregg-preanniversary-phrases","creator_name":"Grascii: Language and Tools for Gregg Shorthand","creator_url":"https://huggingface.co/grascii","description":"\n\t\n\t\t\n\t\tGregg Preanniversary Phrases\n\t\n\nThis Grascii original dataset is derived from the 1924 Gregg Shorthand Phrase Book1.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset contains three columns:\n\nimage: The image of a shorthand form in the dictionary\ngrascii_normalized: The normalized grascii of the shorthand form\nlonghand: The English longhand represented by the shorthand form\n\n\n\t\n\t\t\n\t\tIssues\n\t\n\nIf you notice any problems in the dataset, open an issue in the\ndatasets repository.\n\n1Gregg, John Robert.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/grascii/gregg-preanniversary-phrases.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TDC_training_data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hoar012/TDC_training_data","creator_name":"Haoran Hao","creator_url":"https://huggingface.co/Hoar012","description":"\n\t\n\t\t\n\t\tTraining data used in TDC.\n\t\n\nPaper Link:\nhttps://arxiv.org/pdf/2504.10443\nProject Page:\nhttps://hoar012.github.io/TDC-Project\nCode:\nhttps://github.com/Hoar012/TDC-Video\n","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"ta-wiki-OCR-synth-deduped","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Suchinthana/ta-wiki-OCR-synth-deduped","creator_name":"Wijesundara","creator_url":"https://huggingface.co/Suchinthana","description":"Suchinthana/ta-wiki-OCR-synth-deduped dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Tamil","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"RS-M3Bench","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RemoteReason-JLU/RS-M3Bench","creator_name":"Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education","creator_url":"https://huggingface.co/RemoteReason-JLU","description":"\n\t\n\t\t\n\t\tRS-M3Bench\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tUsing datasets\n\t\n\nfrom datasets import load_dataset\nfw = load_dataset(\"RemoteReason-JLU/RS-M3Bench\", name=\"RS-M3Bench\", split=\"train\", streaming=True)\n\nAttribute explanations in annotation files:\n\nHBB: the coordinates of four object corner points\nOBB: the coordinates of four object corner points\nPolygon: the coordinates of all the object corner points\nNote that the original STAR/ReCom1M datasets use OBB to localize objects, we further uultilize SAM to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RemoteReason-JLU/RS-M3Bench.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NumPro_FT","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Liang0223/NumPro_FT","creator_name":"Yongliang","creator_url":"https://huggingface.co/Liang0223","description":"This repository contains the dataset described in the paper Number it: Temporal Grounding Videos like Flipping Manga.\nCode: https://github.com/yongliang-wu/NumPro\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v194","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v194","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v194.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v212","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v212","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v212.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Sparrow-Synthetic","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic","creator_name":"Shukang Yin","creator_url":"https://huggingface.co/xjtupanda","description":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation\n\n        üíª GitHub¬†¬† | ¬†¬† üìë Paper ¬†¬†  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Sparrow-Synthetic\n\t\n\n\n\nThis is a synthetic \"video\" instruction dataset derived from language data.\nIt is designed to enrich the instruction diversity of video training corpus.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset is curated from longQA text datasets, LongAlpaca-12k and LongQLoRA-39K.\nEach sample can be abstracted as a (long-context, instruction, answer)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2411.19951"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v10","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v17","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v27","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CompreCap","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FanLu31/CompreCap","creator_name":"Fan Lu","creator_url":"https://huggingface.co/FanLu31","description":"\n\t\n\t\t\n\t\tDataset Card for CompreCap\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe CompreCap benchmark is characterized by human-annotated scene graph and focuses on the evaluation of comprehensive image captioning.\nIt provides new semantic segmentation annotations for common objects in images, with an average mask coverage of 95.83%.\nBeyond the careful annotation of objects, CompreCap also includes high-quality descriptions of the attributes bound to the objects, as well as directional relation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FanLu31/CompreCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"food-visual-instructions","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AdaptLLM/food-visual-instructions","creator_name":"AdaptLLM","creator_url":"https://huggingface.co/AdaptLLM","description":"\n\t\n\t\t\n\t\tAdapting Multimodal Large Language Models to Domains via Post-Training\n\t\n\nThis repos contains the food visual instructions for post-training MLLMs in our paper: On Domain-Specific Post-Training for Multimodal Large Language Models.\nThe main project page is: Adapt-MLLM-to-Domains\n\n\t\n\t\t\n\t\n\t\n\t\tData Information\n\t\n\nUsing our visual instruction synthesizer, we generate visual instruction tasks based on the image-caption pairs from extended Recipe1M+ dataset. These synthetic tasks, combined‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AdaptLLM/food-visual-instructions.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"coco-pic","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rasoulasadianub/coco-pic","creator_name":"rasoul asadian","creator_url":"https://huggingface.co/rasoulasadianub","description":"rasoulasadianub/coco-pic dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","image-classification","Persian","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VISTA-400K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VISTA-400K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tVISTA-400K\n\t\n\nThis repo contains all subsets for VISTA-400K. VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\n\n\t\n\t\t\n\t\tThis repo is under construction. Please stay tuned.\n\t\n\nüåê Homepage | üìñ arXiv | üíª GitHub | ü§ó VISTA-400K | ü§ó Models | ü§ó HRVideoBench\n\n\t\n\t\n\t\n\t\tVideo Instruction Data Synthesis Pipeline\n\t\n\n\nVISTA leverages insights from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/VISTA-400K.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"GroundCap","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daniel3303/GroundCap","creator_name":"Daniel Oliveira","creator_url":"https://huggingface.co/daniel3303","description":"\n\t\n\t\t\n\t\tGroundCap Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundCap is a novel grounded image captioning dataset derived from MovieNet, containing 52,350 movie frames with detailed grounded captions. The dataset uniquely features an ID-based system that maintains object identity throughout captions, enables tracking of object interactions, and grounds not only objects but also actions and locations in the scene.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach sample in the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daniel3303/GroundCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GroundCap","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daniel3303/GroundCap","creator_name":"Daniel Oliveira","creator_url":"https://huggingface.co/daniel3303","description":"\n\t\n\t\t\n\t\tGroundCap Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundCap is a novel grounded image captioning dataset derived from MovieNet, containing 52,350 movie frames with detailed grounded captions. The dataset uniquely features an ID-based system that maintains object identity throughout captions, enables tracking of object interactions, and grounds not only objects but also actions and locations in the scene.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach sample in the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daniel3303/GroundCap.","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"laion-coco-13m-molmo-d-7b","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/laion-coco-13m-molmo-d-7b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for laion-coco-13m-molmo-d-7b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 41,409,699 new synthetic captions for the 13,803,233 images found in laion/laion-coco. It includes the original captions from that repository as well as new captions. The dataset was filtered to images >= 512px on the short edge.\nThe long captions were produced using allenai/Molmo-7B-D-0924. Medium and short captions were produced from these captions using allenai/Llama-3.1-Tulu-3-8B-DPO. The dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/laion-coco-13m-molmo-d-7b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"Synthdog-Multilingual-100","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","description":"\n\t\n\t\t\n\t\tSynthdog Multilingual\n\t\n\n\n\nThe Synthdog dataset created for training in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\nUsing the official Synthdog code, we created >1 million training samples for improving OCR capabilities in Large Vision-Language Models.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nWe provide the images for download in two .tar.gz files. Download and extract them in folders of the same name (so cat images.tar.gz.* | tar xvzf -C images; tar xvzf‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100.","first_N":5,"first_N_keywords":["image-to-text","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"MJ-BENCH-VIDEO","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO","creator_name":"MJ-Bench-Team","creator_url":"https://huggingface.co/MJ-Bench","description":"This repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\".\nPaper: https://arxiv.org/abs/2502.01719\nCode: https://github.com/aiming-lab/MJ-Video\nProject Page: https://aiming-lab.github.io/MJ-VIDEO.github.io/\nThis repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\". We create a fine-grained video preference dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO.","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"pl-wiki-printsyn","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pniedzwiedzinski/pl-wiki-printsyn","creator_name":"Patryk Nied≈∫wiedzi≈Ñski","creator_url":"https://huggingface.co/pniedzwiedzinski","description":"pniedzwiedzinski/pl-wiki-printsyn dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Polish","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"outlined-text-captcha","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/muddokon/outlined-text-captcha","creator_name":"Sebasti√°n G√≥mez Rosas","creator_url":"https://huggingface.co/muddokon","description":"This image collection was web scrapped from a single site. \nIt comes from the SimpleCaptcha library, which uses a variety of formats including this one: outlined alphanumeric characters with a stroke.\n\n\t\n\t\t\n\t\tImage description:\n\t\n\n\nHorizontal Gradient.\nRGB Channels.\nJPG Format.\n5 characters.\nCharacters may repeat.\nNumeric and lowercase alphanumeric.\n\n\n\t\n\t\t\n\t\tVocab:\n\t\n\n\nAlpha: a b c d e f g h k m n p r v w x y (17 unique)\nNumber: 2 3 4 5 6 7 8 9 (8 unique)\nTotal chars = 25 unique characters‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/muddokon/outlined-text-captcha.","first_N":5,"first_N_keywords":["image-to-text","token-classification","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"VL3-Syn7M","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M","creator_name":"Language Technology Lab at Alibaba DAMO Academy","creator_url":"https://huggingface.co/DAMO-NLP-SG","description":"\n    \n\n\nThe re-caption dataset used in VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding\n\n If you like our project, please give us a star ‚≠ê on Github for the latest update.  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåü Introduction\n\t\n\nThis dataset is the re-captioned data we used during the training of VideoLLaMA3. It consists of 7 million diverse, high-quality images, each accompanied by a short caption and a detailed caption.\nThe images in this dataset originate from COYO-700M, MS-COCO 2017‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"multimodal_meme_classification_singapore","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aliencaocao/multimodal_meme_classification_singapore","creator_name":"Billy Cao","creator_url":"https://huggingface.co/aliencaocao","description":"\n\t\n\t\t\n\t\tDataset Card for Offensive Memes in Singapore Context\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a collection of memes from various existing datasets, online forums, and freshly scrapped contents. It contains both global-context memes and Singapore-context memes, in different splits. It has textual description and a label stating if it is offensive under Singapore society's standards.\n\nCurated by: Cao Yuxuan, Wu Jiayang, Alistair Cheong, Theodore Lee‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aliencaocao/multimodal_meme_classification_singapore.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-text-to-text","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"fastcup-highlights","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/fastcup-highlights","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Fastcup.net Highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 85,488 video clips from the gaming platform Fastcup.net, with 78,143 clips from Counter-Strike 2 and 7,345 clips from Counter-Strike: Global Offensive. The clips showcase gameplay highlights and include detailed metadata such as player statistics, weapon information, and engagement metrics. The total size of raw video content is approximately 34 TB.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/fastcup-highlights.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\t‚ö†Ô∏è IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tüö´ THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY üö´\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Please‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","image-to-text","visual-question-answering","language-modeling"],"keywords_longer_than_N":true},
	{"name":"Flame-Evo-React","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Evo-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Evo-React: A Diverse Data Synthesis Dataset for Multi-modal React Code Generation\n\t\n\nFlame-Evo-React is a dataset synthesized using the Evolution-Based Synthesis method, leveraging random evolutionary logic to generate a highly diverse set of React components. This approach systematically varies functionality, architecture, and visual style, providing a robust dataset for generalized React code generation.\nThis dataset includes in-breadth (feature expansion) and in-depth‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Evo-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Amazon-2023-GenQ","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/smartcat/Amazon-2023-GenQ","creator_name":"SmartCat","creator_url":"https://huggingface.co/smartcat","description":"\n\t\n\t\t\n\t\tAmazon Reviews Dataset for Query Generation\n\t\n\nThis dataset is designed for training models on tasks such as query generation, reranking, semantic search, and vision-language tasks (e.g., CLIP, VLMS) using Amazon product metadata.The original datasets can be found here: https://amazon-reviews-2023.github.io/\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a curated sample derived from seven filtered Amazon product category datasets \n(Amazon All Beauty, Amazon Fashion, Sports and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smartcat/Amazon-2023-GenQ.","first_N":5,"first_N_keywords":["summarization","sentence-similarity","text-classification","text-generation","image-to-text"],"keywords_longer_than_N":true},
	{"name":"Flame-Eval-React","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Eval-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Eval-React: A Benchmark Dataset for Multi-modal React Code Generation Evaluation\n\t\n\nFlame-Eval-React is the first benchmarking dataset specifically designed to evaluate the accuracy, functionality, and visual fidelity of vision-language models (VLMs) for React code generation.\nThis dataset includes manually curated React components that serve as the gold standard for evaluating the performance of image-to-code translation models.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nHere we provide the code guidance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Eval-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Caption-Anything-InContext","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Caption-Anything-InContext","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"Caption-Anything-InContext is a dataset curated using the model Caption-Pro for improved in-context captioning of images. This model is designed for generating multiple captions for images, ensuring they are contextually accurate.\n\n\t\n\t\t\n\t\tRequired Lib\n\t\n\n!pip install -q transformers qwen-vl-utils==0.0.2\n\nDemo with transformers\nimport os\nimport gdown\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption-Anything-InContext.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Chart-MRAG","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymyang/Chart-MRAG","creator_name":"Young Yurm","creator_url":"https://huggingface.co/ymyang","description":"\n\t\n\t\t\n\t\tBenchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nMultimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, Chart-based MRAG, to address this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymyang/Chart-MRAG.","first_N":5,"first_N_keywords":["question-answering","image-to-text","visual-question-answering","image-captioning","expert-generated"],"keywords_longer_than_N":true},
	{"name":"medmax_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israel‚Ä†, Siyan Zhao‚Ä†, Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angeles‚Ä† Equal Contribution\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilities‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Img2Text-Algorithm-Retrieval","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Img2Text-Algorithm-Retrieval","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tImg2Text-Algorithm-Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThe Img2Text-Algorithm-Retrieval dataset is designed for retrieving text descriptions of algorithms from corresponding images. This dataset consists of structured text, raw text, algorithm images, and metadata such as source URLs and filenames. It can be useful for tasks like OCR-based text retrieval, image-to-text learning, and document understanding.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModality: Image, Text  \nFormat: Parquet‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Img2Text-Algorithm-Retrieval.","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"gc-os-img-art-critic","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpfearnworks/gc-os-img-art-critic","creator_name":"JP","creator_url":"https://huggingface.co/jpfearnworks","description":"\n\t\n\t\t\n\t\tgc-os-img-art-critic\n\t\n\nExample gc dataset with art critic perspective\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains images with associated metadata including captions, tags, and verification information.\n","first_N":5,"first_N_keywords":["English","cc0-1.0","üá∫üá∏ Region: US","image-to-text","computer-vision"],"keywords_longer_than_N":true},
	{"name":"EgoNormia","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/open-social-world/EgoNormia","creator_name":"Open Social World","creator_url":"https://huggingface.co/open-social-world","description":"\n                EgoNormia: Benchmarking Physical-Social Norm Understanding      \n\n    MohammadHossein Rezaei*,¬†\n    Yicheng Fu*,¬†\n    Phil Cuvin*,¬†\n    Caleb Ziems,¬†\n    Yanzhe Zhang,¬†\n    Hao Zhu,¬†\n    Diyi Yang,¬†\n\n\n\n    üåéWebsite |\n    ü§ó Dataset |\n    üìÑ arXiv |\n    üìÑ HF Paper\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tEgoNormia\n\t\n\nEgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context.\nThe datset consists of 1,853 physically grounded egocentric \ninteraction clips from Ego4D‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-social-world/EgoNormia.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","question-answering","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"swedish_fraktur","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Riksarkivet/swedish_fraktur","creator_name":"AI Riksarkivet  / AIRA","creator_url":"https://huggingface.co/Riksarkivet","description":"\n\t\n\t\t\n\t\tSwedish Fraktur\n\t\n\n\n\nThis is a dataset for swedish blackletter from the 19th century. The transcriptions were made by Spr√•kbanken and converted into a text-line\ndataset by the Swedish National Archives\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\nTrain textline-based OCR models for swedish 19th century blackletter\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"image\": Image(),\n  \"text\": str\n}\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tSource Data\n\t\n\nOriginal dataset from Spr√•kbanken -‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Riksarkivet/swedish_fraktur.","first_N":5,"first_N_keywords":["image-to-text","Swedish","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Share4oReasoning/dpo_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data DPO\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nDPO_preview: Contains model generated CoT judged my outcome reward.\n\nImage use same in sft repo: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODO separate readme for setup and train.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSet up:\n\t\n\ngit clone‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/dpo_data.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OVO-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeLeelyf/OVO-Bench","creator_name":"Yifei Li","creator_url":"https://huggingface.co/JoeLeelyf","description":"\n  OVO-Bench: How Far is Your Video-LLMs from Real-World Online VideO Understanding?\n\n\n\n  üî•üî•OVO-Bench is accepted by CVPR 2025!üî•üî•\n\n\n\n   \n    \n  \n\n\n\nImportant Note: Current codebase is modified compared to our initial arXiv paper. We strongly recommend that any use of OVO-Bench should be based on current edition.\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\n\t\n\t\n\t\n\t\tüåü Three distinct problem-solving modes\n\t\n\n\nBackward Tracing: trace back to past events to answer the question.Real-Time Visual Perception:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JoeLeelyf/OVO-Bench.","first_N":5,"first_N_keywords":["video-text-to-text","English","cc-by-sa-4.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"M4-IT","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ColorfulAI/M4-IT","creator_name":"Yuxuan Wang","creator_url":"https://huggingface.co/ColorfulAI","description":"\n\t\n\t\t\n\t\tM4-IT\n\t\n\nThis dataset, M4-IT, is a synthetic instruction finetuning dataset used in the development of the M4 framework, designed to enhance real-time interactive reasoning in multi-modal language models.\nThe M4 framework is evaluated on OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts.\n\n\t\n\t\t\n\t\n\t\n\t\tData Description\n\t\n\nBuilding on the LLaVA-NeXT-Data, we crafted a small video-free synthetic instruction finetuning dataset, M4-IT, with the assistance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ColorfulAI/M4-IT.","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2503.22952","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"CoMT","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/czh-up/CoMT","creator_name":"ZihuiCheng","creator_url":"https://huggingface.co/czh-up","description":"\n   CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models\n\n\n\n      \n    | [ArXiv] | [ü§óHuggingFace] |\n    \n    \n\n\n\n\n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\n\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by AAAI 2025 !\nüî• We have release benchmark on [ü§óHuggingFace].\nüî• The paper is also available on [ArXiv].\n\n\n\t\n\t\n\t\n\t\tüí° Motivation\n\t\n\nLarge Vision-Language Models (LVLMs) have recently demonstrated amazing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/czh-up/CoMT.","first_N":5,"first_N_keywords":["question-answering","image-to-image","image-to-text","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Flame-Additive-React","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Additive-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","description":"\n\t\n\t\t\n\t\tFlame-Additive-React: An Iterative Data Synthesis Dataset for Multi-modal React Code Generation\n\t\n\nFlame-Additive-React is a dataset synthesized using the Additive Development Synthesis method, focusing on real-world React development patterns. This dataset ensures that training data remains grounded in realistic, incrementally enhanced code components.\nInstead of generating synthetic data from scratch, this approach builds upon human-authored React components, progressively increasing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Additive-React.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Newspapers-finlam-La-Liberte","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teklia/Newspapers-finlam-La-Liberte","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","description":"\n\t\n\t\t\n\t\tNewspaper dataset: Finlam La Libert√©\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Finlam La Libert√© dataset includes 1500 issues from La Libert√©, a French newspaper, from 1925 to 1928. \nEach issue contains multiple pages, with one image for each page resized to a fixed height of 2500 pixels.\nThe dataset can be used to train end-to-end newspaper understanding models, with tasks including:\n\nText zone detection and classification\nReading order detection\nArticle separation\n\n\n\t\n\t\t\n\t\n\t\n\t\tSplit‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Newspapers-finlam-La-Liberte.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-classification","image-segmentation","French"],"keywords_longer_than_N":true},
	{"name":"VideoChat-Flash-Training-Data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","description":"\n\t\n\t\t\n\t\tü¶ú VideoChat-Flash-Training-Data\n\t\n\nThis repos contains all annotaions and most videos for training VideoChat-Flash.\n\n\t\n\t\t\n\t\tüìï How to use the LongVid data?\n\t\n\nFor video_dir like longvid_subset/coin_grounding_10k_zip, you need to concat this dir to a zip file as follows:\ncat ego4dhcap_eventunderstanding_2k_zip/* > ego4dhcap_eventunderstanding_2k.zip\n\n\n\t\n\t\t\n\t\t‚úèÔ∏è Citation\n\t\n\n\n@article{li2024videochatflash,\n  title={VideoChat-Flash: Hierarchical Compression for Long-Context Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Datasets"],"keywords_longer_than_N":true},
	{"name":"SARD-Extended","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/riotu-lab/SARD-Extended","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD-Extended.","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","üá∫üá∏ Region: US","dataset"],"keywords_longer_than_N":true},
	{"name":"SARD-Extended","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/riotu-lab/SARD-Extended","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD-Extended.","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","üá∫üá∏ Region: US","dataset"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v182","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v182","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v182.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v183","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v183","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v183.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v184","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v184","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v184.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v185","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v185","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v185.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v187","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v187","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v187.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n\t\n\t\t\n\t\tOpen Image Preferences\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v190","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v190","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v190.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v195","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v195","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v195.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v196","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v196","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v196.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mmevol-zh-hant","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant","creator_name":"SyntaxSynth","creator_url":"https://huggingface.co/syntaxsynth","description":"\n\t\n\t\t\n\t\tMMEvol - Translated Chinese Traditional\n\t\n\nA subset of Tongyi-ConvAI/MMEvol translated using yentinglin/Llama-3-Taiwan-70B-Instruct from english to traditional chinese.\nRead the Note below before use.\nImage source distribution:\n\n\t\n\t\t\nDataset\nCount\nPercentage\n\n\n\t\t\ncoco\n6598\n29.8%\n\n\nQ-Instruct-DB\n5856\n26.4%\n\n\nclevr\n2383\n10.8%\n\n\nchartqa\n1733\n7.8%\n\n\nhfdata\n1296\n5.9%\n\n\ngeo170k\n706\n3.2%\n\n\ndata_engine\n6983.2%\n\n\nmathvision\n644\n2.9%\n\n\ndocvqa\n600\n2.7%\n\n\nalfworld\n401\n1.8%\n\n\narxivqa\n337\n1.5%‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant.","first_N":5,"first_N_keywords":["text-generation","image-to-text","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v202","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v202","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v202.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v203","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v203","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v203.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v205","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v205","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v205.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-Video-large-swift","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malterei/LLaVA-Video-large-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-medium-swift\n\t\n\nA subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v15","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v13","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v7","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v11","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v19","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v8","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 8\n\t\n\nReplaced RLE compressed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v9","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v6","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v164","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v164","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v164.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v166","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v166","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v166.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v169","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v169","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v169.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v170","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v170","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v170.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v171","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v171","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v171.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v173","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v173","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v173.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v175","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v175","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v175.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v178","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v178","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v178.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v179","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v179","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v179.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v186","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v186","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v186.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v188","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v188","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v188.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v189","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v189","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v189.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v191","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v191","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v191.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v192","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v192","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v192.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v193","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v193","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v193.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v197","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v197","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v197.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v198","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v198","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v198.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v199","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v199","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v199.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v200","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v200","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v200.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v201","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v201","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v201.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v204","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v204","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v204.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v206","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v206","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v206.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v207","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v207","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v207.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v208","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v208","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v208.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"lego_minifigure_captions","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions","creator_name":"Alessandro","creator_url":"https://huggingface.co/armaggheddon97","description":"\n\t\n\t\t\n\t\tLEGO Minifigure Captions\n\t\n\nThe LEGO Minifigure Captions dataset contains 12966 images of LEGO minifigures with captions. The dataset contains the following columns:\n\nimage: The jpeg image of the minifigure in the format {\"bytes\": bytes, \"path\": str} so that can be interpreted as PIL.Image objects in the huggingface datasets library.\nshort_caption: The short caption describing the minifigure in the image.\ncaption: The caption describing the minifigure which is generated using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v209","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v209","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v209.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v210","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v210","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v210.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v211","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v211","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v211.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"AVAINT-IMG","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botintel-community/AVAINT-IMG","creator_name":"BotIntel X","creator_url":"https://huggingface.co/botintel-community","description":"botintel-community/AVAINT-IMG dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","image-classification","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"AVAINT-IMG","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botintel-community/AVAINT-IMG","creator_name":"BotIntel X","creator_url":"https://huggingface.co/botintel-community","description":"botintel-community/AVAINT-IMG dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","image-classification","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"emova-alignment-7m","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-Alignment-7M\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-Alignment-7M is a comprehensive dataset curated for omni-modal pre-training, including vision-language and speech-language alignment. \nThis dataset is created using open-sourced image-text pre-training datasets, OCR datasets, and 2,000 hours of ASR and TTS data. \nThis dataset is part of the EMOVA-Datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m.","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"art-museums-pd-440k","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mitsua/art-museums-pd-440k","creator_name":"elanmitsua","creator_url":"https://huggingface.co/Mitsua","description":"\n\t\n\t\t\n\t\tArt Museums PD 440K\n\t\n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a dataset to train text-to-image or any text and image multimodal models with minimized copyright/licensing concerns.\nAll images and texts in this dataset are orignally shared under CC0 or public domain, and no pretrained models or any AI models are used to build this dataset except for our ElanMT model to translate English captions to Japanese.\nElanMT model is trained solely on licensed corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tData sources\n\t\n\nImages and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/art-museums-pd-440k.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"safe-commons-pd-3m","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mitsua/safe-commons-pd-3m","creator_name":"elanmitsua","creator_url":"https://huggingface.co/Mitsua","description":"\n\t\n\t\t\n\t\tSafe Commons PD 3M\n\t\n\n\nThis is a balanced and safe-to-use public domain / CC0 images dataset.\nAll images and texts come from Wikimedia Commons and Wikidata with strict filtering.\nImages license is either Public Domain or CC0 (varies by image).\nTexts license is either CC0 or CC BY-SA (varies by caption source).\nNo synthetic data (AI generated images or captions) is in the dataset.\n\nTo build this dataset, we tried to avoid any knowledge leaks from existing pre-trained models at the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/safe-commons-pd-3m.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Japanese","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-more-results","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for image-preferences-results Original\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CompCap-gpt4","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xchen16/CompCap-gpt4","creator_name":"Xiaohui Chen","creator_url":"https://huggingface.co/xchen16","description":"\n\t\n\t\t\n\t\tCompCap-GPT4: A GPT-4 Captioned Version of CompCap-118K\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: CompCap: Improving Multimodal Large Language Models with Composite Captions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\nDirect Download:The repository includes CI_type.zip and CI_type.json. The JSON file follows the Llava format:\n{\n  \"id\": ID,\n  \"image\": IMAGE_PATH,\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": QUESTION},\n    {\"from\": \"gpt\", \"value\": ANSWER}\n  ]\n}\n\n\nUsing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xchen16/CompCap-gpt4.","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-10K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512√ó512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-20K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512√ó512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Pix2Cap-COCO","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/geshang/Pix2Cap-COCO","creator_name":"geshang","creator_url":"https://huggingface.co/geshang","description":"\n\t\n\t\t\n\t\tPix2Cap-COCO\n\t\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPix2Cap-COCO is the first pixel-level captioning dataset derived from the panoptic COCO 2017 dataset, designed to provide more precise visual descriptions than traditional region-level captioning datasets. It consists of 20,550 images, partitioned into a training set (18,212 images) and a validation set (2,338 images), mirroring the original COCO split. The dataset includes 167,254 detailed pixel-level captions, each averaging‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/geshang/Pix2Cap-COCO.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"mypresentation","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/mypresentation","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for MyPresentation.ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 420,946 educational presentations in Russian extracted from mypresentation.ru website. The content includes presentation slides across various educational topics and categories.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian (ru).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nurl: URL of the presentation page (string)\ntitle:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/mypresentation.","first_N":5,"first_N_keywords":["text-classification","image-to-text","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"pix2tex","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anindya-hf-2002/pix2tex","creator_name":"Anindya Mitra","creator_url":"https://huggingface.co/anindya-hf-2002","description":"Here‚Äôs a completed dataset card for your Pix2Tex dataset based on the provided template:\n\n\n\t\n\t\t\n\t\tPix2Tex\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Pix2Tex Dataset is a high-quality, curated dataset for training and evaluating Vision-Language Models (VLMs) capable of extracting LaTeX code from images of mathematical formulas. This dataset combines both printed and handwritten formula images, offering diverse and challenging samples for tasks involving LaTeX recognition.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anindya-hf-2002/pix2tex.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"OmniMMI","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bigai-nlco/OmniMMI","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","description":"\n\t\n\t\t\n\t\tOmniMMI\n\t\n\nPaper: OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts\nCode\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nwe introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigai-nlco/OmniMMI.","first_N":5,"first_N_keywords":["video-text-to-text","mit","1K - 10K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"MIG-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michael4933/MIG-Bench","creator_name":"You Li","creator_url":"https://huggingface.co/Michael4933","description":"\n    \n\n\n\n\n\n\t\n\t\t\n\t\tMigician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models\n\t\n\nYou Li, Heyu Huang*, Chen Chi, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun\n\n        \nThis repository hosts the usage details of our training dataset MGrounding-630k and benchmark MIG-Bench and the training implementation of Migician, the first competitive Multi-image Grounding MLLM capable of free-form grounding.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Michael4933/MIG-Bench.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Text2Face","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/oguzhanercan/Text2Face","creator_name":"Oƒüuzhan Ercan","creator_url":"https://huggingface.co/oguzhanercan","description":"This is version 1.0 of Text2Face dataset. This dataset generated by using Flux1.dev (Nunchaku 4 bit optimization method). Facial descriptions generated with promptgen.py script. More advanced version of prompt generator is also available as promptgenv2.py. For more detailed facial descriptions, you can generate with that.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MMathCoT-1M","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/URSA-MATH/MMathCoT-1M","creator_name":"URSA-MATH","creator_url":"https://huggingface.co/URSA-MATH","description":"\n\t\n\t\t\n\t\tMMathCoT-1M\n\t\n\nThis repository contains the data presented in URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics.\nCode: https://github.com/URSA-MATH/URSA-MATH\nImage data can be downloaded from the following address:\n\nMAVIS: https://github.com/ZrrSkywalker/MAVIS, https://drive.google.com/drive/folders/1LGd2JCVHi1Y6IQ7l-5erZ4QRGC4L7Nol.\nMultimath: https://huggingface.co/datasets/pengshuai-rin/multimath-300k.\nGeo170k:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/URSA-MATH/MMathCoT-1M.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","gpl-3.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"vqa-rad-ko","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/junyeong-nero/vqa-rad-ko","creator_name":"Junyeong Song","creator_url":"https://huggingface.co/junyeong-nero","description":"junyeong-nero/vqa-rad-ko dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"FaceCaptionHQ-4M","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFaceCaptionHQ-4M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing the Agreement\nFaceCaptionHQ-4M contains about 4M facial image-text pairs that cleaned from FaceCaption-15M .  \n\n\n\n\n\t\n\t\n\t\n\t\tFigure.1 Illustrations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Medical_Multimodal_Evaluation_Data","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/Medical_Multimodal_Evaluation_Data","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\n\t\n\t\t\n\t\tEvaluation Guide\n\t\n\nThis dataset is used to evaluate medical multimodal LLMs, as used in HuatuoGPT-Vision. It includes benchmarks such as VQA-RAD, SLAKE, PathVQA, PMC-VQA, OmniMedVQA, and MMMU-Medical-Tracks.  \nTo get started:  \n\nDownload the dataset and extract the images.zip file.  \nFind evaluation code on our GitHub: HuatuoGPT-Vision.\n\nThis open-source release aims to simplify the evaluation of medical multimodal capabilities in large models. Please cite the relevant benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/Medical_Multimodal_Evaluation_Data.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Tarsier2-Recap-585K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K","creator_name":"omni-research","creator_url":"https://huggingface.co/omni-research","description":"\n\t\n\t\t\n\t\tDataset Card for Tarsier2-Recap-585K\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n‚ú®Tarsier2-Recap-585K‚ú® consists of 585K distinct video clips, lasting for 1972 hours in total, from open-source datasets (e.g. VATEX, TGIF, LSMDC, etc.) and each one with a detailed video description annotated by Tarsier2-7B, which beats GPT-4o in generating detailed and accurate video descriptions for video clips of 5~20 seconds (See the DREAM-1K Leaderboard). Experiments demonstrate its effectiveness in enhancing the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","Video","arxiv:2501.07888"],"keywords_longer_than_N":true},
	{"name":"Data-DeQA-Score","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhiyuanyou/Data-DeQA-Score","creator_name":"zhiyuanyou","creator_url":"https://huggingface.co/zhiyuanyou","description":"\n\t\n\t\t\n\t\tData-DeQA-Score\n\t\n\nDatasets of the DeQA-Score paper \n(\nproject page / \ncodes / \npaper \n) \nin our DepictQA project.\n\n\t\n\t\t\n\t\tDataset Construction\n\t\n\n\nDownload our meta files in this repo. \n\nDownload source images from KonIQ, \nSPAQ, \nKADID, \nPIPAL, \nLIVE-Wild, \nAGIQA, \nTID2013, \nand CSIQ.\n\nArrange the folders as follows:\n\n\n|-- Data-DeQA-Score\n  |-- KONIQ\n    |-- images/*.jpg\n    |-- metas\n  |-- SPAQ\n    |-- images/*.jpg\n    |-- metas\n  |-- KADID10K\n    |-- images/*.png\n    |-- metas\n  |--‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zhiyuanyou/Data-DeQA-Score.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","arxiv:2501.11561"],"keywords_longer_than_N":true},
	{"name":"RSTeller","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SlytherinGe/RSTeller","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","description":"\n\t\n\t\t\n\t\t‚ö†Ô∏è Usage Warning\n\t\n\nThis is the latest version of RSTeller, updated on 2025-01-28. Users who accessed this dataset before this date can find the legacy version, which is preserved for reference. Additionally, we have released the metadata for this dataset.\nFor the details and the usage of the dataset, please refer to our github repository page.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find the dataset and our paper useful, please consider citing our paper:\n@article{ge2025rsteller‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-xm100","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-xm100","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM100\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"windata-vision-synthetics-zh-300k","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/winninghealth/windata-vision-synthetics-zh-300k","creator_name":"Winning Health AI Research","creator_url":"https://huggingface.co/winninghealth","description":"‰ªãÁªç\nÊàë‰ª¨Êï¥ÁêÜÁîüÊàê‰∫Ü‰∏Ä‰∏™‰∏≠ÊñáÂ§öÊ®°ÊÄÅÂõæÊñáÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÂåÖÂê´‰∫ÜÂ§ßÁ∫¶30‰∏áÊù°Êï∞ÊçÆ‰ª•ÂèäÁ∫¶20‰∏áÂº†ÂõæÁâáÔºåÊ∂âÂèäÊñáÊ°£doc„ÄÅÂõæË°®„ÄÅÊï∞Â≠¶„ÄÅOCRÁ≠âÂ§öÁßçÂú∫ÊôØ„ÄÇ\nÈíàÂØπÂºÄÊ∫êÊï∞ÊçÆ‰∏≠‰∏≠ÊñáÂõæÊñáÊåá‰ª§ÈõÜÂ∞ë‰∏îÊåá‰ª§ÈõÜÊèèËø∞ÊôÆÈÅçËøá‰∫éÁÆÄÁü≠Á≠âÈóÆÈ¢òÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂºÄÊ∫êÊ®°ÂûãÁöÑÂêàÊàêÊï∞ÊçÆÁîüÊàêÊñπÊ≥ïÔºåÂà©Áî® Qwen2-vl-72B-Instruct ÁîüÊàêËæÉ‰∏∫ËØ¶ÁªÜÁöÑ‰∏≠ÊñácaptionÊåá‰ª§ÈõÜÔºåÁÑ∂ÂêéÂú®Âêå‰∏ÄÂú∫ÊôØ‰∏≠ÈöèÊú∫ÊåëÈÄâ1-4Âº†ÂõæÁâáÂíåÁõ∏Â∫îÁöÑ‰∏≠ÊñácaptionÔºåÂ∞ÜcaptionÊï∞ÊçÆÁªôÂà∞Êàë‰ª¨ÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã WiNGPT-2.6 ÈÄöËøáËÆæËÆ°Á≥ªÁªüÊåá‰ª§‰ΩøÂÖ∂ÊØèËΩÆËøõË°åÊèêÈóÆÔºåÂ∞ÜÈóÆÈ¢òÂíåÂõæÁâáÁªôÂà∞ Qwen2-vl-72B-Instruct ‰ΩøÂÖ∂ËøõË°åÂõûÁ≠îÔºõÊúÄÂêéËÆæÂÆöÂæ™ÁéØÊ¨°Êï∞ÔºåÂæóÂà∞Â§öËΩÆÂ§öÂõæÁöÑÂØπËØùÊï∞ÊçÆ„ÄÇ \nÂØπ‰∫éÁîüÊàêÂêéÁöÑÊï∞ÊçÆÔºåÊ†πÊçÆÁ≠îÊ°àÁöÑÈïøÂ∫¶„ÄÅËØ≠Âè•ÁöÑÈáçÂ§çÊÄßÁ≠âËøõË°å‰∫ÜËßÑÂàôËøáÊª§ÔºõÊï∞Â≠¶Á±ªÈ¢òÁõÆÔºåÊ†πÊçÆÂéüÂßãÊï∞ÊçÆÁöÑÁ≠îÊ°àËøõË°å‰∫ÜËøáÊª§„ÄÇÂú®Âà∂‰ΩúÊúÄÂêéÁöÑcaptionÊåá‰ª§ÈõÜÊó∂ÔºåÊàë‰ª¨ÈíàÂØπÊØè‰∏Ä‰∏™Âú∫ÊôØÈÉΩËÆæËÆ°‰∫Ü‰∏äÁôæ‰∏™ÈóÆÈ¢òÔºå‰øùËØÅ‰∫ÜcaptionÊï∞ÊçÆÈõÜÁöÑÂ§öÊ†∑ÊÄßÔºõÂú®ÂØπËØùÊï∞ÊçÆÈõÜ‰∏äÔºåÊàë‰ª¨Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÊù•ËÆ©WiNGPT-2.6‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/winninghealth/windata-vision-synthetics-zh-300k.","first_N":5,"first_N_keywords":["image-to-text","Chinese","cc-by-4.0","100K<n<1M","arxiv:2409.11402"],"keywords_longer_than_N":true},
	{"name":"VisNumBench","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GML-FMGroup/VisNumBench","creator_name":"Foundation Model Group at Guangming Laboratory","creator_url":"https://huggingface.co/GML-FMGroup","description":"This dataset is designed for research in Deep Learning for Geometry Problem Solving (DL4GPS) and accompanies the survey paper A Survey of Deep Learning for Geometry Problem Solving. It aims to provide a structured resource for evaluating and training AI models, particularly multimodal large language models (MLLMs), on mathematical reasoning tasks involving geometric contexts.\nThe dataset provides a collection of geometry problems, each consisting of a textual question and a corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GML-FMGroup/VisNumBench.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"minimal_video_pairs","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/minimal_video_pairs","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tMinimal Video Pairs\n\t\n\nA shortcut-aware benchmark for spatio-temporal and intuitive physics video understanding (VideoQA) using minimally different video pairs.\n\nGithub\n\n\n  \n\n\nFor legal reasons, we are unable to upload the videos directly to Huggingface. However, we provide scripts in this repository for downloading the videos in our github repository. Our benchmark is built on top of videos source from 9 domains:\n\n\t\n\t\t\nSubset\nData sources\n\n\n\t\t\nHuman object interactions\nPerceptionTest‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/minimal_video_pairs.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Live-CC-5M","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chenjoya/Live-CC-5M","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","description":"\n\t\n\t\t\n\t\tDataset Card for Live-CC-5M\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for LiveCC-7B-Base model pre-training. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tLive-CC-5M Dataset\n\t\n\n\nStatistics: 5,047,208 YouTube Video-CC 30~240s samples.\n\n\nAnnotation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-CC-5M.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1M<n<10M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"ReflectiVA-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aimagelab/ReflectiVA-Data","creator_name":"AImageLab","creator_url":"https://huggingface.co/aimagelab","description":"In this datasets space, you will find the data of Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering.\nFor more information, visit our ReflectiVA repository, our project page and model space.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you make use of our work, please cite our repo:\n@inproceedings{cocchi2024augmenting,\n  title={{Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering}},\n  author={Cocchi, Federico and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aimagelab/ReflectiVA-Data.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2411.16863","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"VIDGEN-1K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a video-text-to-text dataset used in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\".\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tInstallation\n\t\n\n\nClone the github repository and navigate to LiFT folder\n\ngit clone https://github.com/CodeGoat24/LiFT.git\ncd LiFT\n\n\nInstall packages\n\nbash‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"KYC2","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiteshKamwal/KYC2","creator_name":"Hitesh Kamwal","creator_url":"https://huggingface.co/HiteshKamwal","description":"HiteshKamwal/KYC2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"RefChartQA","keyword":"image-text-to-text","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/omoured/RefChartQA","creator_name":"Omar Moured","creator_url":"https://huggingface.co/omoured","description":"\n\t\n\t\t\n\t\tüß† About\n\t\n\nRefChartQA is a large-scale benchmark for visual grounding in chart-based question answering. It extends the ChartQA and TinyChart-PoT datasets by adding explicit bounding box annotations that link each answer to supporting visual elements in the chart. RefChartQA contains 73,702 annotated samples, including:\n\n55,789 training,\n6,223 validation,\n11,690 testing instances.\n\nFor details, see our paper and GitHub repository.\n\n  \n\n\n\n\t\n\t\n\t\n\t\tüõ†Ô∏è Usage\n\t\n\n\n\t\n\t\n\t\n\t\tüì¶ Environment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omoured/RefChartQA.","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","image-text-to-text","English","agpl-3.0"],"keywords_longer_than_N":true},
	{"name":"unprocessed_hcs","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BenjaminKost/unprocessed_hcs","creator_name":"Benjamin Kostka","creator_url":"https://huggingface.co/BenjaminKost","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is not data that I created. It originally came from the Paper Digitization of Handwritten Chess Scoresheets with a BiLSTM Network\nYou can also find the dataset Chesscorner/HCS_Dataset-csv, Chesscorner/HCS_pictures, here and here\n\n\t\n\t\t\n\t\n\t\n\t\tDatasets\n\t\n\nThere are 2 versions of this dataset\n\nunprocessed_hcs Dataset where you are right now\nprocessed_hcs Dataset where the extracted move boxes are provided which can be found here\n\n\n\t\n\t\n\t\n\t\tDescription\n\t\n\nThe Handwritten‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BenjaminKost/unprocessed_hcs.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"processed_hcs","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BenjaminKost/processed_hcs","creator_name":"Benjamin Kostka","creator_url":"https://huggingface.co/BenjaminKost","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is not data that I created. It originally came from the Paper Digitization of Handwritten Chess Scoresheets with a BiLSTM Network\nYou can also find the dataset here and here\n\n\t\n\t\t\n\t\tDatasets\n\t\n\nThere are 2 versions of this dataset\n\nprocessed_hcs Dataset where you are right now\nunprocessed_hcs where the whole scoresheet can be seen here\n\n\n\t\n\t\t\n\t\tDesciption\n\t\n\nThe Handwritten Chess Scoresheet Datase (HCS) contains a set of single and double paged chess scoresheet‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BenjaminKost/processed_hcs.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"arocrbench_khatt","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ahmedheakl/arocrbench_khatt","creator_name":"Ahmed Heakl","creator_url":"https://huggingface.co/ahmedheakl","description":"KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding\nThis dataset is designed to evaluate the performance of Arabic OCR and document understanding systems. It includes a variety of document types and tasks.\nPlease see paper & code for more information:\n\nGitHub Repository\nProject Page\narXiv Paper\n\n","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MM-RLHF","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/MM-RLHF","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n\n[üìñ arXiv Paper] \n[üìä Training Code] \n[üìù Homepage] \n[üèÜ Reward Model] \n[üîÆ MM-RewardBench] \n[üîÆ MM-SafetyBench] \n[üìà Evaluation Suite] \n\n\n\n\n\t\n\t\t\n\t\tThe Next Step Forward in Multimodal LLM Alignment\n\t\n\n[2025/02/10] üî• We are proud to open-source MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. This release includes:\n\nA high-quality MLLM alignment dataset.\nA strong Critique-Based MLLM reward model and its training algorithm.\nA novel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MM-RLHF.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-16k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-16k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","description":"\n\t\n\t\t\n\t\theb-synthtiger-16k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-16k is a dataset containing 16,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-16k.","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-30k","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-30k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","description":"\n\t\n\t\t\n\t\theb-synthtiger-30k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-30 is a dataset containing 30,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-30k.","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"flickr8k","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tsystems/flickr8k","creator_name":"T-Systems International","creator_url":"https://huggingface.co/tsystems","description":"tsystems/flickr8k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"chartqa-caption-gpt4","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xchen16/chartqa-caption-gpt4","creator_name":"Xiaohui Chen","creator_url":"https://huggingface.co/xchen16","description":"Caption data for ChartQA images.\n","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"allstar","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/allstar","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Allstar.gg Clips\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 47,896 video clips from the gaming platform allstar.gg. The clips primarily focus on Counter-Strike 2 gameplay moments and include detailed metadata such as player information, game statistics, view counts, and related media URLs.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English (en).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/allstar.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"vf-eval","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/songtingyu/vf-eval","creator_name":"Tingyu Song","creator_url":"https://huggingface.co/songtingyu","description":"\n\t\n\t\t\n\t\tDataset Card for VF-Eval Benchmark\n\t\n\nRepository: sighingsnow/vf-eval\nFor the usage of this dataset, please refer to the github repo. \nIf you find this repository helpful, feel free to cite our paper:\n@misc{song2025vfeval,\n      title={VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos}, \n      author={Tingyu Song and Tongyan Hu and Guo Gan and Yilun Zhao},\n      year={2025},\n      eprint={2505.23693},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songtingyu/vf-eval.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","mit","1K<n<10K","Video"],"keywords_longer_than_N":true},
	{"name":"tamily-1","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sasicodes/tamily-1","creator_name":"Sasi","creator_url":"https://huggingface.co/sasicodes","description":"\n\t\n\t\t\n\t\tTamily-1: Ancient Tamil OCR Synthetic Dataset\n\t\n\nTamizhi \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø\"\n\n\t\n\t\t\n\t\tDescription\n\t\n\n\nRepository: sasicodes/tamily-1\nPoint of Contact: @sasicodes\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nTamily-1 is an ancient Tamil OCR synthetic dataset generated from the first 200,000 rows of Solvari-1, a large Tamil text corpus. The dataset contains rendered images of Tamil text with various augmentations and styles, making it suitable for training OCR models.\n\n\t\n\t\t\n\t\tFields\n\t\n\n\nimage: PNG image of rendered Tamil‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sasicodes/tamily-1.","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","sasicodes/solvari-1","Tamil","mit"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTrain","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","description":"\n\t\n\t\t\n\t\tGuardReasoner-VLTrain\n\t\n\nGuardReasoner-VLTrain is the training data for R-SFT of GuardReasoner-VL, as described in the paper GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nCode: https://github.com/yueliu1999/GuardReasoner-VL/\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"yueliu1999/GuardReasoner-VLTrain\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite our paper.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"jedi","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rronan-h/jedi","creator_name":"Ronan Riochet","creator_url":"https://huggingface.co/rronan-h","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nThe JEDI Dataset consists of four carefully designed categories:\n\nIcon\nComponent\nLayout\nRefusal\n\nThis repository includes all the textures and images for these components.\nAdditionally, JEDI processes and improves the data from AGUVIS, calling it AGUVIS++. This repository contains the texture portion of AGUVIS++. For images, please refer to the original repository.\n\n\t\n\t\t\n\t\n\t\n\t\tüìÑ Citation\n\t\n\nIf you find this work useful, please consider citing our paper:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rronan-h/jedi.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"LaTeX-OCR-dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lukbl/LaTeX-OCR-dataset","creator_name":"Lukas Blecher","creator_url":"https://huggingface.co/lukbl","description":"\n\t\n\t\t\n\t\tLaTeX-OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset was created to train LaTeX-OCR, a model for recognizing LaTeX code from images of mathematical formulas. Each sample consists of a synthetically rendered formula image and its corresponding LaTeX formula. The images were generated from scratch using xelatex with multiple fonts, offering more typographic variety than many other datasets that use a single font (typically Computer Modern).\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nFormulas were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lukbl/LaTeX-OCR-dataset.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"visual_genome_revised","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AnnaZ1103/visual_genome_revised","creator_name":"Huiqi Zou","creator_url":"https://huggingface.co/AnnaZ1103","description":"AnnaZ1103/visual_genome_revised dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","object-detection","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ViMUL-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ViMUL-Bench","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n\t\n\t\t\n\t\tViMUL-Bench: A Culturally-diverse Multilingual Multimodal Video Benchmark\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe evaluation toolkit to be used is lmms-eval. This toolkit facilitates the evaluation of models across multiple tasks and languages.\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\nüåç 14 Languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, Japaneseüé≠ 15 Categories: Including 8 culturally diverse categories (lifestyles, festivals, foods‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ViMUL-Bench.","first_N":5,"first_N_keywords":["video-text-to-text","cc-by-sa-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"textureninja","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/textureninja","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Texture Ninja\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,540 texture images from texture.ninja. It includes high-resolution textures of brick, concrete, rock, wood, metal, paint, plaster, ground materials, and other surfaces. The original images were downloaded, processed, and compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/textureninja.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"pbrpx","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/pbrpx","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for PBRPX Asset Library\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 710 physically-based rendering (PBR) assets from pbrpx.com, a CC0 asset library. The dataset includes comprehensive information about 3D models, textures, and materials with detailed file listings, download URLs, texture resolutions, and categorization data. Assets include nature elements (trees, rocks), architectural materials (brick, concrete), and various other 3D models with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pbrpx.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"CC3M_synthetic","keyword":"image-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/timjeffrey10/CC3M_synthetic","creator_name":"ÂºµÂ∫≠Áëú","creator_url":"https://huggingface.co/timjeffrey10","description":"\n\t\n\t\t\n\t\tDataset Card for CC3M_synthetic\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of synthetic captions for Conceptual Captions. The alt_text is the original caption in CC3M dataset. The synthetic_captions were produced by Florence-2-large.\n\n\t\n\t\t\n\t\tProcedure\n\t\n\nWe captioned the images by Florence-2-large. We gave the model DETAILED_CAPTION task with a beam search size of 3. To ensure quality of the captions, we filtered out samples with too short (fewer than 50 characters) or excessively‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timjeffrey10/CC3M_synthetic.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"UniMER","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deepcopy/UniMER","creator_name":"deep copy","creator_url":"https://huggingface.co/deepcopy","description":"\n\t\n\t\t\n\t\tUniMER Dataset\n\t\n\nFor detailed instructions on using the dataset, please refer to the project homepage: UniMERNet Homepage\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe UniMER dataset is a specialized collection curated to advance the field of Mathematical Expression Recognition (MER). It encompasses the comprehensive UniMER-1M training set, featuring over one million instances that represent a diverse and intricate range of mathematical expressions, coupled with the UniMER Test Set, meticulously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepcopy/UniMER.","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"new_real_datasets","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hongin9812/new_real_datasets","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"kaggle_data","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hongin9812/kaggle_data","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"testrtt","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hongin9812/testrtt","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"ChartEdit","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xxxllz/ChartEdit","creator_name":"Xuanle Zhao","creator_url":"https://huggingface.co/xxxllz","description":"After download the data, extract the folder via\ntar -xzvf chartedit.tar.gz\n\nThe folder is organized as\n|-- ChartEdit\n    |-- chartedit/\n        |-- reference/\n            |-- figs/\n            |-- code/\n        |-- source/\n        |--instruction.jsonl\n\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"TextBraTS","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Jupitern52/TextBraTS","creator_name":"SHI XIAOYU","creator_url":"https://huggingface.co/Jupitern52","description":"\n\t\n\t\t\n\t\tTextBraTS\n\t\n\nA volume-level text-image public dataset with novel text-guided 3D brain tumor segmentation from BraTS challenge.\nPaper\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nTextBraTS is an open-access dataset designed to advance research in text-guided 3D brain tumor segmentation. It includes paired multi-modal brain MRI scans and expertly annotated radiology reports, enabling the development and evaluation of multi-modal deep learning models that bridge vision and language in neuro-oncology. Our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jupitern52/TextBraTS.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"dataset_111-220","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hongin9812/dataset_111-220","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"KC-MMbench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kwai-Keye/KC-MMbench","creator_name":"Kwai-Keye","creator_url":"https://huggingface.co/Kwai-Keye","description":"  [üçé Home Page] [üìñ Technical Report] [\\ud83d\\udcca Models] [\\ud83d\\ude80 Demo] \nThis repository contains KC-MMBench, a new benchmark dataset meticulously tailored for real-world short-video scenarios, as presented in the paper \"Kwai Keye-VL Technical Report\". Constructed from Kuaishou short video data, KC-MMBench comprises 6 distinct datasets designed to evaluate the performance of Vision-Language Models (VLMs) like Kwai Keye-VL-8B, Qwen2.5-VL, and InternVL in comprehending dynamic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kwai-Keye/KC-MMbench.","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","cc-by-sa-4.0","arxiv:2507.01949"],"keywords_longer_than_N":true},
	{"name":"OpenDoc-Pdf-Preview","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/OpenDoc-Pdf-Preview","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpenDoc-Pdf-Preview\n\t\n\nOpenDoc-Pdf-Preview is a compact visual preview dataset containing 6,000 high-resolution document images extracted from PDFs. This dataset is designed for Image-to-Text tasks such as document OCR pretraining, layout understanding, and multimodal document analysis.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nModality: Image-to-Text\nContent Type: PDF-based document previews\nNumber of Samples: 6,000\nLanguage: English\nFormat: Parquet\nSplit: train only\nSize: 606 MB\nLicense: Apache‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/OpenDoc-Pdf-Preview.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"blip3o-caption-mini-arrow","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/blip3o-caption-mini-arrow","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tblip3o-caption-mini-arrow\n\t\n\nblip3o-caption-mini-arrow is a high-quality, curated image-caption dataset derived and optimized from the original BLIP3o/BLIP3o-Pretrain-Long-Caption. This dataset is specifically filtered and processed for tasks involving long-form image captioning and vision-language understanding.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nTotal Samples: 91,600\nModality: Image ‚Üî Text\nFormat: Arrow (auto-converted to Parquet)\nLicense: Apache 2.0\nLanguage: English\nSize: ~4.5 GB‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/blip3o-caption-mini-arrow.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"Embodied-Captioning","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TommyBsk/Embodied-Captioning","creator_name":"Tommaso ","creator_url":"https://huggingface.co/TommyBsk","description":"\n\t\n\t\t\n\t\tEmbodied Image Captioning ‚Äì Manually Annotated Test Set\n\t\n\nPaper: Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions (ICCV 2025)Authors: Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo NataleAffiliations: Italian Institute of Technology (IIT), University of GenoaProject Website: https://hsp-iit.github.io/embodied-captioning\n\n\n\t\n\t\t\n\t\n\t\n\t\tüì¶ Dataset Description\n\t\n\nThis repository contains the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TommyBsk/Embodied-Captioning.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"OGC_Geotechnie_Compatible_Negatives","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noebrndl/OGC_Geotechnie_Compatible_Negatives","creator_name":"No√© BRANDOLINI","creator_url":"https://huggingface.co/noebrndl","description":"\n\t\n\t\t\n\t\tOGC_Geotechnie_Corrected\n\t\n\nCorrected version of racineai/OGC_Geotechnie with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/noebrndl/OGC_Geotechnie_Compatible_Negatives.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"OGC_Energy_Compatible_Negatives","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/noebrndl/OGC_Energy_Compatible_Negatives","creator_name":"No√© BRANDOLINI","creator_url":"https://huggingface.co/noebrndl","description":"\n\t\n\t\t\n\t\tOGC_Energy_Corrected\n\t\n\nCorrected version of racineai/OGC_Energy with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Matchone7/OGC_Energy_Corrected\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/noebrndl/OGC_Energy_Compatible_Negatives.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CalliBench","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gtang666/CalliBench","creator_name":"TJQ","creator_url":"https://huggingface.co/gtang666","description":"\n\t\n\t\t\n\t\tüß† CalliReader: Contextualizing Chinese Calligraphy via an Embedding-aligned Vision Language Model\n\t\n\n\n  üìÇ Code\n  üìÑ Paper\n\n\nCalliBench is aimed to comprehensively evaluate VLMs' performance on the recognition and understanding of Chinese calligraphy. \n\n\t\n\t\t\n\t\tüì¶ Dataset Summary\n\t\n\n\nSamples: 3,192 image‚Äìannotation pairs\n\nTasks: Full-page recognition and Contextual VQA (choice of author/layout/style, bilingual interpretation, and intent analysis).\n\nAnnotations:\n\nMetadata of author‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gtang666/CalliBench.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"french-lot-department-captioned-photos","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tLot Department, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from the Lot region of France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from three notable locations in France's Lot department: Rocamadour, Autoire, and Padirac. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"CoF-SFT-Data-5.4k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k","creator_name":"Xintong Zhang","creator_url":"https://huggingface.co/xintongzhang","description":"Overview.\nThis dataset is used for supervised fine-tuning (SFT) in training Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL,\nDetails.\nThis dataset includes 5.4k reasoning samples: 2.4k involve zoom-in behavior, and 3k are text-only reasoning cases. All samples were generated using our Visual Agent and span a variety of image resolutions.\nTraining Code: The SFT code can be found at https://github.com/xtong-zhang/Chain-of-Focus\nProject page:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","Image","arxiv:2505.15436"],"keywords_longer_than_N":true},
	{"name":"SENTINEL","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/psp-dada/SENTINEL","creator_name":"Peng Shangpin","creator_url":"https://huggingface.co/psp-dada","description":"\n\t\n\t\t\n\t\tDataset Card for SENTINEL\n\t\n\nGitHub\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"t2i_safety_dataset","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset","creator_name":"OpenSafetyLab","creator_url":"https://huggingface.co/OpenSafetyLab","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a source for the training and testing of text-to-image safety judge.\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: [https://github.com/adwardlee/t2i_safety]\nPaper: [https://arxiv.org/abs/2501.12612]\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\ncat train.zip.part-a* > train.zip\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n    \"id\": 0,\n    \"image_category\": \"sexual\",\n    \"text_category\": \"sexual\",\n    \"base_category\": \"toxicity\",\n    \"caption\": \"\",\n    \"real\": false‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","apache-2.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"albi-captioned-photos","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tAlbi, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from Albi, France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from Albi, France, including the city center, the Toulouse Lautrec museum, and the Sainte-C√©cile Cathedral. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:\n\nHigh-resolution‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"viggo_french","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/viggo_french","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFrench translation of the ViGGO dataset.According of the English version dataset card, ViGGO is an data-to-text generation dataset in the video game domain, with target responses being more conversational than information-seeking, yet constrained to the information presented in a meaning representation. The dataset is relatively small with about 5,000 datasets but very clean, and can thus serve for evaluating transfer learning, low-resource, or few-shot capabilities of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/viggo_french.","first_N":5,"first_N_keywords":["table-to-text","French","cc-by-sa-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"web_nlg_french","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/web_nlg_french","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFrench translation of the WebNLG dataset.According of the English version dataset card, WebNLG is a bi-lingual dataset (English, Russian) of parallel DBpedia triple sets and short texts that cover about 450 different DBpedia properties. The WebNLG data was originally created to promote the development of RDF verbalisers able to generate short text and to handle micro-planning (i.e., sentence segmentation and ordering, referring expression generation, aggregation); the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/web_nlg_french.","first_N":5,"first_N_keywords":["table-to-text","French","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"llava-test-nonmember-v3","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iammytoo/llava-test-nonmember-v3","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","description":"\n\t\n\t\t\n\t\tLLaVA-1.5 Test Images Not in LAION2B\n\t\n\nThis dataset contains validation and test images from various vision-language datasets that are commonly used to evaluate LLaVA-1.5 and similar models, but are not present in the LAION2B dataset.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"iammytoo/llava-test-nonmember-v3\")\n\n","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ccpd2019train","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/okita-souji/ccpd2019train","creator_name":"okita souji","creator_url":"https://huggingface.co/okita-souji","description":"CCPD2019 training dataset. Used for my training on Kaggle.\nIf you are benefited from this dataset, please cite their paper as follows:\n@inproceedings{xu2018towards,\n  title={Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline},\n  author={Xu, Zhenbo and Yang, Wei and Meng, Ajin and Lu, Nanxue and Huang, Huan},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  pages={255--271},\n  year={2018}\n}\n\n","first_N":5,"first_N_keywords":["object-detection","image-to-text","Chinese","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"PdfParser","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stauroskou/PdfParser","creator_name":"Stavros Koutsoukos","creator_url":"https://huggingface.co/stauroskou","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stauroskou/PdfParser.","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"popp","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thomas-C/popp","creator_name":"Thomas Constum","creator_url":"https://huggingface.co/thomas-C","description":"\n\t\n\t\t\n\t\tPOPP datasets\n\t\n\nThis repository contains 3 datasets created by the LITIS lab (University of Rouen Normandie) within the POPP project (Project for the Oceration of the Paris Population Census) for the task of handwriting text recognition.\nThese datasets have been published in Recognition and information extraction in historical handwritten tables: toward understanding early 20th century Paris census at DAS 2022 from T. Constum et al and are also available on Zenodo.\nThe 3 datasets are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thomas-C/popp.","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ACON","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jiwan-chung/ACON","creator_name":"Jiwan Chung","creator_url":"https://huggingface.co/jiwan-chung","description":"\n\t\n\t\t\n\t\tDataset Card for ACON Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nData from: Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?\n@inproceedings{chung2025are,\n  title={Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?},\n  author={Chung, Jiwan and Yoon, Janghan and Park, Junhyeong and Lee, Sangeyl and Yang, Joowon and Park, Sooyeon and Yu, Youngjae},\n  booktitle={Proceedings of the 63rd Annual Meeting of the Association for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jiwan-chung/ACON.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PCogAlignBench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YongqiLi/PCogAlignBench","creator_name":"Yongqi Li","creator_url":"https://huggingface.co/YongqiLi","description":"\n\t\n\t\t\n\t\tAligning VLM Assistants with Personalized Situated Cognition (ACL 2025 main)\n\t\n\n\n\n\nThis repository contains the constructed benchmark in our ACL 2025 main paper \"Aligning VLM Assistants with Personalized Situated Cognition\". \n\n‚ö†Ô∏è This project is for academic research only and not intended for commercial use.\n\n\n\t\n\t\n\t\n\t\tAbstract\n\t\n\nVision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YongqiLi/PCogAlignBench.","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","Image","arxiv:2506.00930","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"vlms-are-biased","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anvo25/vlms-are-biased","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","description":"\n\t\n\t\t\n\t\tVision Language Models are Biased \n\t\n\n    \n  by \n    An Vo1*,\n    Khai-Nguyen Nguyen2*,\n    Mohammad Reza Taesiri3, \n    Vy Tuong Dang1,\n    Anh Totti Nguyen4‚Ä†,\n    Daeyoung Kim1‚Ä†\n  \n  \n    *Equal contribution¬†¬†¬†¬†‚Ä†Equal advising\n    1KAIST, 2College of William and Mary, 3University of Alberta, 4Auburn University\n  \n\n\n\n\n\n \n\n\n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) perform perfectly on counting tasks with original images but fail catastrophically (e.g., 100% ‚Üí 17.05%‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/vlms-are-biased.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Analysis-Recognition","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tOpenpdf-Analysis-Recognition\n\t\n\nThe Openpdf-Analysis-Recognition dataset is curated for tasks related to image-to-text recognition, particularly for scanned document images and OCR (Optical Character Recognition) use cases. It contains over 6,900 images in a structured imagefolder format suitable for training models on document parsing, PDF image understanding, and layout/text extraction tasks.\n\n\t\n\t\t\nAttribute\nValue\n\n\n\t\t\nTask\nImage-to-Text\n\n\nModality\nImage\n\n\nFormat\nImageFolder‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Analysis-Recognition.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"synthdoc-zh-tw-dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LeeTung/synthdoc-zh-tw-dataset","creator_name":"Lee Tung","creator_url":"https://huggingface.co/LeeTung","description":"\n\t\n\t\t\n\t\tSynthDoG Traditional Chinese Dataset\n\t\n\nThis dataset contains synthetic document-ground truth pairs for Traditional Chinese text recognition training. The dataset is generated using the SynthDoG (Synthetic Document Generation) framework, which creates realistic document images with Traditional Chinese text.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized into three splits:\n\ntrain/: Training data\nvalidation/: Validation data\ntest/: Test data\n\nEach split contains:\n\nImage files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeeTung/synthdoc-zh-tw-dataset.","first_N":5,"first_N_keywords":["image-to-text","document-question-answering","Chinese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"image-captioning-turkish","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ituperceptron/image-captioning-turkish","creator_name":"ITU Perceptron","creator_url":"https://huggingface.co/ituperceptron","description":"\n\t\n\t\t\n\t\tT√ºrk√ße Image Captioning Veri Seti\n\t\n\nBu veri seti BLIP3o modelinin pre-train eƒüitiminde kullanƒ±lan BLIP3o-Pretrain-Long-Caption ve BLIP3o-Pretrain-Short-Caption veri setlerinin T√ºrk√ßeye √ßevirilmi≈ü bir alt par√ßasƒ±dƒ±r. Veri setinin olu≈üturulmasƒ± ile ilgili detaylƒ± bilgiye orijinal veri seti √ºzerinden ula≈üabilirsiniz. \nVeri seti Image-to-Text modellerinin eƒüitilmesinde veya ince ayar s√ºrecinde kullanƒ±labilir. Veri seti, orijinal veri setinin lisansƒ± olan Apache 2.0 altƒ±nda payla≈üƒ±lmƒ±≈ütƒ±r.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ituperceptron/image-captioning-turkish.","first_N":5,"first_N_keywords":["image-to-text","Turkish","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"CyclePrefDB-I2T-Reconstructions","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/carolineec/CyclePrefDB-I2T-Reconstructions","creator_name":"Caroline Chan","creator_url":"https://huggingface.co/carolineec","description":"\n\t\n\t\t\n\t\tImage Reconstructions for CyclePrefDB-I2T\n\t\n\nProject page | Paper | Code\nThis dataset contains reconstruction images used to determine cycle consistency preferences for CyclePrefDB-I2T. You can find the corresponding file paths in the CyclePrefDB-I2T dataset here. Reconstructions are created using Stable Diffusion 3 Medium.\n\n\t\n\t\n\t\n\t\tPreparing the reconstructions\n\t\n\nYou can download the test and validation split .tar files and extract them directly.Use this script to extract the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carolineec/CyclePrefDB-I2T-Reconstructions.","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"alchemist-tr","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/salihfurkaan/alchemist-tr","creator_name":"Salih Furkan Erik","creator_url":"https://huggingface.co/salihfurkaan","description":"\n\t\n\t\t\n\t\tAlchemist-TR üë®‚Äçüî¨\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAlchemist-TR is the Turkish-translated version of the original Alchemist dataset, comprising 3,350 high-quality image-text pairs curated for supervised fine-tuning (SFT) of pre-trained text-to-image (T2I) generative models. This version maintains the original's visual and aesthetic standards while localizing all prompts into Turkish, enabling multilingual and culturally-aware T2I training and evaluation.\nThis dataset facilitates the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/salihfurkaan/alchemist-tr.","first_N":5,"first_N_keywords":["image-to-text","Turkish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"iReason","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rippleripple/iReason","creator_name":"Jackie Y","creator_url":"https://huggingface.co/rippleripple","description":"Paper: Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models\nPaper: https://arxiv.org/abs/2506.00258\nWebsite: https://jackie-2000.github.io/iReason.github.io/\nGitHub: https://github.com/eric-ai-lab/iReason\niReason: Designed to probe MLLMs‚Äô implicit reasoning by evaluating their ability to detect subtle flaws in seemingly valid instructions. Covers 643 real-world scenarios spanning four failure types‚Äîobject absence, ambiguity, contradiction, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rippleripple/iReason.","first_N":5,"first_N_keywords":["image-text-to-text","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ibero-characters-es","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es","creator_name":"Hackathon SomosNLP 2025","creator_url":"https://huggingface.co/somosnlp-hackathon-2025","description":"\n\t\n\t\t\n\t\tConjunto de datos de personajes de mitos y leyendas iberoamericanos.\n\t\n\n\n‚ö†Ô∏è Este dataset se encuentra en desarrollo activo. Se planea expandir significativamente el n√∫mero de registros y mejorar la cobertura de im√°genes.\n\n\n\t\n\t\t\n\t\tüìö Descripci√≥n\n\t\n\nDataset de personajes m√≠ticos y legendarios de Iberoam√©rica, dise√±ado para preservar y promover el patrimonio cultural a trav√©s de la inteligencia artificial.\n\n\t\n\t\t\n\t\tüåü Motivaci√≥n e Impacto\n\t\n\n\nüì± Preservaci√≥n Digital: Conservaci√≥n del‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es.","first_N":5,"first_N_keywords":["video-text-to-text","text-generation","Spanish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"prophet-mosque-library-compressed","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library-compressed","creator_name":"ÿßŸÑŸÉÿ™ÿ® ÿßŸÑŸÖŸèŸäÿ≥Ÿëÿ±ÿ©","creator_url":"https://huggingface.co/ieasybooks-org","description":"\n\t\n\t\t\n\t\tProphet's Mosque Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tüìñ Overview\n\t\n\nProphet‚Äôs Mosque Library is one of the primary resources for Islamic books. It hosts more than 48,000 PDF books across over 70 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tüìä Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/prophet-mosque-library, with one key difference: the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library-compressed.","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n  \n\n\n[üìñ arXiv Paper] \n[üìä R1-Reward Code] \n[üìù R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nüî• We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Real-Complex-Analysis-Math","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Real-Complex-Analysis-Math","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tReal-Complex-Analysis-Math\n\t\n\nThis dataset contains high-quality scanned pages from the classic mathematics textbook by Walter Rudin, widely used in advanced undergraduate and graduate-level courses on real and complex analysis. It is ideal for building OCR systems, digitizing textbooks, or creating educational AI tools for higher mathematics.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Principles of Mathematical Analysis and Real and Complex Analysis by Walter Rudin\n\nTask: Image-to-Text (OCR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Real-Complex-Analysis-Math.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MolA","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhenger/MolA","creator_name":"Jiaxin","creator_url":"https://huggingface.co/Zhenger","description":"\n\t\n\t\t\n\t\tTowards Reliable Optical Chemical Structure Recognition: Better Metrics, Better Data and Better Reconstruction\n\t\n\nWe introduce a comprehensive approach that improves OCSR evaluation, training data, and model reliability.\nKey Contributions:\n\nBetter Metrics: We introduce a comprehensive evaluation scheme that directly quantifies manual correction costs, with Perfectly-Matched Annotation Accuracy (PMAA) as the most stringent metric, ensuring exact alignment between model outputs and input‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhenger/MolA.","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"video-dataset","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProgramerSalar/video-dataset","creator_name":"ProgramerSalar","creator_url":"https://huggingface.co/ProgramerSalar","description":"\n\t\n\t\t\n\t\tVideo Dataset on Hugging Face\n\t\n\nThis repository hosts the  video dataset, a widely used benchmark dataset for human action recognition in videos. The dataset has been processed and uploaded to the Hugging Face Hub for easy access, sharing, and integration into machine learning workflows.\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe  dataset is a large-scale video dataset designed for action recognition tasks. It contains 13,320 video clips across 101 action categories, making it one of the most‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProgramerSalar/video-dataset.","first_N":5,"first_N_keywords":["text-to-video","video-classification","video-text-to-text","voice-activity-detection","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"texturecan","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/texturecan","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for TextureCan Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,037 texture images from texturecan.com. It includes textures of various materials such as brick, paper, fabric, metal, wood, stone, and other surfaces. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/texturecan.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"ambientcg","keyword":"image-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/ambientcg","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for AmbientCG Textures and HDRIs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 14,202 high-quality texture images and HDRI environments from ambientcg.com. It includes a comprehensive collection of materials such as fabric, metal, wood, stone, concrete, nature elements, and HDRI skyboxes for 3D rendering and computer graphics applications. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ambientcg.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"TAMA_Instruct","keyword":"table-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct","creator_name":"LIT @ UMich","creator_url":"https://huggingface.co/MichiganNLP","description":"\n  \n  Dataset Card for TAMA Instruct\n\n\n\n\nThe training and testing data for TAMA models.\nPlease refer to our paper for additional information.\n","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"my_image_caption_dataset","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hongin9812/my_image_caption_dataset","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","description":"\n\t\n\t\t\n\t\tMy image-caption dataset\n\t\n\nThis dataset contains images with English descriptions (captions).\n","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"kzgov-budget-data","keyword":"table-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Adilbai/kzgov-budget-data","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","description":"\n\t\n\t\t\n\t\tKazakhstan Government Budget Data üá∞üáø\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis comprehensive dataset provides detailed insights into Kazakhstan's government budget allocation, execution, and performance across various sectors, regions, and administrative levels for 2024. The dataset enables analysis of fiscal policy, budget efficiency, and resource distribution across the country.\n\n\t\n\t\t\n\t\tüìä Dataset Statistics\n\t\n\n\nTotal Records: 615 entries\nCoverage Period: 2024\nAdministrative Levels:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kzgov-budget-data.","first_N":5,"first_N_keywords":["table-to-text","question-answering","text-retrieval","open-domain-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"Dogs-images-text-pair","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair","creator_name":"Mikdad Mrhij","creator_url":"https://huggingface.co/MikdadMrhij","description":"This dataset is curated from subsets of public datasets such as MS COCO, LAION-ART, and SBU Captions.\nIt specifically filters for samples featuring dog-related content.\nThe goal of this dataset is to support text-to-image generation models focused on dog objects and related scenes. \nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/SBUcaptions.md\nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/mscoco.md‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MindCube","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation-v2","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset (v2)\n\t\n\nÁîªÂÉè/„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅäÈ°å„Å´ÂØæ„Åô„ÇãÈù¢ÁôΩ„ÅÑÂõûÁ≠î„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇboketeÔºàÁîªÂÉè‚Üí„ÉÜ„Ç≠„Çπ„ÉàÔºâ„Å®keitaiÔºà„ÉÜ„Ç≠„Çπ„Éà‚Üí„ÉÜ„Ç≠„Çπ„ÉàÔºâ„ÇíÁµ±Âêà„ÄÇ\n\n\t\n\t\t\n\t\t‰Ωø„ÅÑÊñπ\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"iammytoo/japanese-humor-evaluation-v2\")\n\n\n\t\n\t\t\n\t\t„Éá„Éº„ÇøÊßãÈÄ†\n\t\n\n\nodai_type: 'image' or 'text'\nimage: ÁîªÂÉè„ÅäÈ°åÔºàtext„Çø„Ç§„Éó„Åß„ÅØNoneÔºâ\nodai: „ÉÜ„Ç≠„Çπ„Éà„ÅäÈ°åÔºàimage„Çø„Ç§„Éó„Åß„ÅØNoneÔºâ\nresponse: ÂõûÁ≠î„ÉÜ„Ç≠„Çπ„Éà\nscore: 0-4„ÅÆÊ≠£Ë¶èÂåñ„Çπ„Ç≥„Ç¢\n\n\n\t\n\t\t\n\t\t„ÇΩ„Éº„Çπ\n\t\n\n\nYANS-official/ogiri-bokete\nYANS-official/ogiri-keitai\n\n","first_N":5,"first_N_keywords":["text-generation","image-to-text","Japanese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Medical-Multimodal-EN-TH","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZombitX64/Medical-Multimodal-EN-TH","creator_name":"ZomBitX64","creator_url":"https://huggingface.co/ZombitX64","description":"\n\t\n\t\t\n\t\tHealthGPTVL-Translation Medical-Multimodal-EN-TH\n\t\n\nThis dataset is a bilingual (English-Thai) medical multimodal evaluation dataset containing medical images with corresponding question-answer pairs for visual question answering and translation tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 17,047 medical image-text pairs designed for multimodal medical AI evaluation. It includes medical images from various imaging modalities (MRI, CT, X-Ray‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZombitX64/Medical-Multimodal-EN-TH.","first_N":5,"first_N_keywords":["image-classification","image-to-text","translation","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Perception-R1-Dataset","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tongxiao2002/Perception-R1-Dataset","creator_name":"tongxiao","creator_url":"https://huggingface.co/tongxiao2002","description":"Paper: arxiv.org/abs/2506.07218\nPlease refer to GitHub repo for detailed usage: https://github.com/tongxiao2002/Perception-R1\n","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"TreeBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeBench","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTraceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\n\t\n\nThis repository contains the TreeBench dataset, a diagnostic benchmark for visual grounded reasoning, introduced in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTL; DR: We propose TreeBench, the first benchmark specially designed for evaluating \"thinking with images\" capabilities with traceable visual evidence, and TreeVGR, the current state-of-the-art‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeBench.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tCaption3o-Opt\n\t\n\nCaption3o-Opt is a compact, high-quality image-caption dataset derived from the original BLIP3o/BLIP3o-Pretrain-Long-Caption. This refined subset focuses on optimized long-form captioning, curated for real-world and artistic image understanding across vision-language models.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nTotal Samples: 10,278\nModality: Image ‚Üî Text\nFormat: Arrow (auto-converted to Parquet)\nLicense: Apache 2.0\nLanguage: English\nSize: ~500 MB\n\n\n\t\n\t\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt.","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"CalcTrainer_dataset","keyword":"image-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hoololi/CalcTrainer_dataset","creator_name":"Olivier","creator_url":"https://huggingface.co/hoololi","description":"\n\t\n\t\t\n\t\tCalcTrainer Dataset üßÆ\n\t\n\nHandwritten mathematical answers collected from the CalcTrainer interactive math training application.\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\n\t\n\t\t\n\t\tCore Data\n\t\n\n\n\t\n\t\t\nField\nType\nDescription\n\n\n\t\t\nhandwriting_image\nImage\nHandwritten answer image (~100x100px)\n\n\nocr_prediction\nstring\nRaw OCR output text\n\n\nocr_parsed_number\nint32\nCleaned numeric value from OCR\n\n\nis_correct\nbool\nWhether OCR matches correct answer\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tMathematical Context\n\t\n\n\n\t\n\t\t\nField\nType‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hoololi/CalcTrainer_dataset.","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-RL-37K","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTreeBench Dataset Card\n\t\n\nThis repository contains TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark designed for evaluating \"thinking with images\" capabilities with traceable visual evidence.\nThe dataset was introduced in the paper: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is built on three core principles:\n\nFocused visual perception: of subtle targets in complex scenes.\nTraceable evidence: via bounding box‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"s2lcd","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neuronelab/s2lcd","creator_name":"NeuRoNeLab","creator_url":"https://huggingface.co/neuronelab","description":"\n\t\n\t\t\n\t\tSentinel-2 Land-cover Captioning Dataset\n\t\n\nThe Sentinel-2 Land-cover Captioning Dataset (S2LCD) is a newly proposed dataset specifically designed for deep learning research on remote sensing image captioning. It comprises 1533 image patches, each of size 224 √ó 224 pixels, derived from Sentinel-2 L2A images. The dataset ensures a diverse representation of land cover and land use types in temperate regions, including forests, mountains, agricultural lands, and urban areas, each one with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neuronelab/s2lcd.","first_N":5,"first_N_keywords":["zero-shot-classification","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simple-shapes-svg","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/darknoon/simple-shapes-svg","creator_name":"Andrew Pouliot","creator_url":"https://huggingface.co/darknoon","description":"The goal of this dataset is to measure and improve the ability of VLMs to see accurately in spatial dimensions.\nI've tried to ensure that all of the examples are not too hard\n\nhave sufficient contrast between foreground and background\nshapes are not clipped or ambiguous\nsolid background\ncanvas is square 512x512\n\nInitially, I've kept the \"canvas\" that they're working with 512x512 points, but you can learn more by experimenting with the dimensions as well.\n","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"image-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://sdiffusers.com/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\n‚îú‚îÄ‚îÄ üìÇ images/\n‚îÇ   ‚îú‚îÄ‚îÄ 1.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 2.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 3.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ ....\n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://sdiffusers.com/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\n‚îú‚îÄ‚îÄ üìÇ images/\n‚îÇ   ‚îú‚îÄ‚îÄ 1.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 2.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 3.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ ....\n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"m-popp","keyword":"image-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thomas-C/m-popp","creator_name":"Thomas Constum","creator_url":"https://huggingface.co/thomas-C","description":"\n\t\n\t\t\n\t\tM-POPP datasets\n\t\n\nThis repository contains 2 datasets created within the EXO-POPP project (Optical EXtraction of handwritten named entities for marriage records of the POPulation of Paris) for the task of text recognition and information extraction.\nThese datasets have been published in End-to-end information extraction in handwritten documents: Understanding Paris marriage records from 1880 to 1940 [1] at ICDAR 2024 and are also available on Zenodo.\nThis version contains the labels‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thomas-C/m-popp.","first_N":5,"first_N_keywords":["image-to-text","question-answering","French","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"e2e_nlg_french","keyword":"table-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CATIE-AQ/e2e_nlg_french","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nFrench translation of the E2E NLG dataset.According of the English version dataset card, The E2E NLG dataset is a benchmark dataset for data-to-text models that verbalize a set of 2-9 key-value attribute pairs in the restaurant domain. The version used for GEM is the cleaned E2E NLG dataset, which filters examples with hallucinations and outputs that don't fully cover all input attributes. \nYou can find the main data card on the GEM Website and we invite you to consult‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/e2e_nlg_french.","first_N":5,"first_N_keywords":["table-to-text","French","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true}
]
;
