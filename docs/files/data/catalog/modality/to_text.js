const data_for_modality_to_text = 
[
	{"name":"SMMILE","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v30","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v30.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v30","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"STAIR-Captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for STAIR-Captions\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nSTAIR Captions is a large-scale dataset containing 820,310 Japanese captions. This dataset can be used for caption generation, multimodal retrieval, and image generation.\n\n\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\n\n\n\n\t\n\t\n\t\n\t\tLanguages\n\t\n\nThe language data in JDocQA is in Japanese (BCP-47 ja-JP).\n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shunk031/STAIR-Captions.","url":"https://huggingface.co/datasets/shunk031/STAIR-Captions","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"YFCC15M_page_and_download_urls","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tYFCC15M subset used for VLMs\n\t\n\nThis dataset contains the ~15M subset of YFCC100M used for training the models in the paper Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP. The metadata provided in this repo contains both the page-urls and image-download-urls for downloading the dataset.\nThis dataset can be easily downloaded with img2dataset:\nimg2dataset --url_list yfcc15m_final_split_pageandimageurls.csv --input_format \"csv\" --output_formatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vishaal27/YFCC15M_page_and_download_urls.","url":"https://huggingface.co/datasets/vishaal27/YFCC15M_page_and_download_urls","creator_name":"Vishaal Udandarao","creator_url":"https://huggingface.co/vishaal27","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","image-to-text","English","mit","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"gan_hkr_large","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from HKR train set using ScrabbleGAN\n\t\n\nNumber of images: 2476836\nSources:\n\nHKR dataset\nScrabbleGAN code\n\n","url":"https://huggingface.co/datasets/nastyboget/gan_hkr_large","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Persian-OCR-230k","keyword":"image-to-text","description":".\nâ”œâ”€â”€ Images/\nâ”œâ”€â”€ train.csv (184k)\nâ””â”€â”€ test.csv (46k)\n\n","url":"https://huggingface.co/datasets/ordaktaktak/Persian-OCR-230k","creator_name":"Saeed Biabani","creator_url":"https://huggingface.co/ordaktaktak","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Persian","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"VL3-Syn7M","keyword":"image-to-text","description":"\n    \n\n\nThe re-caption dataset used in VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding\n\n If you like our project, please give us a star â­ on Github for the latest update.  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸŒŸ Introduction\n\t\n\nThis dataset is the re-captioned data we used during the training of VideoLLaMA3. It consists of 7 million diverse, high-quality images, each accompanied by a short caption and a detailed caption.\nThe images in this dataset originate from COYO-700M, MS-COCO 2017â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M.","url":"https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M","creator_name":"Language Technology Lab at Alibaba DAMO Academy","creator_url":"https://huggingface.co/DAMO-NLP-SG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"flickr8k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"flickr8k\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/Naveengo/flickr8k","creator_name":"Naveen Golla","creator_url":"https://huggingface.co/Naveengo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DefectSpectrum","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDefect Spectrum Dataset\n\t\n\nWelcome to the Defect Spectrum dataset repository. This comprehensive benchmark is a granular collection of large-scale defect datasets with rich semantics, designed to push the frontier of industrial defect inspection research and applications.\n\n\t\n\t\t\n\t\tIMPORTANT\n\t\n\nPLEASE SEE OUR NEW REPO FOR THE FULL DATASET: https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum\n\n\t\n\t\t\n\t\tOverview\n\t\n\nDefect inspection is a critical component within the closed-loopâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Andyson/DefectSpectrum.","url":"https://huggingface.co/datasets/Andyson/DefectSpectrum","creator_name":"Syang","creator_url":"https://huggingface.co/Andyson","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"pixmo-cap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPixMo-Cap\n\t\n\nPixMo-Cap is a dataset of very long (roughly 200 words on average), detailed captions.\nIt can be used to pre-train and fine-tune vision-language models. \nPixMo-Cap was created by recording annotators speaking about an image for 60-90 seconds and then using the Claude large language model to turn the audio transcripts(s) into a long caption. \nThe audio transcripts are also included.\nPixMo-Cap is part of the PixMo dataset collection and was used to train the Molmo family ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/allenai/pixmo-cap.","url":"https://huggingface.co/datasets/allenai/pixmo-cap","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","odc-by","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"stackmix_hkr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from HKR train set using Stackmix\n\t\n\nNumber of images: 300000\nSources:\n\nHKR dataset\nStackmix code\n\n","url":"https://huggingface.co/datasets/nastyboget/stackmix_hkr","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"gan_hkr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from HKR train set using ScrabbleGAN\n\t\n\nNumber of images: 300000\nSources:\n\nHKR dataset\nScrabbleGAN code\n\n","url":"https://huggingface.co/datasets/nastyboget/gan_hkr","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"CMDS_Multimodal_Document","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Cyrillic Multimodel Document (CMDS)\n\t\n\nThis is the dataset consists of 3789 pairs of images and text across 31 categories downloaded from the Bulgarian ministry of finance\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nUses this dataset for downstream task like Document Classification, Image Classification or Text Classificationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sitloboi2012/CMDS_Multimodal_Document.","url":"https://huggingface.co/datasets/sitloboi2012/CMDS_Multimodal_Document","creator_name":"Huy Vo","creator_url":"https://huggingface.co/sitloboi2012","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-classification","image-to-text","Bulgarian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestones/photo-anatomy","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"diffusiondb-pixelart","keyword":"image-to-text","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/jainr3/diffusiondb-pixelart","creator_name":"Rahul Jain","creator_url":"https://huggingface.co/jainr3","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"inference-PhD","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tæ•°æ®æ ¼å¼\n\t\n\næ¯ä¸ªæ ·æœ¬åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n\nquestion_id: é—®é¢˜ID\nquestion: é—®é¢˜æ–‡æœ¬\nmodel_output: æ¨¡åž‹è¾“å‡º\nground_truth: çœŸå®žç­”æ¡ˆ\ntask: ä»»åŠ¡ç±»åž‹\nimage_name: å›¾ç‰‡åç§°\nmodel_name: æ¨¡åž‹åç§°\ndetailed_prompt: è¯¦ç»†æç¤º\nimage: å›¾ç‰‡æ•°æ®\n\n\n## åˆ†å‰²é…ç½®\n\n```yaml\n    # R1-Onevision-7B æ¨¡åž‹\n    - split: r1_onevision_7b_phd_ccs\n      path: \"r1_onevision_7b/r1_onevision_7b_phd_ccs.parquet\"\n    - split: r1_onevision_7b_phd_sec\n      path: \"r1_onevision_7b/r1_onevision_7b_phd_sec.parquet\"\n    - split: r1_onevision_7b_phd_icc\n      path:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/1Jin1/inference-PhD.","url":"https://huggingface.co/datasets/1Jin1/inference-PhD","creator_name":"jinxiwei","creator_url":"https://huggingface.co/1Jin1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","visual-question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"Esposalles-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEsposalles - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Marriage Licenses ground-truth is compiled from the Marriage Licenses Books conserved at the Archives of the Cathedral of Barcelona.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Catalan.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1244x128 at 0x1A800E8E190â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Esposalles-line.","url":"https://huggingface.co/datasets/Teklia/Esposalles-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Catalan","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"NSFW-T2I","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIntroduction (Version 1)\n\t\n\nAbout 38k image-text pairs(10k from LAION and 28k from nsfw_detect), and captions are generated by LLaVA-NeXT with prompt \"Describe the photo in detail (attributes of person)\".\nThe \"txt\" column shown in the dataset viewer is originated from LAION, not the captions yielded by LLaVA-NeXT.\n\n\t\n\t\t\n\t\n\t\n\t\tCaption Codes\n\t\n\npretrained = \"lmms-lab/llama3-llava-next-8b\"\nmodel_name = \"llava_llama3\"\ndevice = \"cuda:2\"\ndevice_map = \"auto\"\ntokenizer, model, image_processorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zxbsmk/NSFW-T2I.","url":"https://huggingface.co/datasets/zxbsmk/NSFW-T2I","creator_name":"Jun","creator_url":"https://huggingface.co/zxbsmk","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tç®€ä»‹ Brief Introduction\n\t\n\nè½¬è½½è‡ªwanng/midjourney-v5-202304-clean\néžå®˜æ–¹çš„ï¼Œçˆ¬å–è‡ªmidjourney v5çš„2023å¹´4æœˆçš„æ•°æ®ï¼Œä¸€å…±1701420æ¡ã€‚\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tæ•°æ®é›†ä¿¡æ¯ Dataset Information\n\t\n\nåŽŸå§‹é¡¹ç›®åœ°å€ï¼šhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\næˆ‘åšäº†ä¸€äº›æ¸…æ´—ï¼Œæ¸…ç†å‡ºäº†ä¸¤ä¸ªæ–‡ä»¶ï¼š\n\nori_prompts_df.parquet ï¼ˆ1,255,812å¯¹ï¼Œmidjourneyçš„å››æ ¼å›¾ï¼‰\n\nupscaled_prompts_df.parquet ï¼ˆ445,608å¯¹ï¼Œä½¿ç”¨äº†é«˜æ¸…æŒ‡ä»¤çš„å›¾ï¼Œè¿™æ„å‘³ç€è¿™ä¸ªå›¾æ›´å—æ¬¢è¿Žã€‚ï¼‰\n\n\nOriginalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304.","url":"https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304","creator_name":"JohnTeddy3","creator_url":"https://huggingface.co/JohnTeddy3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304-clean","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tç®€ä»‹ Brief Introduction\n\t\n\néžå®˜æ–¹çš„ï¼Œçˆ¬å–è‡ªmidjourney v5çš„2023å¹´4æœˆçš„æ•°æ®ï¼Œä¸€å…±1701420æ¡ã€‚\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tæ•°æ®é›†ä¿¡æ¯ Dataset Information\n\t\n\nåŽŸå§‹é¡¹ç›®åœ°å€ï¼šhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\næˆ‘åšäº†ä¸€äº›æ¸…æ´—ï¼Œæ¸…ç†å‡ºäº†ä¸¤ä¸ªæ–‡ä»¶ï¼š\n\nori_prompts_df.parquet ï¼ˆ1,255,812å¯¹ï¼Œmidjourneyçš„å››æ ¼å›¾ï¼‰\n\nupscaled_prompts_df.parquet ï¼ˆ445,608å¯¹ï¼Œä½¿ç”¨äº†é«˜æ¸…æŒ‡ä»¤çš„å›¾ï¼Œè¿™æ„å‘³ç€è¿™ä¸ªå›¾æ›´å—æ¬¢è¿Žã€‚ï¼‰\n\n\nOriginal project address:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean.","url":"https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"synthetic_cyrillic","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated using handwritten fonts\n\t\n\nNumber of images: 300000\nSources:\n\nHandwriting generation code\n\nThe code was executed with cyrillic option (more augmentations)\n","url":"https://huggingface.co/datasets/nastyboget/synthetic_cyrillic","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"CroQS-Benchmark","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCroQS: Cross-modal Query Suggestion Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCroQS (Cross-modal Query Suggestion) is a benchmark dataset for evaluating cross-modal query suggestion systems in text-to-image retrieval scenarios. The dataset addresses the novel task of suggesting minimal textual modifications to help users explore visually consistent subsets of image collections, following the premise of \"Maybe you are looking for...\"\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCroQS comprises:\n\n50â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ruggero1912/CroQS-Benchmark.","url":"https://huggingface.co/datasets/Ruggero1912/CroQS-Benchmark","creator_name":"Giacomo Pacini","creator_url":"https://huggingface.co/Ruggero1912","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","visual-document-retrieval","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"windata-vision-synthetics-zh-300k","keyword":"image-to-text","description":"ä»‹ç»\næˆ‘ä»¬æ•´ç†ç”Ÿæˆäº†ä¸€ä¸ªä¸­æ–‡å¤šæ¨¡æ€å›¾æ–‡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«äº†å¤§çº¦30ä¸‡æ¡æ•°æ®ä»¥åŠçº¦20ä¸‡å¼ å›¾ç‰‡ï¼Œæ¶‰åŠæ–‡æ¡£docã€å›¾è¡¨ã€æ•°å­¦ã€OCRç­‰å¤šç§åœºæ™¯ã€‚\né’ˆå¯¹å¼€æºæ•°æ®ä¸­ä¸­æ–‡å›¾æ–‡æŒ‡ä»¤é›†å°‘ä¸”æŒ‡ä»¤é›†æè¿°æ™®éè¿‡äºŽç®€çŸ­ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºŽå¼€æºæ¨¡åž‹çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ Qwen2-vl-72B-Instruct ç”Ÿæˆè¾ƒä¸ºè¯¦ç»†çš„ä¸­æ–‡captionæŒ‡ä»¤é›†ï¼Œç„¶åŽåœ¨åŒä¸€åœºæ™¯ä¸­éšæœºæŒ‘é€‰1-4å¼ å›¾ç‰‡å’Œç›¸åº”çš„ä¸­æ–‡captionï¼Œå°†captionæ•°æ®ç»™åˆ°æˆ‘ä»¬çš„å¤§è¯­è¨€æ¨¡åž‹ WiNGPT-2.6 é€šè¿‡è®¾è®¡ç³»ç»ŸæŒ‡ä»¤ä½¿å…¶æ¯è½®è¿›è¡Œæé—®ï¼Œå°†é—®é¢˜å’Œå›¾ç‰‡ç»™åˆ° Qwen2-vl-72B-Instruct ä½¿å…¶è¿›è¡Œå›žç­”ï¼›æœ€åŽè®¾å®šå¾ªçŽ¯æ¬¡æ•°ï¼Œå¾—åˆ°å¤šè½®å¤šå›¾çš„å¯¹è¯æ•°æ®ã€‚ \nå¯¹äºŽç”ŸæˆåŽçš„æ•°æ®ï¼Œæ ¹æ®ç­”æ¡ˆçš„é•¿åº¦ã€è¯­å¥çš„é‡å¤æ€§ç­‰è¿›è¡Œäº†è§„åˆ™è¿‡æ»¤ï¼›æ•°å­¦ç±»é¢˜ç›®ï¼Œæ ¹æ®åŽŸå§‹æ•°æ®çš„ç­”æ¡ˆè¿›è¡Œäº†è¿‡æ»¤ã€‚åœ¨åˆ¶ä½œæœ€åŽçš„captionæŒ‡ä»¤é›†æ—¶ï¼Œæˆ‘ä»¬é’ˆå¯¹æ¯ä¸€ä¸ªåœºæ™¯éƒ½è®¾è®¡äº†ä¸Šç™¾ä¸ªé—®é¢˜ï¼Œä¿è¯äº†captionæ•°æ®é›†çš„å¤šæ ·æ€§ï¼›åœ¨å¯¹è¯æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬åœ¨ä¸åŒåœºæ™¯ä¸‹æ¥è®©WiNGPT-2.6â€¦ See the full description on the dataset page: https://huggingface.co/datasets/winninghealth/windata-vision-synthetics-zh-300k.","url":"https://huggingface.co/datasets/winninghealth/windata-vision-synthetics-zh-300k","creator_name":"Winning Health AI Research","creator_url":"https://huggingface.co/winninghealth","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Chinese","cc-by-4.0","100K<n<1M","arxiv:2409.11402"],"keywords_longer_than_N":true},
	{"name":"text-to-image-2M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\ttext-to-image-2M: A High-Quality, Diverse Text-to-Image Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\ntext-to-image-2M is a curated text-image pair dataset designed for fine-tuning text-to-image models. The dataset consists of approximately 2 million samples, carefully selected and enhanced to meet the high demands of text-to-image model training. The motivation behind creating this dataset stems from the observation that datasets with over 1 million samples tend to produce betterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jackyhate/text-to-image-2M.","url":"https://huggingface.co/datasets/jackyhate/text-to-image-2M","creator_name":"kzou","creator_url":"https://huggingface.co/jackyhate","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"TOX21-V-SMILES-4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/TOX21-V-SMILES-4.","url":"https://huggingface.co/datasets/molvision/TOX21-V-SMILES-4","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"TOX21-V-SMILES-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/TOX21-V-SMILES-2.","url":"https://huggingface.co/datasets/molvision/TOX21-V-SMILES-2","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"pose-controlnet","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe data is based on DeepFashion; turned into image pairs of the same person in same garment with different poses.\nThis won't preserve the person/garment at all but just want to process the data first and see what kind of controlnet it can train as an exercise for training a controlnet.\nThe controlnet_aux's openpose detector sometimes return black images for occluded human images so there won't be a lot of valid image pairs.\n","url":"https://huggingface.co/datasets/merlinyx/pose-controlnet","creator_name":"Yuxuan Mei","creator_url":"https://huggingface.co/merlinyx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Memories-ai/UGC-VideoCap.","url":"https://huggingface.co/datasets/Memories-ai/UGC-VideoCap","creator_name":"Memories.ai","creator_url":"https://huggingface.co/Memories-ai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","ðŸ‡ºðŸ‡¸ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"midjourney-kaggle-clean","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tç®€ä»‹ Brief Introduction\n\t\n\néžå®˜æ–¹çš„ï¼Œå¯¹Kaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] ä¸Šçš„æ•°æ®é›†è¿›è¡Œäº†æ¸…ç†ï¼Œä¸€å…±æœ‰ 248,167å¯¹ã€‚\nUnofficially, a cleanup of the dataset on Kaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] yielded 248,167 pairs.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-kaggle-clean.","url":"https://huggingface.co/datasets/wanng/midjourney-kaggle-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"test_dataset_final","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n107 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_final.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_final","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"FeTaQA","keyword":"table-to-text","description":"This repo is the unofficial FeTA-QA dataset from paper FeTaQA: Free-form Table Question Answering.\nThe original purpose to make it easier for users to download and use dataset. All the data is publicly avaliable on their offical Github site\nIf there is anything wrong, please raise an issue in the community and I will fix it if I am available.\n","url":"https://huggingface.co/datasets/DongfuJiang/FeTaQA","creator_name":"Dongfu Jiang","creator_url":"https://huggingface.co/DongfuJiang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"synthetic_cyrillic_large","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated using handwritten fonts\n\t\n\nNumber of images: 3700269\nSources:\n\nHandwriting generation code\n\nThe code was executed with cyrillic option (more augmentations)\n","url":"https://huggingface.co/datasets/nastyboget/synthetic_cyrillic_large","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"dm-codes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDM codes dataset\n\t\n\nThe dataset contains photos of Data Matrix (DM) codes and their annotations. The photos were taken on an iPhone and annotated manually by a human.\nThe annotations contain text, which is encoded in the DM code and the pixel coordinates of the DM code vertices.\nThe vertices are: tl = top left, tr = top right, br = bottom right, bl = bottom left.\nAttribute is_clean specifies whether the DM code on the image is expected to be easily readable. For every DM code, there isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shortery/dm-codes.","url":"https://huggingface.co/datasets/shortery/dm-codes","creator_name":"-","creator_url":"https://huggingface.co/shortery","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","object-detection","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"hl","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for the High-Level Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe High-Level (HL) dataset aligns object-centric descriptions from COCO \nwith high-level descriptions crowdsourced along 3 axes: scene, action, rationale\nThe HL dataset contains 14997 images from COCO and a total of 134973 crowdsourced captions (3 captions for each axis) aligned with ~749984 object-centric captions from COCO.\nEach axis is collected by asking the following 3 questions:\n\nWhere is the pictureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michelecafagna26/hl.","url":"https://huggingface.co/datasets/michelecafagna26/hl","creator_name":"Michele Cafagna","creator_url":"https://huggingface.co/michelecafagna26","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","zero-shot-classification","text-scoring","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"ui_refexp_saved","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"ui_refexp_saved_Jan2023\"\n\t\n\nThis is a saved snapshot of the dynamically generated UI Bert dataset. \nMuch faster download time than the dynamic version which pulls and filters large data files from remote sources.\n","url":"https://huggingface.co/datasets/ivelin/ui_refexp_saved","creator_name":"Ivelin Ivanov","creator_url":"https://huggingface.co/ivelin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"sportsett_basketball","keyword":"data-to-text","description":"SportSett:Basketball dataset for Data-to-Text Generation contains NBA games stats aligned with their human written summaries.","url":"https://huggingface.co/datasets/GEM/sportsett_basketball","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"sportsett_basketball","keyword":"table-to-text","description":"SportSett:Basketball dataset for Data-to-Text Generation contains NBA games stats aligned with their human written summaries.","url":"https://huggingface.co/datasets/GEM/sportsett_basketball","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"full-modality-video-caption","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tFull Modality Video Caption Dataset\n\t\n\nThis dataset contains video segments with multi-modal captions including vision, audio, and integrated descriptions.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal segments: 55,940\nTotal duration: 155.39 hours (9323.33 minutes)\nMin duration: 10.0s\nMax duration: 10.0s\nMean duration: 10.0s\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry in the dataset contains:\n\nfile_name: Video segment filename\nvideo_path: Relative path to video file\nstart_time: Start time ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ngqtrung/full-modality-video-caption.","url":"https://huggingface.co/datasets/ngqtrung/full-modality-video-caption","creator_name":"Nguyen Quang Trung","creator_url":"https://huggingface.co/ngqtrung","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"surface_realisation_st_2020","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for GEM/surface_realisation_st_2020\n\t\n\n\n\t\n\t\t\n\t\tLink to Main Data Card\n\t\n\nYou can find the main data card on the GEM Website.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was used as part of the multilingual surface realization shared task in which a model gets full or partial universal dependency structures and has to reconstruct the natural language. This dataset support 11 languages. \nYou can load the dataset via:\nimport datasets\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GEM/surface_realisation_st_2020.","url":"https://huggingface.co/datasets/GEM/surface_realisation_st_2020","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"Creative Commons Attribution 2.5","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.5.html","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"Clintox-V-Train","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tClintox-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for Clintox related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/Clintox-V-Train.","url":"https://huggingface.co/datasets/molvision/Clintox-V-Train","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HIV-V-Train","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHIV-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for HIV related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/HIV-V-Train.","url":"https://huggingface.co/datasets/molvision/HIV-V-Train","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"webcode2m","keyword":"image-to-text","description":"WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs with Layouts\n(This dataset is also called Vision2UI.)\n\nAutomatically generating webpage code from webpage designscan significantly reduce the workload of front-end developers, andrecent Multimodal Large Language Models (MLLMs) have shownpromising potential in this area. However, our investigation revealsthat most existing MLLMs are constrained by the absence of highquality, large-scale, real-world datasets, resulting inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xcodemind/webcode2m.","url":"https://huggingface.co/datasets/xcodemind/webcode2m","creator_name":"xcodemind","creator_url":"https://huggingface.co/xcodemind","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"surface_realisation_st_2020","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for GEM/surface_realisation_st_2020\n\t\n\n\n\t\n\t\t\n\t\tLink to Main Data Card\n\t\n\nYou can find the main data card on the GEM Website.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset was used as part of the multilingual surface realization shared task in which a model gets full or partial universal dependency structures and has to reconstruct the natural language. This dataset support 11 languages. \nYou can load the dataset via:\nimport datasets\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/GEM/surface_realisation_st_2020.","url":"https://huggingface.co/datasets/GEM/surface_realisation_st_2020","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"Creative Commons Attribution 2.5","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.5.html","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"BBBP-V-Train","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBBBP-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BBBP (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BBBP-V-Train.","url":"https://huggingface.co/datasets/molvision/BBBP-V-Train","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Fruit-Disease","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card Authors\n\t\n\nSwayam Kesarkar\n\n\t\n\t\t\n\t\tDataset Card Contact\n\t\n\nhttps://www.linkedin.com/in/swayam-kesarkar-b05261272/\n","url":"https://huggingface.co/datasets/nerd-swayam/Fruit-Disease","creator_name":"Swayam Kesarkar","creator_url":"https://huggingface.co/nerd-swayam","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GRIT","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGRIT: Large-Scale Training Corpus of Grounded Image-Text Pairs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe introduce GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created based on image-text pairs from COYO-700M and LAION-2B. We construct a pipeline to extract and link text spans (i.e., noun phrases, and referring expressions) in the caption to their corresponding image regions. More details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nDuring the construction, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zzliang/GRIT.","url":"https://huggingface.co/datasets/zzliang/GRIT","creator_name":"zhiliang","creator_url":"https://huggingface.co/zzliang","license_name":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","object-detection","zero-shot-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"anime-synthetics","keyword":"image-to-text","description":"Mostly unfiltered anime-style images generated by various text to image models, collected from various sources (some were submitted for inclusion by their creators).\nIncludes a subset of p1atdev/niji-v5, albeit captioned differently than the source. \nContains 2224 image & caption pairs.\nAs it is unfiltered, some adult content may be included.\nCaptions may not be completely accurate.\nIf you wish to submit content, do it as a pull request.\n","url":"https://huggingface.co/datasets/mirav/anime-synthetics","creator_name":"Mira","creator_url":"https://huggingface.co/mirav","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"stackmix_cyrillic_large","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from cyrillic train set using Stackmix\n\t\n\nNumber of images: 3700269\nSources:\n\nCyrillic dataset\nStackmix code\n\n","url":"https://huggingface.co/datasets/nastyboget/stackmix_cyrillic_large","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"VisualWebBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVisualWebBench\n\t\n\nDataset for the paper: VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?\nðŸŒ Homepage | ðŸ GitHub | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nWe introduce VisualWebBench, a multimodal benchmark designed to assess the understanding and grounding capabilities of MLLMs in web scenarios. VisualWebBench consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14â€¦ See the full description on the dataset page: https://huggingface.co/datasets/visualwebbench/VisualWebBench.","url":"https://huggingface.co/datasets/visualwebbench/VisualWebBench","creator_name":"VisualWebBench","creator_url":"https://huggingface.co/visualwebbench","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"vlm-image-captioning-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tkasvnmtp/vlm-image-captioning-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a custom Vision Language Model (VLM) dataset for image captioning tasks. The dataset contains image-text pairs suitable for finetuning vision-language models.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 149,997\nTrain Samples: 74,998\nTest Samples: 74,999\nFeatures: image, text, sample_id\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\nimage: PIL Image object\ntext: Caption/description text for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset.","url":"https://huggingface.co/datasets/kasvnmtp/vlm-image-captioning-dataset","creator_name":"KAUSHAL KUMAR SINGH","creator_url":"https://huggingface.co/kasvnmtp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"othello_shuffle","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe dataset is derived from real Othello game records collected from EOTHELLO. It combines textual move sequences with corresponding visual board states, enabling joint modeling of language and vision in a structured, rule-based environment.\nEach game consists of a sequence of 60 Â± 2 moves on average, with one board image generated after every move. This results in a total of approximately 25,000 games and 1.56 million board images.\nItâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jaagli/othello_shuffle.","url":"https://huggingface.co/datasets/jaagli/othello_shuffle","creator_name":"Jiaang Li","creator_url":"https://huggingface.co/jaagli","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-classification","image-to-text","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"laion-coco-aesthetic","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLAION COCO with aesthetic score and watermark score\n\t\n\nThis dataset contains 10% samples of the LAION-COCO dataset filtered by some text rules (remove url, special tokens, etc.), and image rules (image size > 384x384, aesthetic score>4.75 and watermark probability<0.5). There are total 8,563,753 data instances in this dataset. And the corresponding aesthetic score and watermark score are also included. \nNoted: watermark score in the table means the probability of the existence of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/guangyil/laion-coco-aesthetic.","url":"https://huggingface.co/datasets/guangyil/laion-coco-aesthetic","creator_name":"Guangyi Liu","creator_url":"https://huggingface.co/guangyil","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"car_images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCar Models with Captions\n\t\n\nHighâ€‘quality dataset of car images paired with short English captions for training and evaluating textâ€‘toâ€‘image models.\n\nSplits: 62,027 train / 1,625 test\nColumns: image (Image feature), text (caption string), original_filename (filename string), source (data source as string), aesthetic_score (aesthetic score floar if available)\nSources: AutoEvolution (original photos without background) + MeshFleet (rendered cars)\nCaptions: machineâ€‘generated (Google Gemmaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DamianBoborzi/car_images.","url":"https://huggingface.co/datasets/DamianBoborzi/car_images","creator_name":"Damian Boborzi","creator_url":"https://huggingface.co/DamianBoborzi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PopVQA","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPopVQA: Popular Entity Visual Question Answering\n\t\n\nPopVQA is a dataset designed to study the performance gap in vision-language models (VLMs) when answering factual questions about entities presented in images versus text.\nPaper: https://huggingface.co/papers/2412.14133\nCode: https://github.com/ido-co/vlm-modality-gap\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ” Motivation\n\t\n\n\n\nPopVQA was curated to explore the disparity in model performance when answering factual questions about an entity described in textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/idoco/PopVQA.","url":"https://huggingface.co/datasets/idoco/PopVQA","creator_name":"Ido Cohen","creator_url":"https://huggingface.co/idoco","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v119","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v119.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v119","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mets","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for METS (Multiple Edits and Textual Summaries)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMETS (Multiple Edits and Textual Summaries) is a dataset of image editing sequences with human-annotated textual summaries describing the differences between original and edited images. The dataset captures cumulative changes after sequences of manipulations, providing ground truth for image difference captioning tasks. METS contains images that have undergone 5, 10, or 15 sequential edits, withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AlexBlck/mets.","url":"https://huggingface.co/datasets/AlexBlck/mets","creator_name":"Alexander Black","creator_url":"https://huggingface.co/AlexBlck","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"image-to-text","description":"\n\n\nðŸš€ Project Page | ðŸ“œ arXiv | ðŸ‘¨â€ðŸ’» Github | ðŸ† Leaderboard | ðŸ“ blog | ðŸ¤— HF Paper | ð• Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizationsâ€™ flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"video-text-to-text","description":"\n\n\nðŸš€ Project Page | ðŸ“œ arXiv | ðŸ‘¨â€ðŸ’» Github | ðŸ† Leaderboard | ðŸ“ blog | ðŸ¤— HF Paper | ð• Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizationsâ€™ flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestones/photo-aesthetics","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"TCGA-Cancer-Variant-and-Clinical-Data","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tTCGA Cancer Variant and Clinical Data\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset combines genetic variant information at the protein level with clinical data from The Cancer Genome Atlas (TCGA) project, curated by the International Cancer Genome Consortium (ICGC). It provides a comprehensive view of protein-altering mutations and clinical characteristics across various cancer types.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset includes:\n\nProtein sequence data for both mutated andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/seq-to-pheno/TCGA-Cancer-Variant-and-Clinical-Data.","url":"https://huggingface.co/datasets/seq-to-pheno/TCGA-Cancer-Variant-and-Clinical-Data","creator_name":"Seq-to-Pheno","creator_url":"https://huggingface.co/seq-to-pheno","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","entity-linking-retrieval","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"image-text-to-text","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§  What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Gucci","keyword":"image-to-text","description":"Products from Gucci with images, name, category, description and more\n","url":"https://huggingface.co/datasets/bigdata-pw/Gucci","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v18","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"deepseek-svg-description","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSVG Reasoning and Generation Dataset\n\t\n\nA rich dataset containing SVG graphics, structured reasoning, and generated descriptions.Built from the base of thesantatitan/deepseek-svg-dataset but enhanced with separated SVG codes and detailed reasoning-based descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription Generation Process\n\t\n\nThe dataset has been enhanced by using the reasoning part from the original completion to generate longer, detailed descriptions. The SVG code part of the completion is ignoredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description.","url":"https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description","creator_name":"Shahzeb Khoso","creator_url":"https://huggingface.co/ShahzebKhoso","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"PRISM-DPO","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPRISM: Principled Reasoning for Integrated Safety in Multimodality Datasets\n\t\n\nThis repository provides access to the datasets developed for PRISM (Principled Reasoning for Integrated Safety in Multimodality), a system2-like framework that aligns Vision-Language Models (VLMs) by embedding a structured, safety-aware reasoning process.\n\nPaper: PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality\nCode: https://github.com/SaFoLab-WISC/PRISMâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/andyc03/PRISM-DPO.","url":"https://huggingface.co/datasets/andyc03/PRISM-DPO","creator_name":"Nanxi Li","creator_url":"https://huggingface.co/andyc03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","Image","arxiv:2508.18649"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v46","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v46.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TrueMICL","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tTrueMICL: True Multimodal In-Context Learning Dataset\n\t\n\nA comprehensive multimodal dataset designed to evaluate and improve true multimodal in-context learning capabilities in Multimodal Large Language Models (MLLMs).\nPaper | Code | Project page\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\nTrueMICL addresses a critical limitation in current Multimodal Large Language Models: their tendency to neglect visual information in multimodal demonstrations, leading to superficial text imitation. Thisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShuoChen99/TrueMICL.","url":"https://huggingface.co/datasets/ShuoChen99/TrueMICL","creator_name":"Shuo Chen","creator_url":"https://huggingface.co/ShuoChen99","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"image-to-text","description":"Oyasi/test dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Oyasi/test","creator_name":"oyasi zaki ananta","creator_url":"https://huggingface.co/Oyasi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"KhyentseWangpo","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for KhyentseWangpo\n\t\n\nA line-to-text dataset for Tibetan OCR.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of 13,527 rows with three columns: \n\nid (string): Unique identifier for each line\nimage (image): Image file containing a line of Tibetan text\ntranscription (string): Tibetan text transcription in Unicode format\n\nThe dataset was created as part of the BDRC - MonlamAI OCR Project (B-MOP).\n\nCurated by: Buddhist Digital Resource Center &â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BDRC/KhyentseWangpo.","url":"https://huggingface.co/datasets/BDRC/KhyentseWangpo","creator_name":"Buddhist Digital Resource Center","creator_url":"https://huggingface.co/BDRC","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","Tibetan","odc-by","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Vript","keyword":"video-text-to-text","description":"ðŸŽ¬ Vript: Refine Video Captioning into Video Scripting\nWe construct a fine-grained video-text dataset with 12K annotated high-resolution videos (~400k clips). The annotation of this dataset is inspired by the video script. If we want to make a video, we have to first write a script to organize how to shoot the scenes in the videos. To shoot a scene, we need to decide the content, shot type (medium shot, close-up, etc), and how the camera moves (panning, tilting, etc). Therefore, we extendâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Vript.","url":"https://huggingface.co/datasets/TIGER-Lab/Vript","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"gregg-preanniversary-words","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGregg Preanniversary Words\n\t\n\nThis dataset is derived from the 1916 Gregg Shorthand Dictionary1\nand builds on top of the Gregg1916\ndataset by:\n\nCorrecting the labels of 250+ images\nAdding 550+ images for words missing in the original dataset\nRedoing 100+ poorly cropped images or images with stray marks\n\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset contains three columns:\n\nimage: The image of a shorthand form in the dictionary\ngrascii_normalized: The normalized grascii of the shorthand formâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/grascii/gregg-preanniversary-words.","url":"https://huggingface.co/datasets/grascii/gregg-preanniversary-words","creator_name":"Grascii: Language and Tools for Gregg Shorthand","creator_url":"https://huggingface.co/grascii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Sanskrit","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSanskrit Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a conversion of the Pracalit for Sanskrit and Newar MSS 16th to 19th C., Ground Truth dataset, originally published on Zenodo. The dataset contains pairs of images and corresponding plain text extracted from XML files. This dataset specifically includes the ground truth data from the original repository.\n\n\t\n\t\t\n\t\n\t\n\t\tPotential Use for VLM-based OCR Training\n\t\n\nThe existing ground truth OCR data can be particularly useful forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/Sanskrit.","url":"https://huggingface.co/datasets/davanstrien/Sanskrit","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Sanskrit","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"synthdoc-zh-tw-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynthDoG Traditional Chinese Dataset\n\t\n\nThis dataset contains synthetic document-ground truth pairs for Traditional Chinese text recognition training. The dataset is generated using the SynthDoG (Synthetic Document Generation) framework, which creates realistic document images with Traditional Chinese text.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized into three splits:\n\ntrain/: Training data\nvalidation/: Validation data\ntest/: Test data\n\nEach split contains:\n\nImage filesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LeeTung/synthdoc-zh-tw-dataset.","url":"https://huggingface.co/datasets/LeeTung/synthdoc-zh-tw-dataset","creator_name":"Lee Tung","creator_url":"https://huggingface.co/LeeTung","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","document-question-answering","Chinese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"svg-stack-labeled","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSvg Stack - Labeled\n\t\n\nThis dataset consists of the central storage for all datasets related to the SVG Stack dataset. I found it to be lovely, detailed, and of decent to extremely good quality upon observing many different icons and logos during the labeling process.\nThis is the central dataset, and is currently UNDER CONSTRUCTION.  Use with caution, and be aware that the format HAS NOT been frozen. I will make a post announcing when I freeze this dataset, as that will also be theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-labeled.","url":"https://huggingface.co/datasets/MrOvkill/svg-stack-labeled","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Cosmos-R1-RL-dataset","keyword":"video-text-to-text","description":"mohammadbhat/Cosmos-R1-RL-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/mohammadbhat/Cosmos-R1-RL-dataset","creator_name":"Mohammad Qazim Bhat","creator_url":"https://huggingface.co/mohammadbhat","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Video-MMLU","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tVideo-MMLU Benchmark\n\t\n\n\n  \n    \n    \n    \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: Video-MMLU Benchmark\n\n\n\t\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\tBenchmark Collection and Processing\n\t\n\nVideo-MMLU specifically targets videos that focus on theorem demonstrations and probleming-solving, covering mathematics, physics, and chemistry. The videos deliver dense information through numbers and formulas, pose significant challenges for video LMMs in dynamic OCRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Enxin/Video-MMLU.","url":"https://huggingface.co/datasets/Enxin/Video-MMLU","creator_name":"EnxinSong","creator_url":"https://huggingface.co/Enxin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"lego_minifigure_captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLEGO Minifigure Captions\n\t\n\nThe LEGO Minifigure Captions dataset contains 12966 images of LEGO minifigures with captions. The dataset contains the following columns:\n\nimage: The jpeg image of the minifigure in the format {\"bytes\": bytes, \"path\": str} so that can be interpreted as PIL.Image objects in the huggingface datasets library.\nshort_caption: The short caption describing the minifigure in the image.\ncaption: The caption describing the minifigure which is generated usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions.","url":"https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions","creator_name":"Alessandro","creator_url":"https://huggingface.co/armaggheddon97","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"svg-stack-tmp-alpha-chunk","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSvg Stack Labeled - Temporary Split Alpha ( Chunk )\n\t\n\nThis dataset is a chunk of SVG Stack Labeled, and was uploaded solely because I lacked reliable high-volume cloud storage at the time, and was going to make the dataset available on HuggingFace in any case.\nHowever, while I will be deleting the now defunct and unused chunks, this one received a few users, and I truly appreciate your usage of my dataets. Thus, this dataset will remain, even as the others perish.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk.","url":"https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v69","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v69.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v69","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ghostui","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGhostUI: Unveling Hidden Interactions in Mobile UI\n\t\n\nGhostUI is the first comprehensive dataset specifically designed to capture and analyze hidden interactions in mobile applicationsâ€”interactions that lack visible cues but are triggered by gestures such as swipe, long press, or double tap.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ” What are Hidden Interactions?\n\t\n\nHidden interactions are gesture-based interactions in mobile UIs that:\n\nLack explicit visual cues indicating their presence\nAre triggered byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ghostui/ghostui.","url":"https://huggingface.co/datasets/ghostui/ghostui","creator_name":"ghostui","creator_url":"https://huggingface.co/ghostui","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"LearningPaper24","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tLearningPaper24 Dataset\n\t\n\n\n\nThis dataset contains video recordings and metadata from ICLR and NeurIPS 2024 conference talks. It includes both poster and oral presentations, along with their associated metadata such as titles, abstracts, keywords, and primary areas.\nThe paper list is originally sourced from Paperlists.\n\n\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nlearningpaper24/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ metadata/\nâ”‚   â””â”€â”€ catalog.json\nâ””â”€â”€ video/\n    â”œâ”€â”€ {openreview_id}_{slideslive_id}.mp4\n    â””â”€â”€ ...â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vivianchen98/LearningPaper24.","url":"https://huggingface.co/datasets/vivianchen98/LearningPaper24","creator_name":"Shenghui Chen","creator_url":"https://huggingface.co/vivianchen98","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","video-text-to-text","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"ArXiv-tables","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tArxiv-tables Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Arxiv-tables dataset is a collection of tables extracted from scientific papers published on arXiv, primarily focused on ML papers. It includes both the LaTeX source of the tables and their corresponding rendered images from the PDF versions of the papers.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThis dataset can support several tasks, including but not limited to:\n\nTable structure recognition\nLaTeX to image generation for tables\nImage-to-LaTeXâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/staghado/ArXiv-tables.","url":"https://huggingface.co/datasets/staghado/ArXiv-tables","creator_name":"Said Taghadouini","creator_url":"https://huggingface.co/staghado","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Real-Complex-Analysis-Math","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tReal-Complex-Analysis-Math\n\t\n\nThis dataset contains high-quality scanned pages from the classic mathematics textbook by Walter Rudin, widely used in advanced undergraduate and graduate-level courses on real and complex analysis. It is ideal for building OCR systems, digitizing textbooks, or creating educational AI tools for higher mathematics.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Principles of Mathematical Analysis and Real and Complex Analysis by Walter Rudin\n\nTask: Image-to-Text (OCRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Real-Complex-Analysis-Math.","url":"https://huggingface.co/datasets/prithivMLmods/Real-Complex-Analysis-Math","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ecom-prod-demo","keyword":"image-to-text","description":"ariwala99/ecom-prod-demo dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ariwala99/ecom-prod-demo","creator_name":"Pratham Ariwala","creator_url":"https://huggingface.co/ariwala99","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArXiv-tables","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tArxiv-tables Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Arxiv-tables dataset is a collection of tables extracted from scientific papers published on arXiv, primarily focused on ML papers. It includes both the LaTeX source of the tables and their corresponding rendered images from the PDF versions of the papers.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\nThis dataset can support several tasks, including but not limited to:\n\nTable structure recognition\nLaTeX to image generation for tables\nImage-to-LaTeXâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/staghado/ArXiv-tables.","url":"https://huggingface.co/datasets/staghado/ArXiv-tables","creator_name":"Said Taghadouini","creator_url":"https://huggingface.co/staghado","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestone-horizon/photo-anatomy","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v101","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v101.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v101","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v52","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v52.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Hidream_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Hidream I1 full Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 38k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hidream I1 full across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Hidream_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ImageCaptioning_Catalan","keyword":"image-to-text","description":"The dataset consists of 153,791 images, each accompanied by a description in Catalan. The images have been sourced from two repositories: \n\"yerevann/coco-karpathy\" and \"UCSC-VLAA/Recap-COCO-30K.\" This dataset is ideal for computer vision tasks, as it combines a wide variety of \nimages with detailed descriptions that can be useful for training machine learning models.\nIt is freely accessible to everyone, as long as proper credit is given to the original data sources. Thanks\n","url":"https://huggingface.co/datasets/Marxx01/ImageCaptioning_Catalan","creator_name":"Marc Hurtado Beneyto","creator_url":"https://huggingface.co/Marxx01","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","text-to-image","Catalan","odc-by"],"keywords_longer_than_N":true},
	{"name":"diffusiondb","keyword":"image-to-text","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/jobs-git/diffusiondb","creator_name":"James Guana","creator_url":"https://huggingface.co/jobs-git","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"Jedi","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nNOTE: Before you use this dataset, make sure you understand the logic of absolute coordinates and image processor for Qwen2.5-VL. \nThis dataset is set with the image processor max tokens to be 2700, a.k.a max_pixels=2700x14x14x2x2 , the coordinates were resized to be smaller and you have to resize the image as well within max_pixels=2700x14x14x2x2 via image processor to make them align.\nMake sure you also follow it in your training procedure, otherwise the performance will notâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/Jedi.","url":"https://huggingface.co/datasets/xlangai/Jedi","creator_name":"XLang NLP Lab","creator_url":"https://huggingface.co/xlangai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2502.13923","arxiv:2505.13227","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Spatial457","keyword":"image-text-to-text","description":"\n  \n\n\n\n  \n    Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models\n  \n\n\n\n  Xingrui Wang1,\n  Wufei Ma1,\n  Tiezheng Zhang1,\n  Celso M. de Melo2,\n  Jieneng Chen1,\n  Alan Yuille1\n\n\n\n  1 Johns Hopkins University Â Â Â Â \n  2 DEVCOM Army Research Laboratory\n\n\n\n  ðŸŒ Project Pageâ€¢\n  ðŸ“„ Paper â€¢\n  ðŸ¤— Dataset â€¢\n  ðŸ’» Code\n\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tðŸ§  Introduction\n\t\n\nSpatial457 is a diagnostic benchmark designed to evaluate 6D spatial reasoning in large multimodal models (LMMs). Itâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RyanWW/Spatial457.","url":"https://huggingface.co/datasets/RyanWW/Spatial457","creator_name":"Ryan Wang","creator_url":"https://huggingface.co/RyanWW","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2502.08636"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"portuguese-ocr-dataset","keyword":"image-to-text","description":"task_categories:\n\nimage-to-text\ntask_ids:\noptical-character-recognition\ntext-recognition\n\n\n\t\n\t\t\n\t\tPortuguese OCR Dataset\n\t\n\nA comprehensive dataset for Portuguese OCR (Optical Character Recognition) generated from classic Portuguese literature with diverse fonts and visual styles.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 20000 text images for OCR training, created from Portuguese books from Project Gutenberg. Each image contains a complete Portuguese sentence with properâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mazafard/portuguese-ocr-dataset.","url":"https://huggingface.co/datasets/mazafard/portuguese-ocr-dataset","creator_name":"Mohammadreza Asadollahifard","creator_url":"https://huggingface.co/mazafard","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","Portuguese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ja","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Ja\n\t\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\nä»¥ä¸‹ã®æƒ…å ±ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nLLaVA-JPã‚’æ”¹è‰¯ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç°¡æ˜“ãªæ—¥æœ¬èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³1ã¤\nLLaVA-JPã‚’æ”¹è‰¯ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã§ãã‚‹ã ã‘è©³ç´°ãªæ—¥æœ¬èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³3ã¤ (äºˆå®š)\n\n\n\t\n\t\t\n\t\tSample Code\n\t\n\ndf2=pandas.read_csv(\"cc-by-ja.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nfor i,data in enumerate(tqdm(dataset)):\n    data[\"jpg\"].save(f\"/mnt/my_raid/pixart_jp/InternImgs/{i:09}.jpg\")\n\n    data_info.append({\n        \"height\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja.","url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","Japanese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"gex_dataset_merged","keyword":"image-to-text","description":"MosRat/gex_dataset_merged dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/MosRat/gex_dataset_merged","creator_name":"MosRat","creator_url":"https://huggingface.co/MosRat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Chinese","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v138","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v138.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v138","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"medical-imaging","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tX-ray Reports Dataset\n\t\n\nThis dataset contains high-quality (â€œA-gradeâ€) anonymized X-ray images paired with radiology reports. It has been carefully curated, cleaned, and verified to ensure accuracy, completeness, and compliance with privacy standards (e.g., HIPAA/GDPR), making it suitable for high-stakes or research-grade model training.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:\n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/medical-imaging.","url":"https://huggingface.co/datasets/Kratos-AI/medical-imaging","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v182","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v182.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v182","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"medical-imaging","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tX-ray Reports Dataset\n\t\n\nThis dataset contains high-quality (â€œA-gradeâ€) anonymized X-ray images paired with radiology reports. It has been carefully curated, cleaned, and verified to ensure accuracy, completeness, and compliance with privacy standards (e.g., HIPAA/GDPR), making it suitable for high-stakes or research-grade model training.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:\n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.io\n\n\n\t\n\t\t\n\t\tSupportedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/medical-imaging.","url":"https://huggingface.co/datasets/Kratos-AI/medical-imaging","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Magma-Video-ToM","keyword":"video-text-to-text","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1â€ Â \nReuben Tan1â€ Â \nQianhui Wu1â€ Â \nRuijie Zheng2â€¡Â \nBaolin Peng1â€¡Â \nYongyuan Liang2â€¡\nYu Gu1Â \nMu Cai3Â \nSeonghyeon Ye4Â \nJoel Jang5Â \nYuquan Deng5Â \nLars Liden1Â \nJianfeng Gao1â–½\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  â€  First authors  â€¡ Second authors  â–½ Leadership  \n[arXiv Paper] Â  [Project Page] Â  [Hugging Face Paper] Â  [Github Repo] Â  [Video]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM.","url":"https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","robotics","mit","1M - 10M","arrow"],"keywords_longer_than_N":true},
	{"name":"KOFFVQA_Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAbout this data\n\t\n\nKOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language\nKOFFVQA is a general-purpose VLM benchmark in the Korean language. For more information, refer to our leaderboard page and the official evaluation code.\nThis contains the data for the benchmark consisting of images, their corresponding questions, and response grading criteria.  The benchmark focuses on free-form visual question answering, evaluating theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/maum-ai/KOFFVQA_Data.","url":"https://huggingface.co/datasets/maum-ai/KOFFVQA_Data","creator_name":"maum-ai","creator_url":"https://huggingface.co/maum-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Caption3o-XL-v4","keyword":"image-text-to-text","description":"\n\n\t\n\t\t\n\t\tCaption3o-XL-v4\n\t\n\nCaption3o-XL-v4 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0\nSplit:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-XL-v4.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-XL-v4","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VibeEval","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVibe-Eval\n\t\n\nA benchmark for evaluating multimodal chat models, including especially challenging examples.\n[Link to paper] [Blogpost] [Github]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\nEach example has the following fields:\n\nexample_id: a unique ID for the example\ncategory: the category that this example belongs to, either difficulty-normal or difficulty-hard\nprompt: the user prompt\nreference: a golden reference answer for the prompt\nimage: an image struct (containing bytes and path keys).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RekaAI/VibeEval.","url":"https://huggingface.co/datasets/RekaAI/VibeEval","creator_name":"Reka AI","creator_url":"https://huggingface.co/RekaAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","English","Polish","Chinese"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Caption3o-XL-v4","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tCaption3o-XL-v4\n\t\n\nCaption3o-XL-v4 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0\nSplit:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-XL-v4.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-XL-v4","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Img2Code","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8Bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/Img2Code.","url":"https://huggingface.co/datasets/MathLLMs/Img2Code","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","image-text-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Img2Code","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8Bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/Img2Code.","url":"https://huggingface.co/datasets/MathLLMs/Img2Code","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","image-text-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Hotel_Booking_Screen_Recording_Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tHotel Booking Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on hotel booking platforms. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Hotel_Booking_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Hotel_Booking_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Hotel_Booking_Screen_Recording_Dataset","keyword":"video-to-text","description":"\n\t\n\t\t\n\t\tHotel Booking Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on hotel booking platforms. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Hotel_Booking_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Hotel_Booking_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"GeoReasoning","keyword":"image-to-text","description":"GeoReasoning is composed of 10k carefully constructed image-caption pairs.\npaper: https://arxiv.org/abs/2509.15217\n","url":"https://huggingface.co/datasets/ScaleMath/GeoReasoning","creator_name":"ScaleMath","creator_url":"https://huggingface.co/ScaleMath","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"llava-test-nonmember-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLLaVA-1.5 Test Images Not in LAION2B\n\t\n\nThis dataset contains validation and test images from various vision-language datasets that are commonly used to evaluate LLaVA-1.5 and similar models, but are not present in the LAION2B dataset.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"iammytoo/llava-test-nonmember-v3\")\n\n","url":"https://huggingface.co/datasets/iammytoo/llava-test-nonmember-v3","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v48","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v48.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communitiesâ€™ Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CUPUM/mid-space.","url":"https://huggingface.co/datasets/CUPUM/mid-space","creator_name":"CUPUM","creator_url":"https://huggingface.co/CUPUM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ChartEdit","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing\n\t\n\nPaper | Code\nChartEdit is a novel benchmark designed for chart editing tasks, proposed in the paper \"ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing\". It features $1405$ diverse editing instructions applied to $233$ real-world charts, each manually annotated and validated for accuracy. This benchmark aims to evaluateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xxxllz/ChartEdit.","url":"https://huggingface.co/datasets/xxxllz/ChartEdit","creator_name":"Xuanle Zhao","creator_url":"https://huggingface.co/xxxllz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v21","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v18","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Seedream-3_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Seedream 3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~30'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Seedream-3_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"BIGstockimage35M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for BIGstockimage35M\n\t\n\n~35M stock images.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 35 million stock images and caption. Images are low resolution, ~0.4MP.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tWebDataset Structure\n\t\n\n\nkey: Unique identifier for the image\n.jpg: The image\n.txt: The caption\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{BIGstockimage35M,\n  author = {hlky},\n  title = {BIGstockimage35M}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/BIGstockimage35M.","url":"https://huggingface.co/datasets/bigdata-pw/BIGstockimage35M","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"textureninja","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Texture Ninja\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,540 texture images from texture.ninja. It includes high-resolution textures of brick, concrete, rock, wood, metal, paint, plaster, ground materials, and other surfaces. The original images were downloaded, processed, and compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/textureninja.","url":"https://huggingface.co/datasets/nyuuzyou/textureninja","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"mypresentation","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for MyPresentation.ru\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 420,946 educational presentations in Russian extracted from mypresentation.ru website. The content includes presentation slides across various educational topics and categories.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian (ru).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nurl: URL of the presentation page (string)\ntitle:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/mypresentation.","url":"https://huggingface.co/datasets/nyuuzyou/mypresentation","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","image-to-text","topic-classification","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Japanese-photos","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tæ—¥æœ¬ã®å†™çœŸãŸã¡\n\t\n\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯æ—¥æœ¬ã®å†™çœŸãŸã¡ã‚’å…±æœ‰ã—ã¦ã„ã¾ã™ã€‚\næ—¥æœ¬ã§ã‚ˆãè¦‹ã‚‰ã‚Œã‚‹å…‰æ™¯ã‚’AIã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§\næ—¥æœ¬ã‚‰ã—ã„å¿œç­”ã®ã§ãã‚‹AIã‚’é–‹ç™ºã™ã‚‹ã“ã¨ã‚’ç›®çš„ã«ã€\nCC-0ã§å†™çœŸ800æžšãªã©ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚\nç‰¹ã«æœ€è¿‘å¯¿å¸ã¨ã‹ãŒæ—¥æœ¬ã‚‰ã—ããªã„ã®ãŒæ°—ã«ãªã£ã¦ã„ã¾ã™ã€‚\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯: images.tar\nè‹±èªžã®èª¬æ˜Žæ–‡ : metadata.csv\næ—¥æœ¬èªžã®èª¬æ˜Žæ–‡ : metadata_ja.csv\n\n\t\n\t\t\n\t\n\t\n\t\tãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦\n\t\n\næ’®å½±è€…ã§ã‚ã‚‹ç§ã¯ã“ã‚Œã‚‰ã®å†™çœŸã®è‘—ä½œæ¨©ã‚’æ”¾æ£„ã—ã¾ã™ã€‚\nã„ã‹ã‚ˆã†ã«ã‚‚ä½¿ã£ã¦ãã ã•ã„ã€‚\nã—ã‹ã—ã€è‚–åƒæ¨©ã‚„å•†æ¨™æ¨©ã¯æ®‹ã£ã¦ã„ã¾ã™ã®ã§ã€ã”æ³¨æ„ãã ã•ã„ã€‚\næ—¥æœ¬èªžã¨è‹±èªžã®èª¬æ˜Žæ–‡ã¯ã¨ã‚‚ã«Qwen2.5VLã«ã‚ˆã‚Šä½œã‚‰ã‚Œã¾ã—ãŸã€‚ã—ãŸãŒã£ã¦ã€è‘—ä½œæ¨©ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tå†™çœŸã®æ¦‚è¦\n\t\n\nåŸºæœ¬çš„ã«å„åœ°ã‚’æ—…è¡Œã—ãŸéš›ã®æ—¥æœ¬å„åœ°ã®å†™çœŸãŒå†™ã£ã¦ã„ã¾ã™ã€‚\nå®¤å¤–ã‚‚ã‚ã‚Œã°ã€å®¤å†…ã‚‚ã‚ã‚Šã¾ã™ã€‚å†™çœŸã®ä¾‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚\n\n\t\n\t\n\t\n\t\tå†™çœŸã®ä¾‹\n\t\n\n\næ—¥æœ¬èªžã®èª¬æ˜Žæ–‡â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/Japanese-photos.","url":"https://huggingface.co/datasets/alfredplpl/Japanese-photos","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v110","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v110.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v110","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SocialNav-SUB","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tSocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation\n\t\n\nThis is the accompying dataset for the Social Navigation Scene Understanding Benchmark (SocialNav-SUB) which is a Visual Question Answering (VQA) dataset and benchmark designed to evaluate Vision-Language Models (VLMs) for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines acrossâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michaelmunje/SocialNav-SUB.","url":"https://huggingface.co/datasets/michaelmunje/SocialNav-SUB","creator_name":"Michael Munje","creator_url":"https://huggingface.co/michaelmunje","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","robotics","English","mit"],"keywords_longer_than_N":true},
	{"name":"3d_layout_reasoning","keyword":"image-text-to-text","description":"Dataset for MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse\nGithub: https://github.com/PzySeere/MetaSpatial\n","url":"https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning","creator_name":"Zhenyu Pan","creator_url":"https://huggingface.co/zhenyupan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottomâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HAIC","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tHAIC: Human Action and Interaction Comprehension Dataset\n\t\n\nFrom the paper: \"HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models\"\nRead the Paper\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nHAICBench is a comprehensive video dataset featuring manually annotated, fine-grained human captions that features:\n\nMultiple Human Subjects: Captions detail interactions and activities involving more than one person, capturing the complexity of humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KuaishouHAIC/HAIC.","url":"https://huggingface.co/datasets/KuaishouHAIC/HAIC","creator_name":"KuaishouHAIC","creator_url":"https://huggingface.co/KuaishouHAIC","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","Chinese","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Emobench-M","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tEmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEmoBench-M is a comprehensive benchmark designed to evaluate the Emotional Intelligence (EI) of Multimodal Large Language Models (MLLMs). It provides a challenging testbed for assessing a model's ability to understand and interpret human emotions from video, a critical step towards developing more empathetic and human-like AI systems.\nThe dataset consists of videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GMLHUHE/Emobench-M.","url":"https://huggingface.co/datasets/GMLHUHE/Emobench-M","creator_name":"HU HE","creator_url":"https://huggingface.co/GMLHUHE","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Dogs-images-text-pair","keyword":"image-to-text","description":"This dataset is curated from subsets of public datasets such as MS COCO, LAION-ART, and SBU Captions.\nIt specifically filters for samples featuring dog-related content.\nThe goal of this dataset is to support text-to-image generation models focused on dog objects and related scenes. \nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/SBUcaptions.md\nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/mscoco.mdâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair.","url":"https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair","creator_name":"Mikdad Mrhij","creator_url":"https://huggingface.co/MikdadMrhij","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"otpensource_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for otpensource_dataset\n\t\n\nThis dataset contains curated information about fashion items, focusing on various categories, subcategories, genders, seasons, and unique features. It is intended for use in AI applications related to fashion recommendation systems, trend analysis, and image-to-text generation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset includes 8,998 examples of Korean Mushinsa fashion data, each with detailed attributes such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hateslopacademy/otpensource_dataset.","url":"https://huggingface.co/datasets/hateslopacademy/otpensource_dataset","creator_name":"hateslop_academy","creator_url":"https://huggingface.co/hateslopacademy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-captioning","sentiment-analysis","Korean"],"keywords_longer_than_N":true},
	{"name":"otpensource_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for otpensource_dataset\n\t\n\nThis dataset contains curated information about fashion items, focusing on various categories, subcategories, genders, seasons, and unique features. It is intended for use in AI applications related to fashion recommendation systems, trend analysis, and image-to-text generation.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset includes 8,998 examples of Korean Mushinsa fashion data, each with detailed attributes such asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hateslopacademy/otpensource_dataset.","url":"https://huggingface.co/datasets/hateslopacademy/otpensource_dataset","creator_name":"hateslop_academy","creator_url":"https://huggingface.co/hateslopacademy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-captioning","sentiment-analysis","Korean"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LLaVA-ReCap-676K","keyword":"image-to-text","description":"This is an integrated version of LLaVA-ReCap, sourced from lmms-lab/LLaVA-ReCap-558K and lmms-lab/LLaVA-ReCap-118K.\nIn this version, the conversations field has been split into two separate fields: prompt and response. Additionally, the <image> special token has been removed to facilitate customization.\nInspired by the original paper, the prompt field has been further expanded with human-crafted variations. Specifically, each prompt is sampled from one of the following 30 instructions:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LimeryJorge/LLaVA-ReCap-676K.","url":"https://huggingface.co/datasets/LimeryJorge/LLaVA-ReCap-676K","creator_name":"Limery Jorge","creator_url":"https://huggingface.co/LimeryJorge","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Opendoc1-Analysis-Recognition","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpendoc1-Analysis-Recognition Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Opendoc1-Analysis-Recognition dataset is designed for tasks involving image-to-text, text classification, and image feature extraction. It contains images paired with class labels, making it suitable for vision-language tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModalities: Image\nLanguages: English\nSize: Approximately 1,000 samples (n=1K)\nTags: image, analysis, vision-language\nLicense: Apache 2.0\n\n\n\t\n\t\t\n\t\tTasks\n\t\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Opendoc1-Analysis-Recognition.","url":"https://huggingface.co/datasets/prithivMLmods/Opendoc1-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"vlms-are-biased","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVision Language Models are Biased \n\t\n\n    \n  by \n    An Vo1*,\n    Khai-Nguyen Nguyen2*,\n    Mohammad Reza Taesiri3, \n    Vy Tuong Dang1,\n    Anh Totti Nguyen4â€ ,\n    Daeyoung Kim1â€ \n  \n  \n    *Equal contributionÂ Â Â Â â€ Equal advising\n    1KAIST, 2College of William and Mary, 3University of Alberta, 4Auburn University\n  \n\n\n\n\n\n \n\n\n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) perform perfectly on counting tasks with original images but fail catastrophically (e.g., 100% â†’ 17.05%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/vlms-are-biased.","url":"https://huggingface.co/datasets/anvo25/vlms-are-biased","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v18","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"japanese-humor-evaluation-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tJapanese Multimodal Humor Evaluation Dataset (v2)\n\t\n\nç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®ãŠé¡Œã«å¯¾ã™ã‚‹é¢ç™½ã„å›žç­”ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚boketeï¼ˆç”»åƒâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨keitaiï¼ˆãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’çµ±åˆã€‚\n\n\t\n\t\t\n\t\tä½¿ã„æ–¹\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"iammytoo/japanese-humor-evaluation-v2\")\n\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿æ§‹é€ \n\t\n\n\nodai_type: 'image' or 'text'\nimage: ç”»åƒãŠé¡Œï¼ˆtextã‚¿ã‚¤ãƒ—ã§ã¯Noneï¼‰\nodai: ãƒ†ã‚­ã‚¹ãƒˆãŠé¡Œï¼ˆimageã‚¿ã‚¤ãƒ—ã§ã¯Noneï¼‰\nresponse: å›žç­”ãƒ†ã‚­ã‚¹ãƒˆ\nscore: 0-4ã®æ­£è¦åŒ–ã‚¹ã‚³ã‚¢\n\n\n\t\n\t\t\n\t\tã‚½ãƒ¼ã‚¹\n\t\n\n\nYANS-official/ogiri-bokete\nYANS-official/ogiri-keitai\n\n","url":"https://huggingface.co/datasets/iammytoo/japanese-humor-evaluation-v2","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","Japanese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PersianNumberVersion1","keyword":"image-to-text","description":"initmahdi/PersianNumberVersion1 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/initmahdi/PersianNumberVersion1","creator_name":"m.mahdi","creator_url":"https://huggingface.co/initmahdi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Persian","Arabic","cc-by-4.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"UniMER","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUniMER Dataset\n\t\n\nFor detailed instructions on using the dataset, please refer to the project homepage: UniMERNet Homepage\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe UniMER dataset is a specialized collection curated to advance the field of Mathematical Expression Recognition (MER). It encompasses the comprehensive UniMER-1M training set, featuring over one million instances that represent a diverse and intricate range of mathematical expressions, coupled with the UniMER Test Set, meticulouslyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepcopy/UniMER.","url":"https://huggingface.co/datasets/deepcopy/UniMER","creator_name":"deep copy","creator_url":"https://huggingface.co/deepcopy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v24","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v24.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v187","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v187.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v187","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v58","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v58.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nðŸŒ Homepage | Code | ðŸ¤— Paper | ðŸ“– arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specificâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"HumanSense_Benchmark","keyword":"video-text-to-text","description":"\n\nHumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs\n\n\n    Zheng Qin1,\n    Ruobing Zheng*2,\n    Yabing Wang1,\n    Tianqi Li2,\n    Yi Yuan2,\n    Jingdong Chen2,\n    Le Wangâ€ 1 \n    \n    \n    *Co-first authors. Project Lead.\n    â€ Corresponding Author.\n    \n    1Xiâ€™an Jiaotong University. 2Ant Group.\n    \n    \n\n\n Hugging Face Paper\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n arXiv:2508.10576\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n Homepage\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n GitHub\nÂ Â Â Â Â Â Â Â Â Â Â Â Â Â \n\n  \n    \n    Huggingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/antgroup/HumanSense_Benchmark.","url":"https://huggingface.co/datasets/antgroup/HumanSense_Benchmark","creator_name":"Ant Group","creator_url":"https://huggingface.co/antgroup","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2508.10576","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"OmniSpatial","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tOmniSpatial\n\t\n\nThis repository contains the data presented in OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models.\n\n\t\n\t\t\n\t\tTask Schema Documentation\n\t\n\nThis document provides a structured explanation of the task schema for the visual-spatial reasoning benchmark.\n\n\n\t\n\t\t\n\t\tSchema Structure\n\t\n\nThe schema is represented in JSON format, containing the following key components:\n\n\t\n\t\t\nKey\nDescription\n\n\n\t\t\nid\nIdentifier for the question, formatted asâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/qizekun/OmniSpatial.","url":"https://huggingface.co/datasets/qizekun/OmniSpatial","creator_name":"Zekun Qi","creator_url":"https://huggingface.co/qizekun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v57","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v57.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Single-Image","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Single-Image","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-10k-256x256","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ“„ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256.","url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Prompt-Ablations","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMaCBench-Prompt-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Prompt-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"jedi","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nThe JEDI Dataset consists of four carefully designed categories:\n\nIcon\nComponent\nLayout\nRefusal\n\nThis repository includes all the textures and images for these components.\nAdditionally, JEDI processes and improves the data from AGUVIS, calling it AGUVIS++. This repository contains the texture portion of AGUVIS++. For images, please refer to the original repository.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“„ Citation\n\t\n\nIf you find this work useful, please consider citing our paper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/rronan-h/jedi.","url":"https://huggingface.co/datasets/rronan-h/jedi","creator_name":"Ronan Riochet","creator_url":"https://huggingface.co/rronan-h","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"TON-Math-SFT","keyword":"image-to-text","description":"This is the dataset trained for model cited in the paper: Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models.\n","url":"https://huggingface.co/datasets/kolerk/TON-Math-SFT","creator_name":"jiaqi wang","creator_url":"https://huggingface.co/kolerk","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"HumanEval-V-Benchmark","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tHumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks\n\t\n\n\n    ðŸ“„ Paper  â€¢\n    ðŸ  Home Page â€¢\n    ðŸ’» GitHub Repository  â€¢\n    ðŸ† Leaderboard â€¢\n    ðŸ¤— Dataset Viewer \n\n\nHumanEval-V is a novel benchmark designed to evaluate the diagram understanding and reasoning capabilities of Large Multimodal Models (LMMs) in programming contexts. Unlike existing benchmarks, HumanEval-V focuses on coding tasks that require sophisticated visual reasoning overâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark.","url":"https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark","creator_name":"HumanEval-V","creator_url":"https://huggingface.co/HumanEval-V","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v173","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v173.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v173","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"RICO-ScreenAnnotation-f","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for RICO Screen Annotations\n\t\n\nThis is a standardization of Google's Screen Annotation dataset on a subset of RICO screens, as described in their ScreenAI paper.\nUnlike the original, this version transforms integer-based bounding boxes into floating-point-based bounding boxes of 2 decimal precision. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is an image-to-text annotation format first proscribed in Google's ScreenAI paper. \nThe idea is to standardizeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-ScreenAnnotation-f.","url":"https://huggingface.co/datasets/rootsautomation/RICO-ScreenAnnotation-f","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v112","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v112.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v112","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"flickr8k-pt-br","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŽ‰ Flickr8K Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tðŸ’¾ Dataset Summary\n\t\n\nFlickr8K Portuguese Translation, a multimodal dataset for Portuguese image captioning with 8,000 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tðŸ§‘â€ðŸ’» Hot to Get Started with the Datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br.","url":"https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br","creator_name":"LaboratÃ³rio de InteligÃªncia Computacional e Sistemas de informaÃ§Ã£o","creator_url":"https://huggingface.co/laicsiifes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"SightationReasoning","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSighationReasoning\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationReasoning.","url":"https://huggingface.co/datasets/Sightation/SightationReasoning","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"minecraft-captioning","keyword":"image-to-text","description":"orzhan/minecraft-captioning dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/orzhan/minecraft-captioning","creator_name":"Mikhail","creator_url":"https://huggingface.co/orzhan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"SightationReasoning","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSighationReasoning\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationReasoning.","url":"https://huggingface.co/datasets/Sightation/SightationReasoning","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"beautiVis","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tbeautiVis\n\t\n\n\n\t\n\t\t\n\t\tAbout the Dataset\n\t\n\nbeautiVis is a richly annotated dataset of 50,000+ static images sourced from Reddit's r/dataisbeautiful subreddit between February 2012 and January 2025. The dataset was built through a three-phase pipeline:\nPhase 1: Data CollectionFirst, we downloaded the complete post history from r/dataisbeautiful using the Arctic-Shift Reddit Download Tool, which provided raw JSON data containing post metadata, titles, and image URLs. During this initialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/beautiVis/beautiVis.","url":"https://huggingface.co/datasets/beautiVis/beautiVis","creator_name":"beautiVis","creator_url":"https://huggingface.co/beautiVis","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","cc0-1.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"text_meme","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\ttext_meme\n\t\n\nÐ¡Ð¾ÑÐºÑ€Ð°Ð¿ÐµÐ½Ð¾ Ñ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Telegram ÐºÐ°Ð½Ð°Ð»Ð° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð¼ÐµÐ¼Ñ‹.\n","url":"https://huggingface.co/datasets/d0rj/text_meme","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","monolingual","Russian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"image-to-text","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"table-to-text","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"video-text-to-text","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-Data-ru\n\t\n\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\n\nAlmost all datasets have been translated, except for the following:\n[\"tallyqa(cauldron,llava_format)\", \"clevr(cauldron,llava_format)\", \"VisualWebInstruct(filtered)\", \"figureqa(cauldron,llava_format)\", \"magpie_pro(l3_80b_mt)\", \"magpie_pro(qwen2_72b_st)\", \"rendered_text(cauldron)\", \"ureader_ie\"]\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nimport datasets\n\n\ndata =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru.","url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-to-text","translated","monolingual"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v130","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v130.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v130","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"F-16-NBA","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tF-16 NBA dataset\n\t\n\nNBA caption training set (manually annotated) and NBA shot test set (from NBA metadata) used in F-16. The training set of NBA shot is too large to upload, which could be crawled from NBA api following NSVA.\n","url":"https://huggingface.co/datasets/tsinghua-ee/F-16-NBA","creator_name":"Electronic Engineering @Tsinghua University","creator_url":"https://huggingface.co/tsinghua-ee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2503.13956","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"DoclingMatix","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDoclingMatix\n\t\n\nDoclingMatix is a large-scale, multimodal dataset designed for training vision-language models in the domain of document intelligence. It was created specifically for training the SmolDocling model, an ultra-compact model for end-to-end document conversion.\nThe dataset is constructed by augmenting Hugging Face's Docmatix. Each sample in Docmatix, which consists of a document image and a few questions and answers about it, has been transformed. The text field is nowâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix.","url":"https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix","creator_name":"HuggingFaceM4","creator_url":"https://huggingface.co/HuggingFaceM4","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","cdla-permissive-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v127","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v127.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v127","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Flight_Booking_App_Screen_Recording_Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tFlight Booking App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on flight booking applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Flight_Booking_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Flight_Booking_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Flight_Booking_App_Screen_Recording_Dataset","keyword":"video-to-text","description":"\n\t\n\t\t\n\t\tFlight Booking App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on flight booking applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Flight_Booking_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Flight_Booking_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"DocMMIR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDocMMIR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDocMMIR (Document-level Multimodal Information Retrieval) is a dataset for document-level multimodal information retrieval. This dataset contains image-text pairs from arXiv papers, Wikipedia, and presentations, specifically designed for multimodal retrieval tasks.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\nStatistic\nWiki\nArXiv\nSlide\nTotal\n\n\n\t\t\n#Train\n360,285\n62,764\n27,057\n450,079\n\n\n#Valid\n14,775\n3,000\n1,409\n19,184\n\n\n#Test\n14,805\n3,000\n1,399â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Lord-Jim/DocMMIR.","url":"https://huggingface.co/datasets/Lord-Jim/DocMMIR","creator_name":"Zirui Li","creator_url":"https://huggingface.co/Lord-Jim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MolA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTowards Reliable Optical Chemical Structure Recognition: Better Metrics, Better Data and Better Reconstruction\n\t\n\nWe introduce a comprehensive approach that improves OCSR evaluation, training data, and model reliability.\nKey Contributions:\n\nBetter Metrics: We introduce a comprehensive evaluation scheme that directly quantifies manual correction costs, with Perfectly-Matched Annotation Accuracy (PMAA) as the most stringent metric, ensuring exact alignment between model outputs and inputâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhenger/MolA.","url":"https://huggingface.co/datasets/Zhenger/MolA","creator_name":"Jiaxin","creator_url":"https://huggingface.co/Zhenger","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"VISTA-400K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVISTA-400K\n\t\n\nThis repo contains all subsets for VISTA-400K. VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\n\n\t\n\t\t\n\t\tThis repo is under construction. Please stay tuned.\n\t\n\nðŸŒ Homepage | ðŸ“– arXiv | ðŸ’» GitHub | ðŸ¤— VISTA-400K | ðŸ¤— Models | ðŸ¤— HRVideoBench\n\n\t\n\t\n\t\n\t\tVideo Instruction Data Synthesis Pipeline\n\t\n\n\nVISTA leverages insights fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/VISTA-400K.","url":"https://huggingface.co/datasets/TIGER-Lab/VISTA-400K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"RotBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tRotBench\n\t\n\nData for RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation.\narXiv | GitHub\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nRotBench is a benchmark for evaluating whether multimodal large language models (MLLMs) can identify image orientation. It contains 350 manually filtered images. The dataset includes two subsets:  \n\nLarge: 300 images  \nSmall: 50 images\n\nAll images were drawn from the Spatial-MM dataset and passed a two-stage human verification process toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tianyin/RotBench.","url":"https://huggingface.co/datasets/tianyin/RotBench","creator_name":"Tianyi Niu","creator_url":"https://huggingface.co/tianyin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-40","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-40","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"RSCC","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRSCC\n\t\n\nPaper | Project Page | Code\n\n[!WARNING]Due to xBD Licenses, we do not provide direct xBD images and masks. Users can get it via https://www.xview2.org/.\nThe test set of xBD mentioned in our paper can be directly obtained by selecting the first 26 pre- post- images pairs from 19 distinct xBD events to yield all 988=26 * 2 * 19 images\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe introduce the Remote Sensing Change Caption (RSCC) dataset, a new benchmark designed to advance the development ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BiliSakura/RSCC.","url":"https://huggingface.co/datasets/BiliSakura/RSCC","creator_name":"Sakura","creator_url":"https://huggingface.co/BiliSakura","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v92","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v92.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v92","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MM-K12","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMM-K12\n\t\n\n[ðŸ“‚ GitHub]  [ðŸ“œ Paper] \nMM-K12 is a curated, high-quality dataset containing 10,000 multimodal math problems sourced from K-12 educational content. Each problem includes both textual and visual components, covering a wide range of mathematical topics (e.g., arithmetic, geometry, algebra). All problems have unique, verifiable answers, making the dataset ideal for supervised training, evaluation, and reward modeling in multimodal mathematical reasoning tasks.\nThe datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Cierra0506/MM-K12.","url":"https://huggingface.co/datasets/Cierra0506/MM-K12","creator_name":"Lingxiao Du","creator_url":"https://huggingface.co/Cierra0506","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-missing-words","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Missing Words\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Missing Words, each variation column is affected by the noise, without considering the split between train, validation and test.\n\n\t\n\t\t\n\t\n\t\n\t\tData structure\n\t\n\nThe dataset is composed of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words.","url":"https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words","creator_name":"Alix ChaguÃ©","creator_url":"https://huggingface.co/alix-tz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"trdg_random_en_zh_text_recognition","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"trdg_random_en_zh_text_recognition\"\n\t\n\nThis synthetic dataset was generated using the TextRecognitionDataGenerator(TRDG) open source repo: \nhttps://github.com/Belval/TextRecognitionDataGenerator\nIt contains images of text with random characters from Engilsh(en) and Chinese(zh) languages.\nReference to the documentation provided by the TRDG repo: \nhttps://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html\n","url":"https://huggingface.co/datasets/priyank-m/trdg_random_en_zh_text_recognition","creator_name":"priyank","creator_url":"https://huggingface.co/priyank-m","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v33","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v33.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v118","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v118.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v118","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v80","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v80.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v80","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v47","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v47.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"new_real_datasets","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","url":"https://huggingface.co/datasets/hongin9812/new_real_datasets","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"MMMR","keyword":"image-text-to-text","description":"This repository contains the data presented in MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks.\nProject page: https://mmmr-benchmark.github.io/\n","url":"https://huggingface.co/datasets/csegirl/MMMR","creator_name":"g","creator_url":"https://huggingface.co/csegirl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"data","keyword":"image-to-text","description":"zahidpichen/data dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zahidpichen/data","creator_name":"ninjagamer","creator_url":"https://huggingface.co/zahidpichen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"verify-teaser","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning FidelitY\n\t\n\nVERIFY is the first benchmark explicitly designed to assess the reasoning paths of MLLMs in visual reasoning tasks. \nBy introducing novel evaluation metrics that go beyond mere accuracy, VERIFY highlights critical limitations in current MLLMs and emphasizes the need for a more balanced approach to visual perception and logical reasoning.\nDetails of the benchmark can viewed at theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jing-bi/verify-teaser.","url":"https://huggingface.co/datasets/jing-bi/verify-teaser","creator_name":"jing bi","creator_url":"https://huggingface.co/jing-bi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Sofie","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Sofie","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"processed_hcs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is not data that I created. It originally came from the Paper Digitization of Handwritten Chess Scoresheets with a BiLSTM Network\nYou can also find the dataset here and here\n\n\t\n\t\t\n\t\tDatasets\n\t\n\nThere are 2 versions of this dataset\n\nprocessed_hcs Dataset where you are right now\nunprocessed_hcs where the whole scoresheet can be seen here\n\n\n\t\n\t\t\n\t\tDesciption\n\t\n\nThe Handwritten Chess Scoresheet Datase (HCS) contains a set of single and double paged chess scoresheetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BenjaminKost/processed_hcs.","url":"https://huggingface.co/datasets/BenjaminKost/processed_hcs","creator_name":"Benjamin Kostka","creator_url":"https://huggingface.co/BenjaminKost","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"multicare-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMultiCaRe: Open-Source Clinical Case Dataset\n\t\n\nMultiCaRe is an open-source, multimodal clinical case dataset built from the PubMed Central Open Access (OA) Case Report articles. It aggregates de-identified, open-access case narratives, figure images, captions, and rich article metadata across diverse specialties (radiology, pathology, surgery, ophthalmology, etc.). The data is normalized so images, cases, and articles can be joined via stable IDs.\n\nSource and process: OA case reportsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/multicare-images.","url":"https://huggingface.co/datasets/openmed-community/multicare-images","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"modern","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tDataset Card for CATMuS Modern and Contemporary (McCATMuS)\n\t\n\nJoin our Discord to ask questions about the dataset: \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, enabling researchers and scholars to analyze vast collections efficiently. Despite significant technological progress, establishing consistent ground truth across projects for HTR tasks, particularly for complex andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/modern.","url":"https://huggingface.co/datasets/CATMuS/modern","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","German","English","Italian"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v35","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v35.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Euclid30K","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tEuclid30K Dataset\n\t\n\nPaper | Project Page | Code\nSpatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity.\nHowever, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). \nTo fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LiamLian0727/Euclid30K.","url":"https://huggingface.co/datasets/LiamLian0727/Euclid30K","creator_name":"Shijie Lian","creator_url":"https://huggingface.co/LiamLian0727","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VL-RewardBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/VL-RewardBench.","url":"https://huggingface.co/datasets/MMInstruction/VL-RewardBench","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v2\n\t\n\nCaption3o-Opt-v2 is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. Derived from the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, this optimized subset emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: 10,277 image-caption pairs\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v2.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v2","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v53","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v53.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v82","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v82.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v82","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v193","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v193.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v193","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ICDAR2015_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/icdar2015/metafile.yml\nName: 'Incidental Scene Text IC15'\nPaper:\n  Title: ICDAR 2015 Competition on Robust Reading\n  URL: https://rrc.cvc.uab.es/files/short_rrc_2015.pdf\n  Venue: ICDAR\n  Year: '2015'\n  BibTeX: '@inproceedings{karatzas2015icdar,\n  title={ICDAR 2015 competition on robust reading},\n  author={Karatzas, Dimosthenis and Gomez-Bigorda, Lluis and Nicolaou, Anguelos and Ghosh, Suman and Bagdanov, Andrew andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/ICDAR2015_OCR.","url":"https://huggingface.co/datasets/MiXaiLL76/ICDAR2015_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15.","url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"GUI-Lasagne-L1","keyword":"image-text-to-text","description":"This repository contains the GUI-Lasagne dataset used to train SpiritSight Agent, as described in the paper SpiritSight Agent: Advanced GUI Agent with One Look.\nProject Page: https://hzhiyuan.github.io/SpiritSight-Agent\n","url":"https://huggingface.co/datasets/SenseLLM/GUI-Lasagne-L1","creator_name":"SenseLLM","creator_url":"https://huggingface.co/SenseLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2503.03196","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"POPP-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPOPP - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe POPP dataset includes French civil census from Paris from the early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'Joly Ernest 88 Indre M par EmployÃ© Roblot!18377'\n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/POPP-line.","url":"https://huggingface.co/datasets/Teklia/POPP-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Video-Detailed-Caption","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tVideo Detailed Caption Benchmark\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmark Collection and Processing\n\t\n\nWe building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. Structured detailed captions construction pipeline. We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from variousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wchai/Video-Detailed-Caption.","url":"https://huggingface.co/datasets/wchai/Video-Detailed-Caption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 4-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Art-Vision-Question-Answering-Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tArt Vision Question Answering Dataset\n\t\n\nðŸŽ¨ A curated dataset for training AI models on digital artwork analysis and visual question answering.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset contains 577 question-answer pairs extracted from artwork conversations, designed for training multimodal AI models on art analysis tasks.\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nðŸ–¼ï¸ Visual Thumbnails: Artwork images displayed directly in the dataset viewer\nðŸ’¬ Rich Q&A: Expert-level questions and answers aboutâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OneEyeDJ/Art-Vision-Question-Answering-Dataset.","url":"https://huggingface.co/datasets/OneEyeDJ/Art-Vision-Question-Answering-Dataset","creator_name":"Zeyang Zhang","creator_url":"https://huggingface.co/OneEyeDJ","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ReflectiVA-Data","keyword":"image-text-to-text","description":"In this datasets space, you will find the data of Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering.\nFor more information, visit our ReflectiVA repository, our project page and model space.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you make use of our work, please cite our repo:\n@inproceedings{cocchi2024augmenting,\n  title={{Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering}},\n  author={Cocchi, Federico andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aimagelab/ReflectiVA-Data.","url":"https://huggingface.co/datasets/aimagelab/ReflectiVA-Data","creator_name":"AImageLab","creator_url":"https://huggingface.co/aimagelab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2411.16863","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-mass-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v23","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v23.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"t2i_tiny_nasa","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for t2i_tiny_nasa\n\t\n\n\n\t\n\t\t\n\t\tNASA Image Dataset\n\t\n\nThis dataset is created using images obtained from NASA's official image library. The dataset contains a collection of images along with their corresponding textual descriptions (prompts). This dataset can be used for various applications, including image-to-text tasks, text-to-image generation, and other AI-based image analysis studies.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSource: NASA Image Library\nContent: Images andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaangml/t2i_tiny_nasa.","url":"https://huggingface.co/datasets/kaangml/t2i_tiny_nasa","creator_name":"Kaan GML","creator_url":"https://huggingface.co/kaangml","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PhysBench","keyword":"video-text-to-text","description":"\n  PhysBench \n\n\n    ðŸŒ Homepage | ðŸ¤— Dataset | ðŸ“‘ Paper | ðŸ’» Code | ðŸ”º EvalAI\n\n\nThis repo contains evaluation code for the paper \"PhysBench: Benchmarking and Enhancing VLMs for Physical World Understanding\"\nIf you like our project, please give us a star â­ on GitHub for latest update.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nUnderstanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. Whileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/USC-GVL/PhysBench.","url":"https://huggingface.co/datasets/USC-GVL/PhysBench","creator_name":"USC Geomtry, Vision, and Learning Lab","creator_url":"https://huggingface.co/USC-GVL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2501.16411","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-mass-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MIRB-hf","keyword":"image-to-text","description":"VLLMs/MIRB-hf dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/VLLMs/MIRB-hf","creator_name":"Train VLLMs","creator_url":"https://huggingface.co/VLLMs","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding colorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"brill_iconclass","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Brill Iconclass AI Test Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nA test dataset and challenge to apply machine learning to collections described with the Iconclass classification system.\n\nThis dataset contains 87749 images with Iconclass metadata assigned to the images. The iconclass metadata classification system is intended to provide 'the comprehensive classification system for the content of images.'.\n\nIconclass was developed in the Netherlands as a standardâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/biglam/brill_iconclass.","url":"https://huggingface.co/datasets/biglam/brill_iconclass","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","feature-extraction","multi-class-image-classification","multi-label-image-classification"],"keywords_longer_than_N":true},
	{"name":"MIG-Bench","keyword":"image-text-to-text","description":"\n    \n\n\n\n\n\n\t\n\t\t\n\t\tMigician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models\n\t\n\nYou Li, Heyu Huang*, Chen Chi, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun\n\n        \nThis repository hosts the usage details of our training dataset MGrounding-630k and benchmark MIG-Bench and the training implementation of Migician, the first competitive Multi-image Grounding MLLM capable of free-form grounding.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Michael4933/MIG-Bench.","url":"https://huggingface.co/datasets/Michael4933/MIG-Bench","creator_name":"You Li","creator_url":"https://huggingface.co/Michael4933","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v19","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"BiomedCoOp","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBiomedical Few-shot Image Classification for Vision-Language Models\n\t\n\n \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\n Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. \n  However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt \n  engineering, while full model fine-tuning is costly.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TahaKoleilat/BiomedCoOp.","url":"https://huggingface.co/datasets/TahaKoleilat/BiomedCoOp","creator_name":"Taha Koleilat","creator_url":"https://huggingface.co/TahaKoleilat","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-image-classification","image-to-text","English","apache-2.0","Image"],"keywords_longer_than_N":true},
	{"name":"mm-interp-CompCap-gpt4-data","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCompCap-GPT4: A GPT-4 Captioned Version of CompCap-118K\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: CompCap: Improving Multimodal Large Language Models with Composite Captions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\nDirect Download:The repository includes CI_type.zip and CI_type.json. The JSON file follows the Llava format:\n{\n  \"id\": ID,\n  \"image\": IMAGE_PATH,\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": QUESTION},\n    {\"from\": \"gpt\", \"value\": ANSWER}\n  ]\n}\n\n\nUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/htlou/mm-interp-CompCap-gpt4-data.","url":"https://huggingface.co/datasets/htlou/mm-interp-CompCap-gpt4-data","creator_name":"LHT","creator_url":"https://huggingface.co/htlou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"kzgov-budget-data","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tKazakhstan Government Budget Data ðŸ‡°ðŸ‡¿\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis comprehensive dataset provides detailed insights into Kazakhstan's government budget allocation, execution, and performance across various sectors, regions, and administrative levels for 2024. The dataset enables analysis of fiscal policy, budget efficiency, and resource distribution across the country.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Statistics\n\t\n\n\nTotal Records: 615 entries\nCoverage Period: 2024\nAdministrative Levels:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Adilbai/kzgov-budget-data.","url":"https://huggingface.co/datasets/Adilbai/kzgov-budget-data","creator_name":"Baidalin Adilzhan","creator_url":"https://huggingface.co/Adilbai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","question-answering","text-retrieval","open-domain-qa","expert-generated"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v159","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v159.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v159","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v63","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v63.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v63","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v161","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v161.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v161","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding colorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v165","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v165.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v165","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RuBirdNames","keyword":"image-to-text","description":"Examples of 583 handwritten words (not counting words produced by mistake) that make up the common names of 701 bird species found in Russia. Original bird names were produced using eBird's API.\nThe dataset contains examples of handprinted and cursive scripts as well as various backgrounds (plain white, grid-ruled paper, line-ruled paper). \nThere are currently 21325 images, that is, at least 35 full sets of 583 words. Some of the words have up to 38 different image examples. The sets areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jackeet/RuBirdNames.","url":"https://huggingface.co/datasets/jackeet/RuBirdNames","creator_name":"Polina Grabar","creator_url":"https://huggingface.co/jackeet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Russian","apache-2.0","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"EgoLife","keyword":"video-text-to-text","description":"Data cleaning, stay tuned! Please refer to https://egolife-ai.github.io/ first for general info.\nCheckout the paper EgoLife (https://arxiv.org/abs/2503.03803) for more information.\nCode: https://github.com/egolife-ai/EgoLife\n","url":"https://huggingface.co/datasets/lmms-lab/EgoLife","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","mit","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MJ-BENCH-VIDEO","keyword":"video-text-to-text","description":"This repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\".\nPaper: https://arxiv.org/abs/2502.01719\nCode: https://github.com/aiming-lab/MJ-Video\nProject Page: https://aiming-lab.github.io/MJ-VIDEO.github.io/\nThis repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\". We create a fine-grained video preference datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO.","url":"https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO","creator_name":"MJ-Bench-Team","creator_url":"https://huggingface.co/MJ-Bench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"image-textualization","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage-Textualization Dataset\n\t\n\nExciting to announce the open-sourcing of our Image-Text Matching Dataset, which consists of 220K image-text pairs. We also release fine-grained annotations, which may be helpful for many downstream tasks.\nThis dataset is designed to facilitate research and development in the field of large mutimodal language model, particularly for tasks such as image captioning, visual question answering, and multimodal understanding.\nNote that our framework can beâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/image-textualization.","url":"https://huggingface.co/datasets/Sterzhang/image-textualization","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v157","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v157.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v157","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Analysis-Recognition","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenpdf-Analysis-Recognition\n\t\n\nThe Openpdf-Analysis-Recognition dataset is curated for tasks related to image-to-text recognition, particularly for scanned document images and OCR (Optical Character Recognition) use cases. It contains over 6,900 images in a structured imagefolder format suitable for training models on document parsing, PDF image understanding, and layout/text extraction tasks.\n\n\t\n\t\t\nAttribute\nValue\n\n\n\t\t\nTask\nImage-to-Text\n\n\nModality\nImage\n\n\nFormat\nImageFolderâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Analysis-Recognition.","url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"videomarathon","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VideoMarathon\n\t\n\nVideoMarathon is a large-scale long video instruction-following dataset with a total duration of approximately 9,700 hours, comprising 3.3 million QA pairs across 22 task categories.\nPaper and more resources: [arXiv] [Project Website] [GitHub] [Model]\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Uses\n\t\n\nThis dataset is used for academic research purposes only.\n\n\t\n\t\t\n\t\n\t\n\t\tTask Taxonomy\n\t\n\nThe dataset contains 22 diverse tasks over six fundamental topics, including temporalityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jylins/videomarathon.","url":"https://huggingface.co/datasets/jylins/videomarathon","creator_name":"Jingyang Lin","creator_url":"https://huggingface.co/jylins","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ChemVLM_test_data","keyword":"image-to-text","description":"arxiv.org/abs/2408.07246Using this dataset, please kindly cite:\n@inproceedings{li2025chemvlm,\n  title={Chemvlm: Exploring the power of multimodal large language models in chemistry area},\n  author={Li, Junxian and Zhang, Di and Wang, Xunzhi and Hao, Zeying and Lei, Jingdi and Tan, Qian and Zhou, Cai and Liu, Wei and Yang, Yaotian and Xiong, Xinrui and others},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  volume={39},\n  number={1},\n  pages={415--423}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Duke-de-Artois/ChemVLM_test_data.","url":"https://huggingface.co/datasets/Duke-de-Artois/ChemVLM_test_data","creator_name":"Junxian Li","creator_url":"https://huggingface.co/Duke-de-Artois","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"HRVideoBench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tHRVideoBench\n\t\n\nThis repo contains the test data for HRVideoBench, which is released under the paper \"VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation\". VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\nðŸŒ Homepage | ðŸ“– arXiv | ðŸ’» GitHub | ðŸ¤— VISTA-400K | ðŸ¤— Models | ðŸ¤— HRVideoBenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/HRVideoBench.","url":"https://huggingface.co/datasets/TIGER-Lab/HRVideoBench","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pix2tex","keyword":"image-to-text","description":"Hereâ€™s a completed dataset card for your Pix2Tex dataset based on the provided template:\n\n\n\t\n\t\t\n\t\tPix2Tex\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Pix2Tex Dataset is a high-quality, curated dataset for training and evaluating Vision-Language Models (VLMs) capable of extracting LaTeX code from images of mathematical formulas. This dataset combines both printed and handwritten formula images, offering diverse and challenging samples for tasks involving LaTeX recognition.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/anindya-hf-2002/pix2tex.","url":"https://huggingface.co/datasets/anindya-hf-2002/pix2tex","creator_name":"Anindya Mitra","creator_url":"https://huggingface.co/anindya-hf-2002","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 8-16.\nrectangle size: 3-7.\nnumber of rects: 2-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v147","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v147.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v147","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v121","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v121.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v121","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"BridgeData-V2-Scripted-Videos","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tBridgeData V2 Video Dataset\n\t\n\nThis dataset contains videos from BridgeData V2 trajectories in VideoFolder format.\n\n\t\n\t\t\n\t\tDerived From\n\t\n\nThis dataset is a derivative of the 30 GB scripted subset of BridgeData V2 from RAIL-Berkeley. All rights and original licensing apply.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite the original BridgeData V2 paper:\n@inproceedings{walke2023bridgedata,\n    title={BridgeData V2: A Dataset for Robot Learning at Scale},\n    author={Walkeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VyoJ/BridgeData-V2-Scripted-Videos.","url":"https://huggingface.co/datasets/VyoJ/BridgeData-V2-Scripted-Videos","creator_name":"VyoJ","creator_url":"https://huggingface.co/VyoJ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","cc-by-4.0","1K - 10K","Tabular"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"chess_checkmate_images_big_bench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for BIG-Bench Checkmate In One Move (Images)\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is an adapted version of the BIG-Bench Checkmate in One Move task\nas originally made by Nitish Keskar (nkeskar@salesforce.com).\nCopying the original task description:\n\nThe goal of this task is to probe the ability of language models to play chess in standard algebraic notation (SAN). The input to the model is a sequence of moves such that a next possible move is a checkmate. We curate 3,500 gamesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DylanASHillier/chess_checkmate_images_big_bench.","url":"https://huggingface.co/datasets/DylanASHillier/chess_checkmate_images_big_bench","creator_name":"Dylan Hillier","creator_url":"https://huggingface.co/DylanASHillier","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v55","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v55.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BBBP-V-SMILES-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BBBP-V-SMILES-2.","url":"https://huggingface.co/datasets/molvision/BBBP-V-SMILES-2","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NumPro_FT","keyword":"video-text-to-text","description":"This repository contains the dataset described in the paper Number it: Temporal Grounding Videos like Flipping Manga.\nCode: https://github.com/yongliang-wu/NumPro\n","url":"https://huggingface.co/datasets/Liang0223/NumPro_FT","creator_name":"Yongliang","creator_url":"https://huggingface.co/Liang0223","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v88","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v88.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v88","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v129","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v129.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v129","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v84","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v84.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v84","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Opendoc2-Analysis-Recognition","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpendoc2-Analysis-Recognition Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Opendoc2-Analysis-Recognition dataset is a collection of data designed for tasks involving image analysis and recognition. It is suitable for various machine learning tasks, including image-to-text conversion, text classification, and image feature extraction.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModalities: Likely includes images and associated labels (specific modalities can be confirmed on the dataset's page).\nLanguages:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Opendoc2-Analysis-Recognition.","url":"https://huggingface.co/datasets/prithivMLmods/Opendoc2-Analysis-Recognition","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-retrieval","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"trdg_wikipedia_en_text_recognition","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"trdg_wikipedia_en_zh_text_recognition\"\n\t\n\nThis synthetic dataset was generated using the TextRecognitionDataGenerator(TRDG) open source repo:\nhttps://github.com/Belval/TextRecognitionDataGenerator\nIt contains synthetic images of text randomly sampled from Engilsh(en) Wikipedia pages.\nReference to the documentation provided by the TRDG repo:\nhttps://textrecognitiondatagenerator.readthedocs.io/en/latest/index.html\n","url":"https://huggingface.co/datasets/priyank-m/trdg_wikipedia_en_text_recognition","creator_name":"priyank","creator_url":"https://huggingface.co/priyank-m","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000â€¦ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"outlined-text-captcha","keyword":"image-to-text","description":"This image collection was web scrapped from a single site. \nIt comes from the SimpleCaptcha library, which uses a variety of formats including this one: outlined alphanumeric characters with a stroke.\n\n\t\n\t\t\n\t\tImage description:\n\t\n\n\nHorizontal Gradient.\nRGB Channels.\nJPG Format.\n5 characters.\nCharacters may repeat.\nNumeric and lowercase alphanumeric.\n\n\n\t\n\t\t\n\t\tVocab:\n\t\n\n\nAlpha: a b c d e f g h k m n p r v w x y (17 unique)\nNumber: 2 3 4 5 6 7 8 9 (8 unique)\nTotal chars = 25 unique charactersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/muddokon/outlined-text-captcha.","url":"https://huggingface.co/datasets/muddokon/outlined-text-captcha","creator_name":"SebastiÃ¡n GÃ³mez Rosas","creator_url":"https://huggingface.co/muddokon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","token-classification","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"malaysian-cuisine","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMalaysian Cuisine Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains curated images of Malaysian cuisine, including traditional dishes such as roti telur, nasi lemak, murtabak, gulai ikan, and lemang. It is intended for image classification tasks and cultural food recognition studies.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: Images in .jpg format\nClasses: 10 food categories (roti telur, nasi lemak ayam goreng, murtabak, gulai ikan, lemang, etc.)\nFile naming convention:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ychwodhrey/malaysian-cuisine.","url":"https://huggingface.co/datasets/ychwodhrey/malaysian-cuisine","creator_name":"Yasir Kalim","creator_url":"https://huggingface.co/ychwodhrey","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","English","Malay","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"si-wiki-OCR-synth-deduped","keyword":"image-to-text","description":"Suchinthana/si-wiki-OCR-synth-deduped dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Suchinthana/si-wiki-OCR-synth-deduped","creator_name":"Wijesundara","creator_url":"https://huggingface.co/Suchinthana","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Sinhala","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v25","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dataset_111-220","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","url":"https://huggingface.co/datasets/hongin9812/dataset_111-220","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v20","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wikiart_recaption","keyword":"image-to-text","description":"WikiArt Dataset captioned using vikhyatk/moondream2 model with prompt : Generate a short, simple and only visually descriptive caption for this image.\n","url":"https://huggingface.co/datasets/AterMors/wikiart_recaption","creator_name":"Andrea","creator_url":"https://huggingface.co/AterMors","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"video-dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVideo Dataset on Hugging Face\n\t\n\nThis repository hosts the  video dataset, a widely used benchmark dataset for human action recognition in videos. The dataset has been processed and uploaded to the Hugging Face Hub for easy access, sharing, and integration into machine learning workflows.\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe  dataset is a large-scale video dataset designed for action recognition tasks. It contains 13,320 video clips across 101 action categories, making it one of the mostâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProgramerSalar/video-dataset.","url":"https://huggingface.co/datasets/ProgramerSalar/video-dataset","creator_name":"ProgramerSalar","creator_url":"https://huggingface.co/ProgramerSalar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-video","video-classification","video-text-to-text","voice-activity-detection","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v44","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v44.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenDoc-Null-6K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenDoc-Null-6K\n\t\n\nThe OpenDoc-Null-6K dataset is curated for tasks related to image-to-text recognition, particularly for scanned document images and OCR (Optical Character Recognition) use cases. It contains over 6,900 images in a structured imagefolder format suitable for training models on document parsing, PDF image understanding, and layout/text extraction tasks.\n\n\t\n\t\t\nAttribute\nValue\n\n\n\t\t\nTask\nImage-to-Text\n\n\nModality\nImage\n\n\nFormat\nImageFolder\n\n\nLanguage\nEnglish\n\n\nLicenseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/OpenDoc-Null-6K.","url":"https://huggingface.co/datasets/prithivMLmods/OpenDoc-Null-6K","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ArtUK","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ArtUK\n\t\n\n~237k artworks from ArtUK.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 237 thousand artworks from ArtUK. Entries include image, title, artist and date.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{ArtUK,\n  author = {hlky},\n  title = {ArtUK},\n  year = {2024},\n  publisher = {hlky},\n  journal = {Hugging Face repository},\n  howpublished =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/ArtUK.","url":"https://huggingface.co/datasets/bigdata-pw/ArtUK","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","image-to-text","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"prophet-mosque-library-compressed","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tProphet's Mosque Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nProphetâ€™s Mosque Library is one of the primary resources for Islamic books. It hosts more than 48,000 PDF books across over 70 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/prophet-mosque-library, with one key difference: theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library-compressed.","url":"https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library-compressed","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PlantVillageVQA","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPlantVillageVQA\n\t\n\nAssociated paper: arXiv:2508.17117GitHub repository (Currently Private): SyedNazmusSakib/PlantVillageVQA\nPlantVillageVQA is a multimodal dataset for visual question answering (VQA) in plant pathology. It contains 193,609 questionâ€“answer (QA) items paired with 55,448 leaf images spanning 14 crops and 38 diseases. Questions are organized into nine categories and three cognitive levels: perception/identification, symptom grounding/verification, and higherâ€‘orderâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.","url":"https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA","creator_name":"Syed Nazmus Sakib","creator_url":"https://huggingface.co/SyedNazmusSakib","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","cc-by-4.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"blip3o-caption-mini-arrow","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tblip3o-caption-mini-arrow\n\t\n\nblip3o-caption-mini-arrow is a high-quality, curated image-caption dataset derived and optimized from the original BLIP3o/BLIP3o-Pretrain-Long-Caption. This dataset is specifically filtered and processed for tasks involving long-form image captioning and vision-language understanding.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nTotal Samples: 91,600\nModality: Image â†” Text\nFormat: Arrow (auto-converted to Parquet)\nLicense: Apache 2.0\nLanguage: English\nSize: ~4.5 GBâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/blip3o-caption-mini-arrow.","url":"https://huggingface.co/datasets/prithivMLmods/blip3o-caption-mini-arrow","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"Flickr-Geo","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Flickr-Geo\n\t\n\n217.646.487 images with lat lon coordinates from Flickr. This repo is a filtered version of https://huggingface.co/datasets/bigdata-pw/Flickr for all rows containing a valid lat lon pair. \n\n\n\t\n\t\t\n\t\n\t\n\t\tLoad and Visualize\n\t\n\nLoad the data (2s on my system) with DuckDB:\nimport duckdb \ndf = duckdb.sql(\"\"\"\n    SELECT CAST(latitude AS DOUBLE) AS latitude,\n    CAST(longitude AS DOUBLE) AS longitude\n    FROM 'reduced_flickr_data/*.parquet' \"\"\").df()\ndf\n\nCreateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/do-me/Flickr-Geo.","url":"https://huggingface.co/datasets/do-me/Flickr-Geo","creator_name":"Dominik WeckmÃ¼ller","creator_url":"https://huggingface.co/do-me","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","100M<n<1B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v205","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v205.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v205","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search\n\t\n\nVisualWebInstruct is a large-scale, diverse multimodal instruction dataset designed to enhance vision-language models' reasoning capabilities. The dataset contains approximately 900K question-answer (QA) pairs, with 40% consisting of visual QA pairs associated with 163,743 unique images, while the remaining 60% are text-only QA pairs.\n\n\t\n\t\t\n\t\tLinks\n\t\n\n\nGitHub Repository\nResearch Paper\nProject Websiteâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct.","url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","image-text-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v195","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v195.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v195","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"afri-aya","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAfri-Aya ðŸŒ\n\t\n\nGiving Sight to African LLMs\nAfri-Aya is a community-curated multilingual image dataset covering 13 major African languages with AI-powered categorization, created as part of Expedition Aya - a six-week global open-build challenge hosted by Cohere Labs.\n\n\t\n\t\t\n\t\tProject Background\n\t\n\nThis dataset was developed by the Cohere Labs Regional Africa community during Expedition Aya, aiming to include more African low-resource languages and their cultures in Vision Languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CohereLabsCommunity/afri-aya.","url":"https://huggingface.co/datasets/CohereLabsCommunity/afri-aya","creator_name":"Cohere Labs Community","creator_url":"https://huggingface.co/CohereLabsCommunity","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-generation","English","Ganda"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"flickr30k-fa","keyword":"image-to-text","description":"The Flickr30K dataset filtered and translated to Persian.\nThis dataset was originally made by Sajjad Ayoubi and uploaded to Kaggle at https://www.kaggle.com/datasets/sajjadayobi360/flickrfa.\nThis repo contains the exact dataset split to train/test using a custom sampling criteria and can be directly loaded using HuggingFace datasets or right from Hezar.\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tHugging Face Datasets\n\t\n\npip install datasets\n\nfrom datasets import load_dataset\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hezarai/flickr30k-fa.","url":"https://huggingface.co/datasets/hezarai/flickr30k-fa","creator_name":"Hezar AI","creator_url":"https://huggingface.co/hezarai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Persian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v123","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v123.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v123","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"xflickrco_1k","keyword":"image-to-text","description":"floschne/xflickrco_1k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/floschne/xflickrco_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","German","English","Spanish","Indonesian"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SightationRetrieval","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSighationRetrieval\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationRetrieval.","url":"https://huggingface.co/datasets/Sightation/SightationRetrieval","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v203","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v203.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v203","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"xm3600","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\nIt also includes the image features as PIL Image and has a uniform andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600.","url":"https://huggingface.co/datasets/floschne/xm3600","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"filtered-coyo-700M-beta","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for filterred-coyo-700M-beta\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe texts in the COYO-700M dataset consist of English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach instance in COYO-700M represents single image-text pair information with meta-attributes:\n{\n  'id': 841814333321,\n  'url': 'https://blog.dogsof.com/wp-content/uploads/2021/03/Image-from-iOS-5-e1614711641382.jpg',\n  'text': 'A Pomsky dogâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta.","url":"https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"MaCBench-Ablations","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMaCBench-Ablations\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench-Ablations","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"LLaVA-Video-large-swift","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-medium-swift\n\t\n\nA subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","url":"https://huggingface.co/datasets/malterei/LLaVA-Video-large-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v24","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v24.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"xm3600_1k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM3600 - Crossmodal-3600 - 1K Split\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a 1K split of XM3600!\n\t\n\nFor this, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/floschne/xm3600_1k.","url":"https://huggingface.co/datasets/floschne/xm3600_1k","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"phycics_dataset","keyword":"image-to-text","description":"wzmmmm/phycics_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/wzmmmm/phycics_dataset","creator_name":"wzm","creator_url":"https://huggingface.co/wzmmmm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Chinese","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v55","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v55.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MMIF-23k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\n\nPaper: MM-IFEngine: Towards Multimodal Instruction Following\n\nGithub: SYuan03/MM-IFEngine\n\nProject Page: syuan03.github.io/MM-IFEngine/\n\nMM-IFEval Evaluation: Using VLMEvalKit\n\n\nðŸ˜Š This is the official repo of MM-IFEngine datasets in MM-IFEngine: Towards Multimodal Instruction Following\nðŸš€ We include both the SFT and DPO data in this repo as the v1 dataset (generated mainly by InternVL2.5-78B and Qwen2-VL-7B), which we used to train the model described in our paper.ðŸ’–â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChrisDing1105/MMIF-23k.","url":"https://huggingface.co/datasets/ChrisDing1105/MMIF-23k","creator_name":"Shengyuan Ding","creator_url":"https://huggingface.co/ChrisDing1105","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v115","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v115.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v115","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"texturecan","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for TextureCan Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,037 texture images from texturecan.com. It includes textures of various materials such as brick, paper, fabric, metal, wood, stone, and other surfaces. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/texturecan.","url":"https://huggingface.co/datasets/nyuuzyou/texturecan","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-JohnSlegers","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-JohnSlegers","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"cc0-textures","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for CC0 Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 18,785 texture images from cc0-textures.com. It includes textures of wood, metal, concrete, fabric, stone, ceramic, and other materials. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while keeping good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): Texture titles and tagsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cc0-textures.","url":"https://huggingface.co/datasets/nyuuzyou/cc0-textures","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v90","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v90.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v90","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\nnumber of rects: 2-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v120","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v120.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v120","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v202","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v202.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v202","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v61","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v61.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v61","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RICO-SCA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for RICO SCA (SeeClick cache)\n\t\n\nThis is the SeeClick cache of a syntehtically generated dataset following RICO SCA's generation procedure. \nIt consists of approximately 170k captions across 70k widgets and 18k screens. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a widget captioning (referring expression comprehension/generation) dataset. \n\nCurated by: Google Research, Nanjing University\nLanguage(s) (NLP): en\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-SCA.","url":"https://huggingface.co/datasets/rootsautomation/RICO-SCA","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v207","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v207.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v207","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v18","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v18.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-30k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\theb-synthtiger-30k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-30 is a dataset containing 30,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-30k.","url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-30k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ContextualBench","keyword":"image-to-text","description":"\n  Challenging and Enhancing the Reasoning Capacity of Multimodal LLMs in Context-violating Images\n      \n\n    HongxiÂ Li,Â \n    YuyangÂ Chen,Â \n    YayunÂ Qi,Â \n    XinxiaoÂ Wu,Â \nÂ Beijing Institute of Technology\narXiv 2024\nðŸŒŽWebsite (Comming soon) |\nðŸ§‘â€ðŸ’»Code |\nðŸ“„arXiv  (Comming soon) |\nðŸ† Leaderboard\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nContextualBench consists of 6(categories) Ã— 12 (instances) = 72 context instances, with each context instances containing 7 context-consistent images and 7â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ToughStone/ContextualBench.","url":"https://huggingface.co/datasets/ToughStone/ContextualBench","creator_name":"Hongxi","creator_url":"https://huggingface.co/ToughStone","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v79","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v79.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v79","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v20","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v20.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"cat-memes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCat Memes Dataset\n\t\n\nA collection of famous cat memes and their metadata.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains various cat memes with their associated metadata including:\n\nImage files\nMeme IDs\nAnimation status (GIF vs static images)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nid: Unique identifier for each meme\nfile: The image file\nfile_name: Original filename\nanimated: Boolean indicating if the image is animated (GIF)\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Load theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pielet/cat-memes.","url":"https://huggingface.co/datasets/pielet/cat-memes","creator_name":"Shiyang Jia","creator_url":"https://huggingface.co/pielet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v21","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v21.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"men_women_children_wearing_clothes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 6979 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 6979\nAverage words in long description: 17.3\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes.","url":"https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"men_women_children_wearing_clothes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 6979 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 6979\nAverage words in long description: 17.3\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes.","url":"https://huggingface.co/datasets/AntZet/men_women_children_wearing_clothes","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"cool_images_urban_and_nature","keyword":"image-to-text","description":"This is an amazing dataset. From pythess with love.\n","url":"https://huggingface.co/datasets/AlexandrosChariton/cool_images_urban_and_nature","creator_name":"Alexandros Chariton","creator_url":"https://huggingface.co/AlexandrosChariton","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v33","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v33.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v74","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v74.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v74","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-20.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openinterx/UGC-VideoCap.","url":"https://huggingface.co/datasets/openinterx/UGC-VideoCap","creator_name":"Memories.ai Research","creator_url":"https://huggingface.co/openinterx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","ðŸ‡ºðŸ‡¸ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"TextBraTS","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTextBraTS\n\t\n\nA volume-level text-image public dataset with novel text-guided 3D brain tumor segmentation from BraTS challenge.\nPaper\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nTextBraTS is an open-access dataset designed to advance research in text-guided 3D brain tumor segmentation. It includes paired multi-modal brain MRI scans and expertly annotated radiology reports, enabling the development and evaluation of multi-modal deep learning models that bridge vision and language in neuro-oncology. Ourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jupitern52/TextBraTS.","url":"https://huggingface.co/datasets/Jupitern52/TextBraTS","creator_name":"SHI XIAOYU","creator_url":"https://huggingface.co/Jupitern52","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"znanio-videos","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Videos\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 6,653 educational videos from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-videos.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-videos","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"FrenchCensus-handwritten-texts","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSource\n\t\n\nThis repository contains 3 datasets created within the POPP project (Project for the Oceration of the Paris Population Census) for the task of handwriting text recognition. These datasets have been published in Recognition and information extraction in historical handwritten tables: toward understanding early 20th century Paris census at DAS 2022.\nThe 3 datasets are called â€œGeneric datasetâ€, â€œBellevilleâ€, and â€œChaussÃ©e dâ€™Antinâ€ and contains lines made from the extracted rowsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts.","url":"https://huggingface.co/datasets/agomberto/FrenchCensus-handwritten-texts","creator_name":"Arnault Gombert","creator_url":"https://huggingface.co/agomberto","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_task_3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n4 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_task_3.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_task_3","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sample1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for RedCaps\n\t\n\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nPath /home/daniel.baek/public/common/Data\nContent type image\nTag sensor, common, ai, dataset\nDescription \nHomepage: RedCaps homepage\nRepository: RedCaps repository\nPaper: RedCaps: web-curated image-text data created by the people, for the people\nLeaderboard:\nPoint of Contact: Karan Desai\n\n\n\n\n\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nRedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.\nImages and captions fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/actdan2016/sample1.","url":"https://huggingface.co/datasets/actdan2016/sample1","creator_name":"danana","creator_url":"https://huggingface.co/actdan2016","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","image-captioning","found","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"M-BEIR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUniIR: Training and Benchmarking Universal Multimodal Information Retrievers (ECCV 2024)\n\t\n\nðŸŒ Homepage | ðŸ¤— Model(UniIR Checkpoints) | ðŸ¤— Paper | ðŸ“– arXiv  | GitHub\nHow to download the M-BEIR Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ””News\n\t\n\n\nðŸ”¥[2023-12-21]: Our M-BEIR Benchmark is now available for use.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nM-BEIR, the Multimodal BEnchmark for Instructed Retrieval, is a comprehensive large-scale retrieval benchmark designed to train and evaluate unified multimodal retrievalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/M-BEIR.","url":"https://huggingface.co/datasets/TIGER-Lab/M-BEIR","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","text-to-image","image-to-text","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"coco-narratives","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCOCO Narratives\n\t\n\nOriginal Source | Google Localized Narrative\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset collects the images and annotations from the original MS COCO 2017 and the annotations from the project localized-narratives\n\n\t\n\t\t\n\t\tðŸ™ Acknowledgement\n\t\n\nAll credits to the original COCO project and the localized-narratives teams.\n\n\t\n\t\t\n\t\tðŸ“œ Cite\n\t\n\nPlease consider to cite the following related papers:\n@article{DBLP:journals/corr/LinMBHPRDZ14,\n  author       = {Tsung{-}Yi Lin andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fhrozen/coco-narratives.","url":"https://huggingface.co/datasets/Fhrozen/coco-narratives","creator_name":"Nelson Yalta","creator_url":"https://huggingface.co/Fhrozen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","arrow"],"keywords_longer_than_N":true},
	{"name":"ShahNegar","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tShahNegar (A Plotted version of The Shahnameh)\n\t\n\nThis dataset is a plotted version of Ferdowsi's Shahnameh (which is a highly-regarded ancient set of Farsi poems) generated using DALL-E mini (aka craiyon). You can use this dataset using the code below: \nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sadrasabouri/ShahNegar\")\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains more than 30K images with their corresponding text from the Shahnameh. For each Shahnamehâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sadrasabouri/ShahNegar.","url":"https://huggingface.co/datasets/sadrasabouri/ShahNegar","creator_name":"Sadra Sabouri","creator_url":"https://huggingface.co/sadrasabouri","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-captioning","machine-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"DataSeeds.AI-Sample-Dataset-DSD","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataSeeds.AI Sample Dataset (DSD)\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DataSeeds.AI Sample Dataset (DSD) is a high-fidelity, human-curated computer vision-ready dataset comprised of 7,772 peer-ranked, fully annotated photographic images, 350,000+ words of descriptive text, and comprehensive metadata. While the DSD is being released under an open source license, a sister dataset of over 10,000 fully annotated and segmented images is available for immediate commercial licensing, and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD.","url":"https://huggingface.co/datasets/Dataseeds/DataSeeds.AI-Sample-Dataset-DSD","creator_name":"Dataseeds AI","creator_url":"https://huggingface.co/Dataseeds","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/mvp-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"Mobile Vision Perception Lab","creator_url":"https://huggingface.co/mvp-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"brain-tumor-mri-colorized-ehr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBrain Tumor (MRI) Detection Colourized with EHR\n\t\n\nâš ï¸ SYNTHETIC DATA - For research and education only\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive multimodal medical AI dataset combining 11,505 colorized brain MRI images (embedded as bytes) with synthetic Nigerian Electronic Health Records (EHR). All clinical data is FHIR R4 compliant and includes authentic Nigerian healthcare context.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nðŸ–¼ï¸ Images Embedded: All 11,505 MRI images stored as bytes in parquetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/electricsheepafrica/brain-tumor-mri-colorized-ehr.","url":"https://huggingface.co/datasets/electricsheepafrica/brain-tumor-mri-colorized-ehr","creator_name":"Electric Sheep","creator_url":"https://huggingface.co/electricsheepafrica","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","object-detection","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"damaged-media","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"ARTeFACT\"\n\t\n\nARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage\n\nHere we provide example code for downloading the data, loading it as a PyTorch dataset, splitting by material and/or content, and visualising examples.\n\n\t\n\t\t\n\t\n\t\n\t\tHousekeeping\n\t\n\n!pip install datasets\n!pip install -qqqU wandb transformers pytorch-lightning==1.9.2 albumentations torchmetrics torchinfo\n!pip install -qqq requests gradio\n\nimport os\nfrom glob import glob\n\nimport cv2â€¦ See the full description on the dataset page: https://huggingface.co/datasets/danielaivanova/damaged-media.","url":"https://huggingface.co/datasets/danielaivanova/damaged-media","creator_name":"Daniela Ivanova","creator_url":"https://huggingface.co/danielaivanova","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","image-segmentation","image-to-text","image-classification","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"AROFlickrOrder","keyword":"image-to-text","description":"\n  AROFlickrOrder\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nCompositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\nReference\nhttps://openreview.net/forum?id=KRLUvxh8uaX\n\n\n\t\n\nSource datasets:\n\ngowitheflow/ARO-Flickr-Order\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/AROFlickrOrder.","url":"https://huggingface.co/datasets/mteb/AROFlickrOrder","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","image-to-text","text-to-image","image-captioning","expert-annotated"],"keywords_longer_than_N":true},
	{"name":"synthetic_hkr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated using handwritten fonts\n\t\n\nNumber of images: 300000\nSources:\n\nHandwriting generation code\n\nThe code was executed with hkr option (with fewer augmentations)\n","url":"https://huggingface.co/datasets/nastyboget/synthetic_hkr","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"CSTips","keyword":"image-to-text","description":"8-bit-labs/CSTips dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/8-bit-labs/CSTips","creator_name":"8-Bit Labs","creator_url":"https://huggingface.co/8-bit-labs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"diffusiondb","keyword":"image-to-text","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","url":"https://huggingface.co/datasets/poloclub/diffusiondb","creator_name":"Polo Club of Data Science","creator_url":"https://huggingface.co/poloclub","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"UniMER_Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUniMER Dataset\n\t\n\nFor detailed instructions on using the dataset, please refer to the project homepage: UniMERNet Homepage\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe UniMER dataset is a specialized collection curated to advance the field of Mathematical Expression Recognition (MER). It encompasses the comprehensive UniMER-1M training set, featuring over one million instances that represent a diverse and intricate range of mathematical expressions, coupled with the UniMER Test Set, meticulouslyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wanderkid/UniMER_Dataset.","url":"https://huggingface.co/datasets/wanderkid/UniMER_Dataset","creator_name":"Bin Wang","creator_url":"https://huggingface.co/wanderkid","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"gan_cyrillic","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from Cyrillic train set using ScrabbleGAN\n\t\n\nNumber of images: 300000\nSources:\n\nCyrillic dataset\nScrabbleGAN code\n\n","url":"https://huggingface.co/datasets/nastyboget/gan_cyrillic","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"human-style-preferences-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human preference datasets for text-to-image models, this release contains over 1,200,000 human preferenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-style-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-style-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"license-plate-text-recognition-full","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"license-plate-text-recognition-full\"\n\t\n\n\n\t\n\t\t\n\t\tBackground Information\n\t\n\nThis dataset is generated from keremberke/license-plate-object-detection dataset. What we have done is:\n\nGet the Bounding Boxes for each plate in an image,\nCrop the image to make the plate only visible,\nRun it through the microsoft/trocr-large-printed model to extract the written information.\n\n\n\t\n\t\t\n\t\tStructure of the Dataset\n\t\n\nIt has the same structure as theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full.","url":"https://huggingface.co/datasets/sonnetechnology/license-plate-text-recognition-full","creator_name":"Sonne Technology, Inc.","creator_url":"https://huggingface.co/sonnetechnology","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"open-image-narratives","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tOpen Images V7 + Narratives\n\t\n\nOriginal Source | Google Localized Narrative\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset collects the images and annotations from the original Open Images Dataset V7 and the annotations from the project localized-narratives.\nOut of the 9M images, a subset of 1.9M images have been annotated with: bounding boxes, object segmentations, visual relationships, localized narratives, point-level labels, and image-level labels. (The remaining images have onlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fhrozen/open-image-narratives.","url":"https://huggingface.co/datasets/Fhrozen/open-image-narratives","creator_name":"Nelson Yalta","creator_url":"https://huggingface.co/Fhrozen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","arrow"],"keywords_longer_than_N":true},
	{"name":"PetraAI","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPETRA\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nPETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nData is separated by language into /ar and /en directories\nWithin each language directory, data is separated by task into subdirectories  \nTasks include:\nTranslation\nSummarizationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PetraAI/PetraAI.","url":"https://huggingface.co/datasets/PetraAI/PetraAI","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"PetraAI","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tPETRA\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nPETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nData is separated by language into /ar and /en directories\nWithin each language directory, data is separated by task into subdirectories  \nTasks include:\nTranslation\nSummarizationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PetraAI/PetraAI.","url":"https://huggingface.co/datasets/PetraAI/PetraAI","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"WeatherGov-dataset","keyword":"table-to-text","description":"aazer/WeatherGov-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/aazer/WeatherGov-dataset","creator_name":"aaser fawzy","creator_url":"https://huggingface.co/aazer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"JinaVDROWIDChartsRetrieval","keyword":"image-to-text","description":"\n  JinaVDROWIDChartsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve charts from the OWID dataset based on accompanied text snippets.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/owid_charts_en_beir\n\n\n\t\n\nSource datasets:\n\njinaai/owid_charts_en_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDROWIDChartsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDROWIDChartsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreDocVQARetrieval","keyword":"image-to-text","description":"\n  VidoreDocVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/docvqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreDocVQARetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreDocVQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreDocVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAHealthcareIndustryRetrieval","keyword":"image-to-text","description":"\n  JinaVDRDocQAHealthcareIndustryRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve healthcare industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nMedical\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_healthcare_industry_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_healthcare_industry_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAHealthcareIndustryRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAHealthcareIndustryRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAEnergyRetrieval","keyword":"image-to-text","description":"\n  VidoreSyntheticDocQAEnergyRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_energy_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreSyntheticDocQAEnergyRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAEnergyRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAEnergyRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAGovernmentReportsRetrieval","keyword":"image-to-text","description":"\n  VidoreSyntheticDocQAGovernmentReportsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_government_reports_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAGovernmentReportsRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAGovernmentReportsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRJDocQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRJDocQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Japanese documents in various formats based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/jdocqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/jdocqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRJDocQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRJDocQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRJDocQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRTabFQuadRetrieval","keyword":"image-to-text","description":"\n  JinaVDRTabFQuadRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve tables from industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/tabfquad_beir\n\n\n\t\n\nSource datasets:\n\njinaai/tabfquad_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRTabFQuadRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRTabFQuadRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRTabFQuadRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocVQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRDocVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve industry documents based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docvqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docvqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocVQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocVQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRJina2024YearlyBookRetrieval","keyword":"image-to-text","description":"\n  JinaVDRJina2024YearlyBookRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve pages from the 2024 Jina yearbook based on human annotated questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/jina_2024_yearly_book_beir\n\n\n\t\n\nSource datasets:\n\njinaai/jina_2024_yearly_book_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRJina2024YearlyBookRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRJina2024YearlyBookRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"nordjylland-news-image-captioning","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"nordjylland-news-image-captioning\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is a collection of image-caption pairs from the Danish newspaper TV2 Nord. \n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nImage captioning is the intended task for this dataset. No leaderboard is active at this point.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is available in Danish (da).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nAn example from the dataset looks as follows.\n{\n  \"file_name\": \"1.jpg\",\n  \"caption\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/nordjylland-news-image-captioning.","url":"https://huggingface.co/datasets/alexandrainst/nordjylland-news-image-captioning","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","Danish"],"keywords_longer_than_N":true},
	{"name":"Vidore2BioMedicalLecturesRetrieval","keyword":"image-to-text","description":"\n  Vidore2BioMedicalLecturesRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/biomedical_lectures_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2BioMedicalLecturesRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2BioMedicalLecturesRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typicallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typicallyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"emova-sft-4m","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEMOVA-SFT-4M\n\t\n\n\n\n\nðŸ¤— EMOVA-Models | ðŸ¤— EMOVA-Datasets | ðŸ¤— EMOVA-Demo \nðŸ“„ Paper | ðŸŒ Project-Page | ðŸ’» Github | ðŸ’» EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-SFT-4M is a comprehensive dataset curated for omni-modal instruction tuning, including textual, visual, and audio interactions. This dataset is created by gathering open-sourced multi-modal instruction datasets and synthesizing high-quality omni-modal conversation data to enhance user experience. This dataset isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-sft-4m.","url":"https://huggingface.co/datasets/Emova-ollm/emova-sft-4m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a givenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaNlLegalRetrieval","keyword":"image-to-text","description":"\n  JinaVDREuropeanaNlLegalRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Dutch historical legal documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nLegal\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-nl-legal_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-nl-legal_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaNlLegalRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaNlLegalRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRStudentEnrollmentSyntheticRetrieval","keyword":"image-to-text","description":"\n  JinaVDRStudentEnrollmentSyntheticRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve student enrollment data based on templated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/student-enrollment_beir\n\n\n\t\n\nSource datasets:\n\njinaai/student-enrollment_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRStudentEnrollmentSyntheticRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRStudentEnrollmentSyntheticRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"ScreenSpot","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ScreenSpot\n\t\n\nGUI Grounding Benchmark: ScreenSpot. \nCreated researchers at Nanjing University and Shanghai AI Laboratory for evaluating large multimodal models (LMMs) on GUI grounding tasks on screens given a text-based instruction.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nScreenSpot is an evaluation benchmark for GUI grounding, comprising over 1200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element typesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/ScreenSpot.","url":"https://huggingface.co/datasets/rootsautomation/ScreenSpot","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-turkish-subset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for laion2B-multi-turkish-subset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLAION-5B is a large scale openly accessible image-text dataset contains text from multiple languages. This is a Turkish subset data of laion/laion2B-multi. It's compatible to be used with image2dataset to fetch the images at scale.\n\n\t\n\t\t\n\t\n\t\n\t\tData Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['SAMPLE_ID', 'URL', 'TEXT', 'HEIGHT', 'WIDTH', 'LICENSE', 'LANGUAGE', 'NSFW', 'similarity']â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset.","url":"https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset","creator_name":"Cemil Guney","creator_url":"https://huggingface.co/mcemilg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"Magma-AITW-SoM","keyword":"image-to-text","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1â€ Â \nReuben Tan1â€ Â \nQianhui Wu1â€ Â \nRuijie Zheng2â€¡Â \nBaolin Peng1â€¡Â \nYongyuan Liang2â€¡\nYu Gu1Â \nMu Cai3Â \nSeonghyeon Ye4Â \nJoel Jang5Â \nYuquan Deng5Â \nLars Liden1Â \nJianfeng Gao1â–½\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  â€  First authors  â€¡ Second authors  â–½ Leadership  \n[arXiv Paper] Â  [Project Page] Â  [Hugging Face Paper] Â  [Github Repo] Â  [Video]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM.","url":"https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"stackmix_cyrillic","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from cyrillic train set using Stackmix\n\t\n\nNumber of images: 300000\nSources:\n\nCyrillic dataset\nStackmix code\n\n","url":"https://huggingface.co/datasets/nastyboget/stackmix_cyrillic","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"notre-arte","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNotre Arte Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset comprises images sourced from the Notre Arte Instagram page and is intended to serve as a challenging and intriguing dataset for testing visual language models and large multimodal language models. The images in this dataset are characterized by their unique artistic style and complexity, which can provide a robust test for the capabilities of modern AI models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThis dataset is intended for researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/notre-arte.","url":"https://huggingface.co/datasets/taesiri/notre-arte","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"DataComp_medium_pool_BLIP2_captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for DataComp_medium_pool_BLIP2_captions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nWe have used this dataset for pre-training CLIP models and found that it rivals or outperforms models trained on raw web captions on average across the 38 evaluation tasks proposed by DataComp.\nRefer to the DataComp leaderboard (https://www.datacomp.ai/leaderboard.html) for the top baselines uncovered in our work.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nPrimarily English.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thaottn/DataComp_medium_pool_BLIP2_captions.","url":"https://huggingface.co/datasets/thaottn/DataComp_medium_pool_BLIP2_captions","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaottn","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","zero-shot-classification","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"infovqa_colqwen2_embeddings","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tInfoVQA ColQwen2.5 Embeddings\n\t\n\nThis dataset contains pre-computed embeddings for the InfoVQA dataset using the ColQwen2.5 model.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of three configurations:\n\n\t\n\t\t\n\t\tCorpus Configuration\n\t\n\nContains document images with their embeddings.\nfrom datasets import load_dataset\ncorpus = load_dataset(\"WenxingZhu/infovqa_colqwen2_embeddings\", \"corpus\", split=\"test\")\n\nFields:\n\ncorpus-id (int): Document identifier\nimage (Image): Original documentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings.","url":"https://huggingface.co/datasets/WenxingZhu/infovqa_colqwen2_embeddings","creator_name":"WenxingZhu","creator_url":"https://huggingface.co/WenxingZhu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","feature-extraction","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMArt-PPR10k","keyword":"image-text-to-text","description":"JarvisArt/MMArt-PPR10k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/JarvisArt/MMArt-PPR10k","creator_name":"JarvisArt","creator_url":"https://huggingface.co/JarvisArt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-image","image-to-text","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"BACE-V-Train","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BACE-V-Train.","url":"https://huggingface.co/datasets/molvision/BACE-V-Train","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMArt-PPR10k","keyword":"image-to-text","description":"JarvisArt/MMArt-PPR10k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/JarvisArt/MMArt-PPR10k","creator_name":"JarvisArt","creator_url":"https://huggingface.co/JarvisArt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-image","image-to-text","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"GameplayCaptions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"Gameplay Captions\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/asgaardlab/GameplayCaptions","creator_name":"Analytics of Software, Games and Repository Data (ASGAARD) Lab","creator_url":"https://huggingface.co/asgaardlab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"bioleaflets-biomedical-ner","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for BioLeaflets Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBioLeaflets is a biomedical dataset for Data2Text generation. It is a corpus of 1,336 package leaflets of medicines authorised in Europe, which were obtained by scraping the European Medicines Agency (EMA) website. \nPackage leaflets are included in the packaging of medicinal products and contain information to help patients use the product safely and appropriately. \nThis dataset comprises the large majority (âˆ¼ 90%) ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ruslan/bioleaflets-biomedical-ner.","url":"https://huggingface.co/datasets/ruslan/bioleaflets-biomedical-ner","creator_name":"Ruslan Yermak","creator_url":"https://huggingface.co/ruslan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","language-modeling","machine-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"DoclingMatix_5K","keyword":"image-text-to-text","description":"\n[!NOTE]This dataset is a fork of HuggingFaceM4/DoclingMatix\n\n\n\t\n\t\t\n\t\tDoclingMatix\n\t\n\nDoclingMatix is a large-scale, multimodal dataset designed for training vision-language models in the domain of document intelligence. It was created specifically for training the SmolDocling model, an ultra-compact model for end-to-end document conversion.\nThe dataset is constructed by augmenting Hugging Face's Docmatix. Each sample in Docmatix, which consists of a document image and a few questions andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_5K.","url":"https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_5K","creator_name":"Pranav Murthy","creator_url":"https://huggingface.co/pranavvmurthy26","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","cdla-permissive-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"recruiter_atomic_task_final7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n2 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_task_final7.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_task_final7","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_10steps_final","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n10 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_10steps_final.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_10steps_final","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_perfect4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n16 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect4.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect4","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_task6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n10 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_task6.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_task6","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_perfect","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n2 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"brain-mri-plane-aware-vlm","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBrain MRI Plane-Aware VLM Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is derived from the BRISC 2025 dataset and has been processed specifically for training Vision-Language Models (VLMs) with plane-aware understanding of brain MRI scans.\n\n\t\n\t\t\n\t\tSource Dataset\n\t\n\nOriginal dataset: BRISC 2025\nThe BRISC 2025 dataset contains:\n\n6,000 T1-weighted MRI images\nFour tumor classes: Glioma, Meningioma, Pituitary Tumor, and No Tumor\nPixel-wise segmentation masks validated byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AhmadIshaqai/brain-mri-plane-aware-vlm.","url":"https://huggingface.co/datasets/AhmadIshaqai/brain-mri-plane-aware-vlm","creator_name":"Ahmad Ishaq","creator_url":"https://huggingface.co/AhmadIshaqai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","zero-shot-image-classification","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"iReason","keyword":"image-text-to-text","description":"Paper: Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models\nPaper: https://arxiv.org/abs/2506.00258\nWebsite: https://jackie-2000.github.io/iReason.github.io/\nGitHub: https://github.com/eric-ai-lab/iReason\niReason: Designed to probe MLLMsâ€™ implicit reasoning by evaluating their ability to detect subtle flaws in seemingly valid instructions. Covers 643 real-world scenarios spanning four failure typesâ€”object absence, ambiguity, contradiction, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rippleripple/iReason.","url":"https://huggingface.co/datasets/rippleripple/iReason","creator_name":"Jackie Y","creator_url":"https://huggingface.co/rippleripple","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Food_Delivery_App_Screen_Recording_Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tFood Delivery App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on food delivery applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Food_Delivery_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Food_Delivery_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Food_Delivery_App_Screen_Recording_Dataset","keyword":"video-to-text","description":"\n\t\n\t\t\n\t\tFood Delivery App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on food delivery applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Food_Delivery_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Food_Delivery_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"rovibook","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tRoVI-Book Dataset\n\t\n\n\n  ðŸŽ‰ **CVPR 2025** ðŸŽ‰\n  Official dataset for Robotic Visual Instruction\n\n\n\n\nThis is an example to demonstrate the RoVI Book dataset, adapted from the Open-X Embodiments dataset. The bottom displays the proportion of each task type.\nPaper:Robotic Visual Instruction\nProject Page: https://robotic-visual-instruction.github.io/\nCode: https://github.com/RoboticsVisualInstruction/RoVI-Book\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe RoVI-Book dataset is introduced alongside Roboticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yanbang/rovibook.","url":"https://huggingface.co/datasets/yanbang/rovibook","creator_name":"yanbang li","creator_url":"https://huggingface.co/yanbang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["robotics","image-text-to-text","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v19","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v19.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ReachQA","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tReachQA: Reasoning-Intensive Chart Q&A\n\t\n\n\nDisclaimer: This dataset is composed of synthetic data and may contain inaccuracies. Users are advised to exercise caution when working with this dataset and consider additional filtering.\nWe plan to release a manually curated version in the future.\n\nThis repository contains the ðŸ“ˆReachQA dataset proposed in Distill Visual Chart Reasoning Ability from LLMs to MLLMs.\nReachQA is a multimodal instruction dataset synthesized primarily using LLMs.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/hewei2001/ReachQA.","url":"https://huggingface.co/datasets/hewei2001/ReachQA","creator_name":"Wei He","creator_url":"https://huggingface.co/hewei2001","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v3-Tiny","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v3-Tiny\n\t\n\nCaption3o-Opt-v3-Tiny is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and other curated sources, this optimized tiny version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: 27,048 image-caption pairs\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3-Tiny.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3-Tiny","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"govdocs1-pdf-source","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tgovdocs1: source PDF files\n\t\n\n\n[!NOTE]\nConverted versions of other document types (word, txt, etc) are available in this repo\n\nThis is ~220,000 open-access PDF documents (about 6.6M pages) from the dataset govdocs1. It wants to be OCR'd.\n\nUploaded as tar file pieces of ~10 GiB each due to size/file count limits with an index.csv covering details\n5,000 randomly sampled PDFs are available unarchived in sample/. Hugging Face supports previewing these in-browser, for example this oneâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source.","url":"https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source","creator_name":"BEEspoke Data","creator_url":"https://huggingface.co/BEE-spoke-data","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-feature-extraction","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HANS","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for HANS\n\t\n\nHANS is under development and not suited for use yet.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Handwriting Archive: Nordic Samples dataset is a small Danish-language dataset, primarily consisting of transscribed, publically available documents from the Danish National Archive.\nThe purpose of the dataset is explore how to properly create, maintain and update a repository of HTR training data.\n\nCurated by: [Joen Rommedahl]\nLanguage(s) (NLP):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JoenRommedahl/HANS.","url":"https://huggingface.co/datasets/JoenRommedahl/HANS","creator_name":"Joen Rommedahl","creator_url":"https://huggingface.co/JoenRommedahl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Danish","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v168","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v168.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v168","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v201","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v201.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v201","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v3-Tiny","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v3-Tiny\n\t\n\nCaption3o-Opt-v3-Tiny is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and other curated sources, this optimized tiny version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: 27,048 image-caption pairs\nFormat: Parquet\nImage resolution: 512x512\nLanguages: English\nModality:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3-Tiny.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3-Tiny","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"minimal_video_pairs","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tMinimal Video Pairs\n\t\n\nA shortcut-aware benchmark for spatio-temporal and intuitive physics video understanding (VideoQA) using minimally different video pairs.\n\nGithub\n\n\n  \n\n\nFor legal reasons, we are unable to upload the videos directly to Huggingface. However, we provide scripts in this repository for downloading the videos in our github repository. Our benchmark is built on top of videos source from 9 domains:\n\n\t\n\t\t\nSubset\nData sources\n\n\n\t\t\nHuman object interactions\nPerceptionTestâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/facebook/minimal_video_pairs.","url":"https://huggingface.co/datasets/facebook/minimal_video_pairs","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"historical-danish-handwriting","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Historical Danish handwriting dataset is a Danish-language dataset containing more than 11.000 pages of transcribed and proofread handwritten text.\nThe dataset currently consists of the published minutes from a number of City and Parish Council meetings, all dated between 1841 and 1939.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the text is in Danish. The BCP-47 code for Danish is da.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aarhus-city-archives/historical-danish-handwriting.","url":"https://huggingface.co/datasets/aarhus-city-archives/historical-danish-handwriting","creator_name":"aarhus-city-archives","creator_url":"https://huggingface.co/aarhus-city-archives","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Danish","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"PEBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models\n\t\n\nPaper\nPEBench, a comprehensive benchmark for evaluating machine unlearning in MLLMs, focusing on both personal entities and event scenes to provide a holistic assessment of unlearning efficacy and scope.\nMore details on loading and using the data are at our github page.\nIf you do find our code helpful or use our benchmark dataset, please citing our paper.\n@article{xu2025pebenchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xuzhaopan/PEBench.","url":"https://huggingface.co/datasets/xuzhaopan/PEBench","creator_name":"xu","creator_url":"https://huggingface.co/xuzhaopan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v18","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v18.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"journals","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Historical Russian Technical Journal Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains images of pages from old Russian technical journals with descriptions generated using Google Gemini 2.0 Flash.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nRussian (ru): All journal pages are in Russian with corresponding Russian descriptions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nThe dataset consists of:\n\nImage files (.jpg format)\nCorresponding descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/journals.","url":"https://huggingface.co/datasets/nyuuzyou/journals","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Corvus-OCR-Caption-Mix","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCorvus-OCR-Caption-Mix\n\t\n\nCorvus-OCR-Caption-Mix is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. This collection is derived and optimized from the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, with a focus on long-form captions and mixed OCR tasks across a variety of image types.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe dataset spans over 229,000 image-caption pairs and provides a balanced blend of:\n\nOCR-rich documents featuringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mix.","url":"https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mix","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"globalrg-grounding-task","keyword":"image-to-text","description":"UBC-VL/globalrg-grounding-task dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/UBC-VL/globalrg-grounding-task","creator_name":"UBCVL","creator_url":"https://huggingface.co/UBC-VL","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Himanis-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHimanis - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHimanis (HIstorical MANuscript Indexing for user controlled Search) is a corpus of medieval documents.\nThe historical corpus is described in the following publication:\nStutzmann, D., Moufflet, J-F., & Hamel, S. (2017). La recherche en plein texte dans les sources manuscrites mÃ©diÃ©valesâ€¯: enjeux et perspectives du projet HIMANIS pour lâ€™Ã©dition Ã©lectronique. MÃ©diÃ©valesâ€¯: Langue, textes, histoire 73 (2017): 67â€‘96.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Himanis-line.","url":"https://huggingface.co/datasets/Teklia/Himanis-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Latin","French","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Embodied-Captioning","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEmbodied Image Captioning â€“ Manually Annotated Test Set\n\t\n\nPaper: Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions (ICCV 2025)Authors: Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo NataleAffiliations: Italian Institute of Technology (IIT), University of GenoaProject Website: https://hsp-iit.github.io/embodied-captioningCode: https://github.com/hsp-iit/embodied-captioning\n\n\n\t\n\t\n\t\n\t\tðŸ“¦â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TommyBsk/Embodied-Captioning.","url":"https://huggingface.co/datasets/TommyBsk/Embodied-Captioning","creator_name":"Tommaso ","creator_url":"https://huggingface.co/TommyBsk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"PdfParser","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [Moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/stauroskou/PdfParser.","url":"https://huggingface.co/datasets/stauroskou/PdfParser","creator_name":"Stavros Koutsoukos","creator_url":"https://huggingface.co/stauroskou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"tekno21-brain-stroke-dataset-multi","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for BTX24/tekno21-brain-stroke-dataset-multi\n\t\n\n\n\t\n\t\t\n\t\tðŸ”— Dataset Sources\n\t\n\n\nDataset Source: TEKNOFEST-2021 Stroke Dataset\nKaggle: Ä°nme Veri Seti (Stroke Dataset)\nSaÄŸlÄ±k BakanlÄ±ÄŸÄ± AÃ§Ä±k Veri PortalÄ±: Ä°nme Veri Seti\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: PNG\nTotal Images: 7,369\nCategories:\nhemorajik/ (Hemorrhagic stroke images)\niskemik/ (Ischemic stroke images)\nnormal/ (Non-stroke images)\n\n\nThe dataset is structured in a folder-based format where images are grouped intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-multi.","url":"https://huggingface.co/datasets/BTX24/tekno21-brain-stroke-dataset-multi","creator_name":"BORAN TOKTAY","creator_url":"https://huggingface.co/BTX24","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","feature-extraction","image-to-text","image-feature-extraction","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"PC-Agent-E","keyword":"image-text-to-text","description":"This repository contains the dataset used in the paper Efficient Agent Training for Computer Use.\n","url":"https://huggingface.co/datasets/henryhe0123/PC-Agent-E","creator_name":"Yanheng He","creator_url":"https://huggingface.co/henryhe0123","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenGVLab_Lumina_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Lumina Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Lumina across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenGVLab_Lumina_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-missing-words-train-only","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Missing Words in Train Split only\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Missing Words in Train Split only, each variation column is affected by the noise, only when the split is for training. The validation and test splits are not affected by theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words-train-only.","url":"https://huggingface.co/datasets/alix-tz/noisy-gt-missing-words-train-only","creator_name":"Alix ChaguÃ©","creator_url":"https://huggingface.co/alix-tz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v89","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v89.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v89","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dpo_data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data DPO\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nDPO_preview: Contains model generated CoT judged my outcome reward.\n\nImage use same in sft repo: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODO separate readme for setup and train.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSet up:\n\t\n\ngit cloneâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/dpo_data.","url":"https://huggingface.co/datasets/Share4oReasoning/dpo_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LLaVA-Video-small-swift","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-small-swift\n\t\n\nSmall subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","url":"https://huggingface.co/datasets/malterei/LLaVA-Video-small-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"V1-33K-Old","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nðŸš€  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction ðŸŒŸ\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K-Old.","url":"https://huggingface.co/datasets/haonan3/V1-33K-Old","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LiveMCPBench","keyword":"image-text-to-text","description":"\n\n\n\n  LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?\n\n  \n    Benchmarking the agent in real-world tasks within a large-scale MCP toolset.\n  \n\n\n\n  ðŸŒ Website Â  | Â \n  ðŸ“„ Paper Â  | Â \n  ðŸ’» Code Â  | Â \n  ðŸ† Leaderboard \n  Â  | Â \n  ðŸ™ Citation\n\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\nLiveMCPBench is the first comprehensive benchmark designed to evaluate LLM agents at scale across diverse Model Context Protocol (MCP) servers. It comprises 95 real-world tasks grounded in the MCP ecosystemâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ICIP/LiveMCPBench.","url":"https://huggingface.co/datasets/ICIP/LiveMCPBench","creator_name":"ICIP","creator_url":"https://huggingface.co/ICIP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"btc-candlestick-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCandlestick Chart Dataset - BTCUSDT\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains candlestick chart images paired with textual descriptions and trading labels for the BTCUSDT trading pair.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSymbol: BTCUSDT\nInterval: 1h\nWindow Size: 30 candles per chart\nTotal Records: 13081\nImage Format: PNG (candlestick charts with volume)\nText Format: Structured candle data description\nLabels: Trading signal classification\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach recordâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tuankg1028/btc-candlestick-dataset.","url":"https://huggingface.co/datasets/tuankg1028/btc-candlestick-dataset","creator_name":"Tuan","creator_url":"https://huggingface.co/tuankg1028","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v19","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CyclePrefDB-I2T","keyword":"image-to-text","description":"CyclePrefDB-I2T: Dataset of cycle consistency preferences 398K comparison pairs for image-to-text generation. Images are from DCI and captions are generated using 11 different VLMs. Each image has 11 different generated descriptions and 11 image reconstructions. Reconstruction error is measured with DreamSim (lower score means more similar).","url":"https://huggingface.co/datasets/carolineec/CyclePrefDB-I2T","creator_name":"Caroline Chan","creator_url":"https://huggingface.co/carolineec","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","mit","100K<n<1M","arxiv:2506.02095"],"keywords_longer_than_N":true},
	{"name":"GlitchBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGlitchBench\n\t\n\nThis repository contains the dataset for the paper GlitchBench: Can large multimodal models detect video game glitches?\n    \n     by \n        Mohammad Reza Taesiri, \n        Tianjun Feng,\n        Anh Nguyen, and \n        Cor-Paul Bezemer \n    \n    \n    (CVPR 2024)\n    \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nLarge multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/glitchbench/GlitchBench.","url":"https://huggingface.co/datasets/glitchbench/GlitchBench","creator_name":"GlitchBench","creator_url":"https://huggingface.co/glitchbench","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"coda","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCODa Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/coda.","url":"https://huggingface.co/datasets/mateoguaman/coda","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tiny-cord","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tparlarlax/tiny-cord\n\t\n\nCORD (Consolidated Receipt Dataset) is a dataset for receipt understanding tasks.\nThis dataset contains Indonesian restaurant receipts with structured annotations\nfor menu items, prices, and text extraction with bounding boxes.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe CORD dataset contains receipt images and their corresponding structured annotations.\nEach example includes:\n\nReceipt Image: High-resolution image of Indonesian restaurantâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/parlarlax/tiny-cord.","url":"https://huggingface.co/datasets/parlarlax/tiny-cord","creator_name":"Kraiwit Tongkul","creator_url":"https://huggingface.co/parlarlax","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","object-detection","token-classification","Indonesian","English"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v51","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v51.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vqa-rad-ko","keyword":"image-to-text","description":"junyeong-nero/vqa-rad-ko dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/junyeong-nero/vqa-rad-ko","creator_name":"Junyeong Song","creator_url":"https://huggingface.co/junyeong-nero","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Korean","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Cosmos-Reason1-SFT-Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nThe data format is a pair of video and text annotations. We summarize the data and annotations in Table 4 (SFT), Table 5 (RL), and Table 6 (Benchmark) of the Cosmos-Reason1 paper. â€‹â€‹ We release the annotations for embodied reasoning tasks for BridgeDatav2, RoboVQA, Agibot, HoloAssist, AV, and the videos for the RoboVQA and AV datasets. We additionally release the annotations and videos for the RoboFail dataset for benchmarks. By releasing the dataset, NVIDIAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset.","url":"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"matQnA","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis\n\t\n\nThis repository hosts the MatQnA dataset, a multi-modal benchmark dataset presented in the paper MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis.\nMatQnA is specifically designed to evaluate the capabilities of AI models in the specialized field of materials characterization and analysis. It includes data from tenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/richardhzgg/matQnA.","url":"https://huggingface.co/datasets/richardhzgg/matQnA","creator_name":"richardhzgg","creator_url":"https://huggingface.co/richardhzgg","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v144","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v144.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v144","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"thai-license-plate-ocr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tThai License Plate OCR Dataset ðŸ‡¹ðŸ‡­\n\t\n\nðŸ‡ºðŸ‡¸ English Version\n\nTask: Optical Character Recognition (OCR)\nLanguage: Thai ðŸ‡¹ðŸ‡­  \n\nOCR dataset à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸ªà¸³à¸«à¸£à¸±à¸š PaddleOCR-rec à¹‚à¸”à¸¢à¹€à¸‰à¸žà¸²à¸°\nà¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸žà¸·à¹ˆà¸­à¸à¸¶à¸à¸ªà¸­à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸£à¸¹à¹‰à¸ˆà¸³à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ˆà¸²à¸à¸›à¹‰à¸²à¸¢à¸—à¸°à¹€à¸šà¸µà¸¢à¸™à¸£à¸–à¸¢à¸™à¸•à¹Œà¹„à¸—à¸¢ à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸²à¸¢à¹€à¸¥à¸‚à¸—à¸°à¹€à¸šà¸µà¸¢à¸™à¹à¸¥à¸°à¸Šà¸·à¹ˆà¸­à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸”\n\n\nâš ï¸ à¹ƒà¸Šà¹‰à¹€à¸‰à¸žà¸²à¸°à¸à¸±à¸š PaddleOCR-rec (à¹„à¸¡à¹ˆà¸¡à¸µ detection / classification)\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nthai-license-ocr-dataset/\nâ”œâ”€â”€ images/           # à¸£à¸§à¸¡à¸ à¸²à¸žà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”\nâ”œâ”€â”€ train.txt         # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸¶à¸à¸ªà¸­à¸™\nâ”œâ”€â”€ val.txtâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/morsetechlab/thai-license-plate-ocr.","url":"https://huggingface.co/datasets/morsetechlab/thai-license-plate-ocr","creator_name":"Nuttapong Chimwai","creator_url":"https://huggingface.co/morsetechlab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","manual","monolingual","Thai","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"license_plates","keyword":"image-to-text","description":"PawanKrGunjan/license_plates dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/PawanKrGunjan/license_plates","creator_name":"Pawan Kumar Gunjan","creator_url":"https://huggingface.co/PawanKrGunjan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"flickr30k-pt-br","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŽ‰ Flickr30K Translated for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tðŸ’¾ Dataset Summary\n\t\n\nFlickr30K Portuguese Translated, a multimodal dataset for Portuguese image captioning with 31,014 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\nThe dataset is one of the results of work available at:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br.","url":"https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br","creator_name":"LaboratÃ³rio de InteligÃªncia Computacional e Sistemas de informaÃ§Ã£o","creator_url":"https://huggingface.co/laicsiifes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","text-to-image","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v83","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v83.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v83","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mmCultural","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCultural Competence Dataset for Vision-Language Models\n\t\n\nThis dataset contains culturally diverse images and prompts for evaluating cultural competence in Vision-Language Models (VLMs), as presented in the paper Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation.\nCode: https://github.com/ArkaMukherjee0/mmCultural\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\n\nConcepts: 35 unique concepts (e.g., honesty, empathy, cooperation)\nCulturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ArkaMukherjee/mmCultural.","url":"https://huggingface.co/datasets/ArkaMukherjee/mmCultural","creator_name":"Arka Mukherjee","creator_url":"https://huggingface.co/ArkaMukherjee","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"llava-bench-in-the-wild-ja","keyword":"image-to-text","description":"This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.\nOriginal dataset is liuhaotian/llava-bench-in-the-wild.\n","url":"https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Samael1976","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Samael1976","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"gothenburg-price-tag","keyword":"image-to-text","description":"fangsonglong/gothenburg-price-tag dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/fangsonglong/gothenburg-price-tag","creator_name":"Fangsong Long","creator_url":"https://huggingface.co/fangsonglong","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","Swedish","unlicense","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"fastcup-highlights","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Fastcup.net Highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 85,488 video clips from the gaming platform Fastcup.net, with 78,143 clips from Counter-Strike 2 and 7,345 clips from Counter-Strike: Global Offensive. The clips showcase gameplay highlights and include detailed metadata such as player statistics, weapon information, and engagement metrics. The total size of raw video content is approximately 34 TB.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/fastcup-highlights.","url":"https://huggingface.co/datasets/nyuuzyou/fastcup-highlights","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"PD12M-ru","keyword":"image-to-text","description":"Translated captions from Spawning/PD12M into Russian using Google Translate.\n","url":"https://huggingface.co/datasets/d0rj/PD12M-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["image-feature-extraction","image-to-text","text-to-image","translated","Spawning/PD12M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v24","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v71","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v71.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v71","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"image-text","keyword":"image-to-text","description":"SkillFi/image-text dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/SkillFi/image-text","creator_name":"Alex","creator_url":"https://huggingface.co/SkillFi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ViLBench","keyword":"image-text-to-text","description":"Benchmark Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\narXiv | Project Page\nThere are 600 data collected from 5 existing vision-language tasks\n","url":"https://huggingface.co/datasets/UCSC-VLAA/ViLBench","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","n<1K","arxiv:2503.20271"],"keywords_longer_than_N":true},
	{"name":"Mini-YoChameleon-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMini Yo'Chameleon Data\n\t\n\nThis is a mini-training-data for Yo'Chameleon, with example of personalized subject called <bo> (From Yo'LLaVA).\nWhat you will find:\n\n10/10 positive images for training/ testing\n1000 HARD negative images for training (retrieved from LAION-5B based on similarity with subject)\n1000 random images for training\n\nThe folder structure:\nmini-yochameleon-data\n |_ random_negative_example\n |   |_ [1000 random images example for training recognition abilities]\n |_ test\n |â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data.","url":"https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaoshibe","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"heb-synthtiger-16k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\theb-synthtiger-16k (Synthetic Hebrew Printed Text Dataset)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nheb-synthtiger-16k is a dataset containing 16,000 synthetic Hebrew text images generated using SynthTIGER with 11 different printed fonts.\nThe text in these images consists primarily of single words, sampled from the 10,000 most frequent Hebrew words, along with words from additional sources such as:\n\nIsraeli place names\nBiblical texts\n\nThis dataset is designed to support Hebrew OCR and textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/asafd60/heb-synthtiger-16k.","url":"https://huggingface.co/datasets/asafd60/heb-synthtiger-16k","creator_name":"Asaf Delmedigo","creator_url":"https://huggingface.co/asafd60","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-recaption","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tAuroraCap-recaption\n\t\n\n\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nVideo recaption data by AuroraCap. Continue updating...\nFor some video source, we could upload the raw videos but for the others we could only provide the url since the well-known reason.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@article{chai2024auroracap,\n  title={AuroraCap: Efficient, Performant Video Detailedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-recaption.","url":"https://huggingface.co/datasets/wchai/AuroraCap-recaption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PRISM-CoT","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality\n\t\n\nThis repository contains the PRISM-CoT and PRISM-DPO datasets, which are key components of the PRISM (Principled Reasoning for Integrated Safety in Multimodality) framework. PRISM is a system2-like framework designed to align Vision-Language Models (VLMs) by embedding a structured, safety-aware reasoning process.\n\nPRISM-CoT is a dataset that teaches safety-aware chain-of-thought reasoning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/andyc03/PRISM-CoT.","url":"https://huggingface.co/datasets/andyc03/PRISM-CoT","creator_name":"Nanxi Li","creator_url":"https://huggingface.co/andyc03","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","image-text-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"RICO-Screen2Words-Rename","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Screen2Words\n\t\n\nScreen2Words is a dataset providing screen summaries (i.e., image captions for mobile screens). \nIt uses the RICO image database. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository:\ngoogle-research-datasets/screen2words\nRICO raw downloads\n\n\nPaper:\nScreen2Words: Automatic Mobile UI Summarization with Multimodal Learning\nRico: A Mobile App Dataset for Building Data-Driven Design Applications\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis dataset is forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Billyyer/RICO-Screen2Words-Rename.","url":"https://huggingface.co/datasets/Billyyer/RICO-Screen2Words-Rename","creator_name":"xuleyu","creator_url":"https://huggingface.co/Billyyer","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K<n<100K","arxiv:2108.03353"],"keywords_longer_than_N":true},
	{"name":"CompreCap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for CompreCap\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe CompreCap benchmark is characterized by human-annotated scene graph and focuses on the evaluation of comprehensive image captioning.\nIt provides new semantic segmentation annotations for common objects in images, with an average mask coverage of 95.83%.\nBeyond the careful annotation of objects, CompreCap also includes high-quality descriptions of the attributes bound to the objects, as well as directional relationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FanLu31/CompreCap.","url":"https://huggingface.co/datasets/FanLu31/CompreCap","creator_name":"Fan Lu","creator_url":"https://huggingface.co/FanLu31","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"modern-danish-handwriting","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Modern Danish Handwriting\n\t\n\n\n\nThe Modern Danish Handwriting dataset is a Danish-language dataset containing more than 200 pages of transcribed and proofread handwritten text. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nThe Modern Danish Handwriting dataset currently consists of handwritten samples of text from the ePAROLE dataset. The samples were created by volunteers at the Danish National Archives and guests at the festival Historiske Dage in 2025.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/RA-Data-Science/modern-danish-handwriting.","url":"https://huggingface.co/datasets/RA-Data-Science/modern-danish-handwriting","creator_name":"Danish National Archives, Data Science","creator_url":"https://huggingface.co/RA-Data-Science","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Danish","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-18","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-18","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"E-commerce_Platform_Screen_Recording_Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tE-commerce Platform Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on an e-commerce platform. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for UI/UX research, AI training, and behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/E-commerce_Platform_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/E-commerce_Platform_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"E-commerce_Platform_Screen_Recording_Dataset","keyword":"video-to-text","description":"\n\t\n\t\t\n\t\tE-commerce Platform Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on an e-commerce platform. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for UI/UX research, AI training, and behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.io  \nabhishek.vadapalli@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/E-commerce_Platform_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/E-commerce_Platform_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MM-UPD","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMM-UPD Bench\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed Unsolvable Problem Detection (UPD). Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whetherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MM-UPD/MM-UPD.","url":"https://huggingface.co/datasets/MM-UPD/MM-UPD","creator_name":"MM-UPD","creator_url":"https://huggingface.co/MM-UPD","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","arxiv:2403.20331"],"keywords_longer_than_N":true},
	{"name":"TVC-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for TVC-Data\n\t\n\nThis repository contains the data presented in Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning.\nProject page: https://sun-hailong.github.io/projects/TVC\nCode: https://github.com/sun-hailong/TVC\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nA mixture of 345K multimodal long-chain reasoning data. \nFor more statistics of the dataset, please refer to our paper (coming soon)\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nLLaVA-OneVision:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Allen8/TVC-Data.","url":"https://huggingface.co/datasets/Allen8/TVC-Data","creator_name":"Allen Sun","creator_url":"https://huggingface.co/Allen8","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","apache-2.0","arxiv:2503.13360"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTest","keyword":"image-text-to-text","description":"This repository contains the dataset used in GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nGitHub repository: https://github.com/yueliu1999/GuardReasoner-VL\n","url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTest","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v212","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v212.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v212","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GeoQA-8K-direct-synthesizing","keyword":"image-text-to-text","description":"This dataset supports the unsupervised post-training of multi-modal large language models (MLLMs) as described in the paper Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO. It's designed to enable continual self-improvement without external supervision, using a self-rewarding mechanism based on majority voting over multiple sampled responses. The dataset is used to improve the reasoning ability of MLLMs, as demonstrated by significant improvements on benchmarks like MathVistaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing.","url":"https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"JA-OKVQA-Reasoning","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset supports high-quality Visual Question Answering (VQA) focused on knowledge-based reasoning.\nEach sample is annotated with structured reasoning steps, progressing from image observation to external knowledge retrieval and answer derivation.\nIt is constructed based on and expands upon OK-VQA and A-OKVQA.\nThe dataset is suitable for training and evaluating models in explainable multimodal reasoning tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tSample Data Format (JSON)\n\t\n\n{\n  \"id\": \"0\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MIL-UT/JA-OKVQA-Reasoning.","url":"https://huggingface.co/datasets/MIL-UT/JA-OKVQA-Reasoning","creator_name":"Machine Intelligence Laboratory (The University of Tokyo)","creator_url":"https://huggingface.co/MIL-UT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Japanese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Egyptian-Handwriting-Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEgyptian Handwriting Dataset\n\t\n\nA dataset of 11k+ handwritten Arabic words from Egyptian writers, extracted and tightly cropped from scanned paper forms. This dataset offers diverse handwriting samples ranging from children to elderly contributors, making it ideal for training robust Arabic handwriting recognition models.\n\n  \n\n\n\nEach form contains 6 unique words, resulting in 24 handwritten word images per form.\nEach word is written four times by the same writer to captureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OmarMDiab/Egyptian-Handwriting-Dataset.","url":"https://huggingface.co/datasets/OmarMDiab/Egyptian-Handwriting-Dataset","creator_name":"Omar Diab","creator_url":"https://huggingface.co/OmarMDiab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VideoVista-CulturalLingo","keyword":"video-text-to-text","description":"\n    \n\n\n    \n\n\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tVideoVista-CulturalLingo\n\t\n\nThis repository contains the VideoVista-CulturalLingo, introduced in VideoVista-CulturalLingo: 360Â° Horizons-Bridging Cultures, Languages,\nand Domains in Video Comprehension. \n ðŸŽ‰ Our new VideoVista-CulturalLingo bridges cultures (China, North America, and Europe), languages (Chinese and English), and domains (140+)in video comprehension. \n ðŸŒ Welcome to join us on this journey of video understanding!\n\n\t\n\t\t\n\t\tFiles\n\t\n\nWe provice theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo.","url":"https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo","creator_name":"Uni-MoE","creator_url":"https://huggingface.co/Uni-MoE","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SightationVQA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSighationVQA\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationVQA.","url":"https://huggingface.co/datasets/Sightation/SightationVQA","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v153","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v153.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v153","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"StreetView360AtoZ","keyword":"image-to-text","description":"StreetView 360X is a dataset containing 6342 360 degree equirectangular street view images randomly sampled and downloaded from Google Street View. It is published as part of the paper \"StreetView360X: A Location-Conditioned Latent Diffusion Model for Generating Equirectangular 360 Degree Street Views\" (Princeton COS Senior Independent Work by Everett Shen). Images are labelled with their capture coordinates and panorama IDs. Scripts for extending the dataset (i.e. fetching additional images)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/everettshen/StreetView360AtoZ.","url":"https://huggingface.co/datasets/everettshen/StreetView360AtoZ","creator_name":"Everett Shen","creator_url":"https://huggingface.co/everettshen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-feature-extraction","mit"],"keywords_longer_than_N":true},
	{"name":"engineering-drawings-as1100","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEngineering Drawings AS1100 Compliance Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains engineering drawings with various AS1100 (Australian Standard for Technical Drawing) compliance issues for training AI models to identify missing elements and non-compliance issues in technical drawings.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Engineering Drawings AS1100 Compliance Dataset is designed to train and evaluate vision-language models on identifying compliance issues in technicalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jcrzd/engineering-drawings-as1100.","url":"https://huggingface.co/datasets/jcrzd/engineering-drawings-as1100","creator_name":"JC","creator_url":"https://huggingface.co/jcrzd","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","visual-question-answering","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MM-RLHF","keyword":"image-text-to-text","description":"\n\n[ðŸ“– arXiv Paper] \n[ðŸ“Š Training Code] \n[ðŸ“ Homepage] \n[ðŸ† Reward Model] \n[ðŸ”® MM-RewardBench] \n[ðŸ”® MM-SafetyBench] \n[ðŸ“ˆ Evaluation Suite] \n\n\n\n\n\t\n\t\t\n\t\tThe Next Step Forward in Multimodal LLM Alignment\n\t\n\n[2025/02/10] ðŸ”¥ We are proud to open-source MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. This release includes:\n\nA high-quality MLLM alignment dataset.\nA strong Critique-Based MLLM reward model and its training algorithm.\nA novelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MM-RLHF.","url":"https://huggingface.co/datasets/yifanzhang114/MM-RLHF","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v210","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v210.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v210","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SPATIAL-v1.0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸš€ Aerospace Knowledge Dataset (VLM)\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Œ Overview\n\t\n\nThe Aerospace Knowledge Dataset is a large-scale, multi-modal dataset designed for training Vision-Language Models (VLMs) in the aerospace domain. It is built from over 26,000 pages of technical documents, research papers, engineering reports, and mission data from leading space organizations such as NASA, ArianeGroup, SpaceX, ESA, and others.  \nThis dataset is structured in a query + image format, allowing AI models toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Youlln/SPATIAL-v1.0.","url":"https://huggingface.co/datasets/Youlln/SPATIAL-v1.0","creator_name":"Lalain Youri","creator_url":"https://huggingface.co/Youlln","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"RSTeller","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tâš ï¸ Usage Warning\n\t\n\nThis is the latest version of RSTeller, updated on 2025-01-28. Users who accessed this dataset before this date can find the legacy version, which is preserved for reference. Additionally, we have released the metadata for this dataset.\nFor the details and the usage of the dataset, please refer to our github repository page.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find the dataset and our paper useful, please consider citing our paper:\n@article{ge2025rstellerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller.","url":"https://huggingface.co/datasets/SlytherinGe/RSTeller","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"coco-captions-pt-br","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŽ‰ COCO Captions Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tðŸ’¾ Dataset Summary\n\t\n\nCOCO Captions Portuguese Translation, a multimodal dataset for Portuguese image captioning with 123,287 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tðŸ§‘â€ðŸ’» Hot to Get Started with theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br.","url":"https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br","creator_name":"LaboratÃ³rio de InteligÃªncia Computacional e Sistemas de informaÃ§Ã£o","creator_url":"https://huggingface.co/laicsiifes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding colorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RS-M3Bench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRS-M3Bench\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tUsing datasets\n\t\n\nfrom datasets import load_dataset\nfw = load_dataset(\"RemoteReason-JLU/RS-M3Bench\", name=\"RS-M3Bench\", split=\"train\", streaming=True)\n\nAttribute explanations in annotation files:\n\nHBB: the coordinates of four object corner points\nOBB: the coordinates of four object corner points\nPolygon: the coordinates of all the object corner points\nNote that the original STAR/ReCom1M datasets use OBB to localize objects, we further uultilize SAM toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/RemoteReason-JLU/RS-M3Bench.","url":"https://huggingface.co/datasets/RemoteReason-JLU/RS-M3Bench","creator_name":"Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education","creator_url":"https://huggingface.co/RemoteReason-JLU","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v32","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v32.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v152","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v152.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v152","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"NorHand-v1-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNorHand v1 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v1 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian BokmÃ¥l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'fredagâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v1-line.","url":"https://huggingface.co/datasets/Teklia/NorHand-v1-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Norwegian BokmÃ¥l","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v176","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v176.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v176","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ChartEdit","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSTEM Image Captions Dataset\n\t\n\nThis dataset contains AI-generated captions for STEM images.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in batches:\n\nTotal batches: 1\nEach batch is stored in a separate directory (batch_0000, batch_0001, etc.)\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nTotal items: 97930\nSuccessfully captioned: 97930\nErrors: 0\nSkipped: 0\n\n\n\t\n\t\t\n\t\tLoading the Dataset\n\t\n\nYou can load individual batches:\nfrom datasets import load_dataset\n\n# Load a specific batch\nbatch_0 =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JackyZhuo/ChartEdit.","url":"https://huggingface.co/datasets/JackyZhuo/ChartEdit","creator_name":"Le Zhuo","creator_url":"https://huggingface.co/JackyZhuo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"HueManity","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHueManity: A Benchmark for Testing Human-Like Visual Perception in MLLMs\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nHueManity is a benchmark dataset featuring 83,850 images designed to test the fine-grained visual perception of Multimodal Large Language Models (MLLMs). Each image presents a two-character alphanumeric string embedded within Ishihara-style dot patterns, challenging models to perform precise pattern recognition in visually cluttered environments.\nThe dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Jayant-Sravan/HueManity.","url":"https://huggingface.co/datasets/Jayant-Sravan/HueManity","creator_name":"Jayant Sravan Tamarapalli","creator_url":"https://huggingface.co/Jayant-Sravan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","image-feature-extraction","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relativeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Mixed_VQA_GenQA_EvalQA_1.5M","keyword":"image-text-to-text","description":"This repository contains the data for the paper LOVA3: Learning to Visual Question Answering, Asking and Assessment.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tðŸŽ“ Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuan Zhao and Pan Zhou and Difei Gao and Zechen Bai and Mike Zheng Shou},\n    booktitle={The Thirty-eighth Annual Conference onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M.","url":"https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-symmetry-v26","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"OpenAI-4o_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata OpenAI 4o Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 200'000 human responses from over ~45,000 individual annotators, collected in less than half a day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating OpenAI 4o (version from 26.3.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/OpenAI-4o_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"reverse-instruct-1.3m","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tREVERSE Visual Instruct 1.3M\n\t\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDataset Type:REVERSE Visual Instruct 1.3M is a GPT-generated instruction-following dataset designed for training hallucination-aware vision-language models (VLMs). It builds on the LLaVA Instruct 665K dataset and includes structured annotations to indicate model confidence. We introduce three special tokens:  \n\n<SPAN>: marks the beginning of a key phrase  \n</CN>: denotes a confident (grounded) phrase  \n</UN>: denotes anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m.","url":"https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m","creator_name":"Patrick (Tsung-Han) Wu","creator_url":"https://huggingface.co/tsunghanwu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","mit","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"EndoVis2018LongCap","keyword":"image-to-text","description":"This dataset consists of long-text captions of laparoscopic surgical images, extended from https://github.com/XuMengyaAmy/CIDACaptioning.\nDownload the images: https://endovissub2018-roboticscenesegmentation.grand-challenge.org/Data/\n","url":"https://huggingface.co/datasets/liujiquan/EndoVis2018LongCap","creator_name":"Jiquan Liu","creator_url":"https://huggingface.co/liujiquan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v59","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v59.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v59","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-RL-37K","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tTreeBench Dataset Card\n\t\n\nThis repository contains TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark designed for evaluating \"thinking with images\" capabilities with traceable visual evidence.\nThe dataset was introduced in the paper: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is built on three core principles:\n\nFocused visual perception: of subtle targets in complex scenes.\nTraceable evidence: via bounding boxâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K.","url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage isze: 3-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v32","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v32.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tate","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTate\n\t\n\nArtwork+metadata from Tate\nStats:\n\n~67k\n\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\nItems with no image available are excluded\nMajority of images are licensed under Creative Commons CC BY-NC-ND 4.0\n\n\n\t\n\t\t\n\t\tanother BIG data banger straight from the underground\n\t\n\n\n\t\n\t\t\n\t\twith thanks to Tate, their patrons and the artists â¤ï¸\n\t\n\n","url":"https://huggingface.co/datasets/bigdata-pw/tate","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","odc-by","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v116","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v116.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v116","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v78","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v78.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v78","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hico_det","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for HICO-DET Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHICO-DET is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each personâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhimeng/hico_det.","url":"https://huggingface.co/datasets/zhimeng/hico_det","creator_name":"Zhimeng Guo","creator_url":"https://huggingface.co/zhimeng","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-feature-extraction","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v91","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v91.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v91","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v27","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v181","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v181.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v181","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"civitai-top-nsfw-images-with-metadata","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCivitAI Top NSFW Images Dataset\n\t\n\nThis dataset contains 6k+ top NSFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://diffused.to/Thread-CivitAI-Top-NSFW-Images-Dataset-6k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJune 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\nâ”œâ”€â”€ ðŸ“‚ images/\nâ”‚   â”œâ”€â”€ 1.jpg\nâ”‚   â”œâ”€â”€ 2.jpg\nâ”‚   â”œâ”€â”€ 3.jpg\nâ”‚   â”œâ”€â”€ ....\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-nsfw-images-with-metadata.","url":"https://huggingface.co/datasets/wallstoneai/civitai-top-nsfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Blank-v2.0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenpdf-Blank-v2.0\n\t\n\nOpenpdf-Blank-v2.0 is a small dataset containing blank or near-blank PDF image samples. This dataset is primarily designed to help train and evaluate document processing models, especially in tasks like:\n\nIdentifying and filtering blank or noise-filled documents.\nPreprocessing stages for OCR pipelines.\nReceipt/document classification tasks.\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nModality: Image\nLanguages: English (if applicable)\nSize: Less than 1,000 samples\nLicense:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0.","url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"civitai-top-nsfw-images-with-metadata","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCivitAI Top NSFW Images Dataset\n\t\n\nThis dataset contains 6k+ top NSFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://diffused.to/Thread-CivitAI-Top-NSFW-Images-Dataset-6k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJune 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\nâ”œâ”€â”€ ðŸ“‚ images/\nâ”‚   â”œâ”€â”€ 1.jpg\nâ”‚   â”œâ”€â”€ 2.jpg\nâ”‚   â”œâ”€â”€ 3.jpg\nâ”‚   â”œâ”€â”€ ....\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-nsfw-images-with-metadata.","url":"https://huggingface.co/datasets/wallstoneai/civitai-top-nsfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"generative-negative-mining-dataset","keyword":"image-to-text","description":"This dataset consists of 278 samples. 122 of the images have 4 variations, 139 of the images\nhave 3 variations, and 17 of the images have 2 variations.","url":"https://huggingface.co/datasets/ugursahin/generative-negative-mining-dataset","creator_name":"ugur sahin","creator_url":"https://huggingface.co/ugursahin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BACE-V-SMILES-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BACE-V-SMILES-2.","url":"https://huggingface.co/datasets/molvision/BACE-V-SMILES-2","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v105","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v105.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v105","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"IndustryEQA","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tIndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios\n\t\n\nAuthors: Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong\nPaper ðŸ“ | Code ðŸ’»\nThis benchmark dataset accopmanies our paper of the same title. Built upon the NVIDIA Isaac Sim platform,\nIndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets,\ndynamic human agents, and carefully designed hazardous situations inspired byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IndustryEQA/IndustryEQA.","url":"https://huggingface.co/datasets/IndustryEQA/IndustryEQA","creator_name":"IndustryEQA","creator_url":"https://huggingface.co/IndustryEQA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual gameâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Game-RL","creator_url":"https://huggingface.co/Code2Logic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"human-alignment-preferences-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated alignment datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-alignment-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","reinforcement-learning","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Mathematics-Class10-Tnsb","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMathematics-Class10-Tnsb\n\t\n\nThis dataset contains scanned images from a Class 10 Mathematics textbook under the TNSB (Tamil Nadu State Board) curriculum. It is intended for educational machine learning tasks such as image-to-text (OCR), textbook digitization, or educational content understanding.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Tamil Nadu State Board Class 10 Mathematics textbook\nTask: Image-to-Text\nLanguage: English\nSplit: train only\nRows: 352\nFormat: Images only (scanned textbookâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Mathematics-Class10-Tnsb.","url":"https://huggingface.co/datasets/prithivMLmods/Mathematics-Class10-Tnsb","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"playground-popular","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Playground Popular\n\t\n\nMost popular image generations by number of likes.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA subset of bigdata-pw/playground filtered to the most popular 1 million images by number of likes. Entries include generation details such as prompts and model used, anonymized user information, creation date, and URL to the image.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/playground-popular.","url":"https://huggingface.co/datasets/bigdata-pw/playground-popular","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"MIRe_ViD2R","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMIRe Pre-training Dataset for Multimodal Query Retrieval\n\t\n\nThis repository contains the pre-training dataset used in our work on MIRe: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval. The dataset is designed for training multimodal retrieval systems that integrate both visual and textual cues without fusing text features during the alignment stage.\nNote: This release excludes data from the WiT corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tOverviewâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Y-J-Ju/MIRe_ViD2R.","url":"https://huggingface.co/datasets/Y-J-Ju/MIRe_ViD2R","creator_name":"Yeong-Joon Ju","creator_url":"https://huggingface.co/Y-J-Ju","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","image-to-text","visual-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LLaVa_textcaps_rus","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTextcaps\n\t\n\nImage descriptions in English and Russian.\nRussian descriptions were translated from English.\nOriginal data was task textcaps from https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data\nData Format \n'id': str \n'image': dict[str, bytes]  # {'image': b'image bytes'}\n'conversations': List[Dict[str, str]]  # short dialog for two replics in English with image description\nExample: [{'from': 'human', 'value': 'Provide a one-sentence caption for the provided image.'}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/LLaVa_textcaps_rus.","url":"https://huggingface.co/datasets/Vikhrmodels/LLaVa_textcaps_rus","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Russian","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lexica_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLexicaDataset\n\t\n\nLexicaDataset is a large-scale text-to-image prompt dataset shared in [USENIX'24] Prompt Stealing Attacks Against Text-to-Image Generation Models.\nIt contains 61,467 prompt-image pairs collected from Lexica.\nAll prompts are curated by real users and images are generated by Stable Diffusion.\nData collection details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tData Splits\n\t\n\nWe randomly sample 80% of a dataset as the training dataset and the rest 20% as the testing dataset.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/vera365/lexica_dataset.","url":"https://huggingface.co/datasets/vera365/lexica_dataset","creator_name":"Xinyue Shen","creator_url":"https://huggingface.co/vera365","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v54","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v54.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2024-10","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2024-10","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-Thinking-156k","keyword":"image-text-to-text","description":"This is the SophiaVL-R1-Thinking-156k dataset for training Thinking Reward Model of SophiaVL-R1 (SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward).\nThe data is constructed in sharegpt format. text_only_part.json is text-only data. multimodal_part.json is image-text data. Images can be found in images.\n","url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-symmetry-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"UrbEx1M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for UrbEx1M\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nImages of Urban Exploration\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{UrbEx1M,\n  author = {hlky},\n  title = {UrbEx1M},\n  year = {2024},\n  publisher = {hlky},\n  journal = {Hugging Face repository},\n  howpublished = {\\url{[https://huggingface.co/datasets/bigdata-pw/UrbEx1M](https://huggingface.co/datasets/bigdata-pw/UrbEx1M)}}\n}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/UrbEx1M.","url":"https://huggingface.co/datasets/bigdata-pw/UrbEx1M","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","image-to-text","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ShortVid-Bench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tShortVid-Bench\n\t\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting benchmarks often fall short in capturing the nuanced complexities\nof user-generated content. To rigorously evaluate modelâ€™s ability to understand real-world short videos,\nwe construct a specialized benchmark named ShortVid-Bench. Specifically, we develop an automated pipeline\nto generate multi-dimensional questions for each video, targeting capabilities that signify a deep, holistic\ncomprehension through integrating both visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/ShortVid-Bench.","url":"https://huggingface.co/datasets/TencentARC/ShortVid-Bench","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","Video"],"keywords_longer_than_N":true},
	{"name":"MMPR-Tiny","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMMPR-Tiny\n\t\n\nThis is the training data used during the online RL stage of InternVL3.5, which greatly improves the overall performance of InternVL3.5 across all scales. Our training code is also open-sourced.\nBased on MMPR-v1.2, we compute the accuracy of each query using the provided rollouts and select those whose model accuracy falls between 0.2 and 0.8 for online RL.\nWe further extend the dataset with recent multimodal datasets to enhance diversity.\nPlease refer to our paper forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"furniture-model-extraction","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFurniture Model Number Extraction Dataset\n\t\n\nThis dataset contains furniture inventory images with corresponding model numbers for training vision-language models to extract product model numbers from furniture store photos.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCreated: 2025-09-24T12:26:54.209816\nTask: Vision-Language Model Training for Model Number Extraction\nBase Model: IBM Granite Vision 3.2 2B\nDomain: Furniture Inventory Management\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTraining Samples: 219â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wynnwatson/furniture-model-extraction.","url":"https://huggingface.co/datasets/wynnwatson/furniture-model-extraction","creator_name":"wynn watson","creator_url":"https://huggingface.co/wynnwatson","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SID-0.5k","keyword":"image-to-text","description":"\n\n      \n442 PNG-encoded images generated using a custom SD-XL finetune which I deleted some time ago.  \nPrompts for each image can be found in the image's metadata.\n\nData you might find useful:\n\n| Width | Height | Sampler | Steps |\n| ----- | ------ | ------- | ----- |\n| 768px | 1280px | Euler A | 30    |\n    \n  \n\n  \n","url":"https://huggingface.co/datasets/Hamzah-Asadullah/SID-0.5k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"GAIA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŒ GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis\n\t\n\n\nThis repository contains the pre-trained model weights, associated code, and complete dataset of the paper GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis.\nGAIA is a large-scale vision-language dataset designed to bridge the gap between remote sensing (RS) imagery and natural language understanding. It provides 205,150 image-textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/azavras/GAIA.","url":"https://huggingface.co/datasets/azavras/GAIA","creator_name":"Angelos Zavras","creator_url":"https://huggingface.co/azavras","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"VideoGameQA-Bench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance\n\t\n\nby Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer\n\n\t\n\t\t\n\t\tAbstract:\n\t\n\n\nWith video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/VideoGameQA-Bench.","url":"https://huggingface.co/datasets/taesiri/VideoGameQA-Bench","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VideoGameQA-Bench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance\n\t\n\nby Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer\n\n\t\n\t\t\n\t\tAbstract:\n\t\n\n\nWith video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/VideoGameQA-Bench.","url":"https://huggingface.co/datasets/taesiri/VideoGameQA-Bench","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HUVER","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for HUVER\n\t\n\n\n\nThe dataset is comprised of a 6,051 unique UAV configurations, where each configuration is described by multiple data for-\nmats, including a grammar string, an RGB image, and an GLB file.\nComplementing these representation modalities, we also provide a configuration-based description, i.e., a text descriptor describing the features of each UAV using natural language\n\nCurated by: Abhiram Karri, Gary Stump, Christopher McComb, Binyang Song\n\nLanguage(s) (NLP):â€¦ See the full description on the dataset page: https://huggingface.co/datasets/raiselab/HUVER.","url":"https://huggingface.co/datasets/raiselab/HUVER","creator_name":"RAISE Lab","creator_url":"https://huggingface.co/raiselab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-3d","image-feature-extraction","text-to-3d","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"EmbodiedEval","keyword":"video-text-to-text","description":"This repository contains the dataset of the paper EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents.\nGithub repository: https://github.com/thunlp/EmbodiedEval\nProject Page: https://embodiedeval.github.io/\n","url":"https://huggingface.co/datasets/EmbodiedEval/EmbodiedEval","creator_name":"EmbodiedEval","creator_url":"https://huggingface.co/EmbodiedEval","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["robotics","video-text-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v66","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v66.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v66","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-50","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-50","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OmniMMI","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tOmniMMI\n\t\n\nPaper: OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts\nCode\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nwe introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across sixâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigai-nlco/OmniMMI.","url":"https://huggingface.co/datasets/bigai-nlco/OmniMMI","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","1K - 10K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v49","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v49.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CorDiCas","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCorDiCas\n\t\n\nCorDiCas es un prototipo de corpus diacrÃ³nico cuyos documentos proceden de una colecciÃ³n de mÃ¡s de 120 documentos inÃ©ditos de carÃ¡cter semiprivado, cuya temÃ¡tica gira en torno a la sedentarizaciÃ³n e inserciÃ³n forzosas de la poblaciÃ³n gitana durante el siglo XVIII. \nEn la siguiente tabla se ofrece la informaciÃ³n estructurada sobre los periodos que se abordan en la colecciÃ³n:\n\n\t\n\t\t\nSignatura\nPeriodo\nN.Âº textos\n\n\n\t\t\nAMH_01430\n1745 - 1746\n4 textos\n\n\n\n1748\n14 textos\n\n\n\n1749\nMÃ¡sâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/epuertas94/CorDiCas.","url":"https://huggingface.co/datasets/epuertas94/CorDiCas","creator_name":"Elia Puertas RibÃ©s","creator_url":"https://huggingface.co/epuertas94","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","image-to-text","Spanish","mit"],"keywords_longer_than_N":true},
	{"name":"coda-lm-llava-format","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCODA-LM Dataset Card\n\t\n\nCODA-LM is the multi-modal version of the CODA dataset, used in the CODA-LM paper. Both English and Chinese annotations are available. Check detailed usage in our Github repo.\nThis repo contains the CODA-LM dataset, which has been reorganized in the LLaVA data format. \nYou are also welcome to check the original CODA-LM data which contains more metadata vanilla annotations. \n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# name can be selected fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KaiChen1998/coda-lm-llava-format.","url":"https://huggingface.co/datasets/KaiChen1998/coda-lm-llava-format","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v132","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v132.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v132","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"playground","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Playground\n\t\n\n~273.5M image generations.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 273.5 million images generated using Playground AI. Entries include generation details such as prompts and model used, anonymized user information, creation date, and URL to the image.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nid: Unique identifier for the image\nuserId: Identifer for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/playground.","url":"https://huggingface.co/datasets/bigdata-pw/playground","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"SIV-Bench","keyword":"video-text-to-text","description":"This repository contains the dataset for the paper SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning.\nProject page: https://kfq20.github.io/sivbench/\nCode: https://github.com/kfq20/SIV-Bench\n","url":"https://huggingface.co/datasets/Fancylalala/SIV-Bench","creator_name":"KongFanqi","creator_url":"https://huggingface.co/Fancylalala","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","Video"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"random-graphs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRandom Graphs\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nOver 20â€‰000 randomly generated directed graphs with labels and formatting assigned at random.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nGeneration method: ErdÅ‘sâ€“RÃ©nyi random graph model  \nVertices: 2 to 15 per graph  \nEdge probability: Uniformly random between 0.0 and 1.0  \nLabels: Sampled from the agentlans/noun-phrases dataset\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry includes:\n\nimage: PNG rendering of the graph  \ndot: Graphviz DOT source code  \nlisp:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/random-graphs.","url":"https://huggingface.co/datasets/agentlans/random-graphs","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","feature-extraction","English","apache-2.0","Image"],"keywords_longer_than_N":true},
	{"name":"upvoteweb-posts","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tupvoteweb: posts\n\t\n\nPosts in upvoteweb.\n\n\t\n\t\t\n\t\tconfigs\n\t\n\n\n[!IMPORTANT]There are several configs representing different permutations of this dataset. Load the relevant config for the task you are interested in.\n\nOverview of configs:\n\ndefault: largely unfiltered/unprocessed original data\neduscored: the \"eduscore\" predicted on the text column with huggingface's trained classifier\nen-clean: filter language for en and language_score for > 0.6. Run clean-text on the text col, preservingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BEE-spoke-data/upvoteweb-posts.","url":"https://huggingface.co/datasets/BEE-spoke-data/upvoteweb-posts","creator_name":"BEEspoke Data","creator_url":"https://huggingface.co/BEE-spoke-data","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","feature-extraction","image-to-text","text-to-image","fill-mask"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v196","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v196.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v196","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v19","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v19.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SynthVPT","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVPT-Synth-Objects: A Synthetic Dataset for Visual Perspective Taking\n\t\n\nThis is a proof-of-concept synthetic dataset designed for training socio-cognitive foundational models for robotics, specifically in Visual Perspective Taking (VPT). The core task is to enable a robot to infer an object's 6D pose (position and orientation) relative to another agent, given a single RGB image.\nThis dataset was generated using NVIDIA Isaac Sim and Omniverse Replicator. Each entry provides an imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jwgcurrie/SynthVPT.","url":"https://huggingface.co/datasets/jwgcurrie/SynthVPT","creator_name":"Joel Currie","creator_url":"https://huggingface.co/jwgcurrie","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-3d","robotics","image-feature-extraction","depth-estimation","image-to-text"],"keywords_longer_than_N":true},
	{"name":"RICO-Screen2Words","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Screen2Words\n\t\n\nScreen2Words is a dataset providing screen summaries (i.e., image captions for mobile screens). \nIt uses the RICO image database. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository:\ngoogle-research-datasets/screen2words\nRICO raw downloads\n\n\nPaper:\nScreen2Words: Automatic Mobile UI Summarization with Multimodal Learning\nRico: A Mobile App Dataset for Building Data-Driven Design Applications\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses\n\t\n\nThis dataset is forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-Screen2Words.","url":"https://huggingface.co/datasets/rootsautomation/RICO-Screen2Words","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"imagenet-1k-vl-enriched","keyword":"image-to-text","description":"\n  \n    Visualize on Visual Layer\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tImagenet-1K-VL-Enriched\n\t\n\nAn enriched version of the ImageNet-1K Dataset with image caption, bounding boxes, and label issues!\nWith this additional information, the ImageNet-1K dataset can be extended to various tasks such as image retrieval or visual question answering.\nThe label issues helps to curate a cleaner and leaner dataset.\n\n\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 6 columns:\n\nimage_id: The original filename of the image fromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched.","url":"https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched","creator_name":"Visual Layer","creator_url":"https://huggingface.co/visual-layer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-classification","text-to-image","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"fire_intensity","keyword":"image-to-text","description":"farhadzare/fire_intensity dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/farhadzare/fire_intensity","creator_name":"Farhad Zare","creator_url":"https://huggingface.co/farhadzare","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","English","mit","n<1K"],"keywords_longer_than_N":true},
	{"name":"waqfeya-library-compressed","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tWaqfeya Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nWaqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 10,000 PDF books across over 80 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/waqfeya-library, with one key difference: the contentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/waqfeya-library-compressed.","url":"https://huggingface.co/datasets/ieasybooks-org/waqfeya-library-compressed","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Seed","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the seed dataset we used to conduct Google Search.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Recraft-v3-24-7-25_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Recraft v3 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft v3 (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-v3-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"VLM-1B","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VLM-1B\n\t\n\n\n\nVLM-1B is a large-scale image-text dataset that has been recaptioned using an SFT-enhanced Qwen2VL model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: [https://zxwei.site/hqclip/)\n\n\n\n\t\n\t\t\n\t\tUsage Guide\n\t\n\nSee https://github.com/w1oves/hqclip/blob/main/README.md#dataset-usage-guide.\n","url":"https://huggingface.co/datasets/zhixiangwei/VLM-1B","creator_name":"zhixiangwei","creator_url":"https://huggingface.co/zhixiangwei","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100M<n<1B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Sparrow-Synthetic","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Sparrow-Synthetic\n\t\n\n\n\nProject Links:\nðŸ’» GitHub | ðŸ“‘ Paper | ðŸ¤— Hugging Face Collection\nThis is a synthetic \"video\" instruction dataset derived from language data.\nIt is designed to enrich the instruction diversity of video training corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThis dataset is curated from longQA text datasets, LongAlpaca-12k and LongQLoRA-39K.\nEach sample can be abstracted as a (long-context, instruction, answer) triplet:\n\nlong-context: a series of imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic.","url":"https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic","creator_name":"Shukang Yin","creator_url":"https://huggingface.co/xjtupanda","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2411.19951"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v37","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v37.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"7SEG_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nTRAIN ONLY Synthetic dataset of seven-segment indicators. Useful for detecting numbers on devices.\n","url":"https://huggingface.co/datasets/MiXaiLL76/7SEG_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v72","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v72.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v72","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v99","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v99.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v99","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v40","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v40.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v206","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v206.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v206","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v75","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v75.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v75","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Latex-KIE","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLatex-KIE Dataset\n\t\n\nThe Latex-KIE dataset is a large-scale collection of paired LaTeX formula images and their corresponding LaTeX code. It is specifically designed for training and evaluating models for Image-to-LaTeX, Key Information Extraction (KIE), and Optical Character Recognition (OCR) tasks in scientific domains.\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Summary\n\t\n\n\nImages: Rendered LaTeX math formulas (black text on white background)\nText: Corresponding raw LaTeX code for each image\nSplit: trainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Latex-KIE.","url":"https://huggingface.co/datasets/prithivMLmods/Latex-KIE","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"filtered_deepseek_v31_referring_expression_parsing","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDeepSeek v3.1 Quality-Filtered Referring Expression Parsing + Distractor Labels\n\t\n\nThis dataset contains parsed referring expressions from the RefCOCO, RefCOCOg, and RefCOCO+ validation sets, processed using DeepSeek v3.1 with quality filtering, plus corresponding distractor label annotations in COCO format.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nModel: DeepSeek v3.1 (deepseek-chat)\nProcessing: Quality-filtered results from referring expression parsing\nDatasets: RefCOCOâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dddraxxx/filtered_deepseek_v31_referring_expression_parsing.","url":"https://huggingface.co/datasets/dddraxxx/filtered_deepseek_v31_referring_expression_parsing","creator_name":"Drax","creator_url":"https://huggingface.co/dddraxxx","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","object-detection","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"EditReward-Bench","keyword":"image-to-text","description":"To be released.\n","url":"https://huggingface.co/datasets/TIGER-Lab/EditReward-Bench","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"ChartMimic","keyword":"image-to-text","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic.\n\nKind Note: ChartMimic has been integrated into VLMEvalKit. Welcome to use ChartMimic through VLMEvalKit! Special thanks to the VLMEvalKit team.\n\n\n\t\n\t\t\n\t\n\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChartMimic/ChartMimic.","url":"https://huggingface.co/datasets/ChartMimic/ChartMimic","creator_name":"ChartMimic","creator_url":"https://huggingface.co/ChartMimic","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"AgentSynth","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAgentSynth\n\t\n\n\n\t\n\t\t\n\t\tAgentSynth: Scalable Task Generation for Generalist Computer-Use Agents\n\t\n\nPaper | Project Page | Code\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sunblaze-ucb/AgentSynth.","url":"https://huggingface.co/datasets/sunblaze-ucb/AgentSynth","creator_name":"sunblaze-ucb","creator_url":"https://huggingface.co/sunblaze-ucb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"xflickrco","keyword":"image-to-text","description":"floschne/xflickrco dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/floschne/xflickrco","creator_name":"Flo Schneider","creator_url":"https://huggingface.co/floschne","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","German","English","Spanish","Indonesian"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"finevision-sample","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFineVision Sample Dataset\n\t\n\nA comprehensive multimodal dataset containing samples across multiple categories, designed for visual question answering and multimodal understanding tasks. This dataset follows the same format as the official HuggingFaceM4/FineVision dataset.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset is organized into separate folders for each source category, making it easy to load specific subsets of the data.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach sample contains:\n\nid: Uniqueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/dinesh-vlmrun/finevision-sample.","url":"https://huggingface.co/datasets/dinesh-vlmrun/finevision-sample","creator_name":"vlmrun","creator_url":"https://huggingface.co/dinesh-vlmrun","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"malayalam_ocr_printed_v1","keyword":"image-to-text","description":"ussooraj/malayalam_ocr_printed_v1 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ussooraj/malayalam_ocr_printed_v1","creator_name":"U S Sooraj","creator_url":"https://huggingface.co/ussooraj","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Malayalam","mit","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Vl-RewardBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VLRewardBench\n\t\n\nProject Page:\nhttps://vl-rewardbench.github.io\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nVLRewardBench is a comprehensive benchmark designed to evaluate vision-language generative reward models (VL-GenRMs) across visual perception, hallucination detection, and reasoning tasks. The benchmark contains 1,250 high-quality examples specifically curated to probe model limitations.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach instance consists of multimodal queries spanning three keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhihui/Vl-RewardBench.","url":"https://huggingface.co/datasets/Zhihui/Vl-RewardBench","creator_name":"Xie","creator_url":"https://huggingface.co/Zhihui","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"shamela-waqfeya-library-compressed","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tShamela Waqfeya Library - Compressed\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nShamela Waqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 4,500 PDF books across over 40 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThis dataset is identical to ieasybooks-org/shamela-waqfeya-library, with one keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library-compressed.","url":"https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library-compressed","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"cat_breed","keyword":"image-to-text","description":"zahidpichen/cat_breed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zahidpichen/cat_breed","creator_name":"ninjagamer","creator_url":"https://huggingface.co/zahidpichen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","n<1K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"COCO2017-captions","keyword":"image-to-text","description":"AISNP/COCO2017-captions dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/AISNP/COCO2017-captions","creator_name":"AI Security and Privacy Laboratory","creator_url":"https://huggingface.co/AISNP","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Pexels_Gemini_capitoned","keyword":"image-to-text","description":"This dataset features a collection of high-quality images sourced from Pexels and captioned using the Gemini-1.5-Flash API. This dataset is designed to provide accurate, detailed descriptions of various visual content, suitable for text-to-image tasks, training AI models, and more.\nGemini promt:\n\"Describe this image, for a text-to-image train to be accurate, max 74 tokens. (the common theme between these images is '{theme}'), prefer the use of ',' dont use '.' and there is no need to have aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned.","url":"https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned","creator_name":"Pixel Dust","creator_url":"https://huggingface.co/Pixel-Dust","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"publicdomainpictures","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Public Domain Pictures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 644,412 public domain images from publicdomainpictures.net, a public domain photo sharing platform. The dataset includes detailed image metadata including titles, descriptions, and keywords.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All metadata including titles, descriptions and keywords\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe metadata forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/publicdomainpictures.","url":"https://huggingface.co/datasets/nyuuzyou/publicdomainpictures","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestone-horizon/photo-aesthetics","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"ViDoSeek","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tðŸš€Overview\n\t\n\nThis is the Repo for ViDoSeek, a benchmark specifically designed for visually rich document retrieval-reason-answer, fully suited for evaluation of RAG within large document corpus. \n\nThe paper is available at https://arxiv.org/abs/2502.18017.\nViDoRAG Project: https://github.com/Alibaba-NLP/ViDoRAG\n\nViDoSeek sets itself apart with its heightened difficulty level, attributed to the multi-document context and the intricate nature of its content types, particularly theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/autumncc/ViDoSeek.","url":"https://huggingface.co/datasets/autumncc/ViDoSeek","creator_name":"QiuchenWang","creator_url":"https://huggingface.co/autumncc","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","visual-document-retrieval","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"merit","keyword":"image-to-text","description":"\n  \n\n\n\n\t\n\t\t\n\t\tThe MERIT Dataset ðŸŽ’ðŸ“ƒðŸ†\n\t\n\nThe MERIT Dataset is a multimodal dataset (image + text + layout) designed for training and benchmarking Large Language Models (LLMs) on Visually Rich Document Understanding (VrDU) tasks. It is a fully labeled synthetic dataset generated using our opensource pipeline available on GitHub. You can explore more details about the dataset and pipeline reading our paper.\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction â„¹ï¸\n\t\n\nAI faces some dynamic and technical issues that pushâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/de-Rodrigo/merit.","url":"https://huggingface.co/datasets/de-Rodrigo/merit","creator_name":"de Rodrigo","creator_url":"https://huggingface.co/de-Rodrigo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","image-to-text","English","Spanish","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React-Multi-Images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React-Multi-Images","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CLINTOX-V-SMILES-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/CLINTOX-V-SMILES-2.","url":"https://huggingface.co/datasets/molvision/CLINTOX-V-SMILES-2","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"image-wallpapers-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNavanjana/image-wallpapers-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality images paired with descriptive text annotations, designed for computer vision and multimodal machine learning tasks. Each image has been preprocessed to standard dimensions and paired with detailed descriptions extracted from web sources.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Images: [NUMBER] images\nImage Format: JPEG (RGB)\nImage Dimensions: 224Ã—224 pixels\nText Descriptions: Naturalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset.","url":"https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-feature-extraction","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"coco-pic","keyword":"image-to-text","description":"rasoulasadianub/coco-pic dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/rasoulasadianub/coco-pic","creator_name":"rasoul asadian","creator_url":"https://huggingface.co/rasoulasadianub","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","Persian","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ramp","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Retrieval-Augmented Modular Prompt Tuning for Low-Resource Data-to-Text Generation (RAMP)\n\t\n\nHugging Face Dataset | GitHub Repository | paper | Gitlab Repository \n\n\nRAMP provides a prepared version of a low-resource data-to-text corpus for drone handover message generation: structured sensor records (status + time-step object lists) paired with natural-language â€œhandoverâ€ messages describing critical situations. The release includes raw/filtered splits andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/ramp.","url":"https://huggingface.co/datasets/tonyhong/ramp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","monolingual","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayukaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lodestone-horizon/pixelprose.","url":"https://huggingface.co/datasets/lodestone-horizon/pixelprose","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"prophet-mosque-library","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tProphet's Mosque Library\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nProphetâ€™s Mosque Library is one of the primary resources for Islamic books. It hosts more than 48,000 PDF books across over 70 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThe dataset includes 70,884 PDF files (spanning 23,494,042 pages) representing 48,717 Islamic books. Each book isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library.","url":"https://huggingface.co/datasets/ieasybooks-org/prophet-mosque-library","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"TableEval","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tTableEval dataset\n\t\n\n\n\n\nTableEval is developed to benchmark and compare the performance of (M)LLMs on tables from scientific vs. non-scientific sources, represented as images vs. text. \nIt comprises six data subsets derived from the test sets of existing benchmarks for question answering (QA) and table-to-text (T2T) tasks, containing a total of 3017 tables and 11312 instances. \nThe scienfific subset includes tables from pre-prints and peer-reviewed scholarly publications, while theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/katebor/TableEval.","url":"https://huggingface.co/datasets/katebor/TableEval","creator_name":"Ekaterina Borisova","creator_url":"https://huggingface.co/katebor","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"TDC_training_data","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tTraining data used in TDC.\n\t\n\nPaper Link:\nhttps://arxiv.org/pdf/2504.10443\nProject Page:\nhttps://hoar012.github.io/TDC-Project\nCode:\nhttps://github.com/Hoar012/TDC-Video\nThis repository contains the data used for training and evaluation in Multimodal Long Video Modeling Based on Temporal Dynamic Context.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@misc{hao2025multimodallongvideomodeling,\n        title={Multimodal Long Video Modeling Based on Temporal Dynamic Context}, \n        author={Haoran Hao andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hoar012/TDC_training_data.","url":"https://huggingface.co/datasets/Hoar012/TDC_training_data","creator_name":"Haoran Hao","creator_url":"https://huggingface.co/Hoar012","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[ arXiv paper ]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Details\n\t\n\nTotal number of image-caption pairs: 16,896,214 (16.9M)\n\n6,538,898 (6.5M) pairs in the split of CommonPool\n9,066,455 (9.1M) pairs in the split of CC12M\n1,290,861 (1.3M) pairs inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tomg-group-umd/pixelprose.","url":"https://huggingface.co/datasets/tomg-group-umd/pixelprose","creator_name":"Tom Goldstein's Lab at University of Maryland, College Park","creator_url":"https://huggingface.co/tomg-group-umd","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"xAI_Aurora_t2i_human_preferences","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Aurora Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 400k human responses from over 86k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Aurora across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/xAI_Aurora_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"ChartGenBench_cot","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSTEM Image Captions Dataset\n\t\n\nThis dataset contains AI-generated captions for STEM images.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in batches:\n\nTotal batches: 1\nEach batch is stored in a separate directory (batch_0000, batch_0001, etc.)\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nTotal items: 1042\nSuccessfully captioned: 1042\nErrors: 0\nSkipped: 0\n\n\n\t\n\t\t\n\t\tLoading the Dataset\n\t\n\nYou can load individual batches:\nfrom datasets import load_dataset\n\n# Load a specific batch\nbatch_0 =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JackyZhuo/ChartGenBench_cot.","url":"https://huggingface.co/datasets/JackyZhuo/ChartGenBench_cot","creator_name":"Le Zhuo","creator_url":"https://huggingface.co/JackyZhuo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Book-Scan-OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBest Usage\n\t\n\n\nSuitable for fine-tuning Vision-Language Models (e.g., PaliGemma).\n\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\nThis dataset was generated using Mistral OCR and Google Lens, followed by manual cleaning for improved accuracy.  \n\n\t\n\t\t\n\t\tImage Source\n\t\n\nImages are sourced from Sarvam.ai.  \n","url":"https://huggingface.co/datasets/MLap/Book-Scan-OCR","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v35","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v35.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SynthFormulaNet","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSynthFormulaNet\n\t\n\n\n    \n\n\nSynthFormulaNet is a multimodal dataset designed for training the SmolDocling model. It contains over 6.4 million pairs of synthetically rendered images depicting mathematical formulas and their corresponding LaTeX representations. The LaTeX data was collected from permissively licensed sources, and the images were generated using LaTeX at 120â€¯DPI with diverse rendering styles, fonts, and layout configurations to maximize visual variability. This dataset alsoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ds4sd/SynthFormulaNet.","url":"https://huggingface.co/datasets/ds4sd/SynthFormulaNet","creator_name":"Docling","creator_url":"https://huggingface.co/ds4sd","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cdla-permissive-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v108","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v108.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v108","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HunyuanImage-2.1_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Hunyuan Image 2.1 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~50'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Hunyuan Image 2.1 (version from 19.9.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/HunyuanImage-2.1_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 8\n\t\n\nReplaced RLE compressedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LAION-SG","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for LAION-SG\n\t\n\n\n\n\n LAION-SG is a large-scale dataset with high-quality structural annotations of scene graphs (SG), which precisely describe attributes and relationships of multiple objects, effectively representing the semantic structure in complex scenes.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\n\n\n\n\nLanguage(s) : All of annotations use English as primary language.\n\nLicense: MIT License.\n\n\nRepository: https://github.com/mengcye/LAION-SG?tab=readme-ov-file\n\nPaper:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mengcy/LAION-SG.","url":"https://huggingface.co/datasets/mengcy/LAION-SG","creator_name":"Chenye Meng","creator_url":"https://huggingface.co/mengcy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v194","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v194.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v194","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ShowUI-web-8k","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tShowUI-web-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original ShowUI-web dataset, as mentioned in our paper. It contributes to the training of GUI grounding models, with a focus on realistic web user interfaces collected from diverse websites.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Sampled from ShowUI-web  \nDomain: Web GUI screenshots  \nDiversity: Covers a wide variety of website layouts and components  \nUse case: GUI grounding pretraining for web environmentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/ShowUI-web-8k.","url":"https://huggingface.co/datasets/zonghanHZH/ShowUI-web-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ConsoleScreenshots","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ConsoleScreenshots\n\t\n\n~59k console video game screenshots\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n58909 video game screenshots from older consoles in WebDataset format. Entries include the name and platform. Some entries have more than one platform listed.\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{ConsoleScreenshots,\n  author = {hlky},\n  title = {ConsoleScreenshots},\n  year =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/ConsoleScreenshots.","url":"https://huggingface.co/datasets/bigdata-pw/ConsoleScreenshots","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"megalith-10m-florence2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMegalith-10M with Florence-2 Caption\n\t\n\næ—¥æœ¬èªžã¯ã“ã¡ã‚‰\nThis reposity is the supplymentary of Megalith-10M.\nMegalith-10M is an CC-0 like image dataset. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/megalith-10m-florence2\")\n\n\n\t\n\t\n\t\n\t\tHow to get images\n\t\n\ngit lfs install\ngit clone https://huggingface.co/datasets/drawthingsai/megalith-10mâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/megalith-10m-florence2.","url":"https://huggingface.co/datasets/aipicasso/megalith-10m-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v102","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v102.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v102","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v94","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v94.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v94","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"GenS-Video-150K","keyword":"video-text-to-text","description":"\nðŸ”— Project Page Â· ðŸ“– Paper Â· â­ GitHub Â· ðŸ“Š Dataset Â· ðŸ¤— Checkpoints\n\n\n\n\n\t\n\t\t\n\t\tGenS-Video-150K Dataset\n\t\n\nTo enable effective frame sampling, we introduce GenS-Video-150K, a large-scale synthetic dataset specifically designed for training frame sampling models. Annotated by GPT-4o, this dataset features:\n\nDense coverage: Annotates ~20% of all frames with relevance scores.\nFine-grained assessment: Assigns confidence scores (level 1 to 5) to relevant frames.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yaolily/GenS-Video-150K.","url":"https://huggingface.co/datasets/yaolily/GenS-Video-150K","creator_name":"Linli Yao","creator_url":"https://huggingface.co/yaolily","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2503.09146"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"image-to-text","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the fileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vwp","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Visual Writing Prompts Dataset (VWP)\n\t\n\nWebsite | Github Repository | arXiv e-Print\n\n\nThe Visual Writing Prompts (VWP) dataset contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which are collected via crowdsourcing given the image sequences and up to 5  grounded characters from the corresponding image sequence.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Links\n\t\n\n\n\n\nTACLâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tonyhong/vwp.","url":"https://huggingface.co/datasets/tonyhong/vwp","creator_name":"Xudong Hong","creator_url":"https://huggingface.co/tonyhong","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","monolingual","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"pokemon-gpt4-1k","keyword":"image-to-text","description":"This dataset was modified from diffusers/pokemon-gpt4-captions and contains 1k PokÃ©mon-related image-captioning instruction data points.\nYou can organize content in the dataset_info.json in LLaMA Factory like this:\n\"pokemon_1k\": {\n  \"hf_hub_url\": \"BUAADreamer/pokemon-gpt4-1k\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"messages\",\n    \"images\": \"images\"\n  },\n  \"tags\": {\n    \"role_tag\": \"role\",\n    \"content_tag\": \"content\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BUAADreamer/pokemon-gpt4-1k.","url":"https://huggingface.co/datasets/BUAADreamer/pokemon-gpt4-1k","creator_name":"Zhangchi Feng","creator_url":"https://huggingface.co/BUAADreamer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Medical_Multimodal_Evaluation_Data","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEvaluation Guide\n\t\n\nThis dataset is used to evaluate medical multimodal LLMs, as used in HuatuoGPT-Vision. It includes benchmarks such as VQA-RAD, SLAKE, PathVQA, PMC-VQA, OmniMedVQA, and MMMU-Medical-Tracks.  \nTo get started:  \n\nDownload the dataset and extract the images.zip file.  \nFind evaluation code on our GitHub: HuatuoGPT-Vision.\n\nThis open-source release aims to simplify the evaluation of medical multimodal capabilities in large models. Please cite the relevant benchmarkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/Medical_Multimodal_Evaluation_Data.","url":"https://huggingface.co/datasets/FreedomIntelligence/Medical_Multimodal_Evaluation_Data","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"llava-pretrain-refined-by-data-juicer","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLLaVA pretrain -- LCS-558k (refined by Data-Juicer)\n\t\n\nA refined version of LLaVA pretrain dataset (LCS-558k) by Data-Juicer. Removing some \"bad\" samples from the original dataset to make it higher-quality.\nThis dataset is usually used to pretrain a Multimodal Large Language Model.\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 115MB).\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Information\n\t\n\n\nNumber of samples: 500,380 (Keep ~89.65% from the original dataset)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer.","url":"https://huggingface.co/datasets/datajuicer/llava-pretrain-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MaCBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMaCBench\n\t\n\n\n\n\n\n\n\n\n\n\nA Chemistry and Materials Benchmark for evaluating Vision Large Language Models\n\n\n\n\n\t\n\t\t\n\t\tâš ï¸ IMPORTANT NOTICE - NOT FOR TRAINING\n\t\n\n\n\n\n\t\n\t\t\n\t\tðŸš« THIS DATASET IS STRICTLY FOR EVALUATION PURPOSES ONLY ðŸš«\n\t\n\nDO NOT USE THIS DATASET FOR TRAINING OR FINE-TUNING MODELS\nThis benchmark is designed exclusively for evaluation and testing of existing models. Using this data for training would compromise the integrity of the benchmark and invalidate evaluation results. Pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jablonkagroup/MaCBench.","url":"https://huggingface.co/datasets/jablonkagroup/MaCBench","creator_name":"Lab of Kevin Jablonka at Uni Jena","creator_url":"https://huggingface.co/jablonkagroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multiple-choice","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Live-CC-5M","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Live-CC-5M\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for LiveCC-7B-Base model pre-training. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tLive-CC-5M Dataset\n\t\n\n\nStatistics: 5,047,208 YouTube Video-CC 30~240s samples.\n\n\nAnnotationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-CC-5M.","url":"https://huggingface.co/datasets/chenjoya/Live-CC-5M","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1M<n<10M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"KVG","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n\n \n \n \nThis is the official repository of KVG training data for DeepPerception.\n","url":"https://huggingface.co/datasets/MaxyLee/KVG","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"UnifiedWasteClassificationDataset","keyword":"image-text-to-text","description":"sidmaji/UnifiedWasteClassificationDataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/sidmaji/UnifiedWasteClassificationDataset","creator_name":"Siddhant Maji","creator_url":"https://huggingface.co/sidmaji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-feature-extraction","image-to-image","image-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"IncivilityCaps","keyword":"image-to-text","description":"ZhaoYeP/IncivilityCaps dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/ZhaoYeP/IncivilityCaps","creator_name":"yepyep","creator_url":"https://huggingface.co/ZhaoYeP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"UnifiedWasteClassificationDataset","keyword":"image-to-text","description":"sidmaji/UnifiedWasteClassificationDataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/sidmaji/UnifiedWasteClassificationDataset","creator_name":"Siddhant Maji","creator_url":"https://huggingface.co/sidmaji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-feature-extraction","image-to-image","image-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"CalliTrain","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ§  CalliReader: Contextualizing Chinese Calligraphy via an Embedding-aligned Vision Language Model\n\t\n\n\n  ðŸ“‚ Code\n  ðŸ“„ Paper\n\n\nCalliBench is aimed to comprehensively evaluate VLMs' performance on the recognition and understanding of Chinese calligraphy. \n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Summary\n\t\n\n\nSamples: 3,192 imageâ€“annotation pairs\n\nTasks: Full-page recognition and Contextual VQA (choice of author/layout/style, bilingual interpretation, and intent analysis).\n\nAnnotations:\n\nMetadata of authorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gtang666/CalliTrain.","url":"https://huggingface.co/datasets/gtang666/CalliTrain","creator_name":"TJQ","creator_url":"https://huggingface.co/gtang666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Tecky-UI-Elements-VLM","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tTecky UI Automation Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸ§  Tecky: Your True Virtual Employee\n\t\n\nTecky is an AI-powered virtual teammate that lives on your machine. It learns how you interact with apps and automates workflows across both public tools and internal software.\nTecky acts as a context-aware agent, observing how users complete digital tasks and suggesting or executing them through a human-like interface.\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Overview\n\t\n\nThis dataset is the foundation for trainingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tecky-tech/Tecky-UI-Elements-VLM.","url":"https://huggingface.co/datasets/tecky-tech/Tecky-UI-Elements-VLM","creator_name":"Tecky","creator_url":"https://huggingface.co/tecky-tech","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"popp","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPOPP datasets\n\t\n\nThis repository contains 3 datasets created by the LITIS lab (University of Rouen Normandie) within the POPP project (Project for the Oceration of the Paris Population Census) for the task of handwriting text recognition.\nThese datasets have been published in Recognition and information extraction in historical handwritten tables: toward understanding early 20th century Paris census at DAS 2022 from T. Constum et al and are also available on Zenodo.\nThe 3 datasets areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thomas-C/popp.","url":"https://huggingface.co/datasets/thomas-C/popp","creator_name":"Thomas Constum","creator_url":"https://huggingface.co/thomas-C","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"da-wiki-icc","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for â€œDanish Wikipedia â€” Image, Caption, Contextâ€\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset contains images from Danish Wikipedia articles paired with:\n\ncaptions â€” the local image caption \nneighbouring_context â€” the surrounding section/block text where the image appears in markdown\nfull text â€” the full article markdown, stored once per article, with all other rows pointing to the canonical copy via full_text_row_id\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tFiles & Splitsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/V4ldeLund/da-wiki-icc.","url":"https://huggingface.co/datasets/V4ldeLund/da-wiki-icc","creator_name":"Vladimir Salnikov","creator_url":"https://huggingface.co/V4ldeLund","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","feature-extraction","image-captioning","Danish","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"VideoGameBunny-Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVideoGameBunny Instruction Following Dataset\n\t\n\nPaper - Website\n\n\t\n\t\t\n\t\tOverview\n\t\n\nWe present a comprehensive dataset of 185,259 high-resolution images from 413 video games, sourced from YouTube videos. This dataset addresses the lack of game-specific instruction-following data and aims to improve the ability of open-source models to understand and respond to video game content.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Composition\n\t\n\nOur dataset includes various types of instructions generated for theseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/asgaardlab/VideoGameBunny-Dataset.","url":"https://huggingface.co/datasets/asgaardlab/VideoGameBunny-Dataset","creator_name":"Analytics of Software, Games and Repository Data (ASGAARD) Lab","creator_url":"https://huggingface.co/asgaardlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"APASI-SI-dataset","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAPASI-SI-Dataset: Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations\n\t\n\nThis repository hosts the APASI Self-Injection (SI) Dataset, presented in the paper Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations.\nCode Repository: https://github.com/davidluciolu/APASI\nThe APASI (Autonomous Preference Alignment via Self-Injection) method proposes a novel approach to mitigate hallucinations in Large Vision-Languageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lucio36/APASI-SI-dataset.","url":"https://huggingface.co/datasets/lucio36/APASI-SI-dataset","creator_name":"david lu","creator_url":"https://huggingface.co/lucio36","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"coda-lm","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tCODA-LM Dataset Card\n\t\n\nCODA-LM is the multi-modal version of the CODA dataset, used in the CODA-LM paper. Both English and Chinese annotations are available. Check detailed usage in our Github repo.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@article{li2024automated,\n  title={Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases},\n  author={Li, Yanze and Zhang, Wenhua and Chen, Kai and Liu, Yanxin and Li, Pengxiang and Gao, Ruiyuan and Hong, Lanqing and Tian, Meng andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KaiChen1998/coda-lm.","url":"https://huggingface.co/datasets/KaiChen1998/coda-lm","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Dinosaurs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dinosaurs\n\t\n\nOver 300 dinosaurs ðŸ¦•ðŸ¦–\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n321 dinosaurs ðŸ¦•ðŸ¦– with metadata including name, pronunciation, size, image and more!\nWith thanks to Natural History Museum â¤ï¸\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\t\n\t\t\n\t\tUses\n\t\n\nPotential uses include:\n\nDinosaur classification\nDiffusion model training\n\nIt would be roarsome to use these dinosaur drawings with ControlNet to generate aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/Dinosaurs.","url":"https://huggingface.co/datasets/bigdata-pw/Dinosaurs","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","odc-by","< 1K"],"keywords_longer_than_N":true},
	{"name":"grustnogram","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Grustnogram\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 597,704 posts from Grustnogram.ru, a Russian \"emotional network\" similar to Instagram but with a distinctive black and white filter aesthetic and dark atmosphere. The dataset includes 542,917 image posts with associated metadata and 54,787 anonymous text-only posts.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian (ru).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Splits\n\t\n\nThe dataset is divided intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/grustnogram.","url":"https://huggingface.co/datasets/nyuuzyou/grustnogram","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-classification","multi-label-image-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"pbrpx","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for PBRPX Asset Library\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 710 physically-based rendering (PBR) assets from pbrpx.com, a CC0 asset library. The dataset includes comprehensive information about 3D models, textures, and materials with detailed file listings, download URLs, texture resolutions, and categorization data. Assets include nature elements (trees, rocks), architectural materials (brick, concrete), and various other 3D models withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pbrpx.","url":"https://huggingface.co/datasets/nyuuzyou/pbrpx","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"Perception-Collection","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\nHomepage: https://kaistai.github.io/prometheus-vision/ \nRepository: https://github.com/kaistAI/prometheus-vision \nPaper: https://arxiv.org/abs/2401.06591 \nPoint of Contact: seongyun@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset summary\n\t\n\nPerception Collection is the first multi-modal feedback dataset that could be used to train an evaluator VLM. Perception Collection includes 15K fine-grained criteria that determine the crucial aspect for each instance.\n\n\t\n\t\t\n\t\tLanguagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/Perception-Collection.","url":"https://huggingface.co/datasets/prometheus-eval/Perception-Collection","creator_name":"prometheus-eval","creator_url":"https://huggingface.co/prometheus-eval","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","text2text-generation","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Runway_Frames_t2i_human_preferences","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Frames Preference\n\t\n\n\n\n\n\nThis T2I dataset contains roughly 400k human responses from over 82k individual annotators, collected in just ~2 Days using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Frames across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences.","url":"https://huggingface.co/datasets/Rapidata/Runway_Frames_t2i_human_preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestone-horizon/photo-architecture","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"UGround-V1-8k","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tUGround-WebHybrid-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original UGround-V1-Data (Web-Hybrid), as mentioned in our paper. It serves as part of the training corpus for GUI grounding tasks, focusing on diverse web interface screenshots across resolutions and aspect ratios.\n\n\t\n\t\t\n\t\tPaper and Code\n\t\n\n\nPaper: ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding\nCode: https://github.com/zonghanHZH/ZonUI-3B\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/UGround-V1-8k.","url":"https://huggingface.co/datasets/zonghanHZH/UGround-V1-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMBench-GUI","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tðŸ–¥ï¸ MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents\n\t\n\n\n  ðŸ“– PaperÂ Â  | Â Â ðŸ’» CodeÂ Â  | Â Â ðŸ¤— DatasetÂ Â  | Â Â ðŸ“¢ Leaderboard (coming soon)\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tPaper Abstract\n\t\n\nWe introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMBench-GUI.","url":"https://huggingface.co/datasets/OpenGVLab/MMBench-GUI","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","Image","arxiv:2507.19478","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"Qilin","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tQilin\n\t\n\nQilin is a large-scale multimodal dataset designed for advancing research in search, recommendation, and Retrieval-Augmented Generation (RAG) systems. This repository contains the official implementation of the dataset paper, baseline models, and evaluation tools.  This dataset was presented in Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions.\nGithub: https://github.com/RED-Search/Qilin\nThe image data can be found atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THUIR/Qilin.","url":"https://huggingface.co/datasets/THUIR/Qilin","creator_name":"THUIR","creator_url":"https://huggingface.co/THUIR","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-classification","sentence-similarity","text-retrieval","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"MUSTARD","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for MUSTARD\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nMUSTARD (Multilingual Scanned and Scene Table Structure Recognition Dataset) is a diverse dataset curated for table structure recognition across multiple languages. The dataset consists of tables extracted from magazines, including printed, scanned, and scene-text tables, labeled with Optimized Table Structure Language (OTSL) sequences. It is designed to facilitate research in multilingual tableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/badrivishalk/MUSTARD.","url":"https://huggingface.co/datasets/badrivishalk/MUSTARD","creator_name":"Badri Vishal Kasuba","creator_url":"https://huggingface.co/badrivishalk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Hindi","Telugu","Tamil"],"keywords_longer_than_N":true},
	{"name":"MMPR-v1.2","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMMPR-v1.2\n\t\n\n[ðŸ“‚ GitHub] | [ðŸŒ Project Page] | [ðŸ“œ Paper (InternVL3.5)] | [ðŸ“œ Paper (MMPR/MPO)] | [ðŸ†• Blog (MPO)] | [ðŸ“– Documents]\nThis is a newer version of MMPR and MMPR-v1.1, which includes additional data sources to enhance the data diversity and greatly improves the overall performance of InternVL3.5 across all scales. The prompts used to build this dataset is released in MMPR-v1.2-prompts.\nTo unzip the archive of images, please first run cat images.zip_* > images.zip and then runâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2.","url":"https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1M<n<10M","arxiv:2508.18265"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v49","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v49.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"histogram-comparisons-v1","keyword":"image-to-text","description":"If you want a small subset of this dataset, there is histogram-comparisons-small-v1 with 150k rows.\nThis dataset contains 3000000 items in total. There are 3 curriculums each containing 1000000 items.\nEach item is a markdown document.\nEach item contains between 2 and 6 image comparisons, with a Summary at the bottom.\nThe images are between 3x3 and 14x14.\nThe markdown document contains a ## Response, that separates the prompt from the answer.\nThe structure of the markdown document with 3â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/histogram-comparisons-v1.","url":"https://huggingface.co/datasets/neoneye/histogram-comparisons-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TactileNet","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHow to use TactileNet:\n\t\n\n\n\t\n\t\t\n\t\tStep 1: Download the dataset locally\n\t\n\ngit lfs install\ngit clone https://huggingface.co/datasets/MaiAhmed/TactileNet\n\n\n\t\n\t\t\n\t\tStep 2: Install necessary packages\n\t\n\npip install datasets\n\n\n\t\n\t\t\n\t\tStep 3: Load the dataset\n\t\n\nimport os\nfrom datasets import Dataset, Image\n\n\ndef load_data(dataset_path):\n    data = []\n    for root, dirs, files in os.walk(dataset_path):\n        for file in files:\n            if file.endswith(\".jpg\"):\n                # Extractâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MaiAShaaban/TactileNet.","url":"https://huggingface.co/datasets/MaiAShaaban/TactileNet","creator_name":"Mai A. Shaaban","creator_url":"https://huggingface.co/MaiAShaaban","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-image","image-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 6\n\t\n\nOnlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v151","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v151.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v151","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dataset-kid-fr-lexia","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset KID-FR: Documents d'Informations ClÃ©s Financiers en FranÃ§ais\n\t\n\nCe dataset contient une collection de Documents d'Informations ClÃ©s (KID - Key Information Documents) financiers en franÃ§ais. Il est conÃ§u pour l'entraÃ®nement de modÃ¨les de vision par ordinateur et de traitement du langage naturel dans le domaine financier.\n\n\t\n\t\t\n\t\tContenu\n\t\n\nLe dataset se compose de documents financiers structurÃ©s comprenant :\n\nDes images de documents financiers\nLe texte extrait de ces documentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Marsouuu/dataset-kid-fr-lexia.","url":"https://huggingface.co/datasets/Marsouuu/dataset-kid-fr-lexia","creator_name":"Martial Roberge","creator_url":"https://huggingface.co/Marsouuu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"MR-Video","keyword":"video-text-to-text","description":"This repository contains the data presented in MR. Video: \"MapReduce\" is the Principle for Long Video Understanding.\n","url":"https://huggingface.co/datasets/ziqipang/MR-Video","creator_name":"Ziqi Pang","creator_url":"https://huggingface.co/ziqipang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v174","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v174.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v174","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"cgDDI","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNew Official Repo!\n\t\n\nhttps://huggingface.co/datasets/hcarrion/ControllabeGenDDI\n","url":"https://huggingface.co/datasets/hcarrion/cgDDI","creator_name":"HÃ©ctor CarriÃ³n","creator_url":"https://huggingface.co/hcarrion","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","image-to-image","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"Amazon-2023-GenQ","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAmazon Reviews Dataset for Query Generation\n\t\n\nThis dataset is designed for training models on tasks such as query generation, reranking, semantic search, and vision-language tasks (e.g., CLIP, VLMS) using Amazon product metadata.The original datasets can be found here: https://amazon-reviews-2023.github.io/\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a curated sample derived from seven filtered Amazon product category datasets \n(Amazon All Beauty, Amazon Fashion, Sports andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/smartcat/Amazon-2023-GenQ.","url":"https://huggingface.co/datasets/smartcat/Amazon-2023-GenQ","creator_name":"SmartCat","creator_url":"https://huggingface.co/smartcat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","summarization","sentence-similarity","text-classification","text-generation"],"keywords_longer_than_N":true},
	{"name":"video-SALMONN_2_testset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tvideo-SALMONN 2 Benchmark\n\t\n\nGithub Link\nPaper Link\n\nGenerate the caption corresponding to the video and the audio with video_salmonn2_test.json\nOrganize your results in the format like the following example:\n\n[\n    {\n        \"id\": [\"0.mp4\"], \n        \"pred\": \"Generated Caption\"\n    }\n]\n\n\nReplace res_file in eval.py with your result file.\nRun python3 eval.py\n\n","url":"https://huggingface.co/datasets/tsinghua-ee/video-SALMONN_2_testset","creator_name":"Electronic Engineering @Tsinghua University","creator_url":"https://huggingface.co/tsinghua-ee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","Video","arxiv:2506.15220"],"keywords_longer_than_N":true},
	{"name":"M4-IT","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tM4-IT\n\t\n\nThis dataset, M4-IT, is a synthetic instruction finetuning dataset used in the development of the M4 framework, designed to enhance real-time interactive reasoning in multi-modal language models.\nThe M4 framework is evaluated on OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts.\n\n\t\n\t\t\n\t\n\t\n\t\tData Description\n\t\n\nBuilding on the LLaVA-NeXT-Data, we crafted a small video-free synthetic instruction finetuning dataset, M4-IT, with the assistanceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ColorfulAI/M4-IT.","url":"https://huggingface.co/datasets/ColorfulAI/M4-IT","creator_name":"Yuxuan Wang","creator_url":"https://huggingface.co/ColorfulAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2503.22952","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Video-R1-data","keyword":"video-text-to-text","description":"This repository contains the data presented in Video-R1: Reinforcing Video Reasoning in MLLMs.\nCode: https://github.com/tulerfeng/Video-R1\nVideo data folder: CLEVRER, LLaVA-Video-178K, NeXT-QA, PerceptionTest, STAR\nImage data folder: Chart, General, Knowledge, Math, OCR, Spatial\nVideo-R1-COT-165k.json is for SFT cold start, and Video-R1-260k.json is for RL training.\nData Format in Video-R1-COT-165k:\n  {\n      \"problem_id\": 2,\n      \"problem\": \"What appears on the screen in Russian during theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Video-R1/Video-R1-data.","url":"https://huggingface.co/datasets/Video-R1/Video-R1-data","creator_name":"Video-R1","creator_url":"https://huggingface.co/Video-R1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Image"],"keywords_longer_than_N":true},
	{"name":"MMAT-1M","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMMAT-1M Dataset Card\n\t\n\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\tDataset details\n\t\n\n\n\t\n\t\t\n\t\tDataset type\n\t\n\nMMAT-1M is a million-scale multimodal agent tuning dataset, built by consolidating subsets of five publicly available multimodal question-answer datasets: Visual CoT, LLaVA-CoT, The Cauldron, TabMWP, and Infoseek. It integrates dynamically generated API calls and Retrieval Augmented Generation (RAG) information through a GPT-4o-powered multi-turn paradigm, with rationales refined viaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M.","url":"https://huggingface.co/datasets/VIS-MPU-Agent/MMAT-1M","creator_name":"VIS-MPU-Agent","creator_url":"https://huggingface.co/VIS-MPU-Agent","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","cc-by-4.0","cc-by-nc-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"SAMM","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tResource ðŸ“–\n\t\n\n\nÂ  [ACM MM Paper] | [SAMM HF] | [CAP HF] | [Github Code]\n\n\n\n\n\t\n\t\t\n\t\tNotes âš ï¸\n\t\n\n\nIf you want to import the CAP data into your own dataset, please refer to this.\nIf you want to run RamDG on datasets other than SAMM and use CNCL to incorporate external knowledge, please ensure to configure idx_cap_texts and idx_cap_images in the dataset jsons.\nWe have upgraded the SAMM JSON files. The latest versions (SAMM with CAP or without CAP) are available on July 24, 2025. Pleaseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SJJ0854/SAMM.","url":"https://huggingface.co/datasets/SJJ0854/SAMM","creator_name":"Jinjie Shen","creator_url":"https://huggingface.co/SJJ0854","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["token-classification","text-classification","image-text-to-text","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"OmniCorpus-CC","keyword":"image-to-text","description":"\n  ðŸ³ OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text\n\n\n\nâ­ï¸ NOTE: Several parquet files were marked unsafe (viruses) by official scaning of hf, while they are reported safe by ClamAV and Virustotal. \nWe found many false positive cases of the hf automatic scanning in hf discussions and raise one discussion to ask for a re-scanning.\n\nThis is the repository of OmniCorpus-CC, which contains 988 million image-text interleaved documents collected from Commonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC.","url":"https://huggingface.co/datasets/OpenGVLab/OmniCorpus-CC","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v190","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v190.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v190","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"sensor-ocr-benchmark","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMy Custom Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe original dataset was modified to inserto fake sensor information in bottom of image.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"famousdetectiveadrianmonk/sensor-ocr-benchmark\")\nexample = dataset['train'][0]\nimg = example['pixel_values']\nsensor_zoomin = img.crop((600, 850, 1250, 1050))\n\n\n\t\n\t\t\n\t\n\t\n\t\tAttribution\n\t\n\nThis dataset is based on the original dataset provided by Segments.ai. The original dataset canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/famousdetectiveadrianmonk/sensor-ocr-benchmark.","url":"https://huggingface.co/datasets/famousdetectiveadrianmonk/sensor-ocr-benchmark","creator_name":"seafog winters","creator_url":"https://huggingface.co/famousdetectiveadrianmonk","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size 1-30.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-20K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512Ã—512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K.","url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VideoRoPE","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tV-NIAH-D Benchmark\n\t\n\nA Visual Needle-In-A-Haystack Benchmark with Periodic Distractors. It was presented in VideoRoPE: What Makes for Good Video Rotary Position Embedding?.\nOne can use it by following steps similar to V-NIAH.\n\n\t\n\t\t\n\t\tVideoRoPE Training Data\n\t\n\nTo facilitate the reproduction of our experimental results, we have also uploaded the data used by VideoRoPE. We use a subset of the LLaVA-Video-178K dataset to train VideoRoPE.\nThe LLaVA-Video-178K dataset consists of 178Kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Wiselnn/VideoRoPE.","url":"https://huggingface.co/datasets/Wiselnn/VideoRoPE","creator_name":"Xilin Wei","creator_url":"https://huggingface.co/Wiselnn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["video-text-to-text","apache-2.0","arxiv:2502.05173","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"FaceCaptionHQ-4M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFaceCaptionHQ-4M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing the Agreement\nFaceCaptionHQ-4M contains about 4M facial image-text pairs that cleaned from FaceCaption-15M .  \n\n\n\n\n\t\n\t\n\t\n\t\tFigure.1 Illustrationsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"medical-imaging-combined","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCombined Medical Imaging Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCombined medical imaging dataset with 1893 samples in Alpaca instruction format.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 1893\nTraining Samples: 1514\nValidation Samples: 379\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 1806 samples\nCT: 48 samples\nUnknown: 32 samples\nMRI: 5 samples\nUltrasound: 2 samples\n\n\n\t\n\t\t\n\t\tSource Distribution\n\t\n\n\nVQA-RAD: 1793 samples\nROCO: 100 samples\n\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nROCO (Radiology Objectsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alvinl29/medical-imaging-combined.","url":"https://huggingface.co/datasets/alvinl29/medical-imaging-combined","creator_name":"Alvin Liu","creator_url":"https://huggingface.co/alvinl29","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"gc-os-img-art-critic","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tgc-os-img-art-critic\n\t\n\nExample gc dataset with art critic perspective\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains images with associated metadata including captions, tags, and verification information.\n","url":"https://huggingface.co/datasets/jpfearnworks/gc-os-img-art-critic","creator_name":"JP","creator_url":"https://huggingface.co/jpfearnworks","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","cc0-1.0","ðŸ‡ºðŸ‡¸ Region: US","image-to-text","computer-vision"],"keywords_longer_than_N":true},
	{"name":"docent","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for docent\n\t\n\nThis dataset contains works of art with expert-written detailed descriptions from the U.S. National Gallery of Art, published as part of DOCENT.  It was introduced in \"PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions\".  You can find a full description of its collection methodology in the paper: https://arxiv.org/abs/2510.19060.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\nLanguage: English\nLicense: CC-0\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sourcesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/amitha/docent.","url":"https://huggingface.co/datasets/amitha/docent","creator_name":"Amith Ananthram","creator_url":"https://huggingface.co/amitha","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"yksuniform","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tYKS Uniform\n\t\n\n\n\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills across many tasks and domains. However, their ability to reason over structured, curriculum-based educational questionsâ€”particularly in the context of Turkish high school entrance examinationsâ€”has not been systematically studied.\nTo address this gap, we introduce YKS Uniform, a balanced multimodal benchmark covering the Turkish high school curriculum withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/esrt/yksuniform.","url":"https://huggingface.co/datasets/esrt/yksuniform","creator_name":"ES","creator_url":"https://huggingface.co/esrt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-text","Turkish","English","mit"],"keywords_longer_than_N":true},
	{"name":"Text2Face","keyword":"image-to-text","description":"This is version 1.0 of Text2Face dataset. This dataset generated by using Flux1.dev (Nunchaku 4 bit optimization method). Facial descriptions generated with promptgen.py script. More advanced version of prompt generator is also available as promptgenv2.py. For more detailed facial descriptions, you can generate with that.\n","url":"https://huggingface.co/datasets/oguzhanercan/Text2Face","creator_name":"OÄŸuzhan Ercan","creator_url":"https://huggingface.co/oguzhanercan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"yksuniform","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tYKS Uniform\n\t\n\n\n\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills across many tasks and domains. However, their ability to reason over structured, curriculum-based educational questionsâ€”particularly in the context of Turkish high school entrance examinationsâ€”has not been systematically studied.\nTo address this gap, we introduce YKS Uniform, a balanced multimodal benchmark covering the Turkish high school curriculum withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/esrt/yksuniform.","url":"https://huggingface.co/datasets/esrt/yksuniform","creator_name":"ES","creator_url":"https://huggingface.co/esrt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-text","Turkish","English","mit"],"keywords_longer_than_N":true},
	{"name":"R1-VL-10K","keyword":"image-to-text","description":"Please check our GitHub for more details: https://github.com/jingyi0000/R1-VL.\n","url":"https://huggingface.co/datasets/jingyiZ00/R1-VL-10K","creator_name":"Jingyi Zhang","creator_url":"https://huggingface.co/jingyiZ00","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-ArXiv","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-ArXiv","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"mj-showcase-8K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMJ Showcase 2024\n\t\n\nThis is a dataset of the top voted creations which were manually collected daily between May and August 2024.\n\n  \n    150 most frequent words used in the prompts   \n\n\n\n\n\t\n\t\t\n\t\tã€½ï¸ Stats and Data Composition\n\t\n\n\nDescribed: Both Image and Prompt as input\nExpanded: Only Prompt as input\nCaptioned: Only Image as input\n\n\n\t\n\t\t\nModel-Technique\nRows\nIncomplete\n\n\n\t\t\nllava-1.5-7b-hf-described\n8551\n0\n\n\nclip-described\n8551\n0\n\n\nllama-3.1-405b-expanded\n8551\n0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/shb777/mj-showcase-8K.","url":"https://huggingface.co/datasets/shb777/mj-showcase-8K","creator_name":"SB","creator_url":"https://huggingface.co/shb777","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"tamily-1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTamily-1: Ancient Tamil OCR Synthetic Dataset\n\t\n\nTamizhi \"à®¤à®®à®¿à®´à®¿\"\n\n\t\n\t\t\n\t\tDescription\n\t\n\n\nRepository: sasicodes/tamily-1\nPoint of Contact: @sasicodes\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nTamily-1 is an ancient Tamil OCR synthetic dataset generated from the first 200,000 rows of Solvari-1, a large Tamil text corpus. The dataset contains rendered images of Tamil text with various augmentations and styles, making it suitable for training OCR models.\n\n\t\n\t\t\n\t\tFields\n\t\n\n\nimage: PNG image of rendered Tamilâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sasicodes/tamily-1.","url":"https://huggingface.co/datasets/sasicodes/tamily-1","creator_name":"Sasi","creator_url":"https://huggingface.co/sasicodes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","sasicodes/solvari-1","Tamil","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"KVG-Bench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n \n\n \n \nThis is the official repository of KVG-Bench, a comprehensive benchmark of Knowledge-intensive Visual Grounding (KVG) spanning 10 categories with 1.3K manually curated test cases.\n","url":"https://huggingface.co/datasets/MaxyLee/KVG-Bench","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"AgentNet","keyword":"image-text-to-text","description":"\n  OpenCUA: Open Foundations for Computer-Use Agents\n\n\n\n  \n  \n    ðŸŒ Website\n  \n  \n    ðŸ“ Paper\n  \n  ðŸ’» Code\n  \n\n\n\n\n\n  \n\n\t\n\t\t\n\t\tAgentNet Dataset\n\t\n\n\n\nAgentNet is the first large-scale desktop computer-use agent trajectory dataset, containing 22.6K human-annotated computer-use tasks across Windows, macOS, and Ubuntu systems.\n\n\t\n\t\t\n\t\tApplications\n\t\n\nThis dataset enables training and evaluation of:\n\nVision-language-action (VLA) models for computer use\nMulti-modal agents for desktop automation\nGUIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/AgentNet.","url":"https://huggingface.co/datasets/xlangai/AgentNet","creator_name":"XLang NLP Lab","creator_url":"https://huggingface.co/xlangai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","arxiv:2508.09123","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v50","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v50.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"FlowerEvolver-Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlowerEvolver Dataset\n\t\n\nDataset for flower descriptions for FlowerEvolver-frontend\nformat\n\n{\n    \"annotations\":[\n        {\n            \"ImagePath\":\"flowers/001.png\", \n            \"caption\":\"A flower with 10 petals that are elongated and narrow ovals in shape. The petals are yellow with dark green tones, featuring a striped pattern. The center of the flower has a dense and colorful pattern with red, blue, green, and pink.\"\n        },\n        ...\n    ]\n}\n\n\n\t\n\t\n\t\n\t\tMake more flowersâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cristianglezm/FlowerEvolver-Dataset.","url":"https://huggingface.co/datasets/cristianglezm/FlowerEvolver-Dataset","creator_name":"Cristian Gonzalez","creator_url":"https://huggingface.co/cristianglezm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ImplicitQA","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tImplicitQA Dataset\n\t\n\nThe ImplicitQA dataset was introduced in the paper ImplicitQA: Going beyond frames towards Implicit Video Reasoning.\nProject page: https://implicitqa.github.io/\nImplicitQA is a novel benchmark specifically designed to test models on implicit reasoning in Video Question Answering (VideoQA). Unlike existing VideoQA benchmarks that primarily focus on questions answerable through explicit visual content (actions, objects, events directly observable within individualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ucf-crcv/ImplicitQA.","url":"https://huggingface.co/datasets/ucf-crcv/ImplicitQA","creator_name":"Center for Research in Computer Vision, University of Central Florida","creator_url":"https://huggingface.co/ucf-crcv","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/terminusresearch/photo-anatomy","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"universal-preference-hijacking-datasets","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPhi: Preference Hijacking in Multi-modal Large Language Models at Inference Time\n\t\n\n\n  \n  \n  Figure 1: Examples of Phi, which can hijack MLLM's preference toward the image.\n\n\n\n  \n  \n  Figure 2: Example of a universal hijacking perturbation, which can be transferred across different images.\n\n\nThis dataset is used to train and evaluate the universal hijacking perturbations in the paper \"Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time\", accepted at EMNLPâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yflantmy/universal-preference-hijacking-datasets.","url":"https://huggingface.co/datasets/yflantmy/universal-preference-hijacking-datasets","creator_name":"Yifan Lan","creator_url":"https://huggingface.co/yflantmy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","image-text-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"CLINTOX-V-SMILES-4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/CLINTOX-V-SMILES-4.","url":"https://huggingface.co/datasets/molvision/CLINTOX-V-SMILES-4","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v86","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v86.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v86","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LLaVA-OneVision-1.5-Instruct-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tLLaVA-OneVision-1.5 Instruction Data\n\t\n\nPaper | Code\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset, LLaVA-OneVision-1.5-Instruct, was collected and integrated during the development of LLaVA-OneVision-1.5. LLaVA-OneVision-1.5 is a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. This meticulously curated 22M instruction dataset (LLaVA-OneVision-1.5-Instruct) is part of a comprehensive andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data.","url":"https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-1.5-Instruct-Data","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2509.23661","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v124","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v124.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v124","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MMC4-130k-chinese-image","keyword":"image-to-text","description":"MMC4-130k-chineseæ˜¯å¯¹MMC4ä¸­ï¼ŒæŠ½æ ·äº†130kå·¦å³ simliartyè¾ƒé«˜çš„å›¾æ–‡pairå¾—åˆ°çš„æ•°æ®é›†\nChineseç‰ˆæœ¬æ˜¯å¯¹è¿™é‡Œæ‰€æœ‰çš„captionè¿›è¡Œäº†ç¿»è¯‘ã€‚\næˆ‘ä»¬ä¼šé™†ç»­å°†æ›´å¤šæ•°æ®é›†å‘å¸ƒåˆ°hfï¼ŒåŒ…æ‹¬\n\n Coco Captionçš„ä¸­æ–‡ç¿»è¯‘\n CoQAçš„ä¸­æ–‡ç¿»è¯‘\n CNewSumçš„Embeddingæ•°æ®\n å¢žå¹¿çš„å¼€æ”¾QAæ•°æ®\n WizardLMçš„ä¸­æ–‡ç¿»è¯‘\n\nå¦‚æžœä½ ä¹Ÿåœ¨åšè¿™äº›æ•°æ®é›†çš„ç­¹å¤‡ï¼Œæ¬¢è¿Žæ¥è”ç³»æˆ‘ä»¬ï¼Œé¿å…é‡å¤èŠ±é’±ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\téª†é©¼(Luotuo): å¼€æºä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹\n\t\n\nhttps://github.com/LC1332/Luotuo-Chinese-LLM\néª†é©¼(Luotuo)é¡¹ç›®æ˜¯ç”±å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€, é™ˆå¯æº @ åŽä¸­å¸ˆèŒƒå¤§å­¦ ä»¥åŠ æŽé²é² @ å•†æ±¤ç§‘æŠ€ å‘èµ·çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹å¼€æºé¡¹ç›®ï¼ŒåŒ…å«äº†ä¸€ç³»åˆ—è¯­è¨€æ¨¡åž‹ã€‚\n( æ³¨æ„: é™ˆå¯æº æ­£åœ¨å¯»æ‰¾2024æŽ¨å…å¯¼å¸ˆï¼Œæ¬¢è¿Žè”ç³» )\néª†é©¼é¡¹ç›®ä¸æ˜¯å•†æ±¤ç§‘æŠ€çš„å®˜æ–¹äº§å“ã€‚\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease cite the repo if you use the data or codeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image.","url":"https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Chinese","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MMC4-130k-image-english","keyword":"image-to-text","description":"MMC4-130kæ˜¯å¯¹MMC4ä¸­ï¼ŒæŠ½æ ·äº†130kå·¦å³ simliartyè¾ƒé«˜çš„å›¾æ–‡pairå¾—åˆ°çš„æ•°æ®é›†\næˆ‘ä»¬å‡†å¤‡é™†ç»­ç¿»è¯‘è¿™ä¸ªå­é›†\næˆ‘ä»¬ä¼šé™†ç»­å°†æ›´å¤šæ•°æ®é›†å‘å¸ƒåˆ°hfï¼ŒåŒ…æ‹¬\n\n Coco Captionçš„ä¸­æ–‡ç¿»è¯‘\n CoQAçš„ä¸­æ–‡ç¿»è¯‘\n CNewSumçš„Embeddingæ•°æ®\n å¢žå¹¿çš„å¼€æ”¾QAæ•°æ®\n WizardLMçš„ä¸­æ–‡ç¿»è¯‘\n\nå¦‚æžœä½ ä¹Ÿåœ¨åšè¿™äº›æ•°æ®é›†çš„ç­¹å¤‡ï¼Œæ¬¢è¿Žæ¥è”ç³»æˆ‘ä»¬ï¼Œé¿å…é‡å¤èŠ±é’±ã€‚\n\n\t\n\t\t\n\t\téª†é©¼(Luotuo): å¼€æºä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹\n\t\n\nhttps://github.com/LC1332/Luotuo-Chinese-LLM\néª†é©¼(Luotuo)é¡¹ç›®æ˜¯ç”±å†·å­æ˜‚ @ å•†æ±¤ç§‘æŠ€, é™ˆå¯æº @ åŽä¸­å¸ˆèŒƒå¤§å­¦ ä»¥åŠ æŽé²é² @ å•†æ±¤ç§‘æŠ€ å‘èµ·çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡åž‹å¼€æºé¡¹ç›®ï¼ŒåŒ…å«äº†ä¸€ç³»åˆ—è¯­è¨€æ¨¡åž‹ã€‚\n( æ³¨æ„: é™ˆå¯æº æ­£åœ¨å¯»æ‰¾2024æŽ¨å…å¯¼å¸ˆï¼Œæ¬¢è¿Žè”ç³» )\néª†é©¼é¡¹ç›®ä¸æ˜¯å•†æ±¤ç§‘æŠ€çš„å®˜æ–¹äº§å“ã€‚\n\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease cite the repo if you use the data or code in this repo.\n@misc{alpacaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/silk-road/MMC4-130k-image-english.","url":"https://huggingface.co/datasets/silk-road/MMC4-130k-image-english","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Cars_I_like","keyword":"image-to-text","description":"Basilisk181297/Cars_I_like dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Basilisk181297/Cars_I_like","creator_name":"Basil Minhaj","creator_url":"https://huggingface.co/Basilisk181297","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","depth-estimation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"beninmadrid","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBenin, Madrid Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset comprises images sourced from the beninmadrid Instagram page and is intended to serve as a challenging and intriguing dataset for testing visual language models and large multimodal language models. The images in this dataset are characterized by their unique artistic style and complexity, which can provide a robust test for the capabilities of modern AI models.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThis dataset is intended for researchâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/beninmadrid.","url":"https://huggingface.co/datasets/taesiri/beninmadrid","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Trace_Captioning_Flickr30K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlickr30k Trace Captioning Dataset\n\t\n\nThis dataset contains trace-based captions for images from the Flickr30k dataset. Each sample includes multiple captions paired with spatial-temporal traces that represent mouse scanning patterns over arbitrary image regions. This dataset is a resource introduced in the Patch-ioner paper for evaluating region-based captioning models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Flickr30k Trace Captioning dataset is a resource created for evaluating region-basedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ruggero1912/Trace_Captioning_Flickr30K.","url":"https://huggingface.co/datasets/Ruggero1912/Trace_Captioning_Flickr30K","creator_name":"Giacomo Pacini","creator_url":"https://huggingface.co/Ruggero1912","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaItScansRetrieval","keyword":"image-to-text","description":"\n  JinaVDREuropeanaItScansRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Italian historical articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-it-scans_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-it-scans_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaItScansRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaItScansRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaDeNewsRetrieval","keyword":"image-to-text","description":"\n  JinaVDREuropeanaDeNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve German news articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-de-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-de-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaDeNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaDeNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaFrNewsRetrieval","keyword":"image-to-text","description":"\n  JinaVDREuropeanaFrNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve French news articles from Europeana based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-fr-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-fr-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaFrNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaFrNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDREuropeanaEsNewsRetrieval","keyword":"image-to-text","description":"\n  JinaVDREuropeanaEsNewsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Spanish news articles based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nNews\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/europeana-es-news_beir\n\n\n\t\n\nSource datasets:\n\njinaai/europeana-es-news_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDREuropeanaEsNewsRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDREuropeanaEsNewsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreArxivQARetrieval","keyword":"image-to-text","description":"\n  VidoreArxivQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/arxivqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreArxivQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreArxivQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreArxivQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAAI","keyword":"image-to-text","description":"\n  JinaVDRDocQAAI\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve AI documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_artificial_intelligence_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_artificial_intelligence_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocQAAI\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAAI.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAAI","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Tox21-V-SMILES-0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTox21-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for Tox21 related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/Tox21-V-SMILES-0.","url":"https://huggingface.co/datasets/molvision/Tox21-V-SMILES-0","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"test_dataset_1342","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n17 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_1342.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_1342","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test_dataset_132","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n17 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_132.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_132","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"HatefulMemesI2TRetrieval","keyword":"image-to-text","description":"\n  HatefulMemesI2TRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes to assess OCR abilities.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://arxiv.org/pdf/2005.04790\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_HatefulMemes\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"HatefulMemesI2TRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HatefulMemesI2TRetrieval.","url":"https://huggingface.co/datasets/mteb/HatefulMemesI2TRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAGovReportRetrieval","keyword":"image-to-text","description":"\n  JinaVDRDocQAGovReportRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve government reports based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nGovernment\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_gov_report_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_gov_report_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAGovReportRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAGovReportRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAHealthcareIndustryRetrieval","keyword":"image-to-text","description":"\n  VidoreSyntheticDocQAHealthcareIndustryRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_healthcare_industry_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAHealthcareIndustryRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAHealthcareIndustryRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRInfovqaRetrieval","keyword":"image-to-text","description":"\n  JinaVDRInfovqaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve infographics based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/infovqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/infovqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRInfovqaRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRInfovqaRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRInfovqaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArabicInfographicsVQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRArabicInfographicsVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Arabic infographics based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arabic_infographicsvqa_ar_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arabic_infographicsvqa_ar_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArabicInfographicsVQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArabicInfographicsVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Vidore2EconomicsReportsRetrieval","keyword":"image-to-text","description":"\n  Vidore2EconomicsReportsRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/economics_reports_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2EconomicsReportsRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2EconomicsReportsRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"MemotionT2IRetrieval","keyword":"image-to-text","description":"\n  MemotionT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve memes based on captions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://aclanthology.org/2020.semeval-1.99/\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_Memotion\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MemotionT2IRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MemotionT2IRetrieval.","url":"https://huggingface.co/datasets/mteb/MemotionT2IRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"MemotionI2TRetrieval","keyword":"image-to-text","description":"\n  MemotionI2TRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\ni2t\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://aclanthology.org/2020.semeval-1.99/\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_Memotion\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"MemotionI2TRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/MemotionI2TRetrieval.","url":"https://huggingface.co/datasets/mteb/MemotionI2TRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRMMTabRetrieval","keyword":"image-to-text","description":"\n  JinaVDRMMTabRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve tables from the MMTab dataset based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/MMTab_beir\n\n\n\t\n\nSource datasets:\n\njinaai/MMTab_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRMMTabRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRMMTabRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRMMTabRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"BACE-V-SMILES-0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BACE-V-SMILES-0.","url":"https://huggingface.co/datasets/molvision/BACE-V-SMILES-0","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ArxivCap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ArxivCap\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\nExample-1 of single (image, caption) pairs\n\n\"......\" stands for omitted parts.\n\n{\n    'src': 'arXiv_src_2112_060/2112.08947', \n    'meta': \n    {\n        'meta_from_kaggle': \n        {\n            'journey': '', \n            'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', \n            'categories': 'cs.ET'\n        }, \n        'meta_from_s2': \n        {\n            'citationCount': 8â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMInstruction/ArxivCap.","url":"https://huggingface.co/datasets/MMInstruction/ArxivCap","creator_name":"Multi-modal Multilingual Instruction","creator_url":"https://huggingface.co/MMInstruction","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"visual_genome","keyword":"image-to-text","description":"Visual Genome enable to model objects and relationships between objects.\nThey collect dense annotations of objects, attributes, and relationships within each image.\nSpecifically, the dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects.","url":"https://huggingface.co/datasets/ranjaykrishna/visual_genome","creator_name":"Ranjay Krishna","creator_url":"https://huggingface.co/ranjaykrishna","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","object-detection","visual-question-answering","image-captioning","found"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArabicChartQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRArabicChartQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Arabic charts based on queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arabic_chartqa_ar_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arabic_chartqa_ar_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRArabicChartQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArabicChartQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArabicChartQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"Vidore2ESGReportsHLRetrieval","keyword":"image-to-text","description":"\n  Vidore2ESGReportsHLRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/esg_reports_human_labeled_v2\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"Vidore2ESGReportsHLRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/Vidore2ESGReportsHLRetrieval.","url":"https://huggingface.co/datasets/mteb/Vidore2ESGReportsHLRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreTabfquadRetrieval","keyword":"image-to-text","description":"\n  VidoreTabfquadRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/tabfquad_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreTabfquadRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreTabfquadRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreTabfquadRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRHungarianDocQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRHungarianDocQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve Hungarian documents in various formats based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/hungarian_doc_qa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/hungarian_doc_qa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRHungarianDocQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRHungarianDocQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"docci","keyword":"image-to-text","description":"DOCCI (Descriptions of Connected and Contrasting Images) is a collection of images paired with detailed descriptions. The descriptions explain the key elements of the images, as well as secondary information such as background, lighting, and settings. The images are specifically taken to help assess the precise visual properties of images. DOCCI also includes many related images that vary in having key differences from the others. All descriptions are manually annotated to ensure they adequately distinguish each image from its counterparts.","url":"https://huggingface.co/datasets/google/docci","creator_name":"Google","creator_url":"https://huggingface.co/google","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"coyo-700m","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for COYO-700M\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCOYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models. Our dataset follows a similar strategy to previous vision-and-language datasets, collecting many informative pairs of alt-text and its associated image in HTML documents. We expect COYO to be used to train popular large-scale foundation models \ncomplementary to otherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kakaobrain/coyo-700m.","url":"https://huggingface.co/datasets/kakaobrain/coyo-700m","creator_name":"Kakao Brain","creator_url":"https://huggingface.co/kakaobrain","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"albi-captioned-photos","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAlbi, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from Albi, France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from Albi, France, including the city center, the Toulouse Lautrec museum, and the Sainte-CÃ©cile Cathedral. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:\n\nHigh-resolutionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos.","url":"https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos","creator_name":"NoÃ© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"JinaVDRPlotQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRPlotQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve plots from the PlotQA dataset based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/plotqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/plotqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRPlotQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRPlotQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRPlotQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"tl-caxton","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\t3D Printing Nozzle Images Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTask: Vision-based flow rate estimation and extrusion quality assessment\nDomain: Additive Manufacturing / 3D Printing\nData Type: RGB images with numerical annotations\nTotal Samples: 4,048 images\nTraining: 3,407 samples\nValidation: 331 samples\nTest: 310 samples\n\n\n\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nFlow Rate Regression: Predict the flow rate percentage from camera images of the printing process\nExtrusion Quality Classification:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cemag/tl-caxton.","url":"https://huggingface.co/datasets/cemag/tl-caxton","creator_name":"Christos Margadji","creator_url":"https://huggingface.co/cemag","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","visual-question-answering","image-to-text","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Military-Aircraft-Recognition-dataset","keyword":"image-to-text","description":"This is a remote sensing image Military Aircraft Recognition dataset that include 3842 images, 20 types, and 22341 instances annotated with horizontal bounding boxes and oriented bounding boxes.\n","url":"https://huggingface.co/datasets/Alex5666/Military-Aircraft-Recognition-dataset","creator_name":"Gracio","creator_url":"https://huggingface.co/Alex5666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","image-to-image","object-detection"],"keywords_longer_than_N":true},
	{"name":"Marathi_Handwritten","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Marathi Handwritten OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Marathi Handwritten Text Dataset is a collection of handwritten text images in Marathi (à¤¦à¥‡à¤µà¤¨à¤¾à¤—à¤°à¥€ à¤²à¤¿à¤ªà¥€),\naimed at supporting the development of Optical Character Recognition (OCR) systems, handwriting analysis tools,\nand language research.The dataset was curated from native Marathi speakers to ensure a variety of handwriting styles and character variations.\nThe dataset contains 2520 images with twoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Process-Venue/Marathi_Handwritten.","url":"https://huggingface.co/datasets/Process-Venue/Marathi_Handwritten","creator_name":"ProcessVenue","creator_url":"https://huggingface.co/Process-Venue","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","image-feature-extraction","Marathi","mit"],"keywords_longer_than_N":true},
	{"name":"dp-bench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDP-Bench: Document Parsing Benchmark\n\t\n\n\n  \n\n\n\nDocument parsing refers to the process of converting complex documents, such as PDFs and scanned images, into structured text formats like HTML and Markdown.\nIt is especially useful as a preprocessor for RAG systems, as it preserves key structural information from visually rich documents.\nWhile various parsers are available on the market, there is currently no standard evaluation metric to assess their performance.\nTo address this gap, weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/upstage/dp-bench.","url":"https://huggingface.co/datasets/upstage/dp-bench","creator_name":"upstage","creator_url":"https://huggingface.co/upstage","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["mit","arxiv:1911.10683","ðŸ‡ºðŸ‡¸ Region: US","nlp","Image-to-Text"],"keywords_longer_than_N":false},
	{"name":"table-vqa","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-HTML","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-HTML","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"ESOL-V-SMILES-4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/ESOL-V-SMILES-4.","url":"https://huggingface.co/datasets/molvision/ESOL-V-SMILES-4","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"ESOL-V-SMILES-0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/ESOL-V-SMILES-0.","url":"https://huggingface.co/datasets/molvision/ESOL-V-SMILES-0","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Android-Control-84k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAndroid Control Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis directory contains two dataset files (and_ctrl_train.json and and_ctrl_test.json) derived from the Android Control project by Google Research. These datasets have been formatted specifically for GUI grounding training in LLaMA-Factory.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Android Control dataset consists of episodes where each episode contains multiple steps. Each step includes:\n\nStep instructions: Natural language instructions for UIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OfficerChul/Android-Control-84k.","url":"https://huggingface.co/datasets/OfficerChul/Android-Control-84k","creator_name":"Kyochul Jang","creator_url":"https://huggingface.co/OfficerChul","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"RIMES-2011-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRIMES-2011 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe RIMES-2011 database (Recognition and Indexation of handwritten documents and faxes) was created to evaluate automatic recognition and indexing systems for handwritten letters. \nThe database was collected by asking volunteers to write handwritten letters in exchange for gift certificates. Volunteers were given a fictitious identity (same gender as the real one) and up to 5 scenarios. Each scenario was chosen from among 9â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/RIMES-2011-line.","url":"https://huggingface.co/datasets/Teklia/RIMES-2011-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"unictokens_data","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUniCTokens Dataset\n\t\n\nVersion Â· 2025-10-24\n\n\t\n\t\t\n\t\t1 Data Overview\n\t\n\n\n\t\n\t\t\nItem\nDescription\n\n\n\t\t\nTotal concepts\n20 (Human Ã— 10 Â· Animal Ã— 5 Â· Object Ã— 5)\n\n\nImages per concept\nN â‰ˆ 10 â€“ 15 (already split into train / test)\n\n\nNegative samples\nrandom_images/ (100 random irrelevant images) + negative_example/ (hard negatives)\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t2 Benchmark Tasks\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\t2.1 MMU (Multi-Modal Understanding)\n\t\n\n\n\t\n\t\t\nSub-task\nSource files\nEvaluation focus\n\n\n\t\t\nText-Only QAâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HankYang428/unictokens_data.","url":"https://huggingface.co/datasets/HankYang428/unictokens_data","creator_name":"HankYang","creator_url":"https://huggingface.co/HankYang428","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"DSI-Bench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDSI-Bench: A Benchmark for Dynamic Spatial Intelligence\n\t\n\nPaper | Project Page | Code\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nReasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Viglong/DSI-Bench.","url":"https://huggingface.co/datasets/Viglong/DSI-Bench","creator_name":"ZiangZhang","creator_url":"https://huggingface.co/Viglong","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","cc-by-4.0","1K - 10K","Video","3D"],"keywords_longer_than_N":true},
	{"name":"hl-narratives","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for the High-Level Narratives Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe High-Level Narratives (HL-Narratives) dataset aligns object-centric descriptions from COCO \nwith synthetic high-level narratives captions automatically generated by merging scene, action, rationale captions from the HL Dataset using T5\nThe HL-Naratives dataset contains 14997 images from COCO and a total of 134973 synthetic captions (3 captions per image) aligned with ~749984 object-centric captionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/michelecafagna26/hl-narratives.","url":"https://huggingface.co/datasets/michelecafagna26/hl-narratives","creator_name":"Michele Cafagna","creator_url":"https://huggingface.co/michelecafagna26","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","zero-shot-classification","text-scoring","machine-generated"],"keywords_longer_than_N":true},
	{"name":"Visual-CoT","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVisCoT Dataset Card\n\t\n\n\n\nThere is a shortage of multimodal datasets for training multi-modal large language models (MLLMs) that require to identify specific regions in an image for additional attention to improve response performance. This type of dataset with grounding bbox annotations could possibly help the MLLM output intermediate interpretable attention area and enhance performance.\nTo fill the gap, we curate a visual CoT dataset. This dataset specifically focuses on identifyingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/deepcs233/Visual-CoT.","url":"https://huggingface.co/datasets/deepcs233/Visual-CoT","creator_name":"Hao Shao","creator_url":"https://huggingface.co/deepcs233","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2403.16999","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"ocr-synthetic-images-of-words-danish-v1","keyword":"image-to-text","description":"This repo contains synthetic images with Danish word created for generating an OCR model.\nThere is about 22.9GB of train-data and 4.58GB of validation-data.\nIn order to clone the repo you will need to use the git lfs extension. \nClone:\ngit clone https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1\ncd ocr-synthetic-images-of-words-danish-v1\n\nExtract (this may take some time):\ntar xvfz validation-data.tar.gz\ntar xvfz train-data.tar.gz \n\nInside both train-data andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1.","url":"https://huggingface.co/datasets/diversen/ocr-synthetic-images-of-words-danish-v1","creator_name":"Dennis Iversen","creator_url":"https://huggingface.co/diversen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"test_dataset_13432","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n22 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_13432.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_13432","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"WebRenderBench","keyword":"image-to-text","description":"\n    \n    \n        \n            \n        \n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tWebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning\n\t\n\nPaper | ä¸­æ–‡\n\n\t\n\t\n\t\n\t\tðŸ” Overview\n\t\n\nWebRenderBench is a large-scale benchmark designed to advance WebUI-to-Coderesearch for multimodal large language models (MLLMs) through evaluation on real-world webpages. It provides:\n\n45,100 real webpages collected from public portal websites\nHigh diversity and complexity, covering aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aleversn/WebRenderBench.","url":"https://huggingface.co/datasets/aleversn/WebRenderBench","creator_name":"Peichao Lai","creator_url":"https://huggingface.co/aleversn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mbbank_recaptcha","keyword":"image-to-text","description":"TitanRTX/mbbank_recaptcha dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/TitanRTX/mbbank_recaptcha","creator_name":"TITAN RTX","creator_url":"https://huggingface.co/TitanRTX","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","image-to-text","English","mit","10M<n<100M"],"keywords_longer_than_N":true},
	{"name":"MobileViews","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸš€ MobileViews: A Large-Scale Mobile GUI Dataset\n\t\n\nMobileViews is a large-scale dataset designed to support research on mobile agents and mobile user interface (UI) analysis. The first release, MobileViews-600K, includes over 600,000 mobile UI screenshot-view hierarchy (VH) pairs collected from over 20,000 apps on the Google Play Store. This dataset is based on the DroidBot, which we have optimized for large-scale data collection, capturing more comprehensive interaction details whileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mllmTeam/MobileViews.","url":"https://huggingface.co/datasets/mllmTeam/MobileViews","creator_name":"mllm","creator_url":"https://huggingface.co/mllmTeam","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","task-planning","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"emova-alignment-7m","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEMOVA-Alignment-7M\n\t\n\n\n\n\nðŸ¤— EMOVA-Models | ðŸ¤— EMOVA-Datasets | ðŸ¤— EMOVA-Demo \nðŸ“„ Paper | ðŸŒ Project-Page | ðŸ’» Github | ðŸ’» EMOVA-Speech-Tokenizer-Github\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-Alignment-7M is a comprehensive dataset curated for omni-modal pre-training, including vision-language and speech-language alignment. \nThis dataset is created using open-sourced image-text pre-training datasets, OCR datasets, and 2,000 hours of ASR and TTS data. \nThis dataset is part of the EMOVA-Datasetsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m.","url":"https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"DataComp-12M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for DataComp-12M\n\t\n\n\n\nThis dataset contains a 12M subset of DataComp-1B-BestPool.\nWe distribute the image url-text samples and metadata under a standard Creative Common CC-BY-4.0 license. The individual images are under their own copyrights.\nImage-text models trained on DataComp-12M are significantly better than on CC-12M/YFCC-15M as well as DataComp-Small/Medium.\nDataComp-12M was introduced in MobileCLIP paper and along with the reinforced dataset DataCompDR-12M.\nThe UIDsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/DataComp-12M.","url":"https://huggingface.co/datasets/mlfoundations/DataComp-12M","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","Image"],"keywords_longer_than_N":true},
	{"name":"G-PlanET","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis G-PlanET dataset is built on AI2 ALFRED.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More Information Needed]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yuchenlin/G-PlanET.","url":"https://huggingface.co/datasets/yuchenlin/G-PlanET","creator_name":"Bill Yuchen Lin","creator_url":"https://huggingface.co/yuchenlin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","table-to-text","table-question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Tox21-V-Train","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTox21-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for Tox21 related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/Tox21-V-Train.","url":"https://huggingface.co/datasets/molvision/Tox21-V-Train","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ThaiIDCardSynt","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\nCurated by: Matichon Maneegard\nShared by [optional]: Matichon Maneegard\nLanguage(s) (NLP): image-to-text\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\nThe dataset was entirely synthetic. It does not contain real information or pertain to any specific person.\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\nUsing for tranning OCR or Multimodal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nThis dataset contains 98 x 6 = 588 samples, and theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Float16-cloud/ThaiIDCardSynt.","url":"https://huggingface.co/datasets/Float16-cloud/ThaiIDCardSynt","creator_name":"Float16.cloud","creator_url":"https://huggingface.co/Float16-cloud","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Thai","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"logicnlg","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\tLogicNLG Dataset\n\t\n\nSee the official wenhuchen/LogicNLG release on GitHub.\n","url":"https://huggingface.co/datasets/kasnerz/logicnlg","creator_name":"ZdenÄ›k Kasner","creator_url":"https://huggingface.co/kasnerz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"LaTeX_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLaTeX OCR çš„æ•°æ®ä»“åº“\n\t\n\næœ¬æ•°æ®ä»“åº“æ˜¯ä¸“ä¸º LaTeX_OCR åŠ LaTeX_OCR_PRO åˆ¶ä½œçš„æ•°æ®ï¼Œæ¥æºäºŽ https://zenodo.org/record/56198#.V2p0KTXT6eA ä»¥åŠ https://www.isical.ac.in/~crohme/ ä»¥åŠæˆ‘ä»¬è‡ªå·±æž„å»ºã€‚\nå¦‚æžœè¿™ä¸ªæ•°æ®ä»“åº“æœ‰å¸®åŠ©åˆ°ä½ çš„è¯ï¼Œè¯·ç‚¹äº® â¤ï¸like ++\nåŽç»­è¿½åŠ æ–°çš„æ•°æ®ä¹Ÿä¼šæ”¾åœ¨è¿™ä¸ªä»“åº“ ~~\n\nåŽŸå§‹æ•°æ®ä»“åº“åœ¨github LinXueyuanStdio/Data-for-LaTeX_OCR.\n\n\n\t\n\t\t\n\t\n\t\n\t\tæ•°æ®é›†\n\t\n\næœ¬ä»“åº“æœ‰ 5 ä¸ªæ•°æ®é›†\n\nsmall æ˜¯å°æ•°æ®é›†ï¼Œæ ·æœ¬æ•° 110 æ¡ï¼Œç”¨äºŽæµ‹è¯•\nfull æ˜¯å°åˆ·ä½“çº¦ 100k çš„å®Œæ•´æ•°æ®é›†ã€‚å®žé™…ä¸Šæ ·æœ¬æ•°ç•¥å°äºŽ 100kï¼Œå› ä¸ºç”¨ LaTeX çš„æŠ½è±¡è¯­æ³•æ ‘å‰”é™¤äº†å¾ˆå¤šä¸èƒ½æ¸²æŸ“çš„ LaTeXã€‚\nsynthetic_handwrite æ˜¯æ‰‹å†™ä½“ 100k çš„å®Œæ•´æ•°æ®é›†ï¼ŒåŸºäºŽ full çš„å…¬å¼ï¼Œä½¿ç”¨æ‰‹å†™å­—ä½“åˆæˆè€Œæ¥ï¼Œå¯ä»¥è§†ä¸ºäººç±»åœ¨çº¸ä¸Šçš„æ‰‹å†™ä½“ã€‚æ ·æœ¬æ•°å®žé™…ä¸Šç•¥å°äºŽ 100kï¼Œç†ç”±åŒä¸Šã€‚â€¦ See the full description on the dataset page: https://huggingface.co/datasets/linxy/LaTeX_OCR.","url":"https://huggingface.co/datasets/linxy/LaTeX_OCR","creator_name":"Lin Xueyuan","creator_url":"https://huggingface.co/linxy","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"AniPersonaCaps","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAniPersonaCaps\n\t\n\nWelcome to AniPersonaCaps! This is an immersive collection of anime characters, each paired with detailed, human-crafted descriptions of their looks and personalities.\nThis dataset brings together 45k+ unique characters from 1k+ beloved anime titles, capturing the essence of each character straight from the nijigen realm.\nOur data is lovingly sourced from Fandom Wiki, a true treasure trove thanks to the dedication of anime fans worldwide.\nWe've made a first attempt toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AniPersonaCaps.","url":"https://huggingface.co/datasets/mrzjy/AniPersonaCaps","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K<n<100K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"waqfeya-library","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tWaqfeya Library\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nWaqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 10,000 PDF books across over 80 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThe dataset includes 22,443 PDF files (spanning 8,978,634 pages) representing 10,150 Islamic books. Each book isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/waqfeya-library.","url":"https://huggingface.co/datasets/ieasybooks-org/waqfeya-library","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"soa-full-florence2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSmithsonian Open Access Dataset with Florence-2 Caption\n\t\n\n\næ—¥æœ¬èªžã¯ã“ã¡ã‚‰\nThis dataset is made of soa-full.\nsoa-full is an CC-0 image dataset from Smithsonian Open Access. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/soa-full-florence2\")\n\n\n\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nResearch Vision & Language\nDevelop text-to-image model or image-to-text model.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/soa-full-florence2.","url":"https://huggingface.co/datasets/aipicasso/soa-full-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v65","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v65.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v65","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v189","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v189.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v189","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ViQuAE-JA","keyword":"image-to-text","description":"This dataset was created by machine translating \"ViQuAE\" into Japanese.\noriginal_answer_ja translated from original_answer. I didn't translate answer.\nViQuAE: https://github.com/PaulLerner/ViQuAE\n","url":"https://huggingface.co/datasets/toshi456/ViQuAE-JA","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"termith-eval_fr_prompt_data_to_text","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\ttermith-eval_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\ntermith-eval_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 11,886 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset termith-eval.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/termith-eval_fr_prompt_data_to_text.","url":"https://huggingface.co/datasets/CATIE-AQ/termith-eval_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/termith-eval"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"french-lot-department-captioned-photos","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLot Department, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from the Lot region of France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from three notable locations in France's Lot department: Rocamadour, Autoire, and Padirac. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos.","url":"https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos","creator_name":"NoÃ© Flandre","creator_url":"https://huggingface.co/NoeFlandre","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v41","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v41.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Handwritten-Latex-Datasets","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset\n\t\n\nThis data set includes common handwritten formulas in junior high schools and high schools, and is labeled in Latex format. Can be used to train models that recognize common numbers, fractions, and sets.\n\n\t\n\t\t\n\t\tDataset source\n\t\n\nCollected in various junior high schools and high schools, handwritten by students.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThe label is stored at json folder and scanned hand-writted pictures are stored at pic folder.\nScan the qr code of the picture to get the index andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WindyVerse/Handwritten-Latex-Datasets.","url":"https://huggingface.co/datasets/WindyVerse/Handwritten-Latex-Datasets","creator_name":"WindyVerse","creator_url":"https://huggingface.co/WindyVerse","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"V1-33K","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nðŸš€  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction ðŸŒŸ\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K.","url":"https://huggingface.co/datasets/haonan3/V1-33K","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"swedish_fraktur","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSwedish Fraktur\n\t\n\n\n\nThis is a dataset for swedish blackletter from the 19th century. The transcriptions were made by SprÃ¥kbanken and converted into a text-line\ndataset by the Swedish National Archives\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\nTrain textline-based OCR models for swedish 19th century blackletter\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\n  \"image\": Image(),\n  \"text\": str\n}\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tSource Data\n\t\n\nOriginal dataset from SprÃ¥kbanken -â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Riksarkivet/swedish_fraktur.","url":"https://huggingface.co/datasets/Riksarkivet/swedish_fraktur","creator_name":"AI Riksarkivet  / AIRA","creator_url":"https://huggingface.co/Riksarkivet","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Swedish","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Open-Qwen2VL-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains the data for Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources.\nProject page: https://victorwz.github.io/Open-Qwen2VL\nCode: https://github.com/Victorwz/Open-Qwen2VL\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\n\nccs_ebdataset: CC3M-CC12M-SBU filtered by CLIP, we directly download the webdataset based on the released of curated subset of BLIP-1\ndatacomp_medium_dfn_webdataset: DataComp-Medium-128M filtered by DFN, we justâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data.","url":"https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data","creator_name":"Weizhi Wang","creator_url":"https://huggingface.co/weizhiwang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ave-2","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAVE-2: AudioVisual Event Evaluation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAVE-2 is a comprehensive dataset featuring 570,138 audio-visual clips with detailed five-dimensional alignment quality annotations. This dataset enables systematic investigation of how alignment quality affects multimodal model performance across retrieval and generation tasks.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\nðŸŽ¬ 570k+ annotated clips with granular quality scores (0-10 scale)\nðŸ“ Five-dimensional scoring: temporalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ali-vosoughi/ave-2.","url":"https://huggingface.co/datasets/ali-vosoughi/ave-2","creator_name":"Ali Vosoughi","creator_url":"https://huggingface.co/ali-vosoughi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","text-to-audio","text-to-speech","automatic-speech-recognition"],"keywords_longer_than_N":true},
	{"name":"Medical-Multimodal-EN-TH","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHealthGPTVL-Translation Medical-Multimodal-EN-TH\n\t\n\nThis dataset is a bilingual (English-Thai) medical multimodal evaluation dataset containing medical images with corresponding question-answer pairs for visual question answering and translation tasks.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 17,047 medical image-text pairs designed for multimodal medical AI evaluation. It includes medical images from various imaging modalities (MRI, CT, X-Rayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZombitX64/Medical-Multimodal-EN-TH.","url":"https://huggingface.co/datasets/ZombitX64/Medical-Multimodal-EN-TH","creator_name":"ZomBitX64","creator_url":"https://huggingface.co/ZombitX64","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","translation","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"TextOCR_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tText OCR\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/textocr/metafile.yml\nName: 'Text OCR'\nPaper:\n  Title: 'TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text'\n  URL: https://openaccess.thecvf.com/content/CVPR2021/papers/Singh_TextOCR_Towards_Large-Scale_End-to-End_Reasoning_for_Arbitrary-Shaped_Scene_Text_CVPR_2021_paper.pdf\n  Venue: CVPR\n  Year: '2021'\n  BibTeX: '@inproceedings{singh2021textocr,\n    title={{TextOCR}:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR.","url":"https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"FlowGen","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŒŸ FlowGen\n\t\n\nFlowGen is a controllable flowchart synthesizer that generates diagrams with tunable structural features and supports multiple rendering styles.\n\n\t\n\t\t\n\t\tðŸ“‘ Dataset description\n\t\n\nThis dataset contains different types of renderer flowchart images with different difficulty levels.\n\n\t\n\t\t\nTypes\nTrain (Easy)\nTrain (Medium)\nTrain (Hard)\nTest (Graph Easy)\nTest (Graph Medium)\nTest (Graph Hard)\nTest (Scanned Easy)\nTest (Scanned Medium)\nTest (Scanned Hard)\n\n\n\t\t\nMermaid\n960\n960\n960â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Anonymous112233/FlowGen.","url":"https://huggingface.co/datasets/Anonymous112233/FlowGen","creator_name":"Anonymous","creator_url":"https://huggingface.co/Anonymous112233","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","visual-question-answering","image-captioning","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"ave-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAVE-2: AudioVisual Event Evaluation Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAVE-2 is a comprehensive dataset featuring 570,138 audio-visual clips with detailed five-dimensional alignment quality annotations. This dataset enables systematic investigation of how alignment quality affects multimodal model performance across retrieval and generation tasks.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\nðŸŽ¬ 570k+ annotated clips with granular quality scores (0-10 scale)\nðŸ“ Five-dimensional scoring: temporalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ali-vosoughi/ave-2.","url":"https://huggingface.co/datasets/ali-vosoughi/ave-2","creator_name":"Ali Vosoughi","creator_url":"https://huggingface.co/ali-vosoughi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","text-to-audio","text-to-speech","automatic-speech-recognition"],"keywords_longer_than_N":true},
	{"name":"augmented-recap-datacomp-3m","keyword":"image-to-text","description":"This is an experimental augmentation of about 3 million synthetic captions from Recap-Datacomp-1B. This dataset includes about 2 million multilingual captions. \nIt attempts to balance for gender stereotypes, added occupations, race, union membership, and religion to a subsample. We have also performed hair color and eye color balancing. It also includes some permutations of sentence orders, and modificaitons of the number of items (\"Two\" is changed to \"Three\", \"Four\", etc.)\nWe have also runâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m.","url":"https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"CSTips","keyword":"image-to-text","description":"asiakarysheva/CSTips dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/asiakarysheva/CSTips","creator_name":"Asia Karysheva","creator_url":"https://huggingface.co/asiakarysheva","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"photochat_plus","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for PhotoChat++\n\t\n\n\nðŸš¨ Disclaimer: All models and datasets are intended for research purposes only.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPhotoChat++ is a publicly available multi-modal dialogue dataset, an extended version of PhotoChat. PhotoChat++ contains six intent labels, a triggering sentence, an image description, and salient information (e.g., â€œwordsâ€ or â€œphrasesâ€) to invoke the image-sharing behavior. The purpose of this dataset is to thoroughly assess the image-sharingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/passing2961/photochat_plus.","url":"https://huggingface.co/datasets/passing2961/photochat_plus","creator_name":"Young-Jun Lee","creator_url":"https://huggingface.co/passing2961","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","conversational","monolingual","PhotoChat"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LongVA-TPO-10k","keyword":"video-text-to-text","description":"\n \n \n\n\n\t\n\t\n\t\n\t\t10kTemporal Preference Optimization Dataset for LongVA\n\t\n\nLongVA-TPO-10k, introduced by paper Temporal Preference Optimization for Long-form Video Understanding\n","url":"https://huggingface.co/datasets/ruili0/LongVA-TPO-10k","creator_name":"Rui Li","creator_url":"https://huggingface.co/ruili0","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v44","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v44.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v140","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v140.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v140","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"KHATT_v1.0_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tKHATT_v1.0 - line level\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nKHATT (KFUPM Handwritten Arabic TexT) database is a database of unconstrained handwritten Arabic Text written by 1000 different writers. This research databaseâ€™s development was undertaken by a research group from KFUPM, Dhahran, S audi Arabia headed by Professor Sabri Mahmoud in collaboration with Professor Fink from TU-Dortmund, Germany and Dr. MÃ¤rgner from TU-Braunschweig, Germany.\nThe database includes 2000 similar-textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/johnlockejrr/KHATT_v1.0_dataset.","url":"https://huggingface.co/datasets/johnlockejrr/KHATT_v1.0_dataset","creator_name":"John Locke","creator_url":"https://huggingface.co/johnlockejrr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","Image","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v56","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v56.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PubLayNet","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for PubLayNet\n\t\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nPubLayNet is a dataset for document layout analysis. It contains images of research papers and articles and annotations for various elements in a page such as \"text\", \"list\", \"figure\" etc in these research paper images. The dataset was obtained by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/creative-graphic-design/PubLayNet.","url":"https://huggingface.co/datasets/creative-graphic-design/PubLayNet","creator_name":"Creative Graphic Design Lab","creator_url":"https://huggingface.co/creative-graphic-design","license_name":"Community Data License Agreement Permissive 1.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","question-answering","other"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v43","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v43.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PubLayNet","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for PubLayNet\n\t\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nPubLayNet is a dataset for document layout analysis. It contains images of research papers and articles and annotations for various elements in a page such as \"text\", \"list\", \"figure\" etc in these research paper images. The dataset was obtained by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/creative-graphic-design/PubLayNet.","url":"https://huggingface.co/datasets/creative-graphic-design/PubLayNet","creator_name":"Creative Graphic Design Lab","creator_url":"https://huggingface.co/creative-graphic-design","license_name":"Community Data License Agreement Permissive 1.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","image-to-text","question-answering","other"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v114","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v114.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v114","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BACE-V-SMILES-4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BACE-V-SMILES-4.","url":"https://huggingface.co/datasets/molvision/BACE-V-SMILES-4","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Nike","keyword":"image-to-text","description":"Products from Nike with images, name, category, description and more\n","url":"https://huggingface.co/datasets/bigdata-pw/Nike","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nmax_scale=7.\nimage_size: 1-30.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Quick_Commerce_Grocery_App_Screen_Recording_Dataset","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tQuick Commerce Grocery App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on quick commerce grocery applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Quick_Commerce_Grocery_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Quick_Commerce_Grocery_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Quick_Commerce_Grocery_App_Screen_Recording_Dataset","keyword":"video-to-text","description":"\n\t\n\t\t\n\t\tQuick Commerce Grocery App Screen Recording Dataset\n\t\n\nThis dataset contains high-quality screen recordings of user interactions on quick commerce grocery applications. It has been carefully curated, cleaned, and anonymized to ensure accuracy, completeness, and compliance with privacy standards, making it suitable for AI training, UX research, and user behavior analysis.\n\n\t\n\t\t\n\t\tContact\n\t\n\nFor queries or collaborations related to this dataset, contact:  \n\nanoushka@kgen.ioâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Kratos-AI/Quick_Commerce_Grocery_App_Screen_Recording_Dataset.","url":"https://huggingface.co/datasets/Kratos-AI/Quick_Commerce_Grocery_App_Screen_Recording_Dataset","creator_name":"KratosAI","creator_url":"https://huggingface.co/Kratos-AI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","url":"https://huggingface.co/datasets/lodestones/photo-typography","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"image-text-to-text","description":"\n  \n\n\n[ðŸ“– arXiv Paper] \n[ðŸ“Š R1-Reward Code] \n[ðŸ“ R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nðŸ”¥ We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVisuLogic-Train (OmniTool)\n\t\n\nShort description of the datasetâ€¦\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\nds = load_dataset(\"OmniTool/VisuLogic-Train\", \"solution\", split=\"train\")\"internvl\"\nprint(ds[0])\n\n","url":"https://huggingface.co/datasets/OmniTool/VisuLogic-Train","creator_name":"OmniTool","creator_url":"https://huggingface.co/OmniTool","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"artbench-pd-256x256","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ArtBench Public Domain 256x256\n\t\n\n\næ—¥æœ¬èªžã¯ã“ã¡ã‚‰\nThis repository is the subset of ArtBench.\nArtBench is the dataset for historical arts such as Art Nouveau and Ukiyo-e.\nI picked up public domain images from ArtBench. Then, I create new dataset.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nYou can use huggingface datasets to download the dataset.\nYou can also download the tar file.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"alfredplpl/artbench-pd-256x256\")\n\n\n\t\n\t\t\n\t\tIntended Useâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256.","url":"https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-recap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning\n\t\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚ ä»¥ä¸‹ã®æƒ…å ±ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nPhi-3 Visionã§Dense Captioningã—ãŸè‹±èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³\n\nä¸»ã‚­ãƒ¼ã¯photoidã§ã™ã®ã§ã€CommonCatalog CC-BYã¨çµåˆã™ã‚‹ãªã‚Šã—ã¦ä½¿ã£ã¦ãã ã•ã„ã€‚ streaming=Trueã§èª­ã¿è¾¼ã‚€ã¨åŒã˜é †ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã®ã§ãã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒä¸€ç•ªæ¥½ã§ã™ã€‚\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap.","url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Magma-Mind2Web-SoM","keyword":"image-to-text","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1â€ Â \nReuben Tan1â€ Â \nQianhui Wu1â€ Â \nRuijie Zheng2â€¡Â \nBaolin Peng1â€¡Â \nYongyuan Liang2â€¡\nYu Gu1Â \nMu Cai3Â \nSeonghyeon Ye4Â \nJoel Jang5Â \nYuquan Deng5Â \nLars Liden1Â \nJianfeng Gao1â–½\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  â€  First authors  â€¡ Second authors  â–½ Leadership  \n[arXiv Paper] Â  [Project Page] Â  [Hugging Face Paper] Â  [Github Repo] Â  [Video]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM.","url":"https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v171","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v171.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v171","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"multicare-case-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMultiCaRe: Open-Source Clinical Case Dataset\n\t\n\nMultiCaRe is an open-source, multimodal clinical case dataset built from the PubMed Central Open Access (OA) Case Report articles. It aggregates de-identified, open-access case narratives, figure images, captions, and rich article metadata across diverse specialties. Figures are mapped to their case text with explicit references and to article-level metadata, enabling grounded multimodal use.\n\nSource and process: OA case reports wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openmed-community/multicare-case-images.","url":"https://huggingface.co/datasets/openmed-community/multicare-case-images","creator_name":"OpenMed Community","creator_url":"https://huggingface.co/openmed-community","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","document-question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LaTeX_Image_Pairs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tLaTeX Image Pairs Dataset\n\t\n\nThis dataset comprises a unique collection of LaTeX expressions paired with their corresponding images. The LaTeX expressions were meticulously scraped from a variety of open-source textbooks, ensuring a diverse and comprehensive dataset. Sample references from these textbooks will be provided to illustrate the sources of these expressions.\nIn addition to the raw LaTeX expressions, this dataset includes images of the rendered expressions. Each LaTeXâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/henryholloway/LaTeX_Image_Pairs.","url":"https://huggingface.co/datasets/henryholloway/LaTeX_Image_Pairs","creator_name":"Henry Holloway","creator_url":"https://huggingface.co/henryholloway","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-generation","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"znanio-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 19,060 educational images from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): Theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-images.","url":"https://huggingface.co/datasets/nyuuzyou/znanio-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v76","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v76.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v76","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v39","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v39.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Belfort-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBelfort - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Belfort dataset includes minutes of the municipal council of the French city of Belfort. \nText lines were extracted using an automatic model and may contain segmentation errors. The transcriptions were obtained through a crowdsourcing campaign using the Callico web plateform.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Belfort-line.","url":"https://huggingface.co/datasets/Teklia/Belfort-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"image-text-to-text","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nðŸ™ GitHub Repo: waltonfuture/MM-UPT\nðŸ“œ Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Finna-JOKA-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tOld photographs from Journalistic Picture Archive JOKA\n\t\n\nThis is an image dataset consisting of 4595 old photographs (until 1940) from the collections of the Journalistic Picture Archive JOKA, obtained from the Finna.fi discovery service.\nThe images are intended to be used for different AI/ML tasks such as generating captions or colorizing them.\nThe images themselves are JPEG files under the directory images.\nThe metadata.jsonl file contains metadata about each image, for exampleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NatLibFi/Finna-JOKA-images.","url":"https://huggingface.co/datasets/NatLibFi/Finna-JOKA-images","creator_name":"National Library of Finland","creator_url":"https://huggingface.co/NatLibFi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","Finnish","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"visual_genome_revised","keyword":"image-to-text","description":"AnnaZ1103/visual_genome_revised dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/AnnaZ1103/visual_genome_revised","creator_name":"Huiqi Zou","creator_url":"https://huggingface.co/AnnaZ1103","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","object-detection","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"home_decoration_objects_images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 5125 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 5125\nAverage words in long description: 18.1\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/home_decoration_objects_images.","url":"https://huggingface.co/datasets/AntZet/home_decoration_objects_images","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"home_decoration_objects_images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 5125 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 5125\nAverage words in long description: 18.1\nAverage words in short description: 9.4\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/home_decoration_objects_images.","url":"https://huggingface.co/datasets/AntZet/home_decoration_objects_images","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"danbooru2023-florence2-caption","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDanbooru2023 - Florence2 Caption dataset\n\t\n\nThis dataset contains captions of danbooru2023 images generated by microsoft/Florence-2-large \nI use original one with  task token\n\n\t\n\t\t\n\t\tFormat\n\t\n\nparquet:\n\nkey: the danbooru id of the image\nparsed: parsed florence 2 output of the image\n\n\n\t\n\t\t\n\t\tStat\n\t\n\n\n\t\n\t\t\n\t\tMORE_DETAILED_CAPTION\n\t\n\n\nEntries: 7,438,449\nOutput Tokens (Min/Max/Mean/Median):\nFlan T5 Tokenizer: 19/736/120/114\nDFN CLIP Tokenizer: 19/826/108.7/103\nQwen2 Tokenizer:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption.","url":"https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption","creator_name":"Shih-Ying Yeh","creator_url":"https://huggingface.co/KBlueLeaf","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"llava-1.5-665k-instructions","keyword":"image-text-to-text","description":"This dataset repository, LLaVA-1.5-665K-Instructions, is notably utilized in the paper Zero-Shot Vision Encoder Grafting via LLM Surrogates.\nThe official code repository for the paper can be found here: https://github.com/kaiyuyue/zero\n\n\t\n\t\t\n\t\tLLaVA-1.5-665K-Instructions\n\t\n\nThis dataset repo contains the entire LLaVA-1.5-665K-Instructions in one place, including images and text sequences.\nThe images are in train_split/*.tars and the text sequences are in jsons:\n\nllava_v1_5_mix665k.json is theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kaiyuyue/llava-1.5-665k-instructions.","url":"https://huggingface.co/datasets/kaiyuyue/llava-1.5-665k-instructions","creator_name":"Kaiyu Yue","creator_url":"https://huggingface.co/kaiyuyue","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"M-Attack_AdvSamples","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tM-Attack Adversarial Samples Dataset\n\t\n\nThis dataset contains 100 adversarial samples generated using M-Attack to perturb the images from the NIPS 2017 Adversarial Attacks and Defenses Competition. This dataset is used in the paper A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset consists of total 300 adversarial samples organized in threeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI-LLM/M-Attack_AdvSamples.","url":"https://huggingface.co/datasets/MBZUAI-LLM/M-Attack_AdvSamples","creator_name":"MBZUAI-LLM","creator_url":"https://huggingface.co/MBZUAI-LLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"image-text-to-text","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpen Image Preferences\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1.","url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"amazon-qwen2vl-listing","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAmazon Qwen2-VL Listing Dataset\n\t\n\nThis tiny dataset accompanies the LoRA adapter:\n\nModel: https://huggingface.co/soupstick/qwen2vl-amazon-ft-lora\n\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\ndata/train.json â€” LLaMA-Factory style JSON with fields:\nimages (list of filenames or a single filename)\ninstruction (prompt)\noutput (JSON-formatted string with title/bullets/description)\n\n\neval/eval_predictions.jsonl â€” model generations used for quick evaluation.\n\n\nNote: This repo is intentionally lightweight forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/soupstick/amazon-qwen2vl-listing.","url":"https://huggingface.co/datasets/soupstick/amazon-qwen2vl-listing","creator_name":"Souptik Chakraborty","creator_url":"https://huggingface.co/soupstick","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"image-to-text","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v128","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v128.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v128","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"table-to-text","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"QuantumAI","keyword":"video-text-to-text","description":"Groovy-123/QuantumAI dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/QuantumAI","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Orsta-Data-47k","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tOrsta-Data-47k Dataset\n\t\n\n\nðŸ™ GitHub Repo: MiniMax-AI/One-RL-to-See-Them-All\nðŸ“œ Paper (arXiv): V-Triune: One RL to See Them All (arXiv:2505.18129)\n\n\n\t\n\t\t\n\t\tDataset Description ðŸ“–\n\t\n\nOrsta-Data-47k is a specialized dataset curated for the post-training of Vision-Language Models (VLMs) using our V-Triune unified reinforcement learning system. Its primary purpose is to enable robust joint training across a diverse spectrum of both visual reasoning and visual perception tasks, poweringâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k.","url":"https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k","creator_name":"One-RL-to-See-Them-All","creator_url":"https://huggingface.co/One-RL-to-See-Them-All","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K<n<100K","arxiv:2505.18129"],"keywords_longer_than_N":true},
	{"name":"CUAHarm","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCUAHarm Dataset\n\t\n\nThis repository contains the CUAHarm benchmark, introduced in the paper Measuring Harmfulness of Computer-Using Agents.\nCUAHarm is a benchmark designed to evaluate the safety risks of Computer-Using Agents (CUAs) â€” AI agents that can autonomously control computers to perform multi-step actions.\nCode: https://github.com/db-ol/CUAHarm\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nComputer-using agents (CUAs), which autonomously control computers to perform multi-step actions, might poseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CUAHarm/CUAHarm.","url":"https://huggingface.co/datasets/CUAHarm/CUAHarm","creator_name":"CUAHarm","creator_url":"https://huggingface.co/CUAHarm","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-06","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-06","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","100B<n<1T"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-23","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-23","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v53","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v53.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v158","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v158.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v158","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GPRadar-Defect-MultiTask","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGPRadar-Defect-MultiTask æ•°æ®é›†\n\t\n\næœ¬ä»“åº“åŒ…å«ç”¨äºŽå¾®è°ƒPaLI-GEMMAå¤šæ¨¡æ€æ¨¡åž‹çš„åœ°è´¨é›·è¾¾(GPR)ç¼ºé™·æ£€æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºŽåœ°ä¸‹ç»“æž„ä¸­çš„ç©ºæ´žå’Œè£‚ç¼æ£€æµ‹ä¸Žåˆ†æžã€‚\n\n\t\n\t\t\n\t\tæ•°æ®é›†ç»“æž„\n\t\n\næ•°æ®é›†ç»„ç»‡å¦‚ä¸‹ï¼š\ndataset/\nâ”œâ”€â”€ annotations/ - åŒ…å«JSONå’ŒJSONLæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶\nâ”‚   â”œâ”€â”€ _annotations.train.jsonl - è®­ç»ƒé›†æ ‡æ³¨\nâ”‚   â”œâ”€â”€ _annotations.valid.jsonl - éªŒè¯é›†æ ‡æ³¨\nâ”‚   â”œâ”€â”€ _annotations.test.jsonl - æµ‹è¯•é›†æ ‡æ³¨\nâ”‚   â”œâ”€â”€ p-1.v1i.paligemma/ - ä¸»æ•°æ®é›†å…ƒæ•°æ®\nâ”‚   â””â”€â”€ p-1.v1i.paligemma-multimodal/ - å¤šæ¨¡æ€æ•°æ®é›†å…ƒæ•°æ®\nâ”œâ”€â”€ images/ - åŒ…å«æ‰€æœ‰å›¾åƒæ–‡ä»¶\n\n\n\t\n\t\t\n\t\tç‰¹ç‚¹\n\t\n\n\nåŒ…å«874å¼ å¸¦æ³¨é‡Šçš„åœ°è´¨é›·è¾¾æ‰«æå›¾åƒ\nå›¾åƒé¢„å¤„ç†ä¸º640x640åƒç´ å¤§å°\næ”¯æŒå¤šç§ä»»åŠ¡ç±»åž‹ï¼šç¼ºé™·æ£€æµ‹ã€ä½ç½®å®šä½å’Œæè¿°ç”Ÿæˆâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingqiang/GPRadar-Defect-MultiTask.","url":"https://huggingface.co/datasets/xingqiang/GPRadar-Defect-MultiTask","creator_name":"chen.xingqiang","creator_url":"https://huggingface.co/xingqiang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-to-text","visual-question-answering","image-captioning","English"],"keywords_longer_than_N":true},
	{"name":"wine-images-126k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tWine Images Dataset 126K\n\t\n\nA comprehensive dataset of 107,821 wine bottle images linked to the Wine Text Dataset 126K. This companion dataset provides high-quality wine bottle images for computer vision, multimodal machine learning, and wine recognition tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains wine bottle images scraped from wine retailer websites. Each image is linked to detailed wine information (descriptions, pricing, categories, regions) via stable IDs thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cipher982/wine-images-126k.","url":"https://huggingface.co/datasets/cipher982/wine-images-126k","creator_name":"David Rose","creator_url":"https://huggingface.co/cipher982","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","feature-extraction","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"LLaVA-JP-Instruct-108K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset details\n\t\n\nDataset type: LLaVA JP Instruct 108K created by changing the data sets in Japanese Visual Genome VQA dataset and train data of docci_ja to the LLaVA-Instruct format.\n\n\t\n\t\t\n\t\n\t\n\t\tAcknowledgement\n\t\n\n\nJapanese Visual Genome VQA dataset\nDOCCI\n\n\n\t\n\t\t\n\t\n\t\n\t\tLisence\n\t\n\nApach lisense 2.0\n","url":"https://huggingface.co/datasets/toshi456/LLaVA-JP-Instruct-108K","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Japanese","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Recap-DataComp-100K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nRecap-DataComp-100K is a subset of UCSC-VLAA/Recap-DataComp-1B. \nThis dataset aims to ease the development of vision-language models by providing a readily-available small collection of image-text pairs.\nUse this dataset for sanity checks, developing POCs, or other quick multimodal dev. For serious model training please refer to the original repo linked above.  \n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nAlways cite the original authors . I've copied their citation info here for yourâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K.","url":"https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ChartCap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tChartCap: Mitigating Hallucination of Dense Chart Captioning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nChartCap is a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTraining set: {train_size:,} examples (508,783)\nTest set: {test_size:,} examples (56,486 - human verified)\nTotal: {train_size + test_size:,} examplesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/junyoung-00/ChartCap.","url":"https://huggingface.co/datasets/junyoung-00/ChartCap","creator_name":"junyoung.lim","creator_url":"https://huggingface.co/junyoung-00","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v42","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v42.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenMind2D","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenMind2D: 2D Brain MRI Slices\n\t\n\nOpenMind2D is a 2D medical imaging dataset derived from the OpenMind dataset. It contains 335,754 2D slices extracted from 3D brain MRI volumes in three anatomical orientations (axial, sagittal, coronal).\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Images: 335,754\nResolution: 256Ã—256 pixels\nFormat: JPEG\nSize: ~11.7 GB\nSplits: Train (70%), Validation (20%), Test (10%)\nOrientations: Axial, sagittal, coronal\nModalities: T1w, T2w, FLAIR, DWI, and 19+ additionalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/liamchalcroft/OpenMind2D.","url":"https://huggingface.co/datasets/liamchalcroft/OpenMind2D","creator_name":"Liam Chalcroft","creator_url":"https://huggingface.co/liamchalcroft","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","zero-shot-image-classification","multi-class-image-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v20","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v20.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"s2lcd","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSentinel-2 Land-cover Captioning Dataset\n\t\n\nThe Sentinel-2 Land-cover Captioning Dataset (S2LCD) is a newly proposed dataset specifically designed for deep learning research on remote sensing image captioning. It comprises 1533 image patches, each of size 224 Ã— 224 pixels, derived from Sentinel-2 L2A images. The dataset ensures a diverse representation of land cover and land use types in temperate regions, including forests, mountains, agricultural lands, and urban areas, each one withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neuronelab/s2lcd.","url":"https://huggingface.co/datasets/neuronelab/s2lcd","creator_name":"NeuRoNeLab","creator_url":"https://huggingface.co/neuronelab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish","keyword":"image-to-text","description":"This dataset is generated from TasvirEt[1]. I do not own the copyright of either the images or the captions. Please refer to the paper's webpage for further details.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÌˆruÌˆntuÌˆlerden Otomatik TuÌˆrkcÌ§e AcÌ§Ä±klama OluÅŸturma Ä°cÌ§in Bir DenektaÅŸÄ± Veri KuÌˆmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal Ä°ÅŸleme ve Ä°letiÅŸim UygulamalarÄ±â€¦ See the full description on the dataset page: https://huggingface.co/datasets/atasoglu/flickr8k-turkish.","url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish","keyword":"image-to-text","description":"This dataset is generated from TasvirEt[1]. I do not own the copyright of either the images or the captions. Please refer to the paper's webpage for further details.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÌˆruÌˆntuÌˆlerden Otomatik TuÌˆrkcÌ§e AcÌ§Ä±klama OluÅŸturma Ä°cÌ§in Bir DenektaÅŸÄ± Veri KuÌˆmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal Ä°ÅŸleme ve Ä°letiÅŸim UygulamalarÄ±â€¦ See the full description on the dataset page: https://huggingface.co/datasets/atasoglu/flickr8k-turkish.","url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"video-recs-describe-what-you-see","keyword":"video-text-to-text","description":"More information coming soon\n\n\t\n\t\t\n\t\tFFmpeg processing script\n\t\n\n# Download the list of URLs recursively using wget\ncat video_ids_list.txt | parallel -j10 --line-buffer '\n   # Extract the video name and prepare an output directory\n   video_name=\"https://recsys.westlake.edu.cn/MicroLens-100k-Dataset/MicroLens-100k_videos/{}\"\n   output_file=\"sampled_videos/{}\"\n   # Skip processing if the output file already exists\n   if [ ! -f \"$output_file\" ]; then\n           axel -n 10 --quiet  \"$video_name\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/marcodena/video-recs-describe-what-you-see.","url":"https://huggingface.co/datasets/marcodena/video-recs-describe-what-you-see","creator_name":"Marco De Nadai","creator_url":"https://huggingface.co/marcodena","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"chartqa-caption-gpt4","keyword":"image-to-text","description":"Caption data for ChartQA images.\n","url":"https://huggingface.co/datasets/xchen16/chartqa-caption-gpt4","creator_name":"Xiaohui Chen","creator_url":"https://huggingface.co/xchen16","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 3-12.\noutput pattern image size: 1-4.\npixel count: 1-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ccpd2019train","keyword":"image-to-text","description":"CCPD2019 training dataset. Used for my training on Kaggle.\nIf you are benefited from this dataset, please cite their paper as follows:\n@inproceedings{xu2018towards,\n  title={Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline},\n  author={Xu, Zhenbo and Yang, Wei and Meng, Ajin and Lu, Nanxue and Huang, Huan},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  pages={255--271},\n  year={2018}\n}\n\n","url":"https://huggingface.co/datasets/okita-souji/ccpd2019train","creator_name":"okita souji","creator_url":"https://huggingface.co/okita-souji","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-to-text","Chinese","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"concept_coverage_laion_6m","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ“¦ Freeze-Align Dataset\n\t\n\nThe Freeze-Align Dataset (concept_coverage_laion_6m) is a curated collection of high-quality image-text pairs designed to facilitate efficient multimodal alignment using frozen unimodal encoders. This dataset supports the research presented in our CVPR 2025 paper, \"Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment\", enabling models to achieve CLIP-level performance with significantly reduced computational resources.\nThe dataset is curatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mayug/concept_coverage_laion_6m.","url":"https://huggingface.co/datasets/mayug/concept_coverage_laion_6m","creator_name":"Mayug Maniparambil","creator_url":"https://huggingface.co/mayug","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","text-to-image","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayuka Jayawardhanaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/pixelprose.","url":"https://huggingface.co/datasets/lodestones/pixelprose","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"PORTO","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPost-OCR Resources for Text Optimisation\n\t\n\nResource for evaluation and develop OCRs and Post-OCR focused on historical Portuguese.\nHow to load the dataset:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"LIACC/PORTO\")\n\n","url":"https://huggingface.co/datasets/LIACC/PORTO","creator_name":"LIACC","creator_url":"https://huggingface.co/LIACC","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","fill-mask","text-generation","Portuguese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"NutritionQA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nNutritionQA is a novel benchmark for understanding photos of nutrition labels, with practical applications like aiding users with visual impairments. \nNutritionQA contains 50 photos of nutrition labels, each photo is paired with a descriptive question and a reasoning question (requires multi-hop reasoning).\nThe figure below shows that open VLMs perform poorly on NutritionQA, even after training on millions of images. Our code-guided synthetic data generation system canâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/yyupenn/NutritionQA.","url":"https://huggingface.co/datasets/yyupenn/NutritionQA","creator_name":"Yue Yang","creator_url":"https://huggingface.co/yyupenn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v211","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v211.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v211","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"High-Quality-Synthetic-Images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDataset Name: Goldfish High-Quality AI-Generated Images Dataset\nThe Goldfish High-Quality AI-Generated Images Dataset contains a curated collection of high-resolution images. Each image is paired with an AI-generated prompt, specifically crafted to describe the visual content, rather than using the original prompts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nSource: The images were collected from a single high-quality source specializing in AI-generated art.\nResolution: Allâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Chan-Y/High-Quality-Synthetic-Images.","url":"https://huggingface.co/datasets/Chan-Y/High-Quality-Synthetic-Images","creator_name":"Cihan YalÃ§Ä±n","creator_url":"https://huggingface.co/Chan-Y","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-image","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"laion_cc_sbu","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLLaVA-1.5 Test Images Not in LAION2B\n\t\n\nThis dataset contains validation and test images from various vision-language datasets that are commonly used to evaluate LLaVA-1.5 and similar models, but are not present in the LAION2B dataset.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"iammytoo/laion_cc_sbu\")\n\n","url":"https://huggingface.co/datasets/iammytoo/laion_cc_sbu","creator_name":"Miyamoto Ryoto","creator_url":"https://huggingface.co/iammytoo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ru-filtered-web-captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDiTy/ru-filtered-web-captions\n\t\n\n\n\t\n\t\t\n\t\tDataset summary\n\t\n\nThis is a translated RussianÂ part of the filtered web captions.\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n    'caption': 'gladiator standing in a smoke with torch and sword',\n    'url': 'https://thumb9.shutterstock.com/display_pic_with_logo/78238/155376242/stock-photo-gladiator-standing-in-a-smoke-with-torch-and-sword-155376242.jpg',\n    'translated_caption': 'Ð³Ð»Ð°Ð´Ð¸Ð°Ñ‚Ð¾Ñ€, ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¹ Ð² Ð´Ñ‹Ð¼Ñƒ Ñ Ñ„Ð°ÐºÐµÐ»Ð¾Ð¼ Ð¸ Ð¼ÐµÑ‡Ð¾Ð¼'\n}\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ncaption:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/DiTy/ru-filtered-web-captions.","url":"https://huggingface.co/datasets/DiTy/ru-filtered-web-captions","creator_name":"Dmitry Tishencko","creator_url":"https://huggingface.co/DiTy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","feature-extraction","image-feature-extraction","Russian"],"keywords_longer_than_N":true},
	{"name":"mscoco","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCommon Objects in Context (COCO) Dataset\n\t\n\nThis dataset is English captions of COCO dataset. \nThe splits in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.\n\n\t\n\t\t\n\t\n\t\n\t\tReference\n\t\n\n@misc{lin2015microsoftcococommonobjectsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/mscoco.","url":"https://huggingface.co/datasets/romrawinjp/mscoco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v38","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v38.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"datacomp-small-clip","keyword":"image-to-text","description":"\n    \n        \n    \n\n\n\n    \n        Production-ready \n        data processing made \n        easy \n        and \n        shareable\n    \n    \n    Explore the Fondant docs Â»\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for fondant-ai/datacomp-small-clip\n\t\n\n\n\nThis is a dataset containing image urls and their CLIP embeddings, based on the datacomp_small dataset, and processed with fondant.\n\n\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\nLarge (image) datasets are often unwieldy to use due to theirâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fondant-ai/datacomp-small-clip.","url":"https://huggingface.co/datasets/fondant-ai/datacomp-small-clip","creator_name":"Fondant","creator_url":"https://huggingface.co/fondant-ai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","cc-by-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"STEMGenBench_cot","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSTEM Image Captions Dataset\n\t\n\nThis dataset contains AI-generated captions for STEM images.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is organized in batches:\n\nTotal batches: 1\nEach batch is stored in a separate directory (batch_0000, batch_0001, etc.)\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nTotal items: 1002\nSuccessfully captioned: 1002\nErrors: 0\nSkipped: 0\n\n\n\t\n\t\t\n\t\tLoading the Dataset\n\t\n\nYou can load individual batches:\nfrom datasets import load_dataset\n\n# Load a specific batch\nbatch_0 =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/JackyZhuo/STEMGenBench_cot.","url":"https://huggingface.co/datasets/JackyZhuo/STEMGenBench_cot","creator_name":"Le Zhuo","creator_url":"https://huggingface.co/JackyZhuo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"foods-nutrition-dataset","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tFoods Nutrition Dataset\n\t\n\nA curated dataset of 1,028 food items with detailed nutritional information. Lightweight and easy to use for machine learning, health analytics and educational projects.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\nFeature\nDescription\n\n\n\t\t\nItems\n1,028 rows of common food items\n\n\nColumns\nEnergy kcal, Carbs, Protein, Fat, Free sugar, Fiber, Cholesterol, Calcium\n\n\nSplit\nSingle train split\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis dataset offers a structured and small-scaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/adarshzolekar/foods-nutrition-dataset.","url":"https://huggingface.co/datasets/adarshzolekar/foods-nutrition-dataset","creator_name":"Adarsh Zolekar","creator_url":"https://huggingface.co/adarshzolekar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","table-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"foods-nutrition-dataset","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tFoods Nutrition Dataset\n\t\n\nA curated dataset of 1,028 food items with detailed nutritional information. Lightweight and easy to use for machine learning, health analytics and educational projects.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\nFeature\nDescription\n\n\n\t\t\nItems\n1,028 rows of common food items\n\n\nColumns\nEnergy kcal, Carbs, Protein, Fat, Free sugar, Fiber, Cholesterol, Calcium\n\n\nSplit\nSingle train split\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis dataset offers a structured and small-scaleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/adarshzolekar/foods-nutrition-dataset.","url":"https://huggingface.co/datasets/adarshzolekar/foods-nutrition-dataset","creator_name":"Adarsh Zolekar","creator_url":"https://huggingface.co/adarshzolekar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","table-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"MicroG-4M","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tMicroG-4M Dataset\n\t\n\nThis repository stores the entire content of the  MicroG-4M dataset itself.\nFor more information and details, including training, evaluation, statistics, and related code, please:\n\nRefer to our paper\n\nVisit our GitHub\n\n\nIn addition to the original dataset format, we provide a Parquet format for automatically generating Croissant files on the Hugging Face platform. Loading via Croissant will fetch these Parquet files directly. For detailed information, please checkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LEI-QI-233/MicroG-4M.","url":"https://huggingface.co/datasets/LEI-QI-233/MicroG-4M","creator_name":"Lei Qi","creator_url":"https://huggingface.co/LEI-QI-233","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","visual-question-answering","video-text-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"sft_data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nSFT instruction: Contains GPT-4o distilled chain-of-thought reasoning data covering wide range of tasks. Together with corresponding short-answer prediction data.\n\nImage: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODOâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/sft_data.","url":"https://huggingface.co/datasets/Share4oReasoning/sft_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-10K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512Ã—512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFTâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K.","url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v34","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v34.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AmbiBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAmbiBench Metadata\n\t\n\nThis dataset provides the metadata used in the AmbiBench benchmark.Each row corresponds to an ambiguous image or video sample paired with a questionâ€“answer pair, derived from the benchmark design.\n\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\n\t\n\t\t\n\t\tquestion\n\t\n\n\nThe natural-language question posed to the model.\n\n\n\t\n\t\t\n\t\tanswer\n\t\n\n\nThe reference answer to the question.  \nMay include:\nA single object name (\"snake\"),  \nA semicolon-separated pair for bistable/multi-scene (\"duck; rabbit\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BLNL/AmbiBench.","url":"https://huggingface.co/datasets/BLNL/AmbiBench","creator_name":"BLNL","creator_url":"https://huggingface.co/BLNL","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","object-detection","English","mit","100M<n<1B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"kassenzettel-synth","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tkassenzettel-synth\n\t\n\n\n\nThis dataset contains generate images of receipts.\nIt has been generated using github.com/nimalu/kassenzettel.\nThe current version contains 1000 samples.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nFor each instance, there is the underlying data, an image of the receipt, an image of the masks and both images augmented by adding crinkles.\n \n\n","url":"https://huggingface.co/datasets/nimalu/kassenzettel-synth","creator_name":"Niklas","creator_url":"https://huggingface.co/nimalu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-segmentation","German","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"my_smolvla123456789","keyword":"image-to-text","description":"Amanpatel81/my_smolvla123456789 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Amanpatel81/my_smolvla123456789","creator_name":"Amankumar Patel","creator_url":"https://huggingface.co/Amanpatel81","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","table-question-answering","question-answering","translation","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v27","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v27.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"llava-med-zh-instruct-60k","keyword":"image-to-text","description":"This Chinese dataset was translated from llava-med using Qwen1.5-14B-Chat and contains 60k medical visual instruction data points.\nYou can organize content in the dataset_info.json in LLaMA Factory like this:\n\"llava_med_zh_60k\": {\n  \"hf_hub_url\": \"BUAADreamer/llava-med-zh-instruct-60k\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"messages\",\n    \"images\": \"images\"\n  },\n  \"tags\": {\n    \"role_tag\": \"role\",\n    \"content_tag\": \"content\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/BUAADreamer/llava-med-zh-instruct-60k.","url":"https://huggingface.co/datasets/BUAADreamer/llava-med-zh-instruct-60k","creator_name":"Zhangchi Feng","creator_url":"https://huggingface.co/BUAADreamer","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PersReFex","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Multi-Agent Referential Communication Dataset\n\t\n\n\n\n\nExample scene showing the speaker (left) and listener (right) views.\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains spatial dialogue data for multi-agent referential communication tasks in 3D environments. It includes pairs of images showing speaker and listener views within photorealistic indoor scenes, along with natural language descriptions of target object locations.\nThe keyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZinengTang/PersReFex.","url":"https://huggingface.co/datasets/ZinengTang/PersReFex","creator_name":"Zineng Tang","creator_url":"https://huggingface.co/ZinengTang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"showdown-clicks","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tshowdown-clicks\n\t\n\nGeneral Agents\nðŸ¤— Dataset | GitHub\nshowdown is a suite of offline and online benchmarks for computer-use agents.\nshowdown-clicks is a collection of 5,679 left clicks of humans performing various tasks in a macOS desktop environment. It is intended to evaluate instruction-following and low-level control capabilities of computer-use agents.\nAs of March 2025, we are releasing a subset of the full set, showdown-clicks-dev, containing 557 clicks. All examples areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/generalagents/showdown-clicks.","url":"https://huggingface.co/datasets/generalagents/showdown-clicks","creator_name":"General Agents","creator_url":"https://huggingface.co/generalagents","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Sujet-Finance-Vision-10k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSujet Finance Vision 10k Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Sujet Finance Vision 10k dataset is a comprehensive collection of financial document images along with their associated textual annotations. This dataset is specifically designed to facilitate the training and evaluation of Vision-Language Models (VLMs) in recognizing and describing various types of financial documents.\n\n\t\n\t\t\n\t\tImage Characteristics\n\t\n\nThe dataset consists of 9819 handpicked images of financialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sujet-ai/Sujet-Finance-Vision-10k.","url":"https://huggingface.co/datasets/sujet-ai/Sujet-Finance-Vision-10k","creator_name":"Sujet AI","creator_url":"https://huggingface.co/sujet-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v122","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v122.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v122","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GeoGen","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tGeoGeo: GeoExpand & GeoSynth\n\t\n\nThis repository contains the GeoExpand and GeoSynth datasets, originally introduced in the paper Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration.\nThe datasets are designed to enhance and evaluate the geometric problem-solving capabilities of multimodal large language models.\nGitHub Repository: ycpNotFound/GeoGen\nThese datasets are also referenced and contextualized in the survey paper A Survey of Deepâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ycpNotFound/GeoGen.","url":"https://huggingface.co/datasets/ycpNotFound/GeoGen","creator_name":"Yicheng Pan","creator_url":"https://huggingface.co/ycpNotFound","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Hampsty","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Hampsty","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","license_name":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"webcode2m_purified","keyword":"image-to-text","description":"WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs\nFeatures:\n\nimage: the screenshot of the webpage.\nbbox: the layout information, i.e., the bounding boxes (Bbox) of all the elements in the webpage, which contains the size, position, and hierarchy information. \ntext: the webpage code text including HTML/CSS code.\nscale: the scale of the screenshot, in the format [width, height].\nlang: the main language of the text content displayed on the rendered page (excluding HTML/CSSâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xcodemind/webcode2m_purified.","url":"https://huggingface.co/datasets/xcodemind/webcode2m_purified","creator_name":"xcodemind","creator_url":"https://huggingface.co/xcodemind","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Invoice_dataset","keyword":"image-to-text","description":"Sathyakala/Invoice_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Sathyakala/Invoice_dataset","creator_name":"Sathyakala Muthaiyan","creator_url":"https://huggingface.co/Sathyakala","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Tabular_Cups_Dataset","keyword":"table-to-text","description":"yusenthebot/Tabular_Cups_Dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/yusenthebot/Tabular_Cups_Dataset","creator_name":"yusen","creator_url":"https://huggingface.co/yusenthebot","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfiMM-WebMath-40B","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tInfiMM-WebMath-40B Dataset\n\t\n\nArXiv| PDF\nThis dataset is also discussed in the survey paper A Survey of Deep Learning for Geometry Problem Solving.\nThe accompanying reading list/code for the survey can be found at: https://github.com/majianz/gps-survey\nInfiMM-WebMath-40B is a large-scale, open-source multimodal dataset specifically designed for mathematical reasoning tasks. It incorporates both text and images, extracted from web documents, to advance the pre-training of Multimodalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B.","url":"https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B","creator_name":"InfiMM","creator_url":"https://huggingface.co/Infi-MM","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Pix2Cap-COCO","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPix2Cap-COCO\n\t\n\n\n    \n\n\n\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPix2Cap-COCO is the first pixel-level captioning dataset derived from the panoptic COCO 2017 dataset, designed to provide more precise visual descriptions than traditional region-level captioning datasets. It consists of 20,550 images, partitioned into a training set (18,212 images) and a validation set (2,338 images), mirroring the original COCO split. The dataset includes 167,254 detailed pixel-level captions, each averagingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/geshang/Pix2Cap-COCO.","url":"https://huggingface.co/datasets/geshang/Pix2Cap-COCO","creator_name":"geshang","creator_url":"https://huggingface.co/geshang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v37","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v37.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HOME-Alcar-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHOME-Alcar - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe HOME-Alcar (Aligned and Annotated Cartularies) dataset is a Medieval corpus. The 17 medieval manuscripts in this corpus are cartularies, i.e. books copying charters and legal acts, produced between the 12th and 14th centuries. \nThis dataset comes from the following publication:\nStutzmann, D., Torres Aguilar, S., & Chaffenet, P. (2021). HOME-Alcar: Aligned and Annotated Cartularies [Data set]. Zenodo.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/HOME-Alcar-line.","url":"https://huggingface.co/datasets/Teklia/HOME-Alcar-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Latin","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VidComposition_Benchmark","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVidComposition Benchmark\n\t\n\nðŸ–¥ Project Page | ðŸš€ Evaluation Space\nThe advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interactâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark.","url":"https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark","creator_name":"JunJiaGuo","creator_url":"https://huggingface.co/JunJiaGuo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-text-to-text","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"BigDocs-Sketch2Flow","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ServiceNow/BigDocs-Sketch2Flow\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe BigDocs-Sketch2Flow dataset is introduced in the StarFlow paper. It contains workflow diagrams paired with structured JSON workflows for the task of converting sketches or diagrams into executable workflow representations. Samples include both synthetic programmatically generated diagrams and human-created sketches (manual, digital, whiteboard) as well as user interface screenshots.\nThe dataset supportsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/BigDocs-Sketch2Flow.","url":"https://huggingface.co/datasets/ServiceNow/BigDocs-Sketch2Flow","creator_name":"ServiceNow","creator_url":"https://huggingface.co/ServiceNow","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v104","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v104.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v104","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v126","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v126.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v126","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"scand","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSCAND Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/scand.","url":"https://huggingface.co/datasets/mateoguaman/scand","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v85","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v85.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v85","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BigDocs-Sketch2Flow","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ServiceNow/BigDocs-Sketch2Flow\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe BigDocs-Sketch2Flow dataset is introduced in the StarFlow paper. It contains workflow diagrams paired with structured JSON workflows for the task of converting sketches or diagrams into executable workflow representations. Samples include both synthetic programmatically generated diagrams and human-created sketches (manual, digital, whiteboard) as well as user interface screenshots.\nThe dataset supportsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/BigDocs-Sketch2Flow.","url":"https://huggingface.co/datasets/ServiceNow/BigDocs-Sketch2Flow","creator_name":"ServiceNow","creator_url":"https://huggingface.co/ServiceNow","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Openpdf-Blank-v2.0-Sample","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenpdf-Blank-v2.0-Sample\n\t\n\nOpenpdf-Blank-v2.0-Sample is a sample dataset of blank or near-blank invoice and receipt documents. It contains 255 high-resolution scanned images extracted and cleaned from document PDFs. This dataset is intended to support training and evaluation of OCR, document classification, and layout-based filtering models where blank or structurally minimal pages must be identified and processed.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nFormat: Parquet (auto-converted)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0-Sample.","url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-Blank-v2.0-Sample","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v50","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v50.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recraft-V2_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Recraft-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 47k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Recraft-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Recraft-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"vf-eval","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VF-Eval Benchmark\n\t\n\nRepository: sighingsnow/vf-eval\nFor the usage of this dataset, please refer to the github repo. \nIf you find this repository helpful, feel free to cite our paper:\n@misc{song2025vfeval,\n      title={VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos}, \n      author={Tingyu Song and Tongyan Hu and Guo Gan and Yilun Zhao},\n      year={2025},\n      eprint={2505.23693},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/songtingyu/vf-eval.","url":"https://huggingface.co/datasets/songtingyu/vf-eval","creator_name":"Tingyu Song","creator_url":"https://huggingface.co/songtingyu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","mit","1K<n<10K","Video"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v135","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v135.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v135","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"german_handwriting","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGerman handwriting\n\t\n\nThis dataset contains German handwriting images and corresponding text labels. In total, the dataset contains around 10,000 entries with handwriting from 15 different people.\nThe data was created with the help of transcripts from school and university.\nThe dataset was created as part of a handwriting recognition project at the FH-SWF.\n\n\t\n\t\t\n\t\tHow to use:\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('fhswf/german_handwriting')\n\n","url":"https://huggingface.co/datasets/fhswf/german_handwriting","creator_name":"Fachhochschule SÃ¼dwestfalen","creator_url":"https://huggingface.co/fhswf","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","German","afl-3.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Mono-InternVL-2B-Synthetic-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMono-InternVL-2B Synthetic Data\n\t\n\nThis dataset is used for training the S1.2 stage of Mono-InternVL-2B, as described in the paper Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models.\n\nProject Page: https://internvl.github.io/blog/2024-10-10-Mono-InternVL/\nCode: https://github.com/OpenGVLab/Mono-InternVL\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose\n\t\n\nThis dataset is used for training the S1.2 stage of Mono-InternVL-2B.\n\n\t\n\t\t\n\t\n\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data.","url":"https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v98","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v98.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v98","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v36","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v36.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v57","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v57.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v28","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v28.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v100","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v100.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v100","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Live-WhisperX-526K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Live-WhisperX-526K\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for the training of the LiveCC-7B-Instruct model. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nAfter we finished the pre-training of LiveCC-7B-Base modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K.","url":"https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","100K<n<1M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"SynthCodeNet","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSynthCodeNet\n\t\n\n\n    \n\n\nSynthCodeNet is a multimodal dataset created for training the SmolDocling model. It consists of over 9.3 million synthetically generated image-text pairs, covering code snippets from 56 different programming languages. Text data was sourced from permissively licensed sources, while images were synthetically generated at 120 DPI using LaTeX and Pygments to ensure visual diversity.\n\n\n\t\n\t\n\t\n\t\tDataset Statistics\n\t\n\n\nTotal samples: 9,334,257\n\nTraining set: 8,400â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ds4sd/SynthCodeNet.","url":"https://huggingface.co/datasets/ds4sd/SynthCodeNet","creator_name":"Docling","creator_url":"https://huggingface.co/ds4sd","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cdla-permissive-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"balinese-carving-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Balinese Carving Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains images of Balinese carvings along with their classifications, materials, and color descriptions. It is designed for image classification and retrieval tasks related to Balinese art and cultural heritage.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset supports multi-label image classification for Balinese carving styles and image retrieval based on textual descriptions of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/aegishield/balinese-carving-dataset.","url":"https://huggingface.co/datasets/aegishield/balinese-carving-dataset","creator_name":"Bagus Prasetyo","creator_url":"https://huggingface.co/aegishield","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","multi-label-classification","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-50k-256x256","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ“„ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256.","url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"RSTeller_legacy","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tâ›” Usage Warning\n\t\n\nThis is the legacy version of the RSTeller dataset and is not the latest version referenced in our paper. We are keeping it available here to provide the community with easy access to additional data.\nFor the details and the usage of the dataset, please refer to our github page.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find the dataset and our paper useful, please consider citing our paper:\n@article{ge2025rsteller,\n  title={RSTeller: Scaling up visual language modeling inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy.","url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v46","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v46.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthetic-cyrillic-large","keyword":"image-to-text","description":"pumb-ai/synthetic-cyrillic-large dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/pumb-ai/synthetic-cyrillic-large","creator_name":"ÐŸÐµÑ€ÑˆÐ¸Ð¹ Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹ ÐœÑ–Ð¶Ð½Ð°Ñ€Ð¾Ð´Ð½Ð¸Ð¹ Ð‘Ð°Ð½Ðº","creator_url":"https://huggingface.co/pumb-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Ukrainian","Russian","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"docci_ja","keyword":"image-to-text","description":"This data was translated from the \"DOCCI\" into Japanese by DeepL\nDOCCI: https://google.github.io/docci/\nLisence\nCC-BY-4.0\n","url":"https://huggingface.co/datasets/toshi456/docci_ja","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"mmevol-zh-hant","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMMEvol - Translated Chinese Traditional\n\t\n\nA subset of Tongyi-ConvAI/MMEvol translated using yentinglin/Llama-3-Taiwan-70B-Instruct from english to traditional chinese.\nRead the Note below before use.\nImage source distribution:\n\n\t\n\t\t\nDataset\nCount\nPercentage\n\n\n\t\t\ncoco\n6598\n29.8%\n\n\nQ-Instruct-DB\n5856\n26.4%\n\n\nclevr\n2383\n10.8%\n\n\nchartqa\n1733\n7.8%\n\n\nhfdata\n1296\n5.9%\n\n\ngeo170k\n706\n3.2%\n\n\ndata_engine\n6983.2%\n\n\nmathvision\n644\n2.9%\n\n\ndocvqa\n600\n2.7%\n\n\nalfworld\n401\n1.8%\n\n\narxivqa\n337\n1.5%â€¦ See the full description on the dataset page: https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant.","url":"https://huggingface.co/datasets/syntaxsynth/mmevol-zh-hant","creator_name":"SyntaxSynth","creator_url":"https://huggingface.co/syntaxsynth","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v136","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v136.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v136","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"allstar","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Allstar.gg Clips\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 47,896 video clips from the gaming platform allstar.gg. The clips primarily focus on Counter-Strike 2 gameplay moments and include detailed metadata such as player information, game statistics, view counts, and related media URLs.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English (en).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/allstar.","url":"https://huggingface.co/datasets/nyuuzyou/allstar","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communitiesâ€™ Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mila-ai4h/mid-space.","url":"https://huggingface.co/datasets/mila-ai4h/mid-space","creator_name":"Mila AI4H","creator_url":"https://huggingface.co/mila-ai4h","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Llama-Nemotron-VLM-Dataset-v1","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tLlama-Nemotron-VLM-Dataset v1\n\t\n\n\n\t\n\t\t\n\t\tVersions\n\t\n\n\n\t\n\t\t\nDate\nCommit\nChanges\n\n\n\t\t\n2025-08-11\nbdb3899\nInitial release\n\n\n2025-08-18\n5abc7df\nFixes bug (ocr_1 and ocr_3 images were swapped)\n\n\n2025-08-19\nef85bef\nUpdate instructions for ocr_9\n\n\n2025-08-25\n4e46f2b\nAdded example for Megatron Energon\n\n\n2025-09-02\nhead\nUpdate license headers\n\n\n\t\n\n\n\t\n\t\t\n\t\tQuickstart\n\t\n\nIf you want to dive in right away and load some samples using Megatron Energon, check out this section below.\n\n\t\n\t\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1.","url":"https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","image-to-text","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Llama-Nemotron-VLM-Dataset-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLlama-Nemotron-VLM-Dataset v1\n\t\n\n\n\t\n\t\t\n\t\tVersions\n\t\n\n\n\t\n\t\t\nDate\nCommit\nChanges\n\n\n\t\t\n2025-08-11\nbdb3899\nInitial release\n\n\n2025-08-18\n5abc7df\nFixes bug (ocr_1 and ocr_3 images were swapped)\n\n\n2025-08-19\nef85bef\nUpdate instructions for ocr_9\n\n\n2025-08-25\n4e46f2b\nAdded example for Megatron Energon\n\n\n2025-09-02\nhead\nUpdate license headers\n\n\n\t\n\n\n\t\n\t\t\n\t\tQuickstart\n\t\n\nIf you want to dive in right away and load some samples using Megatron Energon, check out this section below.\n\n\t\n\t\t\n\t\tDataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1.","url":"https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","image-to-text","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v73","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v73.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v73","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"NIH-CXR14-BiomedCLIP-Features","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNIH-CXR14-BiomedCLIP-Features Dataset\n\t\n\nThis dataset is derived from the NIH Chest X-ray Dataset (NIH-CXR14) and processed using the BiomedCLIP-PubMedBERT_256-vit_base_patch16_224 model from Microsoft. It contains image and text features extracted from chest X-ray images and their corresponding textual findings.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe original NIH-CXR14 dataset comprises 112,120 chest X-ray images with disease labels from 30,805 unique patients. This processed datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features.","url":"https://huggingface.co/datasets/Yasintuncer/NIH-CXR14-BiomedCLIP-Features","creator_name":"TunÃ§er","creator_url":"https://huggingface.co/Yasintuncer","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-retrieval","text-classification","image-feature-extraction","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"HistCaps","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHistCaps\n\t\n\nHistCaps is a dataset of 515 historical images with bilingual (English/Chinese) captions.It was constructed as part of our effort to build evaluation benchmarks for visionâ€“language models.\n\n\t\n\t\t\n\t\tRelated Datasets\n\t\n\n\nCompareBench  \nTallyBench\n\n\n\t\n\t\t\n\t\tCode and Benchmark\n\t\n\nFor benchmark tasks and evaluation code, please check out our GitHub repo:ðŸ‘‰ CompareBench on GitHub\n","url":"https://huggingface.co/datasets/qiuzhangTiTi/HistCaps","creator_name":"Jie Cai","creator_url":"https://huggingface.co/qiuzhangTiTi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TON-AITZ-SFT","keyword":"image-to-text","description":"This is the dataset trained in the model in the paper: Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models.\n","url":"https://huggingface.co/datasets/kolerk/TON-AITZ-SFT","creator_name":"jiaqi wang","creator_url":"https://huggingface.co/kolerk","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","Image","arxiv:2505.16854"],"keywords_longer_than_N":true},
	{"name":"PELLET-Casimir-Marius-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tPELLET Casimir Marius - Line level\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe PELLET Casimir Marius dataset includes 100 annotated French letters written between 1914 and 1918.\nAnnotations were done at line-level and all images do not have any text.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in French.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\n{\n  'image':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/PELLET-Casimir-Marius-line.","url":"https://huggingface.co/datasets/Teklia/PELLET-Casimir-Marius-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","mit","Image","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v154","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v154.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v154","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B poweredâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/Recap-DataComp-1B.","url":"https://huggingface.co/datasets/lodestones/Recap-DataComp-1B","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"human-coherence-preferences-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Image Generation Coherence Dataset\n\t\n\n\n\n\n\nThis dataset was collected in ~4 Days using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nOne of the largest human annotated coherence datasets for text-to-image models, this release contains over 1,200,000 humanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images.","url":"https://huggingface.co/datasets/Rapidata/human-coherence-preferences-images","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","question-answering","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"carla-autopilot-multimodal-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCARLA Autopilot Multimodal Dataset\n\t\n\nThis dataset contains synchronized multimodal driving data collected in the CARLA simulator using the autopilot feature. It provides RGB images from multiple cameras, semantic segmentation, LiDAR point clouds, 2D bounding boxes, and ego-vehicle state/control signals across varied weather, maps, and traffic densities.\nThe dataset is designed for research in autonomous driving, sensor fusion, imitation learning, and self-driving evaluation.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/immanuelpeter/carla-autopilot-multimodal-dataset.","url":"https://huggingface.co/datasets/immanuelpeter/carla-autopilot-multimodal-dataset","creator_name":"Immanuel Peter","creator_url":"https://huggingface.co/immanuelpeter","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-classification","image-segmentation","depth-estimation","video-classification"],"keywords_longer_than_N":true},
	{"name":"Caption-Anything-InContext","keyword":"image-to-text","description":"Caption-Anything-InContext is a dataset curated using the model Caption-Pro for improved in-context captioning of images. This model is designed for generating multiple captions for images, ensuring they are contextually accurate.\n\n\t\n\t\t\n\t\tRequired Lib\n\t\n\n!pip install -q transformers qwen-vl-utils==0.0.2\n\nDemo with transformers\nimport os\nimport gdown\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom PIL importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption-Anything-InContext.","url":"https://huggingface.co/datasets/prithivMLmods/Caption-Anything-InContext","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MC2","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMC2 Benchmark for Evaluating and Steering Modality Preferences in Multimodal Large Language Models\n\t\n\nThis repository contains the MC2 benchmark dataset for evaluating modality preference in Multimodal Large Language Models (MLLMs). The benchmark focuses on scenarios with conflicting multimodal evidence, allowing for systematic evaluation of an MLLM's tendency to favor one modality over another.\nThe dataset is described in the paper Evaluating and Steering Modality Preferences inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/271754echo/MC2.","url":"https://huggingface.co/datasets/271754echo/MC2","creator_name":"Yu Zhang","creator_url":"https://huggingface.co/271754echo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","arxiv:2505.20977","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-color-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SightationCompletions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSighationCompletions\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationCompletions.","url":"https://huggingface.co/datasets/Sightation/SightationCompletions","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"portugues_ocr_dataset_full","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPortugues OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe portugues_ocr_dataset_full is a dataset designed for Optical Character Recognition (OCR) tasks. It contains images of text from the Portuguese literary work Os LusÃ­adas by LuÃ­s Vaz de CamÃµes, as well as the corresponding ground truth text labels. This dataset can be used for training and evaluating OCR models for Portuguese text recognition.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset consists of:\n\nImages: Each image is a cropped portionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mazafard/portugues_ocr_dataset_full.","url":"https://huggingface.co/datasets/mazafard/portugues_ocr_dataset_full","creator_name":"Mohammadreza Asadollahifard","creator_url":"https://huggingface.co/mazafard","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","manual","monolingual","Portuguese","English"],"keywords_longer_than_N":true},
	{"name":"Advance","keyword":"image-to-text","description":"Groovy-123/Advance dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/Advance","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Advance","keyword":"table-to-text","description":"Groovy-123/Advance dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/Advance","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"Advance","keyword":"video-text-to-text","description":"Groovy-123/Advance dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/Groovy-123/Advance","creator_name":"KWAME MARFO","creator_url":"https://huggingface.co/Groovy-123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v77","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v77.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v77","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v134","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v134.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v134","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v162","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v162.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v162","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VRBench","keyword":"video-text-to-text","description":"This repository contains the dataset described in the paper VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos.\nProject page: https://VRBench.github.io\n","url":"https://huggingface.co/datasets/OpenGVLab/VRBench","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","apache-2.0","arxiv:2506.10857","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"arocrbench_khatt","keyword":"image-to-text","description":"KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding\nThis dataset is designed to evaluate the performance of Arabic OCR and document understanding systems. It includes a variety of document types and tasks.\nPlease see paper & code for more information:\n\nGitHub Repository\nProject Page\narXiv Paper\n\n","url":"https://huggingface.co/datasets/ahmedheakl/arocrbench_khatt","creator_name":"Ahmed Heakl","creator_url":"https://huggingface.co/ahmedheakl","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MINT-1T-PDF-CC-2023-14","keyword":"image-to-text","description":"\n  ðŸƒ MINT-1T:Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens\n\n\nðŸƒ MINT-1T is an open-source Multimodal INTerleaved dataset with 1 trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. ðŸƒ MINT-1T is designed to facilitate research in multimodal pretraining. ðŸƒ MINT-1T is created by a team from the University of Washington inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14.","url":"https://huggingface.co/datasets/mlfoundations/MINT-1T-PDF-CC-2023-14","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v41","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v41.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"self-alignment","keyword":"video-text-to-text","description":" \n \n \n \n \n\n\t\n\t\t\n\t\n\t\n\t\tVideo sources\n\t\n\nIn the json files, src indicates the video sources which can be downloaded as follows.\n\nvideo-vqa-webvid_qa: WebVid\nvideo-conversation-videochat2: VideoChat2\nvideo-classification-ssv2: SSv2\nvideo-reasoning-clevrer_qa: CLEVRER\nvideo-vqa-tgif_frame_qa: TGIF\nvideo-reasoning-next_qa: NExTQA\nvideo-conversation-videochat1: VideoChat\nvideo-vqa-tgif_transition_qa: TGIF\nvideo-reasoning-clevrer_mc: CLEVRER\nvideo-vqa-ego_qa: EgoQA\nvideo-classification-k710:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/self-alignment.","url":"https://huggingface.co/datasets/pritamqu/self-alignment","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K<n<100K","arxiv:2504.12083","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"BHM-Bengali-Hateful-Memes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nBHM is a novel multimodal dataset for Bengali Hateful Memes detection. The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, \ntailored for two tasks: (i) detecting hateful memes and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society).\n\n\t\n\t\t\n\t\tPaper Information\n\t\n\n\nPaper: https://aclanthology.org/2024.acl-long.454/\nCode:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes.","url":"https://huggingface.co/datasets/Eftekhar/BHM-Bengali-Hateful-Memes","creator_name":"Eftekhar Hossain","creator_url":"https://huggingface.co/Eftekhar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["other","image-classification","image-to-text","Bengali","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"rrvideo-element-highlights","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for rrvideo-element-highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains UI element images extracted from web snapshots, along with captions describing each element.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset contains 61 unique UI elements from 2 different snapshots. Each element is associated with a caption describing its basic properties.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal elements: 61\nUnique snapshots: 2\nAverage element width: 1280.00 pixels\nAverage elementâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights.","url":"https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights","creator_name":"Earl Potters","creator_url":"https://huggingface.co/Slyracoon23","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"playground-liked","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Playground\n\t\n\nImage generations curated by user likes.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA subset of bigdata-pw/playground filtered to approximately 14 million images where likeCount >= 1. Entries include generation details such as prompts and model used, anonymized user information, creation date, and URL to the image.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\nid: Uniqueâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/playground-liked.","url":"https://huggingface.co/datasets/bigdata-pw/playground-liked","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"blip3-kale","keyword":"image-to-text","description":"\n  ðŸ¥¬ BLIP3-KALE:Knowledge Augmented Large-scale Dense Captions\n\nBLIP3-KALE is an open-source dataset of 218 million image-text pairs, featuring knowledge-augmented dense captions combining web-scale knowledge with detailed image descriptions.\n\nPaper: [To be added]\n\n\t\n\t\t\n\t\tUses\n\t\n\nBLIP3-KALE is designed to facilitate research in multimodal pretraining. The dataset can be used for training large multimodal models that require factually grounded, dense image captions. It has already been anâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-kale.","url":"https://huggingface.co/datasets/Salesforce/blip3-kale","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100M - 1B","parquet"],"keywords_longer_than_N":true},
	{"name":"kaggle_data","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","url":"https://huggingface.co/datasets/hongin9812/kaggle_data","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v191","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v191.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v191","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"laion-eus","keyword":"image-to-text","description":"This dataset contains a subset of laion/relaion2B-multi-research where captions are written in Basque. Although language is already detected for its captions, we used another language detector Mike0307/multilingual-e5-language-detection and a high certainty threshold (p > 0.98) to better filter the instances in Basque.\nApart from the same attributes found in laion/relaion2B-multi-research, we add the following to each instance.\n\n\"lang_laion\": language originally detected by the classifier usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/laion-eus.","url":"https://huggingface.co/datasets/HiTZ/laion-eus","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Basque","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v117","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v117.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v117","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"KYC2","keyword":"image-to-text","description":"HiteshKamwal/KYC2 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/HiteshKamwal/KYC2","creator_name":"Hitesh Kamwal","creator_url":"https://huggingface.co/HiteshKamwal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"SARD-Extended","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD-Extended.","url":"https://huggingface.co/datasets/riotu-lab/SARD-Extended","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","dataset"],"keywords_longer_than_N":true},
	{"name":"SARD-Extended","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD-Extended.","url":"https://huggingface.co/datasets/riotu-lab/SARD-Extended","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","ðŸ‡ºðŸ‡¸ Region: US","dataset"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v109","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v109.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v109","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"IIIT5K_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/iiit5k/metafile.yml\nName: 'IIIT5K'\nPaper:\n  Title: Scene Text Recognition using Higher Order Language Priors\n  URL: http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/Home/mishraBMVC12.pdf\n  Venue: BMVC\n  Year: '2012'\n  BibTeX: '@InProceedings{MishraBMVC12,\n  author    = \"Mishra, A. and Alahari, K. and Jawahar, C.~V.\",\n  title     = \"Scene Text Recognition using Higher Order Language Priors\",\n  booktitle = \"BMVC\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/IIIT5K_OCR.","url":"https://huggingface.co/datasets/MiXaiLL76/IIIT5K_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"FaceCaption-15M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFacaCaption-15M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing Agreement\n\n\nFaceCaption-15M, a large-scale, diverse, and high-quality dataset of facial images accompanied by their natural language descriptionsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"BBBP-V-SMILES-4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BBBP-V-SMILES-4.","url":"https://huggingface.co/datasets/molvision/BBBP-V-SMILES-4","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v148","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v148.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v148","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Defect_Spectrum","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDefect Spectrum Dataset\n\t\n\nWelcome to the Defect Spectrum dataset repository. This comprehensive benchmark is a granular collection of large-scale defect datasets with rich semantics, designed to push the frontier of industrial defect inspection research and applications.\nPaper: https://huggingface.co/papers/2310.17316\nGithub repository: https://github.com/EnVision-Research/Defect_Spectrum\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nDefect inspection is a critical component within the closed-loopâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum.","url":"https://huggingface.co/datasets/DefectSpectrum/Defect_Spectrum","creator_name":"DefectSpectrum","creator_url":"https://huggingface.co/DefectSpectrum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"100k-random-memes","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\t100K Memes Dataset\n\t\n\nThis is a massive collection of 100,000 haha (un)funny meme images packed into a 15GB .7z archive. Think of it as a time capsule of internet culture that was used to create that \"top 100000 memes\" video on YouTube\n(https://www.youtube.com/watch?v=D__PT7pJohU).\n","url":"https://huggingface.co/datasets/kuzheren/100k-random-memes","creator_name":"Roman Kuzheren","creator_url":"https://huggingface.co/kuzheren","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","English","Russian","mit"],"keywords_longer_than_N":true},
	{"name":"VideoChat2","keyword":"video-text-to-text","description":"Video training data of LongVU downloaded from\nhttps://huggingface.co/datasets/OpenGVLab/VideoChat2-IT\n\n\t\n\t\t\n\t\tVideo\n\t\n\nPlease download the original videos from the provided links:\n\nBDD100K: bdd.zip\nShareGPTVideo: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction/tree/main/train_300k\nCLEVRER: clevrer_qa.zip\nDiDeMo: didemo.zip\nEgoQA: https://huggingface.co/datasets/ynhe/videochat2_data/resolve/main/egoqa_split_videos.zipKinetics-710: k400.zip\nMovieChat: moviechat.zipâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shenxq/VideoChat2.","url":"https://huggingface.co/datasets/shenxq/VideoChat2","creator_name":"Xiaoqian Shen","creator_url":"https://huggingface.co/shenxq","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"temporal-vqa","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Temporal-VQA dataset is a challenging benchmark designed to evaluate the temporal reasoning capabilities of Multimodal Large Language Models (MLLMs) in tasks requiring visual temporal understanding. It emphasizes real-world temporal dynamics through two core evaluation tasks:- \n\nTemporal Order Understanding: This task presents MLLMs with temporally consecutive frames from video sequences. The models must analyze and determine the correct sequence of eventsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/fazliimam/temporal-vqa.","url":"https://huggingface.co/datasets/fazliimam/temporal-vqa","creator_name":"Fazli Imam","creator_url":"https://huggingface.co/fazliimam","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"EditReward-Data","keyword":"image-to-text","description":"To be released.\n","url":"https://huggingface.co/datasets/TIGER-Lab/EditReward-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-combine-v21","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v21.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v36","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v36.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"medmax_data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israelâ€ , Siyan Zhaoâ€ , Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angelesâ€  Equal Contribution\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilitiesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"JeanPaulGaultier","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tJean Paul Gaultier\n\t\n\nFirst in a series of fashion datasets.\n\n\t\n\t\t\n\t\tSource\n\t\n\nJean Paul Gaultier's webshop\n\n\t\n\t\t\n\t\tContent + example\n\t\n\nProduct metadata and images\n{\n    \"id\": \"gid://shopify/Product/9119640486225\",\n    \"lang\": \"en\",\n    \"name\": \"The Silver 56-8171 Sunglasses\",\n    \"descriptions\": \"EYEWEAR CollectionSunglasses with round silver frames, spring detail on temples and Jean Paul et Gaultier logo.\",\n    \"category\": \"\",\n    \"colors\": [\n      \"Silver\"\n    ],\n    \"locale\": \"en\"â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/JeanPaulGaultier.","url":"https://huggingface.co/datasets/bigdata-pw/JeanPaulGaultier","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","< 1K"],"keywords_longer_than_N":true},
	{"name":"SCALAR-VG","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSCALAR_VG\n\t\n\nWhile the world model continues to advance, existing datasets remain inadequate for supporting large-scale multi-modal training, particularly in comprehensive multi-dimensional scene-aware understanding. Therefore, we have built the SCALAR-VG through the SCALAR, integrating and extending many open-source image datasets to meet this demandImportantly, It contains about 240K images with comprehensive, hierarchical and multi-dimensional annotations. \nCompared with existingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiaoMiaoYang/SCALAR-VG.","url":"https://huggingface.co/datasets/MiaoMiaoYang/SCALAR-VG","creator_name":"Guluguluha","creator_url":"https://huggingface.co/MiaoMiaoYang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["keypoint-detection","image-segmentation","object-detection","visual-question-answering","zero-shot-object-detection"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v43","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v43.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CyclePrefDB-I2T-Reconstructions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Reconstructions for CyclePrefDB-I2T\n\t\n\nProject page | Paper | Code\nThis dataset contains reconstruction images used to determine cycle consistency preferences for CyclePrefDB-I2T. You can find the corresponding file paths in the CyclePrefDB-I2T dataset here. Reconstructions are created using Stable Diffusion 3 Medium.\n\n\t\n\t\n\t\n\t\tPreparing the reconstructions\n\t\n\nYou can download the test and validation split .tar files and extract them directly.Use this script to extract theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/carolineec/CyclePrefDB-I2T-Reconstructions.","url":"https://huggingface.co/datasets/carolineec/CyclePrefDB-I2T-Reconstructions","creator_name":"Caroline Chan","creator_url":"https://huggingface.co/carolineec","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"ABC-Pretraining-Data","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tABC Pretraining Data\n\t\n\nThis dataset contains the pretraining data for ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions, advancing the state of visual embeddings with natural language control.\nThis dataset is derived from Google's Conceptual Captions dataset.\nEach item in the dataset contains a URL where the corresponding image can be downloaded and mined negatives for eachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ECDBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tECDBench: A Benchmark for Evaluating MLLM Chart Understanding\n\t\n\nThis repository hosts ECDBench, a high-quality benchmark dataset for evaluating the chart understanding capabilities of Multimodal Large Language Models (MLLMs), as presented in the paper Effective Training Data Synthesis for Improving MLLM Chart Understanding.\nThe full code for data generation, model fine-tuning, and evaluation can be found on the GitHub repository.\n\n\t\n\t\t\n\t\n\t\n\t\tâœ¨ Abstract\n\t\n\nBeing able to effectivelyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChartFoundation/ECDBench.","url":"https://huggingface.co/datasets/ChartFoundation/ECDBench","creator_name":"Chart Foundation Research","creator_url":"https://huggingface.co/ChartFoundation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","visual-question-answering","expert/gpt-4o-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt\n\t\n\nCaption3o-Opt is a compact, high-quality image-caption dataset derived from the original BLIP3o/BLIP3o-Pretrain-Long-Caption. This refined subset focuses on optimized long-form captioning, curated for real-world and artistic image understanding across vision-language models.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nTotal Samples: 10,278\nModality: Image â†” Text\nFormat: Arrow (auto-converted to Parquet)\nLicense: Apache 2.0\nLanguage: English\nSize: ~500 MB\n\n\n\t\n\t\t\n\t\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pxhere","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for pxhere Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a large collection of high-quality photographs sourced from pxhere.com, a free stock photo website. The dataset includes approximately 1,100,000 images in full resolution covering a wide range of subjects including nature, people, urban environments, objects, animals, and landscapes. All images are provided under the Creative Commons Zero (CC0) license, making them freely available for personal andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pxhere.","url":"https://huggingface.co/datasets/nyuuzyou/pxhere","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller imagesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"image-text-to-text","description":"\n ðŸ¦„ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ðŸ¤—HuggingFace] | [Website]\n    \n    \n\n\nðŸŒŸ Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tðŸ”¥News\n\t\n\n\nðŸŽ–ï¸ Our work is accepted by ACL2024.\n\nðŸ”¥ We have release benchmark on [ðŸ¤—HuggingFace].\n\nðŸ”¥ The paper is also available on [ArXiv].\n\nðŸ”®â€¦ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v87","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v87.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v87","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Diffusion1B","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Diffusion1B\n\t\n\n~1.2B image generations.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 1.2 billion images generated by diffusion models. Entries include generation details such as prompts and model used, anonymized user information, creation date, and URL to the image. This dataset is a combination of bigdata-pw/leonardo and bigdata-pw/playground.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/Diffusion1B.","url":"https://huggingface.co/datasets/bigdata-pw/Diffusion1B","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","odc-by","1B - 10B","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CHiTab","keyword":"image-text-to-text","description":"\n\nCHiTab\n\nðŸ“‘ A benchmark for Hierarchical Table Structure Recognition with VLLMs\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nCHiTab (Complex Hierarchical Tables) is a benchmark derived from PubTables-1M that focuses on hierarchical structure recognition in complex tables.Unlike previous benchmarks that only evaluate cell detection or grid reconstruction, CHiTab explicitly targets the parentâ€“child relationships in table headers.  \nThe dataset reformulates Table Structure Recognition (TSR) intoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AILab-UniFi/CHiTab.","url":"https://huggingface.co/datasets/AILab-UniFi/CHiTab","creator_name":"AI Laboratory University of Florence","creator_url":"https://huggingface.co/AILab-UniFi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v166","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v166.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v166","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v93","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v93.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v93","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Finna-HKM-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOld photographs from Helsinki City Museum\n\t\n\nThis is an image dataset consisting of 5947 old photographs (until 1917) from the collections of the Helsinki City Museum, obtained from the Finna.fi discovery service.\nThe images are intended to be used for different AI/ML tasks such as generating captions or colorizing them.\nThe images themselves are JPEG files under the directory images.\nThe metadata.jsonl file contains metadata about each image, for example descriptive captions (mostlyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/NatLibFi/Finna-HKM-images.","url":"https://huggingface.co/datasets/NatLibFi/Finna-HKM-images","creator_name":"National Library of Finland","creator_url":"https://huggingface.co/NatLibFi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","Finnish","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v186","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v186.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v186","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v34","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v34.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/bâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PIXELPROSE_HU","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\nThis dataset is an extension of an existing image captioning dataset, enhanced for PixelProse and augmented with Hungarian translations. It provides a valuable resource for researchers and developers working on image captioning, especially those interested in PixelProse and cross-lingual applications. ðŸŒ\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics\n\t\n\nWe report below the number of successfully fetched images and the number of failedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU.","url":"https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU","creator_name":"Obscure Entropy","creator_url":"https://huggingface.co/Obscure-Entropy","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Hungarian","mit"],"keywords_longer_than_N":true},
	{"name":"listybox-etsy-listing-json","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tEtsy Product Listing Dataset (JSON SFT Format)\n\t\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Optimized for Fine-tuning\n\t\n\nClean, minimal dataset for training vision-language models to generate Etsy product listings.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nJSON SFT Format: Ready for training with standard SFT trainers\nMinimal Instructions: ~200 chars average (vs 2000+ in verbose versions)\nToken Efficient: 90% reduction in instruction tokens\nProduction Ready: Model learns the task, not prompt engineering\n\n\n\t\n\t\t\n\t\tDataset Structureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/burakaktna/listybox-etsy-listing-json.","url":"https://huggingface.co/datasets/burakaktna/listybox-etsy-listing-json","creator_name":"Burak AKTUNA","creator_url":"https://huggingface.co/burakaktna","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v160","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v160.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v160","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TAMMs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting\n\t\n\nTAMMs is a large-scale dataset derived from the Functional Map of the World (fMoW) dataset, curated to support multimodal and temporal reasoning tasks such as change detection and future prediction.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 37,003 high-quality temporal sequences, each consisting of at least four distinct satellite images of the same location captured at differentâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/IceInPot/TAMMs.","url":"https://huggingface.co/datasets/IceInPot/TAMMs","creator_name":"é”…ä¸­å†°","creator_url":"https://huggingface.co/IceInPot","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding colorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was somethingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v81","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v81.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v81","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RewardData","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tLoad Dataset\n\t\n\nDirectly download this repo, and unzip the video zip files.\ngit clone https://huggingface.co/datasets/DeepTraceReward/RewardData\nunzip real_video.zip\ncat videos.zip.00* > videos.zip\nunzip videos.zip\n\n\n\n\t\n\t\n\t\n\t\tlicense: apache-2.0\ndataset_info:\n  config_name: all_fake_video_annotations\n  features:\n  - name: label_id\n    dtype: string\n  - name: video_source\n    dtype: string\n  - name: video_prompt\n    dtype: string\n  - name: video_id\n    dtype: string\n  - name: heightâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeeptraceReward/RewardData.","url":"https://huggingface.co/datasets/DeeptraceReward/RewardData","creator_name":"Deeptrace Reward Bench","creator_url":"https://huggingface.co/DeeptraceReward","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"lego_minifigure_captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLEGO Minifigure Captions\n\t\n\nThe LEGO Minifigure Captions dataset contains 12966 images of LEGO minifigures with captions. The dataset contains the following columns:\n\nimage: The jpeg image of the minifigure in the format {\"bytes\": bytes, \"path\": str} so that can be interpreted as PIL.Image objects in the huggingface datasets library.\nshort_caption: The short caption describing the minifigure in the image.\ncaption: The caption describing the minifigure which is generated usingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Armaggheddon/lego_minifigure_captions.","url":"https://huggingface.co/datasets/Armaggheddon/lego_minifigure_captions","creator_name":"Alessandro","creator_url":"https://huggingface.co/Armaggheddon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v183","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v183.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v183","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PENCIL","keyword":"image-to-text","description":"kausthubkannan17/PENCIL dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/kausthubkannan17/PENCIL","creator_name":"Kausthub Kannan","creator_url":"https://huggingface.co/kausthubkannan17","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"keystroke-typing-videos","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tKeystroke Typing Videos of Reuters\n\t\n\nRecordings of typing randomly sampled sentences (<= 150 characters) from nltk Reuters dataset. Keystroke data is provided too.\n","url":"https://huggingface.co/datasets/andrewt28/keystroke-typing-videos","creator_name":"Andrew Tran","creator_url":"https://huggingface.co/andrewt28","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","afl-3.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v178","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v178.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v178","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ChartGen-200K","keyword":"image-to-text","description":"SD122025/ChartGen-200K dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/SD122025/ChartGen-200K","creator_name":"Chart","creator_url":"https://huggingface.co/SD122025","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"thai_handwriting_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tThai Handwriting Dataset\n\t\n\nThis dataset combines two major Thai handwriting datasets:\n\nBEST 2019 Thai Handwriting Recognition dataset (train-0000.parquet)\nThai Handwritten Free Dataset by Wang (train-0001.parquet onwards)\n\n\n\t\n\t\t\n\t\tMaintainer\n\t\n\nkobkrit@iapp.co.th\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tBEST 2019 Dataset\n\t\n\nContains handwritten Thai text images along with their ground truth transcriptions. The images have been processed and standardized for machine learning tasks.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/iapp/thai_handwriting_dataset.","url":"https://huggingface.co/datasets/iapp/thai_handwriting_dataset","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Thai","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"noisy-gt-xxx-words-train-only","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNoisy Ground Truth - Words Replaced with XXX in Train Split only\n\t\n\nDataset of synthetic data for experimentation with noisy ground truth. The text in the dataset is based on Colette's Sido and Les Vignes, also the data was processed prior to generating images with the TextRecognitionDataGenerator.\nIn Noisy Ground Truth - Words Replaced with XXX in Train Split only, each variation column is affected by the noise, without considering the split between train, validation and test.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alix-tz/noisy-gt-xxx-words-train-only.","url":"https://huggingface.co/datasets/alix-tz/noisy-gt-xxx-words-train-only","creator_name":"Alix ChaguÃ©","creator_url":"https://huggingface.co/alix-tz","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v125","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v125.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v125","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v107","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v107.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v107","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ICDAR2013_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/icdar2013/metafile.yml\nName: 'Incidental Scene Text IC13'\nPaper:\n  Title: ICDAR 2013 Robust Reading Competition\n  URL: https://www.imlab.jp/publication_data/1352/icdar_competition_report.pdf\n  Venue: ICDAR\n  Year: '2013'\n  BibTeX:'@inproceedings{karatzas2013icdar,\n  title={ICDAR 2013 robust reading competition},\n  author={Karatzas, Dimosthenis and Shafait, Faisal and Uchida, Seiichi and Iwamura, Masakazu and i Bigordaâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/ICDAR2013_OCR.","url":"https://huggingface.co/datasets/MiXaiLL76/ICDAR2013_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"movies_CLIP_ViT-L14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŽ¬ Movie Frame & Caption Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Introduction\n\t\n\nThis dataset was created from multiple movies across 10 genres, with approximately 3 movies per genre.From each movie, frames were extracted periodically, and AI-generated captions (BLIP) were assigned to each frame.A total of 93,813 frames were extracted.\nThis dataset can be used for tasks such as:\n\nVideo understanding\nMultimodal learning (image + text)\nImage captioning\nVision-language retrieval\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“‚ Dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thaotien/movies_CLIP_ViT-L14.","url":"https://huggingface.co/datasets/thaotien/movies_CLIP_ViT-L14","creator_name":"BÃ¹i Ngá»c Tháº£o TiÃªn","creator_url":"https://huggingface.co/thaotien","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-retrieval","image-to-text","Vietnamese","English"],"keywords_longer_than_N":true},
	{"name":"DocGenome","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models\n\t\n\npaper link: DocGenome\nWe present DocGenome, a structured document dataset constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline DocParser. DocGenome features four characteristics:\n\n\nCompleteness: It is the first dataset to structure data from all modalities includingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/U4R/DocGenome.","url":"https://huggingface.co/datasets/U4R/DocGenome","creator_name":"Alpha-Innovator Lab","creator_url":"https://huggingface.co/U4R","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","cc-by-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"commoncanvas-cc-by-recap-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning 2\n\t\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚ ä»¥ä¸‹ã®æƒ…å ±ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nFlorence-2-large-ftã§Dense Captioning (More detailed caption) ã—ãŸè‹±èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³\n\nstreaming=Trueã§èª­ã¿è¾¼ã‚€ã¨åŒã˜é †ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã®ã§ãã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒä¸€ç•ªæ¥½ã§ã™ã€‚\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nforâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2.","url":"https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Alignment_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Alignment Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Preference dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset.","url":"https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Alignment_Dataset","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v163","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v163.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v163","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CircuitSketchTextAnnotations","keyword":"image-to-text","description":"edesaras/CircuitSketchTextAnnotations dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/edesaras/CircuitSketchTextAnnotations","creator_name":"Aras EdeÅŸ","creator_url":"https://huggingface.co/edesaras","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","German","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SVG","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSurprising Visual Genome (SVG)\n\t\n\nThis repository contains the dataset used in the paper \"Complexity in Complexity: Understanding Visual Complexity Through Structure, Color, and Surprise\". The dataset includes visual complexity ratings along with various features that help predict perceived visual complexity.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset includes the following features:\n\n\t\n\t\t\nColumn\nDescription\n\n\n\t\t\nimage_id\nUnique identifier for each image\n\n\ncomplexity\nHuman-rated visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mortdecai/SVG.","url":"https://huggingface.co/datasets/Mortdecai/SVG","creator_name":"Karahan","creator_url":"https://huggingface.co/Mortdecai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v175","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v175.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v175","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"UI-128","keyword":"image-to-text","description":"henryhe0123/UI-128 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/henryhe0123/UI-128","creator_name":"Yanheng He","creator_url":"https://huggingface.co/henryhe0123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them â€” the content remains exactly the same. The only difference lies in the compression method, which now allows for quickerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v64","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v64.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v64","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v156","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v156.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v156","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16.","url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"fle_vqa","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFactorio Visual Question Answering (VQA) Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains visual question-answering pairs for the Factorio Learning Environment (FLE). It is designed to train and evaluate vision-language models on understanding Factorio game elements, spatial relationships, and factory designs.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Samples: 4,522\nNumber of Splits: 14\nTask Categories: 4\nLanguages: English\nLicense: MIT\nCreated: 2025-08-05\n\n\n\t\n\t\t\n\t\tTaskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Noddybear/fle_vqa.","url":"https://huggingface.co/datasets/Noddybear/fle_vqa","creator_name":"Jack Hopkins","creator_url":"https://huggingface.co/Noddybear","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish-detailed-captions","keyword":"image-to-text","description":"Detailed captions were genereted by gpt-4o-mini using OpenAI API.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÌˆruÌˆntuÌˆlerden Otomatik TuÌˆrkcÌ§e AcÌ§Ä±klama OluÅŸturma Ä°cÌ§in Bir DenektaÅŸÄ± Veri KuÌˆmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal Ä°ÅŸleme ve Ä°letiÅŸim UygulamalarÄ± KurultayÄ± (SIU 2016), Zonguldak, Mayis 2016\n\n","url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish-detailed-captions","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"flickr8k-turkish-detailed-captions","keyword":"image-to-text","description":"Detailed captions were genereted by gpt-4o-mini using OpenAI API.\n\nM. E. Unal, B. Citamak, S. Yagcioglu, A. Erdem, E. Erdem, N. Ikizler Cinbis and R. Cakici. TasvirEt: GoÌˆruÌˆntuÌˆlerden Otomatik TuÌˆrkcÌ§e AcÌ§Ä±klama OluÅŸturma Ä°cÌ§in Bir DenektaÅŸÄ± Veri KuÌˆmesi (TasvirEt: A Benchmark Dataset for Automatic Turkish Description Generation from Images). 24. IEEE Sinyal Ä°ÅŸleme ve Ä°letiÅŸim UygulamalarÄ± KurultayÄ± (SIU 2016), Zonguldak, Mayis 2016\n\n","url":"https://huggingface.co/datasets/atasoglu/flickr8k-turkish-detailed-captions","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v149","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v149.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v149","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Magma-820K","keyword":"image-to-text","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1â€ Â \nReuben Tan1â€ Â \nQianhui Wu1â€ Â \nRuijie Zheng2â€¡Â \nBaolin Peng1â€¡Â \nYongyuan Liang2â€¡\nYu Gu1Â \nMu Cai3Â \nSeonghyeon Ye4Â \nJoel Jang5Â \nYuquan Deng5Â \nLars Liden1Â \nJianfeng Gao1â–½\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  â€  First authors  â€¡ Second authors  â–½ Leadership  \n[arXiv Paper] Â  [Project Page] Â  [Hugging Face Paper] Â  [Github Repo]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-820K.","url":"https://huggingface.co/datasets/MagmaAI/Magma-820K","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","mit","100K<n<1M","arxiv:2502.13130"],"keywords_longer_than_N":true},
	{"name":"MovieCORE","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tMovieCORE: COgnitive REasoning in Movies\n\t\n\nPaper | Project Page | Code\nMovieCORE is a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. This dataset aims to advance movie understanding in AI systems and provides valuable insights into theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MovieCORE/MovieCORE.","url":"https://huggingface.co/datasets/MovieCORE/MovieCORE","creator_name":"MovieCORE","creator_url":"https://huggingface.co/MovieCORE","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","arxiv:2508.19026","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"seraiki-handwritten-numerals","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ“Š Seraiki Handwritten Numbers (1â€“99)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains handwritten Seraiki numbers from 1 to 99, written in words using the Perso-Arabic Seraiki script.It was created to support Optical Character Recognition (OCR) research and to promote AI development for the underrepresented Seraiki language.  \nUnlike digit-only datasets (e.g., MNIST), this dataset includes full word forms of numbers, making it suitable for sequence-based recognition tasks (TrOCRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tahaListens/seraiki-handwritten-numerals.","url":"https://huggingface.co/datasets/tahaListens/seraiki-handwritten-numerals","creator_name":"Taha Arif","creator_url":"https://huggingface.co/tahaListens","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","10K - 100K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Reve-AI-Halfmoon_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Reve AI Halfmoon Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 51k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Reve AI Halfmoon across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider likingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Reve-AI-Halfmoon_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v199","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v199.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v199","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"olfaction-vision-language-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOlfaction-Vision-Language Learning: A Multimodal Dataset\n\t\n\n\n\nOlfaction â€¢ Vision â€¢ Language\n\n\n\n\n\n\n\nAn open-sourced dataset and dataset builder for prototyping and exploratory olfaction-vision-language tasks within the AI, robotics, and AR/VR domains.\nWhether this dataset is used for better vision-scent navigation with drones, triangulating the source of an odor in an image, extracting aromas from a scene, or augmenting a VR experience with scent, we hope its release will catalyzeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kordelfrance/olfaction-vision-language-dataset.","url":"https://huggingface.co/datasets/kordelfrance/olfaction-vision-language-dataset","creator_name":"Kordel France","creator_url":"https://huggingface.co/kordelfrance","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","robotics","expert-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"NewsEye-Austrian-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNewsEye Austrian - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset comprises Austrian newspaper pages from 19th and early 20th century. The images were provided by the Austrian National Library.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe documents are in Austrian German with the Fraktur font.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 atâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line.","url":"https://huggingface.co/datasets/Teklia/NewsEye-Austrian-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","German","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models\n\t\n\nA Challenging Visual-centric Benchmark for Evaluating Multimodal Reasoning in MLLMs!\nThis is the Benchmark data repo of VisuLogic.\nFor more details, please refer to the project page with dataset exploration and visualization tools: https://visulogic-benchmark.github.io/VisuLogic/.\n\n\t\n\t\t\n\t\n\t\n\t\tVisuLogic Resouces\n\t\n\nðŸŒ Homepage | ðŸ† Leaderboard | ðŸ“– Paper | ðŸ¤— Benchmark | ðŸ¤— Train Data \nðŸ’» Evalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VisuLogic/VisuLogic-Train.","url":"https://huggingface.co/datasets/VisuLogic/VisuLogic-Train","creator_name":"VisuLogic-Benchmark","creator_url":"https://huggingface.co/VisuLogic","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v56","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v56.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v197","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v197.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v197","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create highâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Flame-Evo-React","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Evo-React: A Diverse Data Synthesis Dataset for Multi-modal React Code Generation\n\t\n\nFlame-Evo-React is a dataset synthesized using the Evolution-Based Synthesis method, leveraging random evolutionary logic to generate a highly diverse set of React components. This approach systematically varies functionality, architecture, and visual style, providing a robust dataset for generalized React code generation.\nThis dataset includes in-breadth (feature expansion) and in-depthâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Evo-React.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Evo-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"CalcTrainer_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCalcTrainer Dataset ðŸ§®\n\t\n\nHandwritten mathematical answers collected from the CalcTrainer interactive math training application.\n\n\t\n\t\t\n\t\tDataset Fields\n\t\n\n\n\t\n\t\t\n\t\tCore Data\n\t\n\n\n\t\n\t\t\nField\nType\nDescription\n\n\n\t\t\nhandwriting_image\nImage\nHandwritten answer image (~100x100px)\n\n\nocr_prediction\nstring\nRaw OCR output text\n\n\nocr_parsed_number\nint32\nCleaned numeric value from OCR\n\n\nis_correct\nbool\nWhether OCR matches correct answer\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tMathematical Context\n\t\n\n\n\t\n\t\t\nField\nTypeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hoololi/CalcTrainer_dataset.","url":"https://huggingface.co/datasets/hoololi/CalcTrainer_dataset","creator_name":"Olivier","creator_url":"https://huggingface.co/hoololi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v133","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v133.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v133","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 1-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-11.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NorHand-v3-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNorHand v3 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v3 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian BokmÃ¥l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'Tilâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v3-line.","url":"https://huggingface.co/datasets/Teklia/NorHand-v3-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Norwegian BokmÃ¥l","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"pubtabnet-html","keyword":"image-to-text","description":"apoidea/pubtabnet-html dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/apoidea/pubtabnet-html","creator_name":"Apoidea","creator_url":"https://huggingface.co/apoidea","license_name":"Community Data License Agreement Permissive 1.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-1.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","text-generation","cdla-permissive-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v67","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v67.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v67","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v23","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SynthKhmer-10k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynthKhmer 10k\n\t\n\nImage Size: 896x672\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\nBackground images are from https://picsum.photos/\nProfile Photos are from https://thispersondoesnotexist.com/\nNames are sampled from Khmer Dictionary\n\n","url":"https://huggingface.co/datasets/seanghay/SynthKhmer-10k","creator_name":"seanghay","creator_url":"https://huggingface.co/seanghay","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Khmer","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"pentomino-easy-vsft","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIndividual Module\n\t\n\nThis dataset is created for instruction tuning llava models based on the dataset created here - llava-instruct-mix-vsft\nThis dataset is based on a Pentomino game - More details -> github\n","url":"https://huggingface.co/datasets/Koshti10/pentomino-easy-vsft","creator_name":"Koshti","creator_url":"https://huggingface.co/Koshti10","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-RL-Data","keyword":"image-text-to-text","description":"Overview.\nThe RL data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data includes 15,402 training queries with verifierable answers. The key fields include:\n\nquestion, answer, qid\nis_video: a flag to distinguish video and image queries\nimage: a list of image paths.\nFor video-based queries, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data.","url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v184","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v184.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v184","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Facecaption-15M-Embeddings","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFacecaption-15M-Embeddings\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing Agreement\nWe chose about 5M image-text pairs with the highest resolution from Facecaption-15M, extracted the embeddings of the [CLS]â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v4-rev3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\nTEST rev3. I'm making yet anotherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3.","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v180","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v180.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v180","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"TEMPURA-VER","keyword":"video-text-to-text","description":"This repository contains the data for the paper TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action.\nProject Page: https://andy-cheng.github.io/TEMPURA/\nCode: https://github.com/andy-cheng/TEMPURA\n","url":"https://huggingface.co/datasets/andaba/TEMPURA-VER","creator_name":"Jen-Hao (Andy) Cheng","creator_url":"https://huggingface.co/andaba","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["video-text-to-text","cc-by-4.0","arxiv:2505.01583","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"A-Bench","keyword":"image-text-to-text","description":"Project Page\n\n\n\t\n\t\t\n\t\tGlance at A-Bench Performance\n\t\n\nFor open-source models, LLaVA-NeXT (Qwen-110B) takes the first place. For closed-source models, GEMINI 1.5 PRO takes the first place.\n\n\n\t\n\t\t\n\t\n\t\n\t\tEvaluate your model on A-Bench\n\t\n\nFirst download the dataset and meta information from Huggingface.\nThe imgs.zip contains all the AI-generated images and Abench.json contains all the meta information including the img_path, questions, answers, and categories. The item of Abench.json isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/q-future/A-Bench.","url":"https://huggingface.co/datasets/q-future/A-Bench","creator_name":"Q-Future","creator_url":"https://huggingface.co/q-future","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"housey-home","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHousey Home v1 ( DEFUNCT )\n\t\n\nThe data has a flaw, it occurred during the initial synthesis. The erroneous fields have been removed, and the data is currently being selectively re-synthesized.\nAs of 07-15-2024, this dataset is now defunct. It will no longer receive stability, content, or null row fixes. However, as all images are sourced from generative AI models, open source ones at that, I have decided to make this MIT, and will perform tasks upon request if needed. Just ask.\n-<3\n","url":"https://huggingface.co/datasets/MrOvkill/housey-home","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["unconditional-image-generation","text-to-image","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"paligemma-multitask-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPaliGemma Multitask Dataset\n\t\n\nThis dataset is designed for training and evaluating the PaliGemma multitask model for defect detection and analysis. It combines a base set of annotated samples with an extended collection of 874 real-world structural inspection images.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe dataset contains images of structural defects along with their corresponding annotations for:\n\nObject detection (bounding boxes)\nDefect classification\nDescriptiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xingqiang/paligemma-multitask-dataset.","url":"https://huggingface.co/datasets/xingqiang/paligemma-multitask-dataset","creator_name":"chen.xingqiang","creator_url":"https://huggingface.co/xingqiang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["object-detection","image-to-text","visual-question-answering","image-captioning","English"],"keywords_longer_than_N":true},
	{"name":"NLVR-JA","keyword":"image-to-text","description":"This dataset was created by machine translating \"nlvr\" into Japanese.\nnlvr\nhttps://github.com/lil-lab/nlvr/tree/master/nlvr\n","url":"https://huggingface.co/datasets/toshi456/NLVR-JA","creator_name":"toshi456","creator_url":"https://huggingface.co/toshi456","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Japanese","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"MolLangBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"Plot2Code","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPlot2Code Benchmark\n\t\n\nPlot2Code benchmark is now open-sourced at huggingface (ARC Lab) and GitHub. More information can be found in our paper. \n\n\t\n\t\t\n\t\tWhy we need Plot2Code?\n\t\n\n\nðŸ§ While MLLMs have demonstrated potential in visual contexts, their capabilities in visual coding tasks have not been thoroughly evaluated. Plot2Code offers a platform for comprehensive assessment of these models.\n\nðŸ¤— To enable individuals to ascertain the proficiency of AI assistants in generating code thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/Plot2Code.","url":"https://huggingface.co/datasets/TencentARC/Plot2Code","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","text-to-image","image-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"SPEC","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\t[CVPR 2024] SPEC Benchmark: Evaluating VLMs in Fine-grained and Compositional Understanding\n\t\n\nintroduced in the CVPR 2024 paper Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding\nCode | ðŸ¤— Paper | ðŸ“– arXiv\nTo evaluate the understanding capability of visual-language models on fine-grained concepts, we propose a new benchmark, SPEC, \nwhich consists of six distinct subsets, distributed across the dimensions of Size, Position, Existence, and Count.\nEachâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wjpoom/SPEC.","url":"https://huggingface.co/datasets/wjpoom/SPEC","creator_name":"Wujian Peng","creator_url":"https://huggingface.co/wjpoom","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v31","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v31.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v31","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"DeepEyes-Datasets-47k","keyword":"image-text-to-text","description":"This repository contains the datasets used in the paper DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning.\nCode: https://github.com/Visual-Agent/DeepEyes\n","url":"https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k","creator_name":"ChenShawn","creator_url":"https://huggingface.co/ChenShawn","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.14362","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"acb_recaptcha","keyword":"image-to-text","description":"TitanRTX/acb_recaptcha dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/TitanRTX/acb_recaptcha","creator_name":"TITAN RTX","creator_url":"https://huggingface.co/TitanRTX","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-classification","image-to-text","English","mit","10M<n<100M"],"keywords_longer_than_N":true},
	{"name":"HatefulMemesT2IRetrieval","keyword":"image-to-text","description":"\n  HatefulMemesT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve captions based on memes to assess OCR abilities.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic\n\n\nReference\nhttps://arxiv.org/pdf/2005.04790\n\n\n\t\n\nSource datasets:\n\nAhren09/MMSoc_HatefulMemes\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"HatefulMemesT2IRetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/HatefulMemesT2IRetrieval.","url":"https://huggingface.co/datasets/mteb/HatefulMemesT2IRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreShiftProjectRetrieval","keyword":"image-to-text","description":"\n  VidoreShiftProjectRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/shiftproject_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreShiftProjectRetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreShiftProjectRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreShiftProjectRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"RotoWire_English-German","keyword":"data-to-text","description":"Dataset for the WNGT 2019 DGT shared task on \"Document-Level Generation and Translationâ€.","url":"https://huggingface.co/datasets/GEM/RotoWire_English-German","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","automatically-created","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"red_caps","keyword":"image-to-text","description":"RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.\nImages and captions from Reddit depict and describe a wide variety of objects and scenes.\nThe data is collected from a manually curated set of subreddits (350 total),\nwhich give coarse image labels and allow steering of the dataset composition\nwithout labeling individual instances.","url":"https://huggingface.co/datasets/kdexd/red_caps","creator_name":"Karan Desai","creator_url":"https://huggingface.co/kdexd","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","image-captioning","found","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"RotoWire_English-German","keyword":"table-to-text","description":"Dataset for the WNGT 2019 DGT shared task on \"Document-Level Generation and Translationâ€.","url":"https://huggingface.co/datasets/GEM/RotoWire_English-German","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-to-text","automatically-created","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"CASIA-HWDB2-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCASIA-HWDB2 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe offline Chinese handwriting database (CASIA-HWDB2) was built by the National Laboratory of Pattern Recognition (NLPR), Institute of Automation of Chinese Academy of Sciences (CASIA). \nThe handwritten samples were produced by 1,020 writers using Anoto pen on papers, such that both online and offline data were obtained.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line.","url":"https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Chinese","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Characterâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ESOL-V-SMILES-2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBACE-V-SMILES Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BACE (Beta-secretase 1) related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILESâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/ESOL-V-SMILES-2.","url":"https://huggingface.co/datasets/molvision/ESOL-V-SMILES-2","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"aftdb","keyword":"image-to-text","description":"The Arxiv Figure Table Database (AFTdb) facilitates the linking of documentary\nobjects, such as figures and tables, with their captions. This enables a\ncomprehensive description of document-oriented images (excluding images from\ncameras). For the table component, the character structure is preserved in\naddition to the image of the table and its caption. This database is ideal\nfor multimodal processing of documentary images.","url":"https://huggingface.co/datasets/cmarkea/aftdb","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","French","English"],"keywords_longer_than_N":true},
	{"name":"sbucaptions","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tsbuCaptions\n\t\n\nSBU captions :images and captions\nOriginal Source\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset collects the images and annotations from the original SBUcaptions project.\n\n\t\n\t\t\n\t\tðŸ™ Acknowledgement\n\t\n\nAll credits to the original SBUcaptions project teams.\n","url":"https://huggingface.co/datasets/Fhrozen/sbucaptions","creator_name":"Nelson Yalta","creator_url":"https://huggingface.co/Fhrozen","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","100K - 1M","arrow"],"keywords_longer_than_N":true},
	{"name":"flickr30k-narratives","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tFlick30K Narratives\n\t\n\nOriginal Source | Google Localized Narrative\n\n\t\n\t\t\n\t\tðŸ“Œ Introduction\n\t\n\nThis dataset collects the images and annotations from the original Flickr30K and the annotations from the project localized-narratives\n\n\t\n\t\t\n\t\tðŸ™ Acknowledgement\n\t\n\nAll credits to the original Flickr30K project and the localized-narratives teams.\n\n\t\n\t\t\n\t\tðŸ“œ Cite\n\t\n\nPlease consider to cite the following related papers:\n@article{young2014image,\n  title={From image descriptions to visualâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fhrozen/flickr30k-narratives.","url":"https://huggingface.co/datasets/Fhrozen/flickr30k-narratives","creator_name":"Nelson Yalta","creator_url":"https://huggingface.co/Fhrozen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"COCO-HF","keyword":"image-to-text","description":"yusalei/COCO-HF dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/yusalei/COCO-HF","creator_name":"leizhang","creator_url":"https://huggingface.co/yusalei","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"MMHal-Bench","keyword":"image-to-text","description":"MMHal-Bench is a new evaluation benchmark specifically designed for hallucintation in Large Multimodal Models (LMM). It contains 96 challenging questions based on images from OpenImages, and their corresponding ground-truth answers and image contents.","url":"https://huggingface.co/datasets/Shengcao1006/MMHal-Bench","creator_name":"Shengcao Cao","creator_url":"https://huggingface.co/Shengcao1006","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"elementor-layout-vlm-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tElementor Layout VLM Dataset\n\t\n\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Summary\n\t\n\nDataset para fine-tuning de modelos Vision-Language (VLM) para geraÃ§Ã£o de layouts Elementor a partir de imagens.\n\nTask: Visual Question Answering (VQA)\nFormat: VLM VQA (image, question, answer)\nTotal: 30 exemplos\nTraining: 24 exemplos\nValidation: 6 exemplos\n\n\n\t\n\t\t\n\t\tðŸŽ¯ Uso Recomendado\n\t\n\n\n\t\n\t\t\n\t\tAutoTrain Configuration\n\t\n\nTask: VLM VQA\nBase Model: google/paligemma-3b-pt-448\nDataset: vinicios94/elementor-layout-vlm-datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vinicios94/elementor-layout-vlm-dataset.","url":"https://huggingface.co/datasets/vinicios94/elementor-layout-vlm-dataset","creator_name":"Vinicios Rabaioli","creator_url":"https://huggingface.co/vinicios94","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","Portuguese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"JinaVDRDocQAEnergyRetrieval","keyword":"image-to-text","description":"\n  JinaVDRDocQAEnergyRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve energy industry documents based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/docqa_energy_beir\n\n\n\t\n\nSource datasets:\n\njinaai/docqa_energy_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRDocQAEnergyRetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRDocQAEnergyRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRDocQAEnergyRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRArxivQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRArxivQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve figures from scientific papers from arXiv based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/arxivqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/arxivqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRArxivQARetrieval\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRArxivQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRArxivQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","LM-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"BBBP-V-SMILES-0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tBBBP-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for BBBP related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statisticsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/BBBP-V-SMILES-0.","url":"https://huggingface.co/datasets/molvision/BBBP-V-SMILES-0","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VidoreInfoVQARetrieval","keyword":"image-to-text","description":"\n  VidoreInfoVQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/infovqa_test_subsampled_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreInfoVQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreInfoVQARetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreInfoVQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"JinaVDRTatQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRTatQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve financial reports based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/tatqa_beir\n\n\n\t\n\nSource datasets:\n\njinaai/tatqa_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRTatQARetrieval\")\nevaluator = mteb.MTEB([task])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRTatQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRTatQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreTatdqaRetrieval","keyword":"image-to-text","description":"\n  VidoreTatdqaRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/tatdqa_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"VidoreTatdqaRetrieval\")\nevaluator = mteb.MTEB([task])\n\nmodel =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreTatdqaRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreTatdqaRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"PixLore-Rich-Captions","keyword":"image-to-text","description":"Rich image captioning dataset used for training PixLore model: https://arxiv.org/abs/2312.05349\n\"image_path\" contains the path to the COCO dataset image (change the path accordingly),\n\"rich_caption\" contains the rich caption created using the technique described in the paper.\nThe rest of the columns are used for debugging or improving the prompt.\n","url":"https://huggingface.co/datasets/Boni98/PixLore-Rich-Captions","creator_name":"Diego Bonilla","creator_url":"https://huggingface.co/Boni98","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ui_refexp","keyword":"image-to-text","description":"This dataset is intended for UI understanding, referring expression and action automation model training. It's based on the UIBert RefExp dataset from Google Research, which is based on the RICO dataset.","url":"https://huggingface.co/datasets/ivelin/ui_refexp","creator_name":"Ivelin Ivanov","creator_url":"https://huggingface.co/ivelin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K<n<100K","arxiv:2107.13731"],"keywords_longer_than_N":true},
	{"name":"train_video_and_instruction","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tShareGPTVideo Training Data\n\t\n\nAll dataset and models can be found at ShareGPTVideo.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nTrain 300k video frames: contains video frames used for SFT and DPO model, which is a subset of total 900k.\nActivityNet 50k + vidal 150k + webvid 100k.\n\nTrain 600k video frames: contains the rest 600k frames, the total 900k frames are used for pre-training stage. If you just do finetuning using our video QA, you can just download the 300k above.\n900k composition is 400k WebVid +â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction.","url":"https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction","creator_name":"ShareGPTVideo","creator_url":"https://huggingface.co/ShareGPTVideo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"allava4v-train-regenerated","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tALLaVA-4V Train Dataset (Regenerated)\n\t\n\nThis dataset is a regenerated version of the ALLaVA-4V training dataset, processed using Qwen/Qwen3-VL-8B-Instruct model.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: ALLaVA-4V training data\nModel Used: Qwen/Qwen3-VL-8B-Instruct\nOriginal Format: JSONL (252,924 samples)\nOutput Format: Parquet\nTemperature: 0.0 (deterministic generation)\nProcessing Status: In progress (~21% complete as of upload)\n\n\n\t\n\t\t\n\t\n\t\n\t\tGeneration Details\n\t\n\nThe dataset wasâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/vincent-4/allava4v-train-regenerated.","url":"https://huggingface.co/datasets/vincent-4/allava4v-train-regenerated","creator_name":"Vincent","creator_url":"https://huggingface.co/vincent-4","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"synthetic_hkr_large","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated using handwritten fonts\n\t\n\nNumber of images: 2634473\nSources:\n\nHandwriting generation code\n\nThe code was executed with hkr option (with fewer augmentations)\n","url":"https://huggingface.co/datasets/nastyboget/synthetic_hkr_large","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"UIT-ViIC","keyword":"image-to-text","description":"COCO is a large-scale object detection, segmentation, and captioning dataset.","url":"https://huggingface.co/datasets/ttrung1402/UIT-ViIC","creator_name":"thanh trung","creator_url":"https://huggingface.co/ttrung1402","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Vietnamese","cc-by-4.0","1K<n<10K","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"Yolo-Text-Detection","keyword":"image-to-text","description":"The text scene detection dataset is marked up manually in yolo format and contains 20,799 images, mostly with Latin and Russian text.\n\n\n\n\n  \n","url":"https://huggingface.co/datasets/DonkeySmall/Yolo-Text-Detection","creator_name":"Donkey Small","creator_url":"https://huggingface.co/DonkeySmall","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","English","Russian","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"JinaVDRMPMQARetrieval","keyword":"image-to-text","description":"\n  JinaVDRMPMQARetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve product manuals based on human annotated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/mpmqa_small_beir\n\n\n\t\n\nSource datasets:\n\njinaai/mpmqa_small_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_task(\"JinaVDRMPMQARetrieval\")\nevaluator =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRMPMQARetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRMPMQARetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","human-annotated","monolingual"],"keywords_longer_than_N":true},
	{"name":"Clintox-V-SMILES-0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tClintox-V-SMILES Train Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains molecular data with visual representations for Clintox related compounds.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nQuestion: Question related to the molecule\nAnswer: Corresponding answer\nTargetMolecule: SMILES representation of the target molecule  \nSampleMethod: Method used for sampling\nSampleNum: Sample number\nSampleRep: Sample repetition\nimage: Generated molecular structure image from SMILES\n\n\n\t\n\t\t\n\t\n\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/molvision/Clintox-V-SMILES-0.","url":"https://huggingface.co/datasets/molvision/Clintox-V-SMILES-0","creator_name":"Molvision","creator_url":"https://huggingface.co/molvision","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"JinaVDRShiftProjectRetrieval","keyword":"image-to-text","description":"\n  JinaVDRShiftProjectRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve documents with graphs from the Shift Project based on LLM generated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/shiftproject_beir\n\n\n\t\n\nSource datasets:\n\njinaai/shiftproject_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRShiftProjectRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRShiftProjectRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"VidoreSyntheticDocQAAIRetrieval","keyword":"image-to-text","description":"\n  VidoreSyntheticDocQAAIRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve associated pages according to questions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nAcademic\n\n\nReference\nhttps://arxiv.org/pdf/2407.01449\n\n\n\t\n\nSource datasets:\n\nvidore/syntheticDocQA_artificial_intelligence_test_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAAIRetrieval.","url":"https://huggingface.co/datasets/mteb/VidoreSyntheticDocQAAIRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","monolingual"],"keywords_longer_than_N":true},
	{"name":"dn_dataset","keyword":"image-to-text","description":"adeaven/dn_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/adeaven/dn_dataset","creator_name":"hexiuwen","creator_url":"https://huggingface.co/adeaven","license_name":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":null,"first_N":5,"first_N_keywords":["text-to-image","image-to-text","object-detection","zero-shot-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"amazon-products","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Creation and Processing Overview\n\t\n\nThis dataset underwent a comprehensive process of loading, cleaning, processing, and preparing, incorporating a range of data manipulation and NLP techniques to optimize its utility for machine learning models, particularly in natural language processing.\n\n\t\n\t\t\n\t\tData Loading and Initial Cleaning\n\t\n\n\nSource: Loaded from the Hugging Face dataset repository bprateek/amazon_product_description.\nConversion to Pandas DataFrame: For ease of dataâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ckandemir/amazon-products.","url":"https://huggingface.co/datasets/ckandemir/amazon-products","creator_name":"CK","creator_url":"https://huggingface.co/ckandemir","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Trace_Captioning_COCO","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCOCO Trace Captioning Dataset\n\t\n\nThis dataset contains trace-based captions for images from the COCO dataset. Each sample includes multiple captions paired with spatial-temporal traces that represent mouse scanning patterns over arbitrary image regions. This dataset is a resource introduced in the Patch-ioner paper for evaluating region-based captioning models.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe COCO Trace Captioning dataset is a resource created for evaluating region-based imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ruggero1912/Trace_Captioning_COCO.","url":"https://huggingface.co/datasets/Ruggero1912/Trace_Captioning_COCO","creator_name":"Giacomo Pacini","creator_url":"https://huggingface.co/Ruggero1912","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"medieval","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for CATMuS Medieval\n\t\n\n\nJoin our Discord to ask questions about the dataset: \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nHandwritten Text Recognition (HTR) has emerged as a crucial tool for converting manuscripts images into machine-readable formats, \nenabling researchers and scholars to analyse vast collections efficiently. \nDespite significant technological progress, establishing consistent ground truth across projects for HTR tasks, \nparticularly for complex and heterogeneousâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATMuS/medieval.","url":"https://huggingface.co/datasets/CATMuS/medieval","creator_name":"CATMuS: Consistent Approach to Transcribing ManuScripts","creator_url":"https://huggingface.co/CATMuS","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","English","Dutch","Italian"],"keywords_longer_than_N":true},
	{"name":"DataComp_large_pool_BLIP2_captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for DataComp_large_pool_BLIP2_captions\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nWe have used this dataset for pre-training CLIP models and found that it rivals or outperforms models trained on raw web captions on average across the 38 evaluation tasks proposed by DataComp.\nRefer to the DataComp leaderboard (https://www.datacomp.ai/leaderboard.html) for the top baselines uncovered in our work.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nPrimarily English.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/thaottn/DataComp_large_pool_BLIP2_captions.","url":"https://huggingface.co/datasets/thaottn/DataComp_large_pool_BLIP2_captions","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaottn","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","zero-shot-classification","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"dart","keyword":"data-to-text","description":"DART is a large and open-domain structured DAta Record to Text generation corpus\nwith high-quality sentence annotations with each input being a set of\nentity-relation triples following a tree-structured ontology. It consists of\n82191 examples across different domains with each input being a semantic RDF\ntriple set derived from data records in tables and the tree ontology of table\nschema, annotated with sentence description that covers all facts in the triple set.","url":"https://huggingface.co/datasets/GEM/dart","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"dart","keyword":"table-to-text","description":"DART is a large and open-domain structured DAta Record to Text generation corpus\nwith high-quality sentence annotations with each input being a set of\nentity-relation triples following a tree-structured ontology. It consists of\n82191 examples across different domains with each input being a semantic RDF\ntriple set derived from data records in tables and the tree ontology of table\nschema, annotated with sentence description that covers all facts in the triple set.","url":"https://huggingface.co/datasets/GEM/dart","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-to-text","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B.","url":"https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"stackmix_hkr_large","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset generated from HKR train set using Stackmix\n\t\n\nNumber of images: 2476836\nSources:\n\nHKR dataset\nStackmix code\n\n","url":"https://huggingface.co/datasets/nastyboget/stackmix_hkr_large","creator_name":"Anastasiya Zykina (Bogatenkova)","creator_url":"https://huggingface.co/nastyboget","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["image-to-text","Russian","mit","1M<n<10M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"captioned_images","keyword":"image-to-text","description":"Dataset Summary\nThis is a 660+ image dataset captioned professionally, part of a 450M image dataset with 780M records of ground truth. This is a highly diverse, interleaved dataset.  Many images are highly aesthetic and many are everyday photos taken by tens of millions of people across 8 years with different cameras in different settings, captioned descriptively and accurately by hand. They were used to train ML Vision models.\nPII and images of humans have been removed from this sampleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Optasia/captioned_images.","url":"https://huggingface.co/datasets/Optasia/captioned_images","creator_name":"Optasia Corp","creator_url":"https://huggingface.co/Optasia","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["Arabic","English","Spanish","cc-by-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"histogram-comparisons-small-v1","keyword":"image-to-text","description":"This is a small subset of the huge histogram-comparisons-v1 dataset with 3M rows.\nThis dataset contains 150000 items in total. There are 3 curriculums each containing 50000 items.\nEach item is a markdown document.\nEach item contains between 2 and 6 image comparisons, with a Summary at the bottom.\nThe images are between 3x3 and 14x14.\nThe markdown document contains a ## Response, that separates the prompt from the answer.\nThe structure of the markdown document with 3 comparisons: A, B, C.\n#â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/histogram-comparisons-small-v1.","url":"https://huggingface.co/datasets/neoneye/histogram-comparisons-small-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"test_dataset_12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n21 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_12.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_12","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test_dataset_13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n29 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_13.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_13","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test_dataset_17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n115 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_17.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_17","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test_dataset_16","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n106 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_16.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_16","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test_dataset_14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n32 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/test_dataset_14.","url":"https://huggingface.co/datasets/KMH158-QLU/test_dataset_14","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"newyorker_caption_contest","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for New Yorker Caption Contest Benchmarks\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSee capcon.dev for more!\nData from:\nDo Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest\n@inproceedings{hessel2023androids,\n  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''\n         Benchmarks from {The New Yorker Caption Contest}},\n  author={Hessel, Jack and Marasovi{\\'c}, Ana and Hwang, Jena D. and Lee, Lillian\n          andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jmhessel/newyorker_caption_contest.","url":"https://huggingface.co/datasets/jmhessel/newyorker_caption_contest","creator_name":"Jack Hessel","creator_url":"https://huggingface.co/jmhessel","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","multiple-choice","text-classification","text-generation","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Khatt-Dataset-Unique-lines-full","keyword":"image-to-text","description":"","url":"https://huggingface.co/datasets/Nada2125/Khatt-Dataset-Unique-lines-full","creator_name":"Abbas","creator_url":"https://huggingface.co/Nada2125","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","text"],"keywords_longer_than_N":true},
	{"name":"artelingo-dummy","keyword":"image-to-text","description":"ArtELingo is a benchmark and dataset introduced in a research paper aimed at promoting work on diversity across languages and cultures. It is an extension of ArtEmis, which is a collection of 80,000 artworks from WikiArt with 450,000 emotion labels and English-only captions. ArtELingo expands this dataset by adding 790,000 annotations in Arabic and Chinese. The purpose of these additional annotations is to evaluate the performance of \"cultural-transfer\" in AI systems.\nThe dataset in ArtELingoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/youssef101/artelingo-dummy.","url":"https://huggingface.co/datasets/youssef101/artelingo-dummy","creator_name":"mohamed","creator_url":"https://huggingface.co/youssef101","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-classification","text-to-image","text-generation"],"keywords_longer_than_N":true},
	{"name":"recruiter_101steps_final","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n10 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_101steps_final.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_101steps_final","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SSLQ-Version-1.600","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSSLQ Version 1.600\n\t\n\nSSLQ Version 1.600, â€œSynthetic Scenic Lore Image Qualityâ€, is a hand-curated dataset of 600 manually annotated images drawn from the SSLQ archive, which is a long-term study in synthetic image aesthetics \nand visual coherence. Each image was labeled in Label Studio 1.13 using a structured XML schema and enriched with human and LLM commentary \ndescribing visual qualities, stylistic alignment, and subjective evaluations of quality and mood.\n\n\t\n\t\t\n\t\n\t\n\t\tAnnotationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/shalvers/SSLQ-Version-1.600.","url":"https://huggingface.co/datasets/shalvers/SSLQ-Version-1.600","creator_name":"Steven Halverson","creator_url":"https://huggingface.co/shalvers","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","text-retrieval","image-to-text","expert-generated","English"],"keywords_longer_than_N":true},
	{"name":"StockImages-CC0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCC0 Stock Images Dataset\n\t\n\nThis dataset contains a collection of stock images that are covered by the Creative Commons Zero (CC0) License, meaning they are free for personal and commercial use with no attribution required. It is designed to support a variety of computer vision tasks such as image tagging, categorization, and machine learning model training.\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nWhile every effort has been made to ensure the reliability and correctness of the data presented, theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KoalaAI/StockImages-CC0.","url":"https://huggingface.co/datasets/KoalaAI/StockImages-CC0","creator_name":"Koala AI","creator_url":"https://huggingface.co/KoalaAI","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"Screen2Coord","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tScreen2Coord_denorm_extend Dataset\n\t\n\nScreen2Coord is a dataset for training models that take a screenshot, screen dimensions, and a textual action description as input and output the coordinates of the target bounding box on the screen. This dataset is intended for image-text-to-text LLMs applied to user interface interactions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tNew feature! Windows, MacOS, Linux-Ubuntu subsets!\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nA typical data instance in Screen2Coordâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cybertruck32489/Screen2Coord.","url":"https://huggingface.co/datasets/cybertruck32489/Screen2Coord","creator_name":"cybertruck","creator_url":"https://huggingface.co/cybertruck32489","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","object-detection","visual-question-answering","instance-segmentation","English"],"keywords_longer_than_N":true},
	{"name":"Flickr8k","keyword":"image-to-text","description":"shivangibithel/Flickr8k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/shivangibithel/Flickr8k","creator_name":"Shivangi Bithel","creator_url":"https://huggingface.co/shivangibithel","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"RICO-WidgetCaptioning","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for RICO Widget Captioning\n\t\n\nWidget Captioning is a dataset for providing captions for UI elements on mobile screens. \nIt uses the RICO image database. \n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository:\ngoogle-research-datasets/widget-caption\nRICO raw downloads\n\n\nPaper:\nWidget Captioning: Generating Natural Language Description for Mobile User Interface Elements\nRico: A Mobile App Dataset for Building Data-Driven Design Applications\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rootsautomation/RICO-WidgetCaptioning.","url":"https://huggingface.co/datasets/rootsautomation/RICO-WidgetCaptioning","creator_name":"Roots Automation","creator_url":"https://huggingface.co/rootsautomation","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-generation","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Pexels-400k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPexels 400k\n\t\n\nDataset of 400,476 videos, their thumbnails, viewcounts, explicit classification, and caption.\nNote: The Pexels-320k dataset in the repo is this dataset, with videos <10s removed.\n","url":"https://huggingface.co/datasets/jovianzm/Pexels-400k","creator_name":"Jovian","creator_url":"https://huggingface.co/jovianzm","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","English"],"keywords_longer_than_N":true},
	{"name":"VARAG","keyword":"image-to-text","description":"DeAllGamer/VARAG dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/DeAllGamer/VARAG","creator_name":"Adithya TG","creator_url":"https://huggingface.co/DeAllGamer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["text-generation","text-classification","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"recruiter_atomic_final_chronological","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n2 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_final_chronological.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_final_chronological","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recreuiter_task_2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n24 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recreuiter_task_2.","url":"https://huggingface.co/datasets/KMH158-QLU/recreuiter_task_2","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_atomic_final_correct","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n3 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_final_correct.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_final_correct","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_perfect2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n2 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect2.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_perfect2","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recreuiter_task","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n17 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recreuiter_task.","url":"https://huggingface.co/datasets/KMH158-QLU/recreuiter_task","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"recruiter_atomic_task_final8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUI Automation Dataset (Multi-Video)\n\t\n\n2 examples from 1 videos - UI automation tasks from screen recordings.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry contains:\n\nvideo_id: Sequential ID for each video (video_001, video_002, etc.)\nstep: Step number within that video (0, 1, 2, ...)\nsystem: System prompt for the GUI agent\nuser: Task instruction + previous actions\nassistant: Model's reasoning and action\nimage: Screenshot of the UI state\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_task_final8.","url":"https://huggingface.co/datasets/KMH158-QLU/recruiter_atomic_task_final8","creator_name":"Murad","creator_url":"https://huggingface.co/KMH158-QLU","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-Vietnamese-subset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for LAION-2B-multi Vietnamese subset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFilter the Vietnamese subset from Laion2B-multi\nTo get the subset of your language, check out this notebook\n","url":"https://huggingface.co/datasets/imthanhlv/laion2B-multi-Vietnamese-subset","creator_name":"ThÃ nh LÃª","creator_url":"https://huggingface.co/imthanhlv","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Vietnamese","cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"MM-Math-Align","keyword":"image-to-text","description":"\nHard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models\n\n\n\n| ðŸ™ Github Code |\nðŸ“ƒ Paper |\n\n\n\n\t\n\t\t\n\t\tDataset description:\n\t\n\nWe release MM-Math-Align, a dataset built upon MM-Math, which is derived from actual geometry questions used in middle school exams. Each sample contains the original geometric diagram(original_image), a Python script's image(positive_image) that approximately reconstructs the diagram, a caption(positive_caption) describingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/MM-Math-Align.","url":"https://huggingface.co/datasets/THU-KEG/MM-Math-Align","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottomâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BIGstockimage2M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for BIGstockimage2M\n\t\n\n~2M stock images.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 2 million stock images and caption. Images are low resolution, ~0.4MP.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tWebDataset Structure\n\t\n\n\nkey: Unique identifier for the image\n.jpg: The image\n.txt: The caption\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{BIGstockimage2M,\n  author = {hlky},\n  title = {BIGstockimage2M}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/BIGstockimage2M.","url":"https://huggingface.co/datasets/bigdata-pw/BIGstockimage2M","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"synthetic-watch-faces-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynthetic Watch Faces Dataset\n\t\n\nA synthetic dataset of analog watch faces displaying various times for training vision models in time recognition tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of randomly generated analog watch faces showing different times. Each image contains a watch with hour and minute hands positioned to display a specific time. The dataset is designed to help train and evaluate computer vision models and Vision-Language Models (VLMs) for timeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/elischwartz/synthetic-watch-faces-dataset.","url":"https://huggingface.co/datasets/elischwartz/synthetic-watch-faces-dataset","creator_name":"Eli Schwartz","creator_url":"https://huggingface.co/elischwartz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","image-classification","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Mathematics-Class12-Vol.1.n.2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMathematics-Class12-Vol.1.n.2\n\t\n\nThis dataset contains high-quality scanned pages from the Tamil Nadu State Board (TNSB) Class 12 Mathematics Volume 1 & 2 textbook. It is intended for use in machine learning applications involving OCR (optical character recognition), educational AI tools, and digitization of state board curriculum materials.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nSource: Tamil Nadu State Board Class 12 Mathematics â€“ Volume 1 & 2\n\nModality: Image\n\nFormat: imagefolder\n\nSplit:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Mathematics-Class12-Vol.1.n.2.","url":"https://huggingface.co/datasets/prithivMLmods/Mathematics-Class12-Vol.1.n.2","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"JinaVDRAirbnbSyntheticRetrieval","keyword":"image-to-text","description":"\n  JinaVDRAirbnbSyntheticRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve rendered tables from Airbnb listings based on templated queries.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nWeb\n\n\nReference\nhttps://huggingface.co/datasets/jinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\nSource datasets:\n\njinaai/airbnb-synthetic-retrieval_beir\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval.","url":"https://huggingface.co/datasets/mteb/JinaVDRAirbnbSyntheticRetrieval","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"EvalQABench","keyword":"image-text-to-text","description":"This repository contains the data for LOVA3: Learning to Visual Question Answering, Asking and Assessment. \nLOVA3 is a framework designed to equip MLLMs with the capabilities to answer, ask, and assess questions in the context of images.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tðŸŽ“ Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuanâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/EvalQABench.","url":"https://huggingface.co/datasets/hhenryz/EvalQABench","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-shape-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Ego-R1-Data","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tðŸ§  Ego-R1 Data\n\t\n\nWelcome to the Ego-R1 Data, a comprehensive collection designed to facilitate the training of large language models for tool-augmented reasoning and reinforcement learning. This dataset will be used for Ego-R1 Codebase, presented in the paper Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ“Š Dataset Overview\n\t\n\nThe Ego-R1 Dataset consists of two main components:\n\nEgo-CoTT-25K: 25,000 Chain-of-Tool-Thought examples for Supervisedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Ego-R1/Ego-R1-Data.","url":"https://huggingface.co/datasets/Ego-R1/Ego-R1-Data","creator_name":"Ego-R1","creator_url":"https://huggingface.co/Ego-R1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2506.13654","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"LSDBench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for LSDBench: Long-video Sampling Dilemma Benchmark\n\t\n\nA benchmark that focuses on the sampling dilemma in long-video tasks. Through well-designed tasks, it evaluates the sampling efficiency of long-video VLMs.\nArxiv Paper: ðŸ“– Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?\nGithub : https://github.com/dvlab-research/LSDBench\n(Left) In Q1, identifying a camera wearer's visited locations requires analyzing the entire video. However, key framesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TainU/LSDBench.","url":"https://huggingface.co/datasets/TainU/LSDBench","creator_name":"QU Tianyuan","creator_url":"https://huggingface.co/TainU","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMScan-llava-form","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tMMScan LLaVA-Form Data\n\t\n\nThis repository provides the processed LLaVA-formatted dataset for the MMScan Question Answering Benchmark.\n\n\t\n\t\t\n\t\tDataset Contents\n\t\n\n(1) All image data(Depth&RGB) is distributed in split ZIP archives. Please combine the split ZIP files into a single archive and extract the merged ZIP file using the following command:\ncat mmscan_val8.z* > mmscan_va.zip\nunzip mmscan_va.zip\n\n(2) Under ./annotations, we provide the MMScan Question Answering validation set withâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/rbler/MMScan-llava-form.","url":"https://huggingface.co/datasets/rbler/MMScan-llava-form","creator_name":"Jingli Lin","creator_url":"https://huggingface.co/rbler","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"SynthChartNet","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tSynthChartNet\n\t\n\n\n    \n\n\nSynthChartNet is a multimodal dataset designed for training the SmolDocling model on chart-based document understanding tasks. It consists of 1,981,157 synthetically generated samples, where each image depicts a chart (e.g., line chart, bar chart, pie chart, stacked bar chart), and the associated ground truth is given in OTSL format.\nCharts were rendered at 120 DPI using a diverse set of visualization libraries: Matplotlib, Seaborn, and Pyecharts, enablingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ds4sd/SynthChartNet.","url":"https://huggingface.co/datasets/ds4sd/SynthChartNet","creator_name":"Docling","creator_url":"https://huggingface.co/ds4sd","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cdla-permissive-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Flickr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Flickr\n\t\n\n~5B images from Flickr.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 5 billion images from Flickr. Entries include URLs to images at various resolutions and available metadata such as license, geolocation, dates, description and machine tags (camera info).\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{flickr,\n  author = {hlky},\n  title = {Flickr},\n  year =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/Flickr.","url":"https://huggingface.co/datasets/bigdata-pw/Flickr","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","1B - 10B"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v167","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v167.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v167","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v145","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v145.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v145","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Synthdog-Multilingual-100","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynthdog Multilingual\n\t\n\n\n\nThe Synthdog dataset created for training in Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model.\nUsing the official Synthdog code, we created >1 million training samples for improving OCR capabilities in Large Vision-Language Models.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nWe provide the images for download in two .tar.gz files. Download and extract them in folders of the same name (so cat images.tar.gz.* | tar xvzf -C images; tar xvzfâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100.","url":"https://huggingface.co/datasets/WueNLP/Synthdog-Multilingual-100","creator_name":"WÃ¼NLP","creator_url":"https://huggingface.co/WueNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","multilingual","Afrikaans","Amharic","Arabic"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-more-results","keyword":"image-to-text","description":"\n\n\n\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for image-preferences-results Original\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian worldâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results.","url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v97","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v97.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v97","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"jee-neet-benchmark","keyword":"image-text-to-text","description":"A benchmark dataset for evaluating Large Language Models (LLMs) on Joint Entrance Examination (JEE)\nand National Eligibility cum Entrance Test (NEET) questions from India. Questions are provided as\nimages, and metadata includes exam details, subject, and correct answers.","url":"https://huggingface.co/datasets/Reja1/jee-neet-benchmark","creator_name":"Md Rejaullah","creator_url":"https://huggingface.co/Reja1","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","question-answering","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v3","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v3\n\t\n\nCaption3o-Opt-v3 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: ~100,000 image-caption pairs (estimated)\nFormat: Parquet\nImage resolution: 512x512\nLanguages: Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"art-museums-pd-440k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tArt Museums PD 440K\n\t\n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a dataset to train text-to-image or any text and image multimodal models with minimized copyright/licensing concerns.\nAll images and texts in this dataset are orignally shared under CC0 or public domain, and no pretrained models or any AI models are used to build this dataset except for our ElanMT model to translate English captions to Japanese.\nElanMT model is trained solely on licensed corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tData sources\n\t\n\nImages andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/art-museums-pd-440k.","url":"https://huggingface.co/datasets/Mitsua/art-museums-pd-440k","creator_name":"elanmitsua","creator_url":"https://huggingface.co/Mitsua","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v179","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v179.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v179","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Caption3o-Opt-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCaption3o-Opt-v3\n\t\n\nCaption3o-Opt-v3 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nSize: ~100,000 image-caption pairs (estimated)\nFormat: Parquet\nImage resolution: 512x512\nLanguages: Englishâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-Opt-v3","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"IAM-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIAM - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe IAM Handwriting Database contains forms of handwritten English text which can be used to train and test handwritten text recognizers and to perform writer identification and verification experiments.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image':â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/IAM-line.","url":"https://huggingface.co/datasets/Teklia/IAM-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"pokemoncards","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Pokemon Cards TCG\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about Pokemon Trading Card Game (TCG) cards, decks, and sets. It is designed to be used for training machine learning models to classify and analyze Pokemon cards.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nTasks: \nImage Classification\nText Classification\nInformation Retrieval\nImage-to-Text\nImage Feature Extraction\nImage Segmentation\nImage Classificationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/tooni/pokemoncards.","url":"https://huggingface.co/datasets/tooni/pokemoncards","creator_name":"toni","creator_url":"https://huggingface.co/tooni","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-segmentation","sentence-similarity","image-to-text","image-feature-extraction"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type.\nThe LLM learned some of the types fine. However histograms are causing problems.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. Since this is what my LLM is struggling with.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v113","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v113.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v113","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wikinews-fr-100_fr_prompt_data_to_text","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\twikinews-fr-100_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nwikinews-fr-100_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 3,000 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset wikinews-fr-100.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/wikinews-fr-100_fr_prompt_data_to_text.","url":"https://huggingface.co/datasets/CATIE-AQ/wikinews-fr-100_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/wikinews-fr-100"],"keywords_longer_than_N":true},
	{"name":"MagicData340k","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMagicData340K: A Large-Scale Dataset for Fine-Grained Artifacts Assessment in Text-to-Image Generation\n\t\n\nThis repository hosts MagicData340K, a large-scale human-annotated dataset central to the MagicMirror framework. The MagicMirror framework introduces a comprehensive approach for the systematic and fine-grained evaluation of physical artifacts (such as anatomical and structural flaws) in Text-to-Image (T2I) generation.\nMagicData340K is the first human-annotated large-scale datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wj-inf/MagicData340k.","url":"https://huggingface.co/datasets/wj-inf/MagicData340k","creator_name":"Jia Wang","creator_url":"https://huggingface.co/wj-inf","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","text-to-image","mit","arxiv:2509.10260","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AVAINT-IMG","keyword":"image-to-text","description":"botintel-community/AVAINT-IMG dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/botintel-community/AVAINT-IMG","creator_name":"BotIntel X","creator_url":"https://huggingface.co/botintel-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"AVAINT-IMG","keyword":"image-to-text","description":"botintel-community/AVAINT-IMG dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/botintel-community/AVAINT-IMG","creator_name":"BotIntel X","creator_url":"https://huggingface.co/botintel-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","visual-question-answering","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"Visco-Attack","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tVisCo Attack: Visual Contextual Jailbreak Dataset\n\t\n\nðŸ“„ arXiv:2507.02844 Â· ðŸ’» Code â€“ Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\nThis dataset contains the adversarial contexts, prompts, and images from the paper: \"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\".\n\n\t\n\t\t\n\t\n\t\n\t\tâš ï¸ Content Warning\n\t\n\nThis dataset contains content that is offensive and/or harmful. It was created for research purposes to study the safetyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/miaozq/Visco-Attack.","url":"https://huggingface.co/datasets/miaozq/Visco-Attack","creator_name":"miaozq","creator_url":"https://huggingface.co/miaozq","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","Image","arxiv:2507.02844"],"keywords_longer_than_N":true},
	{"name":"MMLongBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly\n\t\n\nðŸŒ Homepage | ðŸ¤— Dataset | ðŸ¤— Paper | ðŸ“– arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nThe rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverseâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ZhaoweiWang/MMLongBench.","url":"https://huggingface.co/datasets/ZhaoweiWang/MMLongBench","creator_name":"Zhaowei Wang","creator_url":"https://huggingface.co/ZhaoweiWang","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K<n<100K","arxiv:2505.10610"],"keywords_longer_than_N":true},
	{"name":"Chanel","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tChanel\n\t\n\n357 Chanel RTW looks with model image/video and details of the items being worn, split across 6 seasons.\n2023/24 mÃ©tiers dâ€™art, spring-summer 2024, cruise 2023/24 feature models from diverse backgrounds and has video. chanel coco beach 2024 and spring-summer 2024 pre-collection has a range of looks for 2 models and has no video. Video is of model on the catwalk. Images show the entire outfit.\nItems have a class-like name e.g. \"Dress\", \"Rectangle Sunglasses\" and a descriptionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/Chanel.","url":"https://huggingface.co/datasets/bigdata-pw/Chanel","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","< 1K"],"keywords_longer_than_N":true},
	{"name":"Data-DeQA-Score","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tData-DeQA-Score\n\t\n\nDatasets of the DeQA-Score paper \n(\nproject page / \ncodes / \npaper \n) \nin our DepictQA project.\n\n\t\n\t\t\n\t\tDataset Construction\n\t\n\n\nDownload our meta files in this repo. \n\nDownload source images from KonIQ, \nSPAQ, \nKADID, \nPIPAL, \nLIVE-Wild, \nAGIQA, \nTID2013, \nand CSIQ.\n\nArrange the folders as follows:\n\n\n|-- Data-DeQA-Score\n  |-- KONIQ\n    |-- images/*.jpg\n    |-- metas\n  |-- SPAQ\n    |-- images/*.jpg\n    |-- metas\n  |-- KADID10K\n    |-- images/*.png\n    |-- metas\n  |--â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhiyuanyou/Data-DeQA-Score.","url":"https://huggingface.co/datasets/zhiyuanyou/Data-DeQA-Score","creator_name":"zhiyuanyou","creator_url":"https://huggingface.co/zhiyuanyou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K<n<100K","arxiv:2501.11561"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v68","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v68.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v68","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHumanCaption-10M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing the Agreement\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text).â€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"Newspapers-finlam-La-Liberte","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNewspaper dataset: Finlam La LibertÃ©\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Finlam La LibertÃ© dataset includes 1500 issues from La LibertÃ©, a French newspaper, from 1925 to 1928. \nEach issue contains multiple pages, with one image for each page resized to a fixed height of 2500 pixels.\nThe dataset can be used to train end-to-end newspaper understanding models, with tasks including:\n\nText zone detection and classification\nReading order detection\nArticle separation\n\n\n\t\n\t\t\n\t\n\t\n\t\tSplitâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/Newspapers-finlam-La-Liberte.","url":"https://huggingface.co/datasets/Teklia/Newspapers-finlam-La-Liberte","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-classification","image-segmentation","French"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v58","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v58.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1â€“4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict objectâ€“surfaceâ€“scene compatibility mappings.\nA usecase of theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"confab","keyword":"image-to-text","description":"sujal110/confab dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/sujal110/confab","creator_name":"sujal panchal","creator_url":"https://huggingface.co/sujal110","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","image-to-text","English","Hindi","Gujarati"],"keywords_longer_than_N":true},
	{"name":"multilingual-coco","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMultilingual Common Objects in Context (COCO) Dataset\n\t\n\nThis dataset is a collection of multiple language open-source captions of COCO dataset. \nThe split in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/multilingual-coco.","url":"https://huggingface.co/datasets/romrawinjp/multilingual-coco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Thai","Russian","Japanese"],"keywords_longer_than_N":true},
	{"name":"VisualDataset100K","keyword":"image-to-text","description":"ä¸­æ–‡\n\n\t\n\t\t\n\t\n\t\n\t\tLocal Deployment of Large Models and Construction of VisualDataset100K Dataset\n\t\n\nDeploy large models locally using vllm and utilize them to construct the VisualDataset100K dataset.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Local Deployment of Large Models (vllm + nginx)\n\t\n\nUses multi GPUs, loads the Qwen/Qwen2-VL-2B-Instructã€Qwen/Qwen2-VL-7B-Instructã€Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4 models through vllm, and uses nginx for load balancing.\n1.1 Launch vllm instances:\nRun a vllm instance on each GPUâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/REILX/VisualDataset100K.","url":"https://huggingface.co/datasets/REILX/VisualDataset100K","creator_name":"SunForlight","creator_url":"https://huggingface.co/REILX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-to-text","Chinese","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"Diffuse_Map_Surfaces","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Diffuse Map Surface\n\t\n\nDetailed surface textures without shadows or hotspots. (Diffuse Maps)\nAn LORA for Flux.1-Dev using this dataset is aviable at https://civitai.com/models/939491\n","url":"https://huggingface.co/datasets/alastandy/Diffuse_Map_Surfaces","creator_name":"Andrew Smith","creator_url":"https://huggingface.co/alastandy","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"aircraft-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for High-Resolution Aircraft Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 165,340 high-resolution aircraft images collected from the internet, along with machine-generated captions. The captions were generated using Gemini Flash 1.5 AI model and are stored in separate text files matching the image filenames.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All image captions are in English\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/aircraft-images.","url":"https://huggingface.co/datasets/nyuuzyou/aircraft-images","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"ambientcg","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for AmbientCG Textures and HDRIs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 14,202 high-quality texture images and HDRI environments from ambientcg.com. It includes a comprehensive collection of materials such as fabric, metal, wood, stone, concrete, nature elements, and HDRI skyboxes for 3D rendering and computer graphics applications. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compressionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ambientcg.","url":"https://huggingface.co/datasets/nyuuzyou/ambientcg","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"programmerhumor","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ProgrammerHumor.io Memes\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains programming-related memes and humor content collected from programmerhumor.io, along with associated metadata such as titles, categories, tags, and image captions.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): All meme content and descriptions are primarily in English\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Files\n\t\n\nThe dataset consists of:\n\nImage files (stored inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/programmerhumor.","url":"https://huggingface.co/datasets/nyuuzyou/programmerhumor","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v17","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mirabest-radio-astronomy-unofficial","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMiraBest Radio Astronomy Dataset (Unofficial)\n\t\n\nâš ï¸ IMPORTANT: This is an unofficial repository containing a processed version of the MiraBest dataset formatted for stable diffusion fine-tuning. This repository is not affiliated with the original authors.\nUnofficial processing of the MiraBest radio astronomy dataset with original classification labels and natural language captions for diffusion fine-tuning. Original dataset by Porter & Scaife (2023).\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal Dataset\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kwazzi-jack/mirabest-radio-astronomy-unofficial.","url":"https://huggingface.co/datasets/kwazzi-jack/mirabest-radio-astronomy-unofficial","creator_name":"Brian Welman","creator_url":"https://huggingface.co/kwazzi-jack","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"mirabest-radio-astronomy-unofficial","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMiraBest Radio Astronomy Dataset (Unofficial)\n\t\n\nâš ï¸ IMPORTANT: This is an unofficial repository containing a processed version of the MiraBest dataset formatted for stable diffusion fine-tuning. This repository is not affiliated with the original authors.\nUnofficial processing of the MiraBest radio astronomy dataset with original classification labels and natural language captions for diffusion fine-tuning. Original dataset by Porter & Scaife (2023).\n\n\t\n\t\t\n\t\n\t\n\t\tOriginal Dataset\n\t\n\nTheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/kwazzi-jack/mirabest-radio-astronomy-unofficial.","url":"https://huggingface.co/datasets/kwazzi-jack/mirabest-radio-astronomy-unofficial","creator_name":"Brian Welman","creator_url":"https://huggingface.co/kwazzi-jack","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-trainset","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tAuroraCap Trainset\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nWe use over 20 million high-quality image/video-text pairs to train AuroraCap in three stages. \nPretraining stage. We first align visual features with the word embedding space of LLMs. To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector.\nVision stage. Weâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-trainset.","url":"https://huggingface.co/datasets/wchai/AuroraCap-trainset","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"food-visual-instructions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAdapting Multimodal Large Language Models to Domains via Post-Training (EMNLP 2025)\n\t\n\nThis repos contains the food visual instructions for post-training MLLMs in our paper: On Domain-Specific Post-Training for Multimodal Large Language Models.\nThe main project page is: Adapt-MLLM-to-Domains\n\n\t\n\t\t\n\t\n\t\n\t\tData Information\n\t\n\nUsing our visual instruction synthesizer, we generate visual instruction tasks based on the image-caption pairs from extended Recipe1M+ dataset. These syntheticâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AdaptLLM/food-visual-instructions.","url":"https://huggingface.co/datasets/AdaptLLM/food-visual-instructions","creator_name":"AdaptLLM","creator_url":"https://huggingface.co/AdaptLLM","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v29","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v29.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v29","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Ideogram-V2_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Ideogram-V2 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 42k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Ideogram-V2 across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Ideogram-V2_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"tmp","keyword":"image-to-text","description":"\n\n ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation \n\n\nThis is the official dataset repository of ChartMimic. \n\n\t\n\t\t\n\t\t1. Data Overview\n\t\n\nChartMimic aims at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.\nChartMimic includes 1000 human-curatedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YANG-Cheng/tmp.","url":"https://huggingface.co/datasets/YANG-Cheng/tmp","creator_name":"Cheng Yang","creator_url":"https://huggingface.co/YANG-Cheng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v28","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"OpenDoc-Pdf-Preview","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenDoc-Pdf-Preview\n\t\n\nOpenDoc-Pdf-Preview is a compact visual preview dataset containing 6,000 high-resolution document images extracted from PDFs. This dataset is designed for Image-to-Text tasks such as document OCR pretraining, layout understanding, and multimodal document analysis.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nModality: Image-to-Text\nContent Type: PDF-based document previews\nNumber of Samples: 6,000\nLanguage: English\nFormat: Parquet\nSplit: train only\nSize: 606 MB\nLicense: Apacheâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/OpenDoc-Pdf-Preview.","url":"https://huggingface.co/datasets/prithivMLmods/OpenDoc-Pdf-Preview","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"VIDGEN-1K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a video-text-to-text dataset used in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\".\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tInstallation\n\t\n\n\nClone the github repository and navigate to LiFT folder\n\ngit clone https://github.com/CodeGoat24/LiFT.git\ncd LiFT\n\n\nInstall packages\n\nbashâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K.","url":"https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"SENTINEL","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ICCV2025 | SENTINEL:Mitigating Object Hallucinations via Sentence-Level Early Intervention \n\t\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tPaper Abstract\n\t\n\nMultimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/psp-dada/SENTINEL.","url":"https://huggingface.co/datasets/psp-dada/SENTINEL","creator_name":"Peng Shangpin","creator_url":"https://huggingface.co/psp-dada","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2507.12455"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"objaverse_processed_renders_and_captions","keyword":"image-to-text","description":"Contains rendered views and captions from Objaverse XL objects. the objects are from the alignment and TRELLIS500K (over 1 Millionen processed objects) dataset. We downloaded and rendered 4 views of each object. We added TRELLIS and CAP3D Captions where available. If there were no captions we generated new captions with the large version of Florence 2. This is the base dataset we used to generate MeshFleet which is described in MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DamianBoborzi/objaverse_processed_renders_and_captions.","url":"https://huggingface.co/datasets/DamianBoborzi/objaverse_processed_renders_and_captions","creator_name":"Damian Boborzi","creator_url":"https://huggingface.co/DamianBoborzi","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","1M - 10M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v198","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v198.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v198","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTrain","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tGuardReasoner-VLTrain\n\t\n\nGuardReasoner-VLTrain is the training data for R-SFT of GuardReasoner-VL, as described in the paper GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nCode: https://github.com/yueliu1999/GuardReasoner-VL/\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"yueliu1999/GuardReasoner-VLTrain\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite our paper.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain.","url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/terminusresearch/photo-aesthetics","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"danbooru2023","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\t[Mirror]Danbooru2023: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset\n\t\n\nDanbooru2023 is an extension of Danbooru2021, featuring over 6.8 million anime-style images, totaling more than 8.3 TB. \nEach image is accompanied by community-contributed tags that provide detailed descriptions of its content, including characters, \nartists, copyright information, concepts, and attire. \nThis makes it a crucial resource for stylized computer vision tasks and transfer learning.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/danbooru2023.","url":"https://huggingface.co/datasets/zenless-lab/danbooru2023","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-to-image","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"ACON","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for ACON Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nData from: Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?\n@inproceedings{chung2025are,\n  title={Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?},\n  author={Chung, Jiwan and Yoon, Janghan and Park, Junhyeong and Lee, Sangeyl and Yang, Joowon and Park, Sooyeon and Yu, Youngjae},\n  booktitle={Proceedings of the 63rd Annual Meeting of the Association forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jiwan-chung/ACON.","url":"https://huggingface.co/datasets/jiwan-chung/ACON","creator_name":"Jiwan Chung","creator_url":"https://huggingface.co/jiwan-chung","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v42","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v42.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/lodestones/photo-architecture","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MileBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMileBench\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. \nThis benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. \nWe establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMsâ€™ long-context adaptation capacity and their ability to completetasks in long-context scenarios\n \n\nToâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/MileBench.","url":"https://huggingface.co/datasets/FreedomIntelligence/MileBench","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","text-generation","image-to-text","video-classification"],"keywords_longer_than_N":true},
	{"name":"alchemist-tr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAlchemist-TR ðŸ‘¨â€ðŸ”¬\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAlchemist-TR is the Turkish-translated version of the original Alchemist dataset, comprising 3,350 high-quality image-text pairs curated for supervised fine-tuning (SFT) of pre-trained text-to-image (T2I) generative models. This version maintains the original's visual and aesthetic standards while localizing all prompts into Turkish, enabling multilingual and culturally-aware T2I training and evaluation.\nThis dataset facilitates theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/salihfurkaan/alchemist-tr.","url":"https://huggingface.co/datasets/salihfurkaan/alchemist-tr","creator_name":"Salih Furkan Erik","creator_url":"https://huggingface.co/salihfurkaan","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"KOSMOS-VNews","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for KOSMOS-VNews\n\t\n\n\nThis dataset comprises over 170,000 English vision-language samples. The image captions were generated using the KOSMOS-2 model .\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThe images are keyframes extracted from Vietnamese news broadcasts (specifically, \"60 Giay\" news from HTV) aired in 2023 and used in the AIChallenge 2023.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nThe dataset consists of keyframes extracted from approximately 600 news videos, with an average of 300 framesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Oztobuzz/KOSMOS-VNews.","url":"https://huggingface.co/datasets/Oztobuzz/KOSMOS-VNews","creator_name":"Tran Ngoc Oanh","creator_url":"https://huggingface.co/Oztobuzz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Vietnamese","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_innerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VLM-150M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for VLM-150M\n\t\n\n\n\nVLM-150M is a large-scale image-text dataset that has been recaptioned using an SFT-enhanced Qwen2VL model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\n\n\nRepository: [https://zxwei.site/hqclip/)\n\n\n\n\t\n\t\t\n\t\tUsage Guide\n\t\n\nSee https://github.com/w1oves/hqclip/blob/main/README.md#dataset-usage-guide.\n","url":"https://huggingface.co/datasets/zhixiangwei/VLM-150M","creator_name":"zhixiangwei","creator_url":"https://huggingface.co/zhixiangwei","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"textures3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nThis is the third iteration and official release of a dataset curated to power the Materializer model for Blender. The dataset contains a range of labeled texture images that were sourced from ambientCG under their Creative Commons CC0 1.0 Universal License. These textures are designed to help in the classification of various material maps, which are essential for creating realistic 3D materials in Blender.\n\n\t\n\t\t\n\t\tFuture Plans\n\t\n\nThe dataset is still evolving, and Iâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeathDaDev/textures3.","url":"https://huggingface.co/datasets/DeathDaDev/textures3","creator_name":"Death the Developer","creator_url":"https://huggingface.co/DeathDaDev","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"CompCap-gpt4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCompCap-GPT4: A GPT-4 Captioned Version of CompCap-118K\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nPaper: CompCap: Improving Multimodal Large Language Models with Composite Captions\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\nDirect Download:The repository includes CI_type.zip and CI_type.json. The JSON file follows the Llava format:\n{\n  \"id\": ID,\n  \"image\": IMAGE_PATH,\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": QUESTION},\n    {\"from\": \"gpt\", \"value\": ANSWER}\n  ]\n}\n\n\nUsingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/xchen16/CompCap-gpt4.","url":"https://huggingface.co/datasets/xchen16/CompCap-gpt4","creator_name":"Xiaohui Chen","creator_url":"https://huggingface.co/xchen16","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","summarization","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"nocaps-pt-br","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸŽ‰ nocaps Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tðŸ’¾ Dataset Summary\n\t\n\nnocaps Portuguese Translation, a multimodal dataset for Portuguese image captioning benchmark, each image accompanied by ten descriptive captions\nthat have been generated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tðŸ§‘â€ðŸ’» Hot to Get Started with the Dataset\n\t\n\nfromâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/nocaps-pt-br.","url":"https://huggingface.co/datasets/laicsiifes/nocaps-pt-br","creator_name":"LaboratÃ³rio de InteligÃªncia Computacional e Sistemas de informaÃ§Ã£o","creator_url":"https://huggingface.co/laicsiifes","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"MultiFinBen-SpanishOCR","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tDataset Card for SpanishOCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe SpanishOCR dataset contains images derived from regulatory documents from Peru government in pdf format. This dataset is used for benchmarkingg and evaluating Large Language Models ability on converting unstructured dcuments, such as pdfs and images, into machine readable format, particularly in finance domain, where the conversion task is more complex and valuable.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask: Image-to-Textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR.","url":"https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Spanish","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v96","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v96.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v96","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"taln-archives_fr_prompt_data_to_text","keyword":"data-to-text","description":"\n\t\n\t\t\n\t\ttaln-archives_fr_prompt_data_to_text\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\ntaln-archives_fr_prompt_data_to_text is a subset of the Dataset of French Prompts (DFP).It contains 35,370 rows that can be used for a data-to-text task.The original data (without prompts) comes from the dataset taln-archives.A list of prompts (see below) was then applied in order to build the input and target columns and thus obtain the same format as the xP3 dataset by Muennighoff et al.\n\n\t\n\t\t\n\t\n\t\n\t\tPrompts usedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/CATIE-AQ/taln-archives_fr_prompt_data_to_text.","url":"https://huggingface.co/datasets/CATIE-AQ/taln-archives_fr_prompt_data_to_text","creator_name":"CATIE","creator_url":"https://huggingface.co/CATIE-AQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","found","found","monolingual","taln-ls2n/taln-archives"],"keywords_longer_than_N":true},
	{"name":"GroundCap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGroundCap Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundCap is a novel grounded image captioning dataset derived from MovieNet, containing 52,350 movie frames with detailed grounded captions. The dataset uniquely features an ID-based system that maintains object identity throughout captions, enables tracking of object interactions, and grounds not only objects but also actions and locations in the scene.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach sample in the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/daniel3303/GroundCap.","url":"https://huggingface.co/datasets/daniel3303/GroundCap","creator_name":"Daniel Oliveira","creator_url":"https://huggingface.co/daniel3303","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GroundCap","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGroundCap Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGroundCap is a novel grounded image captioning dataset derived from MovieNet, containing 52,350 movie frames with detailed grounded captions. The dataset uniquely features an ID-based system that maintains object identity throughout captions, enables tracking of object interactions, and grounds not only objects but also actions and locations in the scene.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach sample in the datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/daniel3303/GroundCap.","url":"https://huggingface.co/datasets/daniel3303/GroundCap","creator_name":"Daniel Oliveira","creator_url":"https://huggingface.co/daniel3303","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v95","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v95.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v95","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ðŸ¤—The Original Images, are Released Completing Agreement\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared toâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HiPhO","keyword":"image-text-to-text","description":"\n\nðŸ¥‡ HiPhO: High School Physics Olympiad Benchmark\n\n[ðŸ† Leaderboard]\n[ðŸ“Š Dataset]\n[âœ¨ GitHub]\n[ðŸ“„ Paper]\n\n\n\n\n\nðŸ† New (Sep. 16): We launched \"PhyArena\", a physics reasoning leaderboard incorporating the HiPhO benchmark.\n\n\t\n\t\n\t\n\t\tðŸŒ Introduction\n\t\n\nHiPhO (High School Physics Olympiad Benchmark) is the first benchmark specifically designed to evaluate the physical reasoning abilities of (M)LLMs on real-world Physics Olympiads from 2024â€“2025.\n\n  \n\n\n\n\t\n\t\t\n\t\tâœ¨ Key Features\n\t\n\n\nUp-to-date Coverage:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SciYu/HiPhO.","url":"https://huggingface.co/datasets/SciYu/HiPhO","creator_name":"Fangchen Yu","creator_url":"https://huggingface.co/SciYu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v139","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v139.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v139","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Video-Holmes","keyword":"video-text-to-text","description":"\n\n\n\t\n\t\t\n\t\tVideo-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?\n\t\n\nJunhao Cheng1,2, \nYuying Ge1,âœ‰, \nTeng Wang1,âœ‰, \nYixiao Ge1, \nJing Liao2, \nYing Shan1\n\n1ARC Lab, Tencent PCG, \n2City University of Hong Kong\n\n\n    \n\n\n\n\n\n\n    \n\n\n\n\t\n\t\t\n\t\tðŸ”Ž Introduction\n\t\n\nVideo-Holmes is a benchmark designed to evaluate the complex video reasoning capabilities of MLLMs. \nVideo-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films (ranging from 1 to 5 minutes)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/Video-Holmes.","url":"https://huggingface.co/datasets/TencentARC/Video-Holmes","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K<n<10K","Video"],"keywords_longer_than_N":true},
	{"name":"ViRL39K","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\t1. Overview of ViRL39K\n\t\n\nViRL39K (pronounced as \"viral\") provides a curated collection of 38,870 verifiable QAs for Vision-Language RL training. \nIt is built on top of newly collected problems and existing datasets (\nLlava-OneVision, \nR1-OneVision,\nMM-Eureka,\nMM-Math,\nM3CoT,\nDeepScaleR,\nMV-Math)\nthrough cleaning, reformatting, rephrasing and verification.ViRL39K lays the foundation for SoTA Vision-Language Reasoning Model VL-Rethinker. It has the following merits:\n\nhigh-quality andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ViRL39K.","url":"https://huggingface.co/datasets/TIGER-Lab/ViRL39K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v10","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"medical-imaging-combined","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCombined Medical Imaging Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nCombined medical imaging dataset with 6793 samples in Alpaca instruction format.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 6793\nTraining Samples: 5434\nValidation Samples: 1359\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 2691 samples\nCT: 2257 samples\nUnknown: 1329 samples\nMRI: 369 samples\nUltrasound: 147 samples\n\n\n\t\n\t\t\n\t\tSource Distribution\n\t\n\n\nROCO: 5000 samples\nVQA-RAD: 1793 samples\n\n\n\t\n\t\t\n\t\tSources\n\t\n\n\nROCOâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/robailleo/medical-imaging-combined.","url":"https://huggingface.co/datasets/robailleo/medical-imaging-combined","creator_name":"Robail Yasrab ","creator_url":"https://huggingface.co/robailleo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ext","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Extention\n\t\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯CommonCatalog CC-BYã‚’æ‹¡å¼µã—ã¦ã€è¿½åŠ ã®æƒ…å ±ã‚’å…¥ã‚ŒãŸã‚‚ã®ã§ã™ã€‚\nä»¥ä¸‹ã®æƒ…å ±ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nPhi-3 Visionã§Dense Captioningã—ãŸè‹±èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³\nè‹±èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’Phi-3 Mediumã§æ—¥æœ¬èªžåŒ–ã—ãŸæ—¥æœ¬èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³\n\nä¸»ã‚­ãƒ¼ã¯photoidã§ã™ã®ã§ã€CommonCatalog CC-BYã¨çµåˆã™ã‚‹ãªã‚Šã—ã¦ä½¿ã£ã¦ãã ã•ã„ã€‚\nstreaming=Trueã§èª­ã¿è¾¼ã‚€ã¨åŒã˜é †ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã®ã§ãã‚Œã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒä¸€ç•ªæ¥½ã§ã™ã€‚\n\n\t\n\t\t\n\t\tLicense\n\t\n\nç”»åƒãŒCC BYãªãŸã‚ã€ã‚ã‹ã‚Šã‚„ã™ãCC BYã«ã—ã¦ã„ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€å•†ç”¨åˆ©ç”¨å¯èƒ½ã§ã™ã€‚\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3-ja.csv\")\n\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext.","url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v45","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v45.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ECD-10k-Images","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tEffective Training Data Synthesis for Improving MLLM Chart Understanding\n\t\n\nThe Effective Chart Dataset (ECD-10k-Images) is a high-quality, multimodal dataset designed to enhance chart understanding capabilities in Multimodal Large Language Models (MLLMs). This dataset includes over 10,000 synthetic chart images and 321,544 QA pairs (both descriptive and reasoning) spanning 29 chart types, 25 themes, and 252 unique chart combinations. By addressing data realism, complexity, andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ChartFoundation/ECD-10k-Images.","url":"https://huggingface.co/datasets/ChartFoundation/ECD-10k-Images","creator_name":"Chart Foundation Research","creator_url":"https://huggingface.co/ChartFoundation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","GPT-4o-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MMR1-RL","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nThis repository introduces the MMR1 project, focusing on enhancing large multimodal reasoning models. While rapid progress has been made, advancements are constrained by two major limitations:\n\nThe absence of open, large-scale, high-quality long chain-of-thought (CoT) data.\nThe instability of reinforcement learning (RL) algorithms in post-training, where standard Groupâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MMR1/MMR1-RL.","url":"https://huggingface.co/datasets/MMR1/MMR1-RL","creator_name":"MMR1","creator_url":"https://huggingface.co/MMR1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v39","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v39.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LaTeX-OCR-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tLaTeX-OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset was created to train LaTeX-OCR, a model for recognizing LaTeX code from images of mathematical formulas. Each sample consists of a synthetically rendered formula image and its corresponding LaTeX formula. The images were generated from scratch using xelatex with multiple fonts, offering more typographic variety than many other datasets that use a single font (typically Computer Modern).\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nFormulas wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/lukbl/LaTeX-OCR-dataset.","url":"https://huggingface.co/datasets/lukbl/LaTeX-OCR-dataset","creator_name":"Lukas Blecher","creator_url":"https://huggingface.co/lukbl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"image-text-to-text","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"VideoChat-Flash-Training-Data","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tðŸ¦œ VideoChat-Flash-Training-Data\n\t\n\nThis repos contains all annotaions and most videos for training VideoChat-Flash.\n\n\t\n\t\t\n\t\tðŸ“• How to use the LongVid data?\n\t\n\nFor video_dir like longvid_subset/coin_grounding_10k_zip, you need to concat this dir to a zip file as follows:\ncat ego4dhcap_eventunderstanding_2k_zip/* > ego4dhcap_eventunderstanding_2k.zip\n\n\n\t\n\t\t\n\t\tâœï¸ Citation\n\t\n\n\n@article{li2024videochatflash,\n  title={VideoChat-Flash: Hierarchical Compression for Long-Context Videoâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data.","url":"https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Datasets"],"keywords_longer_than_N":true},
	{"name":"CoF-SFT-Data-5.4k","keyword":"image-text-to-text","description":"Overview.\nThis dataset is used for supervised fine-tuning (SFT) in training Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL,\nDetails.\nThis dataset includes 5.4k reasoning samples: 2.4k involve zoom-in behavior, and 3k are text-only reasoning cases. All samples were generated using our Visual Agent and span a variety of image resolutions.\nTraining Code: The SFT code can be found at https://github.com/xtong-zhang/Chain-of-Focus\nProject page:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k.","url":"https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k","creator_name":"Xintong Zhang","creator_url":"https://huggingface.co/xintongzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","Image","arxiv:2505.15436"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations haveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"shamela-waqfeya-library","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tShamela Waqfeya Library\n\t\n\n\n\t\n\t\t\n\t\tðŸ“– Overview\n\t\n\nShamela Waqfeya is one of the primary online resources for Islamic books, similar to Shamela. It hosts more than 4,500 PDF books across over 40 categories.\nIn this dataset, we processed the original PDF files using Google Document AI APIs and extracted their contents into two additional formats: TXT and DOCX.\n\n\t\n\t\t\n\t\tðŸ“Š Dataset Contents\n\t\n\nThe dataset includes 12,877 PDF files (spanning 5,138,027 pages) representing 4,661 Islamic books.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library.","url":"https://huggingface.co/datasets/ieasybooks-org/shamela-waqfeya-library","creator_name":"Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙÙŠØ³Ù‘Ø±Ø©","creator_url":"https://huggingface.co/ieasybooks-org","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","mit","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Anime-Labeled","keyword":"image-to-text","description":"\n[!NOTE]\nIn addition to the MIT-license's restrictions placed upon this processed version of the dataset, it's prohibited to use this dataset to train a model which is intended to generate sexual and or misleading content.\n\n\n[!NOTE]\nFinally, please note, that even though this dataset is labeled \"license: mit\" (in addition to the above restrictions), this license ONLY APPLIES TO data.json, main.py, and this README.md. I do not own copyright to images posted in this repo (even if processed), andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Hamzah-Asadullah/Anime-Labeled.","url":"https://huggingface.co/datasets/Hamzah-Asadullah/Anime-Labeled","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ui-vision","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tUI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction\n\t\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nAutonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challengesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/ui-vision.","url":"https://huggingface.co/datasets/ServiceNow/ui-vision","creator_name":"ServiceNow","creator_url":"https://huggingface.co/ServiceNow","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v177","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v177.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v177","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"jp-univ-essay","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOkayama University Japanese Essay Data\n\t\n\nThis repository contains Japanese essays collected from Okayama University. The dataset is intended for research and analysis purposes, such as natural language processing, text mining, or educational studies.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\nEssay texts in Japanese\nScores of four traints\nData files in jsonl and png image files for essays\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset is provided in JSON Lines (.jsonl) format.Each line corresponds to one student's essayâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/cl-okayama/jp-univ-essay.","url":"https://huggingface.co/datasets/cl-okayama/jp-univ-essay","creator_name":"CL Research Group in Okayama, Japan","creator_url":"https://huggingface.co/cl-okayama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","image-to-text","Japanese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tVideoGameBunny Instruction Following Dataset\n\t\n\n[Website] \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe present a comprehensive dataset of 185,259 high-resolution images from 413 video games, sourced from YouTube videos. This dataset addresses the lack of game-specific instruction-following data and aims to improve the ability of open-source models to understand and respond to video game content.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Composition\n\t\n\nOur dataset includes various types of instructions generated forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/VideoGameBunny/Dataset.","url":"https://huggingface.co/datasets/VideoGameBunny/Dataset","creator_name":"VideoGameBunny","creator_url":"https://huggingface.co/VideoGameBunny","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Flame-Additive-React","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Additive-React: An Iterative Data Synthesis Dataset for Multi-modal React Code Generation\n\t\n\nFlame-Additive-React is a dataset synthesized using the Additive Development Synthesis method, focusing on real-world React development patterns. This dataset ensures that training data remains grounded in realistic, incrementally enhanced code components.\nInstead of generating synthetic data from scratch, this approach builds upon human-authored React components, progressively increasingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Additive-React.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Additive-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MultiFinBen-EnglishOCR","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tDataset Card for EnglishOCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe EnglishOCR dataset contains images derived from regulatory documents from SEC EDGAR company filings. This dataset is used for benchmarkingg and evaluating Large Language Models ability on converting unstructured dcuments, such as pdfs and images, into machine readable format, particularly in finance domain, where the conversion task is more complex and valuable.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nTask: Image-to-Textâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR.","url":"https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR","creator_name":"The Fin AI","creator_url":"https://huggingface.co/TheFinAI","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Spanish","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v45","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v45.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SVHN_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSVHN\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttp://ufldl.stanford.edu/housenumbers/\nName: 'SVHN'\nData:\n  Title: The Street View House Numbers (SVHN) Dataset \n  URL: http://ufldl.stanford.edu/housenumbers/\n\n","url":"https://huggingface.co/datasets/MiXaiLL76/SVHN_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"TinyLLaVA-Video-v1-training-data","keyword":"video-text-to-text","description":"TinyLLaVA-Video\n\nThis dataset combines data from multiple sources for pre-training and fine-tuning.\nPretrain Data: Four subsets of LLaVA-Video-178K (0_30_s_academic_v0_1, 30_60_s_academic_v0_1, 0_30_s_youtube_v0_1, 30_60_s_youtube_v0_1), supplemented with filtered Video-LLaVA data (https://huggingface.co/datasets/LanguageBind/Video-LLaVA) and data from Valley (https://github.com/RupertLuo/Valley). The video data can be downloaded from the linked datasets, and cleaned annotations are providedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data.","url":"https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data","creator_name":"Zhang Xingjian","creator_url":"https://huggingface.co/Zhang199","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","apache-2.0","arxiv:2501.15513","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-compress-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"isl_synthetic_ocr","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tIcelandic Synthetic OCR Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains synthetically generated images of Icelandic text for Optical Character Recognition (OCR) tasks. It was created to address the lack of publicly available, large-scale OCR datasets for the Icelandic language. The images are generated from a corpus of Icelandic text, rendered with various settings to create a diverse set of training examples.\nThe key feature of this dataset is its focus on Icelandicâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sigurdur/isl_synthetic_ocr.","url":"https://huggingface.co/datasets/Sigurdur/isl_synthetic_ocr","creator_name":"Sigurdur Haukur Birgisson","creator_url":"https://huggingface.co/Sigurdur","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Icelandic","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNOTE: A newer version of this dataset is available Imagen3_Flux1.1_Flux1_SD3_MJ_Dalle_Human_Preference_Dataset\n\t\n\n\n\t\n\t\t\n\t\tRapidata Image Generation Preference Dataset\n\t\n\n\n\n\n\nThis Dataset is a 1/3 of a 2M+ human annotation dataset that was split into three modalities: Preference, Coherence, Text-to-Image Alignment. \n\nLink to the Coherence dataset: https://huggingface.co/datasets/Rapidata/Flux_SD3_MJ_Dalle_Human_Coherence_Dataset\nLink to the Text-2-Image Alignment dataset:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3.","url":"https://huggingface.co/datasets/Rapidata/700k_Human_Preference_Dataset_FLUX_SD3_MJ_DALLE3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-to-image","image-classification","reinforcement-learning"],"keywords_longer_than_N":true},
	{"name":"LLaVA_Train","keyword":"image-to-text","description":"\nFor llava-scaled-0.5b\n\nTraining scripts see henrywch/Vision_LLM. Images are combined with text instructions in .parquets of each Stage's directory. \n","url":"https://huggingface.co/datasets/henrywch2huggingface/LLaVA_Train","creator_name":"Chenghao Wang","creator_url":"https://huggingface.co/henrywch2huggingface","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-image-v62","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v62.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v62","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Imagen4_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over 195k human responses from over 70k individual annotators, collected in just ~1 Day using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 (imagen-4.0-ultra-generate-exp-05-20) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the futureâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen4_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"uk_retail_store_synthetic_dataset","keyword":"table-to-text","description":"\n\t\n\t\t\n\t\tSynthetic Data Generation Demo â€” UK Retail Dataset\n\t\n\nWelcome to this synthetic data generation demo repository by Syncora.ai. This project showcases how to generate synthetic data using real-world tabular structures, demonstrated on a UK retail dataset with columns such as:\n\nCountry  \nCustomerID  \nUnitPrice  \nInvoiceDate  \nQuantity  \nStockCode\n\nThis dataset is designed for dataset for LLM training and AI development, enabling developers to work with privacy-safe, high-qualityâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/syncora/uk_retail_store_synthetic_dataset.","url":"https://huggingface.co/datasets/syncora/uk_retail_store_synthetic_dataset","creator_name":"Syncora.ai - Agentic Synthetic Data Platform","creator_url":"https://huggingface.co/syncora","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["tabular-classification","tabular-regression","table-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Perception-Bench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\nHomepage: https://kaistai.github.io/prometheus-vision/ \nRepository: https://github.com/kaistAI/prometheus-vision \nPaper: https://arxiv.org/abs/2401.06591 \nPoint of Contact: seongyun@kaist.ac.kr\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset summary\n\t\n\nPerception-Bench is a benchmark for evaluating the long-form response of a VLM (Vision Language Model) across various domains of images, and it is a held-out test\nset of the Perception-Collection\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\tDatasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prometheus-eval/Perception-Bench.","url":"https://huggingface.co/datasets/prometheus-eval/Perception-Bench","creator_name":"prometheus-eval","creator_url":"https://huggingface.co/prometheus-eval","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","text2text-generation","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v155","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v155.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v155","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"leonardo","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Leonardo\n\t\n\n~958M image generations.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 958 million images generated using Leonardo AI. Entries include generation details such as prompts and model used, anonymized user information, creation date, and URL to the image.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nid: Unique identifier for the image\nuser_id: Identifer for the userâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/leonardo.","url":"https://huggingface.co/datasets/bigdata-pw/leonardo","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","odc-by","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"TheSimpsons","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for TheSimpsons\n\t\n\nFrames from The Simpsons.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nFrames from each episode of The Simpsons. Entries include image and Florence-2-large captions; caption, detailed_caption and more_detailed_caption.\nFrames were extracted using ffmepg with \"black bar removal\" applied with ffmpeg's cropdetect.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation Informationâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/TheSimpsons.","url":"https://huggingface.co/datasets/bigdata-pw/TheSimpsons","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","image-to-text","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","url":"https://huggingface.co/datasets/lodestone-horizon/photo-typography","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VisNumBench","keyword":"image-text-to-text","description":"This dataset is designed for research in Deep Learning for Geometry Problem Solving (DL4GPS) and accompanies the survey paper A Survey of Deep Learning for Geometry Problem Solving. It aims to provide a structured resource for evaluating and training AI models, particularly multimodal large language models (MLLMs), on mathematical reasoning tasks involving geometric contexts.\nThe dataset provides a collection of geometry problems, each consisting of a textual question and a correspondingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/GML-FMGroup/VisNumBench.","url":"https://huggingface.co/datasets/GML-FMGroup/VisNumBench","creator_name":"Foundation Model Group at Guangming Laboratory","creator_url":"https://huggingface.co/GML-FMGroup","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v143","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v143.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v143","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v38","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v38.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"unprocessed_hcs","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThis is not data that I created. It originally came from the Paper Digitization of Handwritten Chess Scoresheets with a BiLSTM Network\nYou can also find the dataset Chesscorner/HCS_Dataset-csv, Chesscorner/HCS_pictures, here and here\n\n\t\n\t\t\n\t\n\t\n\t\tDatasets\n\t\n\nThere are 2 versions of this dataset\n\nunprocessed_hcs Dataset where you are right now\nprocessed_hcs Dataset where the extracted move boxes are provided which can be found here\n\n\n\t\n\t\n\t\n\t\tDescription\n\t\n\nThe Handwrittenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BenjaminKost/unprocessed_hcs.","url":"https://huggingface.co/datasets/BenjaminKost/unprocessed_hcs","creator_name":"Benjamin Kostka","creator_url":"https://huggingface.co/BenjaminKost","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v54","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v54.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tartandrive","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTartanDrive Navigation Dataset\n\t\n\nThis dataset contains navigation trajectory data for robotic navigation tasks. Each example includes an RGB image, a language goal describing the desired navigation target, and 2D/3D trajectories showing the path to the goal.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: RGB image from the robot's viewpoint\nlang_goal: Natural language instruction describing the navigation goal\ntrajectory_2d: 2D trajectory coordinates (pixel space)\ntrajectory_3d: 3D trajectoryâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mateoguaman/tartandrive.","url":"https://huggingface.co/datasets/mateoguaman/tartandrive","creator_name":"Mateo Guaman Castro","creator_url":"https://huggingface.co/mateoguaman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","robotics","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MSTS","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThe MSTS dataset contains content that may be offensive or upsetting in nature. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matter. \nPlease only engage with the data in accordance with your own personal risk tolerance. The data are intended for research purposes, especially research that can make models less harmful.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/felfri/MSTS.","url":"https://huggingface.co/datasets/felfri/MSTS","creator_name":"Felix Friedrich","creator_url":"https://huggingface.co/felfri","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","Arabic","French","German"],"keywords_longer_than_N":true},
	{"name":"m-popp","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tM-POPP datasets\n\t\n\nThis repository contains 2 datasets created within the EXO-POPP project (Optical EXtraction of handwritten named entities for marriage records of the POPulation of Paris) for the task of text recognition and information extraction.\nThese datasets have been published in End-to-end information extraction in handwritten documents: Understanding Paris marriage records from 1880 to 1940 [1] at ICDAR 2024 and are also available on Zenodo.\nThis version contains the labelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thomas-C/m-popp.","url":"https://huggingface.co/datasets/thomas-C/m-popp","creator_name":"Thomas Constum","creator_url":"https://huggingface.co/thomas-C","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","question-answering","French","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v142","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v142.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v142","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"umamusume","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSafebooru Anime Dataset\n\t\n\nThis dataset contains 389 curated anime-style images from Safebooru with descriptive captions, prepared for fine-tuning text-to-image models like Stable Diffusion and FLUX.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach example in the dataset contains:\n\nimage: The anime artwork image\ntext: Descriptive caption/tags for the image\nfile_name: The filename of the image\noriginal_post_id: Original Safebooru post ID\noriginal_filename: Original filename from Safebooruâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/littlestronomer/umamusume.","url":"https://huggingface.co/datasets/littlestronomer/umamusume","creator_name":"littlestronomer","creator_url":"https://huggingface.co/littlestronomer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on globalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dave","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for DAVE ðŸ‘¨ðŸ¿â€ðŸ”¬: Diagnostic benchmark for Audio Visual Evaluation\n\t\n\nDAVE is a diagnostic benchmark for evaluating audio-visual models, ensuring both modalities are required and providing fine-grained error analysis to reveal specific failures.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDAVE (Diagnostic Audio-Visual Evaluation) is a benchmark dataset designed to systematically evaluate audio-visual models by addressing key limitations in existing datasets.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/gorjanradevski/dave.","url":"https://huggingface.co/datasets/gorjanradevski/dave","creator_name":"Gorjan Radevski","creator_url":"https://huggingface.co/gorjanradevski","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on globalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"PCogAlignBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAligning VLM Assistants with Personalized Situated Cognition (ACL 2025 main)\n\t\n\n\n\n\nThis repository contains the constructed benchmark in our ACL 2025 main paper \"Aligning VLM Assistants with Personalized Situated Cognition\". \n\nâš ï¸ This project is for academic research only and not intended for commercial use.\n\n\n\t\n\t\n\t\n\t\tAbstract\n\t\n\nVision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humansâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/YongqiLi/PCogAlignBench.","url":"https://huggingface.co/datasets/YongqiLi/PCogAlignBench","creator_name":"Yongqi Li","creator_url":"https://huggingface.co/YongqiLi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","Image","arxiv:2506.00930","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"TextVQA_GT_bbox","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTextVQA validation set with grounding truth bounding box\n\t\n\nThe dataset used in the paper MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs for studying MLLMs' attention patterns.\nThe dataset is sourced from TextVQA and annotated manually with ground-truth bounding boxes. \nWe consider questions with a single area of interest in the image so that 4370 out of 5000 samples are kept.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find our paper and code usefulâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox.","url":"https://huggingface.co/datasets/jrzhang/TextVQA_GT_bbox","creator_name":"jiarui zhang","creator_url":"https://huggingface.co/jrzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v185","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v185.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v185","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"myanmar-ocr-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMyanmar OCR Dataset\n\t\n\nA synthetic dataset for training and fine-tuning Optical Character Recognition (OCR) models specifically for the Myanmar language.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains synthetically generated OCR images created specifically for Myanmar text recognition tasks. The images were generated using myanmar-ocr-data-generator, a fork of TextRecognitionDataGenerator with fixes for proper Myanmar character splitting.\n\n\t\n\t\t\n\t\n\t\n\t\tDirect Download\n\t\n\nAvailableâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/chuuhtetnaing/myanmar-ocr-dataset.","url":"https://huggingface.co/datasets/chuuhtetnaing/myanmar-ocr-dataset","creator_name":"Chuu Htet Naing","creator_url":"https://huggingface.co/chuuhtetnaing","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Burmese","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbiggerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"anime-with-caption-cc0","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAnime with caption CC-0 dataset\n\t\n\nã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¤ãƒ©ã‚¹ãƒˆã«å¯¾ã™ã‚‹æ—¥æœ¬èªžã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’\nå€«ç†çš„ã«å­¦ç¿’ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚\nã“ã“ã«æŽ²è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ©ã‚¹ãƒˆã¯è‡ªå¾‹çš„ã«AIãŒä½œæˆã—ãŸã‚‚ã®ã§ã‚ã‚Šã€\nè‘—ä½œæ¨©ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãŸã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚‚è‡ªå¾‹çš„ã«ã¤ã‘ã‚‰ã‚ŒãŸã‚‚ã®ãªã®ã§ã€\nè‘—ä½œæ¨©ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã—ãŸãŒã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‘—ä½œæ¨©ã‚’ç§ã¯æ”¾æ£„ã—ã¾ã™ã€‚\nå‹æ‰‹ã«ä½¿ã£ã¦ãã ã•ã„ã€‚\n\n\t\n\t\t\n\t\tãƒ©ã‚¤ã‚»ãƒ³ã‚¹\n\t\n\nãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³\n\n\t\n\t\t\n\t\tãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆ\n\t\n\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã®åˆ—ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n\nimage: Emi 2ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã—ãŸç”»åƒ\nprompt: è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ(ãŸã ã—ã€ç”»åƒã¨ã‚ã¾ã‚Šä¸€è‡´ã—ã¦ã„ãªã„ãŸã‚ã€ã‚ã¦ã«ãªã‚‰ãªã„)\nphi3_caption: Phi-3 Visionã§Dense captioningã—ãŸçµæžœ\nphi3_caption_ja: phi3_captionã‚’Phi-3 Mediumã§æ—¥æœ¬èªžè¨³ã—ãŸçµæžœ\n\n\n\t\n\t\t\n\t\tã‚¤ãƒ©ã‚¹ãƒˆã®ä½œã‚Šæ–¹â€¦ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0.","url":"https://huggingface.co/datasets/alfredplpl/anime-with-caption-cc0","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Japanese","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SightationPreference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSighationPreference\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nðŸ“„ arXiv\nðŸ¤— Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationPreference.","url":"https://huggingface.co/datasets/Sightation/SightationPreference","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v192","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v192.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v192","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v172","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v172.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v172","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"qvhighlights-25frames","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tQVHighlights\n\t\n\n\nSets: train and val. Test set will be available soon.\nNumber of frames per video: 25 frames.\nFrame variations: 3\nAnswer format: 25 frame segmentation mask (e.g. 1010100101100101011110011)\nImages are in videos.tar.gz (no inside folders).\nEach image is noted by {video name}_frame{frame:03d}_var{var:d}.jpg where var2 is the middle one. The variations are separated 0.125 seconds or around 7 frames for 30fps videos (most are 30fps).\nThe subset name p{n} is the n^th promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/jwnt4/qvhighlights-25frames.","url":"https://huggingface.co/datasets/jwnt4/qvhighlights-25frames","creator_name":"bagas jwnt","creator_url":"https://huggingface.co/jwnt4","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"DataDepictQA","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataDepictQA\n\t\n\nDatasets of the papers in DepictQA project:\n\nDepictQA-Wild (DepictQA-v2): paper / project page / code.\nZhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, Tianfan Xue, \"Descriptive Image Quality Assessment in the Wild,\" arXiv preprint arXiv:2405.18842, 2024.\n\nDepictQA-v1: paper / project page / code.\nZhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, Chao Dong, \"Depicting beyond scores: Advancing image quality assessment through multi-modalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zhiyuanyou/DataDepictQA.","url":"https://huggingface.co/datasets/zhiyuanyou/DataDepictQA","creator_name":"zhiyuanyou","creator_url":"https://huggingface.co/zhiyuanyou","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K<n<1M","arxiv:2405.18842"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHereâ€™s the PVIT-3M statistics:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v19","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding colorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"t2i_safety_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tT2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation\n\t\n\nThis dataset, T2ISafety, is a comprehensive safety benchmark designed to evaluate Text-to-Image (T2I) models across three key domains: toxicity, fairness, and bias. It provides a detailed hierarchy of 12 tasks and 44 categories, built from meticulously collected 70K prompts. Based on this taxonomy and prompt set, T2ISafety includes 68K manually annotated images, serving as a robust resource forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset.","url":"https://huggingface.co/datasets/OpenSafetyLab/t2i_safety_dataset","creator_name":"OpenSafetyLab","creator_url":"https://huggingface.co/OpenSafetyLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"image-to-text","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response. Argh I forgot to enable this. Using RLE compression.\nimageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v137","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v137.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v137","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v23","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v23.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-130k","keyword":"image-text-to-text","description":"This is the SophiaVL-7B-130k dataset of SophiaVL-R1 (https://arxiv.org/abs/2505.17018). The textual data is stored in JSON files (located in ./json/), and the corresponding images are contained in ZIP archives.\nCode: https://github.com/kxfan2002/SophiaVL-R1\nData is in the following format:\n    {\n        \"problem_id\": 1, # id in current class\n        \"problem\": \"Subtract 0 cyan cubes. How many objects are left?\", # textual question\n        \"data_type\": \"image\", # text-only data(\"text\") orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k.","url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"MindCube","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'â€¦ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v26","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v26.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v106","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v106.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v106","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"flickr8k","keyword":"image-to-text","description":"tsystems/flickr8k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/tsystems/flickr8k","creator_name":"T-Systems International","creator_url":"https://huggingface.co/tsystems","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Perception-R1-Dataset","keyword":"image-text-to-text","description":"Paper: arxiv.org/abs/2506.07218\nPlease refer to GitHub repo for detailed usage: https://github.com/tongxiao2002/Perception-R1\n","url":"https://huggingface.co/datasets/tongxiao2002/Perception-R1-Dataset","creator_name":"tongxiao","creator_url":"https://huggingface.co/tongxiao2002","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"simple-shapes-svg","keyword":"image-to-text","description":"The goal of this dataset is to measure and improve the ability of VLMs to see accurately in spatial dimensions.\nI've tried to ensure that all of the examples are not too hard\n\nhave sufficient contrast between foreground and background\nshapes are not clipped or ambiguous\nsolid background\ncanvas is square 512x512\n\nInitially, I've kept the \"canvas\" that they're working with 512x512 points, but you can learn more by experimenting with the dimensions as well.\n","url":"https://huggingface.co/datasets/darknoon/simple-shapes-svg","creator_name":"Andrew Pouliot","creator_url":"https://huggingface.co/darknoon","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-timeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"SARD","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSARD: Synthetic Arabic Recognition Dataset\n\t\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSARD (Synthetic Arabic Recognition Dataset) is a large-scale, synthetically generated dataset designed for training and evaluating Optical Character Recognition (OCR) models for Arabic text. This dataset addresses the critical need for comprehensive Arabic text recognition resources by providing controlled, diverse, and scalable training data that simulates real-world book layouts.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nMassiveâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/riotu-lab/SARD.","url":"https://huggingface.co/datasets/riotu-lab/SARD","creator_name":"Robotics and Interne-of-Things","creator_url":"https://huggingface.co/riotu-lab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","apache-2.0","100K<n<1M","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"TAMA_Instruct","keyword":"table-to-text","description":"\n  \n  Dataset Card for TAMA Instruct\n\n\n\n\nThe training and testing data for TAMA models.\nPlease refer to our paper for additional information.\n","url":"https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct","creator_name":"LIT @ UMich","creator_url":"https://huggingface.co/MichiganNLP","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","table-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-timeâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Mulberry-SFT","keyword":"image-to-text","description":"Please check our GitHub for more details.: https://github.com/HJYao00/Mulberry\n\n\t\n\t\t\n\t\tTraining\n\t\n\nWe use LLaMA-Factory to fine-tune the Mulberry models. We provide the training instructions and configs here.\nFirst, install LLaMA-Factory according to the official_instruction.\nThen, refer here and update the following customized dataset into dataset_info.json in LLaMA-Factory.\n\"mulberry\": {\n    \"file_name\": \"./mulberry_sft.json\",\n    \"formatting\": \"sharegpt\",\n    \"columns\": {\n      \"messages\":â€¦ See the full description on the dataset page: https://huggingface.co/datasets/HuanjinYao/Mulberry-SFT.","url":"https://huggingface.co/datasets/HuanjinYao/Mulberry-SFT","creator_name":"Huanjin Yao","creator_url":"https://huggingface.co/HuanjinYao","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Visual-TableQA","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tðŸ§  Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images\n\t\n\nWelcome to Visual-TableQA, a project designed to generate high-quality synthetic question-answer datasets associated to images of tables. This resource is ideal for training and evaluating models on visually-grounded table understanding tasks such as document QA, table parsing, and multimodal reasoning.\n\n\t\n\t\t\n\t\n\t\n\t\tðŸš€ Latest Update\n\t\n\nWe have refreshed the dataset with newly generated QA pairs created byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AI-4-Everyone/Visual-TableQA.","url":"https://huggingface.co/datasets/AI-4-Everyone/Visual-TableQA","creator_name":"AI 4 Everyone","creator_url":"https://huggingface.co/AI-4-Everyone","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ShotBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tShotBench: Expert-Level Cinematic Understanding in Vision-Language Models\n\t\n\nThis is the official test set of ShotBench, comprising 3,572 question-answer pairs. Each sample is paired with either an image or a video clip. In total, ShotBench includes 3,049 images and 464 videos, primarily sourced from films that received Oscar nominations for Best Cinematography, ensuring high visual quality and strong cinematic style.\n\nPaper: ShotBench: Expert-Level Cinematic Understanding inâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Vchitect/ShotBench.","url":"https://huggingface.co/datasets/Vchitect/ShotBench","creator_name":"Vchitect","creator_url":"https://huggingface.co/Vchitect","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","url":"https://huggingface.co/datasets/terminusresearch/photo-typography","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"UniEM-3M","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tUniEM-3M\n\t\n\n\n\t\n\t\t\n\t\tðŸ“˜ Dataset Summary\n\t\n\nUniEM-3M is the first large-scale multimodal electron microscopy (EM) dataset for instance-level microstructural understanding, which is proposed in our paper \"UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation\". \nIt provides high-resolution electron micrographs with expert-curated annotations and textual descriptions, aiming to accelerate research in automated materials analysis and deep learningâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/UniParser/UniEM-3M.","url":"https://huggingface.co/datasets/UniParser/UniEM-3M","creator_name":"Uni-Parser Team","creator_url":"https://huggingface.co/UniParser","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-segmentation","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TreeBench","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tTraceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\n\t\n\nThis repository contains the TreeBench dataset, a diagnostic benchmark for visual grounded reasoning, introduced in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTL; DR: We propose TreeBench, the first benchmark specially designed for evaluating \"thinking with images\" capabilities with traceable visual evidence, and TreeVGR, the current state-of-the-artâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeBench.","url":"https://huggingface.co/datasets/HaochenWang/TreeBench","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tensorart","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for tensorart\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGenerations from tensorart. Includes full metadata.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{tensorart,\n  author = {hlky},\n  title = {tensorart},\n  year = {2024},\n  publisher = {hlky},\n  journal = {Hugging Face repository},\n  howpublished =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/tensorart.","url":"https://huggingface.co/datasets/bigdata-pw/tensorart","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","image-to-text","odc-by","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CTW1500_OCR","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCTW1500\n\t\n\n\n\t\n\t\t\n\t\tMETA\n\t\n\nhttps://github.com/open-mmlab/mmocr/blob/main/dataset_zoo/ctw1500/metafile.yml\nName: 'CTW1500'\nPaper:\n  Title: Curved scene text detection via transverse and longitudinal sequence connection\n  URL: https://www.sciencedirect.com/science/article/pii/S0031320319300664\n  Venue: PR\n  Year: '2019'\n  BibTeX: '@article{liu2019curved,\n  title={Curved scene text detection via transverse and longitudinal sequence connection},\n  author={Liu, Yuliang and Jin, Lianwen andâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/MiXaiLL76/CTW1500_OCR.","url":"https://huggingface.co/datasets/MiXaiLL76/CTW1500_OCR","creator_name":"Mikhail Stepanov","creator_url":"https://huggingface.co/MiXaiLL76","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Img2Text-Algorithm-Retrieval","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImg2Text-Algorithm-Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThe Img2Text-Algorithm-Retrieval dataset is designed for retrieving text descriptions of algorithms from corresponding images. This dataset consists of structured text, raw text, algorithm images, and metadata such as source URLs and filenames. It can be useful for tasks like OCR-based text retrieval, image-to-text learning, and document understanding.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nModality: Image, Text  \nFormat: Parquetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Img2Text-Algorithm-Retrieval.","url":"https://huggingface.co/datasets/prithivMLmods/Img2Text-Algorithm-Retrieval","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-feature-extraction","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"medical-vision-llm-dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCombined Medical Vision-Language Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nComprehensive medical vision-language dataset with 4793 samples for vision-based LLM training.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Samples: 4793\nTraining Samples: 3834\nValidation Samples: 959\n\n\n\t\n\t\t\n\t\tModality Distribution\n\t\n\n\nX-ray: 2325 samples\nCT: 1351 samples\nUnknown: 812 samples\nMRI: 231 samples\nUltrasound: 70 samples\nMicroscopy: 2 samples\nEndoscopy: 2 samples\n\n\n\t\n\t\t\n\t\tBody Part Distribution\n\t\n\n\nUnknown:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset.","url":"https://huggingface.co/datasets/robailleo/medical-vision-llm-dataset","creator_name":"Robail Yasrab ","creator_url":"https://huggingface.co/robailleo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v48","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v48.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v200","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v200.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v200","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SynCap-Flickr8k","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynCap-Flickr8k\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe SynCap-Flickr8k is a unique collection designed to benchmark Vision Language Models (VLMs) in the image captioning task. This dataset was created using the Flickr8k dataset, which contains 8,000 images, each accompanied by five human-generated captions. By leveraging advanced models such as GPT-4o and LLaMA 3.2, we generated synthetic captions that enhance the understanding of how well VLMs can interpret and describe visual content.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/kargwalaryan/SynCap-Flickr8k.","url":"https://huggingface.co/datasets/kargwalaryan/SynCap-Flickr8k","creator_name":"Aryan Kargwal","creator_url":"https://huggingface.co/kargwalaryan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v13","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v13.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v15","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v51","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v51.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v6","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v6.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"synthetic_sami_ocr_data","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSynthetic text images for North, South, Lule and Inari SÃ¡mi\n\t\n\nThis dataset contains synthetic line images meant for fitting OCR models for North, South, Lule and Inari SÃ¡mi.\nClean line images are created using Pillow and they are subsequently distorted using Augraphy [1].\n\n\t\n\t\t\n\t\tText sources\n\t\n\nThe text in this dataset comes from Giellatekno's corpus. Specifically, we used the data files of the converted/-directories of [2][3][4][5] (commit hashesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Sprakbanken/synthetic_sami_ocr_data.","url":"https://huggingface.co/datasets/Sprakbanken/synthetic_sami_ocr_data","creator_name":"Nasjonalbiblioteket SprÃ¥kbanken","creator_url":"https://huggingface.co/Sprakbanken","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","Southern Sami","Northern Sami","Lule Sami","Inari Sami"],"keywords_longer_than_N":true},
	{"name":"ECG-Grounding","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tECG-Grounding dataset\n\t\n\nECG-Grounding provides more accurate, holistic, and evidence-driven interpretations with diagnoses grounded in measurable ECG features.Â Currently, it contains 30,000 instruction pairs annotated with heartbeat-level physiological features. This is the first high-granularity ECG grounding dataset, enabling evidence-based diagnosis and improving the trustworthiness of medical AI. We will continue to release more ECG-Grounding data and associated beat-levelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/LANSG/ECG-Grounding.","url":"https://huggingface.co/datasets/LANSG/ECG-Grounding","creator_name":"LAN XIANG","creator_url":"https://huggingface.co/LANSG","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v146","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v146.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v146","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v52","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v52.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OGC_Geotechnie_Compatible_Negatives","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOGC_Geotechnie_Corrected\n\t\n\nCorrected version of racineai/OGC_Geotechnie with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset =â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Matchone7/OGC_Geotechnie_Compatible_Negatives.","url":"https://huggingface.co/datasets/Matchone7/OGC_Geotechnie_Compatible_Negatives","creator_name":"NoÃ© BRANDOLINI","creator_url":"https://huggingface.co/Matchone7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these areâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottomâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Imagen-4-ultra-24-7-25_t2i_human_preference","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tRapidata Imagen 4 Ultra 24.7.25 Preference\n\t\n\n\n\n\n\nThis T2I dataset contains over ~400'000 human responses from over ~83'000 individual annotators, collected in less than 7h using the Rapidata Python API, accessible to anyone and ideal for large scale evaluation.\nEvaluating Imagen 4 Ultra (version from 24.7.2025) across three categories: preference, coherence, and alignment.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see moreâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference.","url":"https://huggingface.co/datasets/Rapidata/Imagen-4-ultra-24-7-25_t2i_human_preference","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","license_name":"Community Data License Agreement Permissive 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cdla-permissive-2.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","reinforcement-learning","English"],"keywords_longer_than_N":true},
	{"name":"my_image_caption_dataset","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMy image-caption dataset\n\t\n\nThis dataset contains images with English descriptions (captions).\n","url":"https://huggingface.co/datasets/hongin9812/my_image_caption_dataset","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"RSTeller_metadata","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tMetadata for RSTeller\n\t\n\nThis dataset contains the necessary metadata for the dataset SlytherinGe/RSTeller.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe metadata table provides detailed information for the RSTeller dataset, with the following columns:\n\npatch_id: The primary key of the table, corresponding to the \"__key__\" or the \"patch_id\" field in the JSON of the RSTeller dataset.\n\npatch_lat and patch_lon: The latitude and longitude coordinates of the patch center in WGS84â€¦ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata.","url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v188","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v188.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v188","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Corvus-OCR-Caption-Mini-Mix","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCorvus-OCR-Caption-Mini-Mix\n\t\n\nCorvus-OCR-Caption-Mini-Mix is a high-quality, compact image-caption dataset designed for training and evaluating image-to-text models. It is a carefully curated subset of the larger BLIP3o/BLIP3o-Pretrain-Long-Caption, optimized for mixed OCR and long-form captioning tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a balanced mix of:\n\nLong-form natural language captions\nOCR-heavy samples with scientific, mathematical, and document-styleâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mini-Mix.","url":"https://huggingface.co/datasets/prithivMLmods/Corvus-OCR-Caption-Mini-Mix","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","Chinese","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had aâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthchat","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tRendered synthetic chats from llama3.1\n\t\n\nThis dataset contains 2.2k screenshots of multi-turn conversations generated by Llama-3.1-70B-Instruct. Each conversation consists of 3-4 short exchanges between a User and an AI Assistant about a certain topic.\nThe original dataset comprising of pure text exchanges can be found here: HuggingFaceTB/everyday-conversations-llama3.1-2k\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nThis dataset aims to improve the OCR performance of vision-language models in terms ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/synthchat.","url":"https://huggingface.co/datasets/nnethercott/synthchat","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v169","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v169.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v169","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v208","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v208.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v208","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v9","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v150","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v150.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v150","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"testrtt","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSample image-caption dataset\n\t\n\nImages and their English descriptions.\n","url":"https://huggingface.co/datasets/hongin9812/testrtt","creator_name":"hongin kim","creator_url":"https://huggingface.co/hongin9812","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-captioning","human-annotated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v22","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v22.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"flickr-cc-by","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlickr-CC-BY Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset includes URLs of images uploaded to Flickr between December 31, 2003, 15:00, and July 27, 2024, 24:00. These URLs are part of JSON response results containing images under the CC-BY-2.0 license. The rights to each image belong to the respective Flickr users.\n\n\t\n\t\t\n\t\tImportant Notices\n\t\n\n\nPersonal Project: This dataset was created as a personal project by Atsuma, driven by individual interest and concern. This dataset has noâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/images9/flickr-cc-by.","url":"https://huggingface.co/datasets/images9/flickr-cc-by","creator_name":"images","creator_url":"https://huggingface.co/images9","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc0-1.0","100K - 1M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v12","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"biomed-visual-instructions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tAdapting Multimodal Large Language Models to Domains via Post-Training (EMNLP 2025)\n\t\n\nThis repos contains the biomedicine visual instructions for post-training MLLMs in our paper: On Domain-Specific Post-Training for Multimodal Large Language Models.\nThe main project page is: Adapt-MLLM-to-Domains\n\n\t\n\t\t\n\t\n\t\n\t\tData Information\n\t\n\nUsing our visual instruction synthesizer, we generate visual instruction tasks based on the image-caption pairs from PubMedVision (referred to as PMC_refinedâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AdaptLLM/biomed-visual-instructions.","url":"https://huggingface.co/datasets/AdaptLLM/biomed-visual-instructions","creator_name":"AdaptLLM","creator_url":"https://huggingface.co/AdaptLLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ViLReward-73K","keyword":"image-text-to-text","description":"Process Reward Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\nPaper | Project Page\nThere are 73K vision-language process reward data sourcing from five training sets.\n","url":"https://huggingface.co/datasets/UCSC-VLAA/ViLReward-73K","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OGC_Energy_Compatible_Negatives","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOGC_Energy_Corrected\n\t\n\nCorrected version of racineai/OGC_Energy with proper Image() types and 16 negative image columns for ColPali compatibility.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nid: Unique identifier\nquery: Text query about the document  \nlanguage: Language of the query\nimage: Main document image (corrected Image() type)\nnegative_image_0 to negative_image_15: Negative image columns\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Matchone7/OGC_Energy_Corrected\")â€¦ See the full description on the dataset page: https://huggingface.co/datasets/Matchone7/OGC_Energy_Compatible_Negatives.","url":"https://huggingface.co/datasets/Matchone7/OGC_Energy_Compatible_Negatives","creator_name":"NoÃ© BRANDOLINI","creator_url":"https://huggingface.co/Matchone7","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_taskâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v60","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v60.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v60","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"NorHand-v2-line","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tNorHand v2 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe NorHand v2 dataset comprises Norwegian letter and diary line images and text from 19th and early 20th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Norwegian BokmÃ¥l.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128 at 0x1A800E8E190,\n  'text': 'ogâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Teklia/NorHand-v2-line.","url":"https://huggingface.co/datasets/Teklia/NorHand-v2-line","creator_name":"Teklia","creator_url":"https://huggingface.co/Teklia","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Norwegian BokmÃ¥l","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"clothes_for_men_women_children","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 3082 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 3082\nAverage words in long description: 17.5\nAverage words in short description: 8.8\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/clothes_for_men_women_children.","url":"https://huggingface.co/datasets/AntZet/clothes_for_men_women_children","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"clothes_for_men_women_children","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tImage Description Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 3082 images with their corresponding descriptions in both long and short formats. \nThe descriptions were generated using the BLIP-large model.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal images: 3082\nAverage words in long description: 17.5\nAverage words in short description: 8.8\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nEnglish (en)\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach record in the dataset contains:\n\nfile_name: Relative path to theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AntZet/clothes_for_men_women_children.","url":"https://huggingface.co/datasets/AntZet/clothes_for_men_women_children","creator_name":"Ant_Z (AntheZ)","creator_url":"https://huggingface.co/AntZet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Flame-Eval-React","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Eval-React: A Benchmark Dataset for Multi-modal React Code Generation Evaluation\n\t\n\nFlame-Eval-React is the first benchmarking dataset specifically designed to evaluate the accuracy, functionality, and visual fidelity of vision-language models (VLMs) for React code generation.\nThis dataset includes manually curated React components that serve as the gold standard for evaluating the performance of image-to-code translation models.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nHere we provide the code guidanceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Eval-React.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Eval-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"image-caption pairs","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","url":"https://huggingface.co/datasets/terminusresearch/photo-architecture","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"viexam","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?\n\t\n\n    \n  by \n    Vy Tuong Dang*,\n    An Vo*,\n    Quang Tau, \n    Duc Dm, \n    Daeyoung Kim,\n  \n  \n    *Equal contributionÂ \n    KAIST\n  \n\n\n\n\n\n    \n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) demonstrate remarkable capabilities on English multimodal tasks but significantly underperform on Vietnamese educational assessments. ViExam reveals that SOTA VLMs achieve only 57.74% accuracyâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/viexam.","url":"https://huggingface.co/datasets/anvo25/viexam","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","Vietnamese","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v70","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v70.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v70","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Juicy","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tJuicy Couture\n\t\n\nData includes product and model wearing product images, long form descriptions including materials and height of model.\n\n\n","url":"https://huggingface.co/datasets/bigdata-pw/Juicy","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","odc-by","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v40","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v40.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CalliBench","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tðŸ§  CalliReader: Contextualizing Chinese Calligraphy via an Embedding-aligned Vision Language Model\n\t\n\n\n  ðŸ“‚ Code\n  ðŸ“„ Paper\n\n\nCalliBench is aimed to comprehensively evaluate VLMs' performance on the recognition and understanding of Chinese calligraphy. \n\n\t\n\t\t\n\t\tðŸ“¦ Dataset Summary\n\t\n\n\nSamples: 3,192 imageâ€“annotation pairs\n\nTasks: Full-page recognition and Contextual VQA (choice of author/layout/style, bilingual interpretation, and intent analysis).\n\nAnnotations:\n\nMetadata of authorâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gtang666/CalliBench.","url":"https://huggingface.co/datasets/gtang666/CalliBench","creator_name":"TJQ","creator_url":"https://huggingface.co/gtang666","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","Chinese","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-xm100","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\n\t\n\t\tXM100\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tThis is a copy from https://google.github.io/crossmodal-3600/\n\t\n\nIf you use this dataset, please cite the original authors:\n@inproceedings{ThapliyalCrossmodal2022,\n  author = {Ashish Thapliyal and Jordi Pont-Tuset and Xi Chen and Radu Soricut},\n  title = {{Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset}},\n  booktitle = {EMNLP},\n  year = {2022}\n}\n\n","url":"https://huggingface.co/datasets/neulab/PangeaBench-xm100","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Arabic","Bengali","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v7","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3.\nThe image sizes are between 1 and 30 pixels.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Jigsaw-Puzzles","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tJigsaw-Puzzles Dataset\n\t\n\nJigsaw-Puzzles is a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity, designed to rigorously evaluate Vision-Language Models' (VLMs) spatial perception, structural understanding, and reasoning capabilities. The dataset minimizes reliance on domain-specific knowledge to better isolate and assess general spatial reasoning, positioning itself as a challenging and diagnostic benchmark for advancing spatialâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zesen01/Jigsaw-Puzzles.","url":"https://huggingface.co/datasets/zesen01/Jigsaw-Puzzles","creator_name":"zesen01","creator_url":"https://huggingface.co/zesen01","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","mit","Image","arxiv:2505.20728","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":true},
	{"name":"vibeeval_greek","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Vibe-Eval Greek\n\t\n\nThe Vibe-Eval Greek dataset is a benchmark of 269 examples for evaluating multimodal chat models, including especially challenging examples. It has been manually translated into Greek from the VibeEval dataset.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nEach example has the following fields:\n\nmedia_url: a URL where the file is hosted publicly\nexample_id: a unique ID for the example\ncategory: the category that this example belongs to, either difficulty-normal orâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ilsp/vibeeval_greek.","url":"https://huggingface.co/datasets/ilsp/vibeeval_greek","creator_name":"Institute for Language and Speech Processing","creator_url":"https://huggingface.co/ilsp","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-classification","monolingual","Greek","English"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v22","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"chitralekha","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tChitralekha\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Version\n\t\n\nSome of the fonts do not have proper letters/rendering of different telugu letter combinations. Those have been removed as much as I can find them. If there are any other mistakes that you notice, please raise an issue and I will try my best to look into it\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis extensive dataset, hosted on Huggingface, is a comprehensive resource for Optical Character Recognition (OCR) in the Teluguâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/gksriharsha/chitralekha.","url":"https://huggingface.co/datasets/gksriharsha/chitralekha","creator_name":"Krishna Sriharsha Gundu","creator_url":"https://huggingface.co/gksriharsha","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Telugu","mit","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v14","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v14.","url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LoC-meme-generator","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for \"LoC-meme-generator\"\n\t\n\nThis is an official meme dataset from the library of congress.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tMeme Dataset Exploratory Data Analysis Report\n\t\n\n\ncourtesy of chatGPT data analysis\n\n\n\t\n\t\t\n\t\n\t\n\t\tBasic Dataset Information\n\t\n\n\nNumber of Entries: 57685\nNumber of Columns: 10\nColumns:\nMeme ID\nArchived URL\nBase Meme Name\nMeme Page URL\nMD5 Hash\nFile Size (In Bytes)\nAlternate Text\nDisplay Name\nUpper Text\nLower Text\n\n\n\t\t\n\t\tFile Size Summary\n\t\n\n{\n    \"count\": 57685.0â€¦ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/LoC-meme-generator.","url":"https://huggingface.co/datasets/pszemraj/LoC-meme-generator","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Russian","Spanish"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v22","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v22.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"gregg-preanniversary-phrases","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tGregg Preanniversary Phrases\n\t\n\nThis Grascii original dataset is derived from the 1924 Gregg Shorthand Phrase Book1.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThe dataset contains three columns:\n\nimage: The image of a shorthand form in the dictionary\ngrascii_normalized: The normalized grascii of the shorthand form\nlonghand: The English longhand represented by the shorthand form\n\n\n\t\n\t\t\n\t\tIssues\n\t\n\nIf you notice any problems in the dataset, open an issue in the\ndatasets repository.\n\n1Gregg, John Robert.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/grascii/gregg-preanniversary-phrases.","url":"https://huggingface.co/datasets/grascii/gregg-preanniversary-phrases","creator_name":"Grascii: Language and Tools for Gregg Shorthand","creator_url":"https://huggingface.co/grascii","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ScreenSpot-v2","keyword":"image-text-to-text","description":"zonghanHZH/ScreenSpot-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zonghanHZH/ScreenSpot-v2","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v5","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v8","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5â€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Hindi-Captions","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset information\n\t\n\nThis dataset is primarily for training a image captioning model ,this model was filtered from damerajee/Hindi-LLaVA-CC3M-Pretrain-595K-3 to only include hindi captions \n\nThe first column Images contains images which are  224 pixels wide and 224 pixels tall\nThe second column Captions contains captions corresponding to the text\n\nThe Average Length of the captions\n\n","url":"https://huggingface.co/datasets/damerajee/Hindi-Captions","creator_name":"dame rajee","creator_url":"https://huggingface.co/damerajee","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hindi","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v111","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v111.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v111","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SteamScreenshots","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Steam Screenshots\n\t\n\n~24M video game screenshots from Steam.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nApproximately 24 million video game screenshots from Steam. Entries include image URL, title, anonymized username, Steam app id, video game name and Steam page URL.\n\nCurated by: hlky\nLicense: Open Data Commons Attribution License (ODC-By) v1.0\n\n\n\t\n\t\t\n\t\tCitation Information\n\t\n\n@misc{SteamScreenshots,\n  author = {hlky},\n  title = {SteamScreenshots}â€¦ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/SteamScreenshots.","url":"https://huggingface.co/datasets/bigdata-pw/SteamScreenshots","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-classification","text-to-image","image-to-text","odc-by","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v3","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nInstead of making tasks out of input/output images.\nI'm now focusing on preserving the original puzzle, with some transformations applied.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"SEED-Bench-R1","keyword":"video-text-to-text","description":"This repository contains the datasets presented in Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1.\n","url":"https://huggingface.co/datasets/TencentARC/SEED-Bench-R1","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2503.24376","ðŸ‡ºðŸ‡¸ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-combine-v164","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v164.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v164","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VIP","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Video Infilling and Prediction (VIP)\n\t\n\nVideo Infilling and Prediction (VIP) is a benchmark dataset for assessing the sequential commonsense reasoning capabilities of vision-language models by generating explanations of videos.\nSee our EMNLP 2023 paper introducing this work\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nCurated by: Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Wang (UC Santa Barbara)\nFunded by: Amazonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ryanhe/VIP.","url":"https://huggingface.co/datasets/ryanhe/VIP","creator_name":"Ryan He","creator_url":"https://huggingface.co/ryanhe","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-classification","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ibero-characters-es","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tConjunto de datos de personajes de mitos y leyendas iberoamericanos.\n\t\n\n\nâš ï¸ Este dataset se encuentra en desarrollo activo. Se planea expandir significativamente el nÃºmero de registros y mejorar la cobertura de imÃ¡genes.\n\n\n\t\n\t\t\n\t\tðŸ“š DescripciÃ³n\n\t\n\nDataset de personajes mÃ­ticos y legendarios de IberoamÃ©rica, diseÃ±ado para preservar y promover el patrimonio cultural a travÃ©s de la inteligencia artificial.\n\n\t\n\t\t\n\t\tðŸŒŸ MotivaciÃ³n e Impacto\n\t\n\n\nðŸ“± PreservaciÃ³n Digital: ConservaciÃ³n delâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es.","url":"https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es","creator_name":"Hackathon SomosNLP 2025","creator_url":"https://huggingface.co/somosnlp-hackathon-2025","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","text-generation","Spanish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://diffused.to/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\nâ”œâ”€â”€ ðŸ“‚ images/\nâ”‚   â”œâ”€â”€ 1.jpeg\nâ”‚   â”œâ”€â”€ 2.jpeg\nâ”‚   â”œâ”€â”€ 3.jpeg\nâ”‚   â”œâ”€â”€ ....\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"GameQA-5K","keyword":"image-text-to-text","description":"In this repository, we specifically provide the 5k training samples from the complete GameQA-140K dataset used in our work for GRPO training of the models.\nRefer to our paper for details. And our code for training and evaluation is at https://github.com/tongjingqi/Code2Logic.\n\n\t\n\t\t\n\t\n\t\n\t\tCode2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning\n\t\n\nThis is the first work, to the best of our knowledge, that leverages game code to synthesize multimodal reasoning data forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-5K.","url":"https://huggingface.co/datasets/Code2Logic/GameQA-5K","creator_name":"Game-RL","creator_url":"https://huggingface.co/Code2Logic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v141","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v141.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v141","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://diffused.to/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\nâ”œâ”€â”€ ðŸ“‚ images/\nâ”‚   â”œâ”€â”€ 1.jpeg\nâ”‚   â”œâ”€â”€ 2.jpeg\nâ”‚   â”œâ”€â”€ 3.jpeg\nâ”‚   â”œâ”€â”€ ....\nâ”œâ”€â”€â€¦ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AMEX-8k","keyword":"image-text-to-text","description":"\n\t\n\t\t\n\t\tAMEX-8K\n\t\n\nThis dataset is a curated 8K-sample subset from the original AMEX dataset, as mentioned in our paper. It serves as part of the training corpus for GUI grounding tasks, specifically capturing mobile app interfaces across diverse platforms and screen densities.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nSource: Sampled from AMEX  \nDomain: Mobile GUI screenshots  \nDiversity: Includes a variety of app types and device form factors  \nUse case: GUI grounding pretraining, especially for mobileâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zonghanHZH/AMEX-8k.","url":"https://huggingface.co/datasets/zonghanHZH/AMEX-8k","creator_name":"Hsieh ZongHan","creator_url":"https://huggingface.co/zonghanHZH","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v25","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v25.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"COCO_Person","keyword":"image-to-text","description":"This Dataset is a subsets of COCO 2017 -train- images using \"Crowd\" & \"person\" Labels With the First Caption of Each one\n\nCOCO Summary:\nThe COCO dataset is a comprehensive collection designed for object detection, segmentation, and captioning tasks.\nIt comprises over 200,000 images, encompassing a diverse array of everyday scenes and objects.\nEach image features multiple objects and scenes across 80 distinct object categories, all of which are annotated with descriptive image captions.\n","url":"https://huggingface.co/datasets/Hamdy20002/COCO_Person","creator_name":"Abdelrahman Hamdy","creator_url":"https://huggingface.co/Hamdy20002","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"UI-Genie-Agent-5k","keyword":"image-text-to-text","description":"This repository contains the dataset from the paper UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\nMobile GUI Agents.\nGithub: https://github.com/Euphoria16/UI-Genie\n","url":"https://huggingface.co/datasets/HanXiao1999/UI-Genie-Agent-5k","creator_name":"HanXiao","creator_url":"https://huggingface.co/HanXiao1999","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v47","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now onâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v47.","url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"function_graph","keyword":"image-to-text","description":"alikirec/function_graph dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/alikirec/function_graph","creator_name":"Ali Kirecligol","creator_url":"https://huggingface.co/alikirec","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v170","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v170.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v170","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v4","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput imageâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v204","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v204.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v204","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"samaritan_v1","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tSamaritan v1 - line level\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Samaritanv1 dataset comprises Samaritan Biblical manuscripts line images and text from 14th and early 17th century.\nNote that all images are resized to a fixed height of 128 pixels.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nAll the documents in the dataset are written in Hebrew, Samaritan Hebrew and Samaritan Aramaic.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n  'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4300x128â€¦ See the full description on the dataset page: https://huggingface.co/datasets/johnlockejrr/samaritan_v1.","url":"https://huggingface.co/datasets/johnlockejrr/samaritan_v1","creator_name":"John Locke","creator_url":"https://huggingface.co/johnlockejrr","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Hebrew","Samaritan Aramaic","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Openpdf-MultiReceipt-1K","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tOpenpdf-MultiReceipt-1K\n\t\n\nOpenpdf-MultiReceipt-1K is a dataset consisting of over 1,000 receipt documents in PDF format. This dataset is designed for use in image-to-text and document understanding tasks, particularly Optical Character Recognition (OCR), receipt parsing, and layout analysis.\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\nNo text annotations or metadata are provided â€” only the raw PDFs.\nIdeal for tasks requiring raw document inputs like PDF-to-Text pipelines.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nSize: 1â€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Openpdf-MultiReceipt-1K.","url":"https://huggingface.co/datasets/prithivMLmods/Openpdf-MultiReceipt-1K","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","German","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v131","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v131.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v131","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v103","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v103.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v103","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Flame-Waterfall-React","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tFlame-Waterfall-React: A Structured Data Synthesis Dataset for Multimodal React Code Generation\n\t\n\nFlame-Waterfall-React is a dataset synthesized using the Waterfall-Model-Based Synthesis method, Advancing Vision-Language Models in Front-End Development via Data Synthesis. This dataset is designed to train vision-language models (VLMs) for React code generation from UI design mockups and specifications.\nThe Waterfall synthesis approach mimics real-world software development byâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React.","url":"https://huggingface.co/datasets/Flame-Code-VLM/Flame-Waterfall-React","creator_name":"Flame-Code-VLM","creator_url":"https://huggingface.co/Flame-Code-VLM","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"image-captioning-turkish","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tTÃ¼rkÃ§e Image Captioning Veri Seti\n\t\n\nBu veri seti BLIP3o modelinin pre-train eÄŸitiminde kullanÄ±lan BLIP3o-Pretrain-Long-Caption ve BLIP3o-Pretrain-Short-Caption veri setlerinin TÃ¼rkÃ§eye Ã§evirilmiÅŸ bir alt parÃ§asÄ±dÄ±r. Veri setinin oluÅŸturulmasÄ± ile ilgili detaylÄ± bilgiye orijinal veri seti Ã¼zerinden ulaÅŸabilirsiniz. \nVeri seti Image-to-Text modellerinin eÄŸitilmesinde veya ince ayar sÃ¼recinde kullanÄ±labilir. Veri seti, orijinal veri setinin lisansÄ± olan Apache 2.0 altÄ±nda paylaÅŸÄ±lmÄ±ÅŸtÄ±r.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/ituperceptron/image-captioning-turkish.","url":"https://huggingface.co/datasets/ituperceptron/image-captioning-turkish","creator_name":"ITU Perceptron","creator_url":"https://huggingface.co/ituperceptron","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","Turkish","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Microsoft_Learn","keyword":"table-to-text","description":"PetraAI/Microsoft_Learn dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/PetraAI/Microsoft_Learn","creator_name":"Shady BA","creator_url":"https://huggingface.co/PetraAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","fill-mask","sentence-similarity","summarization"],"keywords_longer_than_N":true},
	{"name":"Tarsier2-Recap-585K","keyword":"video-text-to-text","description":"\n\t\n\t\t\n\t\tDataset Card for Tarsier2-Recap-585K\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nâœ¨Tarsier2-Recap-585Kâœ¨ consists of 585K distinct video clips, lasting for 1972 hours in total, from open-source datasets (e.g. VATEX, TGIF, LSMDC, etc.) and each one with a detailed video description annotated by Tarsier2-7B, which beats GPT-4o in generating detailed and accurate video descriptions for video clips of 5~20 seconds (See the DREAM-1K Leaderboard). Experiments demonstrate its effectiveness in enhancing theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K.","url":"https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K","creator_name":"omni-research","creator_url":"https://huggingface.co/omni-research","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","Video","arxiv:2501.07888"],"keywords_longer_than_N":true},
	{"name":"Caption3o-LongCap-v4","keyword":"image-text-to-text","description":"\n\n\t\n\t\t\n\t\tCaption3o-LongCap-v4\n\t\n\nCaption3o-LongCap-v4 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0\nSplit: trainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-LongCap-v4.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-LongCap-v4","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-text-to-text","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"DigitConfuse-23k","keyword":"image-text-to-text","description":"ðŸ“Š DigitConfuse-23k: A Synthetic Dataset of Digit Confusion Patterns\n    ...DigitConfuse-23k is a synthetic dataset containing 23,000 images of digit pairs designed to capture visual anomalies and confusion cases commonly encountered in OCR, CAPTCHA recognition, optical illusions and human digit interpretation tasks. \n    ...Each image contains two-digit numbers generated using the Humor-Sans font (font_size=32, cell_w=60, cell_h=40). For each confusion category, ~1000 images are included.\nðŸ”¢â€¦ See the full description on the dataset page: https://huggingface.co/datasets/chandrabhuma/DigitConfuse-23k.","url":"https://huggingface.co/datasets/chandrabhuma/DigitConfuse-23k","creator_name":"Chandra Mohan Bhuma","creator_url":"https://huggingface.co/chandrabhuma","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v2","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pixmo-cap-images","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tPixMo-Cap\n\t\n\nBig thanks to Ai2 for releasing the original PixMo-Cap dataset. To preserve the images and simplify usage of the dataset, we are releasing this version, which includes downloaded images. \nPixMo-Cap is a dataset of very long (roughly 200 words on average), detailed captions.\nIt can be used to pre-train and fine-tune vision-language models. \nPixMo-Cap was created by recording annotators speaking about an image for 60-90 seconds and then using the Claude large language modelâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/anthracite-org/pixmo-cap-images.","url":"https://huggingface.co/datasets/anthracite-org/pixmo-cap-images","creator_name":"Anthracite","creator_url":"https://huggingface.co/anthracite-org","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["image-to-text","odc-by","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Caption3o-LongCap-v4","keyword":"image-to-text","description":"\n\n\t\n\t\t\n\t\tCaption3o-LongCap-v4\n\t\n\nCaption3o-LongCap-v4 is a large-scale, high-quality image-caption dataset designed for training and evaluating image-to-text models. Derived from prithivMLmods/blip3o-caption-mini-arrow and additional curated sources, this optimized version emphasizes long-form captions and covers a wide range of real-world and artistic scenes.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\n\nImage resolution: 512x512\nLanguages: English\nModality: Image-to-Text\nLicense: Apache-2.0\nSplit: trainâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Caption3o-LongCap-v4.","url":"https://huggingface.co/datasets/prithivMLmods/Caption3o-LongCap-v4","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v1","keyword":"image-to-text","description":"neoneye/simon-arc-rle-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v11","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercisingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11.","url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-to-text","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-to-text","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and isâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"LAION-High-Qualtiy-Pro-6M-VLV","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tLAION-High-Qualtiy-Pro-6M Dataset\n\t\n\nThis repository hosts LAION-High-Quality-Pro-6M, the image-text dataset we used to train Vision-Language-Vision models.\n\n\t\n\t\t\n\t\tExample Usage:\n\t\n\n\n# pip install -U datasets pillow\nfrom datasets import load_dataset\nfrom PIL import Image\nimport base64\nimport io\n\n\n\n# Robust decoder: works if the column is base64 *or* raw bytes\nimport io\nimportâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ccvl/LAION-High-Qualtiy-Pro-6M-VLV.","url":"https://huggingface.co/datasets/ccvl/LAION-High-Qualtiy-Pro-6M-VLV","creator_name":"CCVL at JHU","creator_url":"https://huggingface.co/ccvl","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["image-to-text","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v209","keyword":"image-to-text","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higherâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v209.","url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v209","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true}
]
;
