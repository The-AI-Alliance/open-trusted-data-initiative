const data_for_modality_reasoning = 
[
	{"name":"vulnerability-intelligence-diagrammatic-reasoning","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning","creator_name":"David Quispe","creator_url":"https://huggingface.co/daqc","description":"\n \n\n\n\n\t\n\t\t\n\t\tVulnerability Intelligence with Diagrammatic Reasoning\n\t\n\n\n[!Important]\nThis dataset was created as a proof-of-concept for the Reasoning Datasets Competition (May 2025). If you have any feedback or suggestions, please feel free to open a discussion! Access the Github repository here.\n\n\n\t\n\t\t\n\t\n\t\n\t\tA. Overview\n\t\n\n\n\n\n\n\n\nThis dataset focuses on security vulnerability analysis through a multi-dimensional approach that combines four types of reasoning to generate valuable insights for… See the full description on the dataset page: https://huggingface.co/datasets/daqc/vulnerability-intelligence-diagrammatic-reasoning.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ChartMuseum","keyword":"reasoning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lytang/ChartMuseum","creator_name":"Liyan Tang","creator_url":"https://huggingface.co/lytang","description":"\n\t\n\t\t\n\t\tChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models\n\t\n\nAuthors: Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett\nLeaderboard 🥇 | Paper 📃 | Code 💻\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nChartMuseum is a chart question answering benchmark designed to evaluate reasoning capabilities of large vision-language models… See the full description on the dataset page: https://huggingface.co/datasets/lytang/ChartMuseum.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SOPBench","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zekunli/SOPBench","creator_name":"Zekun Li","creator_url":"https://huggingface.co/Zekunli","description":"\n\t\n\t\t\n\t\tSOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints\n\t\n\n\n\t\n\t\t\n\t\tPurpose and scope\n\t\n\nAs language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline with: (1) executable environments containing 167… See the full description on the dataset page: https://huggingface.co/datasets/Zekunli/SOPBench.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"reasoning-conversations","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/syntaxsynth/reasoning-conversations","creator_name":"SyntaxSynth","creator_url":"https://huggingface.co/syntaxsynth","description":"\n\t\n\t\t\n\t\tMultilingual Reasoning Dataset\n\t\n\n\nInclude languages from German, Korean, Spanish, Japanese, French, Simplified Chinese, Traditional Chinese\n\nReasoning traces from Deepseek-v3-R1, Deepseek-v3-R1-Zero\n\n\nCredits sponsored by Currents API\n","first_N":5,"first_N_keywords":["text2text-generation","English","Spanish","Korean","German"],"keywords_longer_than_N":true},
	{"name":"gs8k_thai_r1_example","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/StelleX/gs8k_thai_r1_example","creator_name":"StelleX","creator_url":"https://huggingface.co/StelleX","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"LSDBench","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TainU/LSDBench","creator_name":"QU Tianyuan","creator_url":"https://huggingface.co/TainU","description":"\n\t\n\t\t\n\t\tDataset Card for LSDBench: Long-video Sampling Dilemma Benchmark\n\t\n\nA benchmark that focuses on the sampling dilemma in long-video tasks. Through well-designed tasks, it evaluates the sampling efficiency of long-video VLMs.\nArxiv Paper: 📖 Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?\nGithub : https://github.com/dvlab-research/LSDBench\n(Left) In Q1, identifying a camera wearer's visited locations requires analyzing the entire video. However, key frames… See the full description on the dataset page: https://huggingface.co/datasets/TainU/LSDBench.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"DrivingVQA","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA","creator_name":"EPFL-DrivingVQA","creator_url":"https://huggingface.co/EPFL-DrivingVQA","description":"\n\t\n\t\t\n\t\tDataset Card for DrivingVQA\n\t\n\n🏠 Homepage\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDrivingVQA is a dataset designed to assist candidates preparing for the French driving theory exam, which requires passing both a theoretical and a practical test. The theoretical aspect consists of analyzing 40 multiple-choice questions (MCQs) with real-world images to test the candidates' knowledge of traffic laws, road signs, and safe driving practices. This dataset focuses on visual… See the full description on the dataset page: https://huggingface.co/datasets/EPFL-DrivingVQA/DrivingVQA.","first_N":5,"first_N_keywords":["visual-question-answering","multiple-choice","English","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"MMMU-LLM-R1-format","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xDAN-Vision/MMMU-LLM-R1-format","creator_name":"xDAN-RL-Group","creator_url":"https://huggingface.co/xDAN-Vision","description":"\n\t\n\t\t\n\t\tMMMU-LLM-R1 Reformatted Dataset\n\t\n\n","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"TPBench","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhiqiGao/TPBench","creator_name":"Zhiqi Gao","creator_url":"https://huggingface.co/ZhiqiGao","description":"\n\t\n\t\t\n\t\tTP Bench – Theoretical Physics Benchmark for AI\n\t\n\n\n\nTPBench is a curated dataset and evaluation suite designed to measure the reasoning capabilities of AI models in theoretical physics. Our test problems span multiple difficulty levels—from undergraduate to frontier research—and cover topics such as cosmology, high-energy theory, general relativity, and more. By providing a unified framework for problem-solving and auto-verifiable answers, TPBench aims to drive progress in AI-based… See the full description on the dataset page: https://huggingface.co/datasets/ZhiqiGao/TPBench.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2-Pause1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1","creator_name":"James Begin","creator_url":"https://huggingface.co/JamesBegin","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\n🌐 Project Page: https://longbench2.github.io\n💻 Github Repo: https://github.com/THUDM/LongBench\n📚 Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to… See the full description on the dataset page: https://huggingface.co/datasets/JamesBegin/LongBench-v2-Pause1.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"verify-teaser","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jing-bi/verify-teaser","creator_name":"jing bi","creator_url":"https://huggingface.co/jing-bi","description":"\n\t\n\t\t\n\t\tVERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning FidelitY\n\t\n\nVERIFY is the first benchmark explicitly designed to assess the reasoning paths of MLLMs in visual reasoning tasks. \nBy introducing novel evaluation metrics that go beyond mere accuracy, VERIFY highlights critical limitations in current MLLMs and emphasizes the need for a more balanced approach to visual perception and logical reasoning.\nDetails of the benchmark can viewed at the… See the full description on the dataset page: https://huggingface.co/datasets/jing-bi/verify-teaser.","first_N":5,"first_N_keywords":["image-to-text","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"math_problem_traces_test2","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Wendong-Fan/math_problem_traces_test2","creator_name":"Wendong.Fan","creator_url":"https://huggingface.co/Wendong-Fan","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"PyRe-v2","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/PyRe-v2","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"\n\t\n\t\t\n\t\tPyRe 2\n\t\n\nThis data set is a mix of samples from a number of public data sets (sources indidcated in the actual data). The goal with this set was to create a smaller set focused on coding (primarily Python), math, and reasoning.\n","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"V1-33K-Old","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K-Old","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\n🚀  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction 🌟\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of… See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K-Old.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"camel_loong_medicine","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zikaixiao1/camel_loong_medicine","creator_name":"zikaixiao","creator_url":"https://huggingface.co/zikaixiao1","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный ассистент. Отвечай на вопросы, сохраняя следующую структуру: <think> Твои мысли и рассуждения </think> \nТвой конечный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"Combined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный ассистент. Отвечай на вопросы, сохраняя следующую структуру: <think> Твои мысли и рассуждения </think> \nТвой конечный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ru-thinking-reasoning-r1-v2","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2","creator_name":"ZeroAgency","creator_url":"https://huggingface.co/ZeroAgency","description":"This is a modified version of ZeroAgency/ru-thinking-reasoning-r1 with addition of Egor-AI/CoT-XLang dataset.\nCombined dataset of mostly Russian thinking/reasoning/reflection dialogs in form of conversation suitable for LLM fine-tuning scenarios. All responses are mapped to same format.\nThe format of reasoning in most cases is:\n<think>\nReasoning...\n</think>\nResponse\n\nFor reflection dataset - there can be also <reflection> tags inside <think>.\nCommon system prompt for think:\nТы полезный… See the full description on the dataset page: https://huggingface.co/datasets/ZeroAgency/ru-thinking-reasoning-r1-v2.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"camel_loong_medicine_medcal_train30","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/realliyifei/camel_loong_medicine_medcal_train30","creator_name":"NLP GO","creator_url":"https://huggingface.co/realliyifei","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"GAIR_LIMO_topics","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dvilasuero/GAIR_LIMO_topics","creator_name":"Daniel Vila","creator_url":"https://huggingface.co/dvilasuero","description":"\n\t\n\t\t\n\t\tLIMO topics\n\t\n\nThe LIMO dataset augmented with topics, using Llama3.3-70B-Instruct with Hugging Face Inference Providers and this pipeline configuration.\n\n","first_N":5,"first_N_keywords":["text-classification","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"bootstrap-latent-thought-data","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryoungj/bootstrap-latent-thought-data","creator_name":"Yangjun Ruan","creator_url":"https://huggingface.co/ryoungj","description":"This dataset is associated with the paper Reasoning to Learn from Latent Thoughts. It contains data used for pretraining language models with a focus on improving data efficiency by modeling and inferring latent thoughts underlying the text generation process, such as on reasoning-intensive math corpus. An expectation-maximization algorithm is developed for models to self-improve their self-generated thoughts and data efficiency. \n","first_N":5,"first_N_keywords":["text-generation","apache-2.0","10M - 100M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"competitive-game-dataset","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yyyyyyjjjjzzz/competitive-game-dataset","creator_name":"Albert","creator_url":"https://huggingface.co/yyyyyyjjjjzzz","description":"\n\t\n\t\t\n\t\tCompetitive Game Dataset\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n  \n\n\nThis dataset collects move trajectories and accompanying natural-language explanations from three classic board games—Tic-Tac-Toe, Connect Four, and Chess—played by a diverse ensemble of large language models. It can be used for training or evaluating large language models (LLMs). This dataset contains\nGame trajectories generated by pair-wise… See the full description on the dataset page: https://huggingface.co/datasets/yyyyyyjjjjzzz/competitive-game-dataset.","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","arrow"],"keywords_longer_than_N":true},
	{"name":"CoTton-38k-6525-Collective","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective","creator_name":"Newstar Research ASIA","creator_url":"https://huggingface.co/NewstaR","description":"\n\t\n\t\t\n\t\tCoTton-38k-6525-Collective\n\t\n\nCoTton-38k is a 38,350-example dataset of soft reasoning conversations in the ShareGPT format. Each entry contains an exchange between a user and a model, showcasing high-quality Chain-of-Thought (CoT) reasoning in natural language.\nThe dataset is distilled from open LLMs:\n\nQwen3 235B A22B\nAM Thinking\nQwQ 32B\nDeepseek R1\nR1 0528\n\nThe name CoTton encodes multiple layers of meaning:\n\nCoT: Chain-of-Thought is embedded in the name\nTON: The dataset contains a… See the full description on the dataset page: https://huggingface.co/datasets/NewstaR/CoTton-38k-6525-Collective.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"altered-riddles","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/altered-riddles","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Altered Riddles Dataset\n\t\n\nWhile working on the academic-chains dataset, I tested a well-known alteration of a common riddle, \"just for fun\":\n\nThe surgeon, who is the boy's father, says, 'I cannot operate on this boy—he's my son!'. Who is the surgeon to the boy?\n\n(Below is the original riddle for reference)\n\nA man and his son are in a terrible accident and are rushed to the hospital in critical condition. The doctor looks at the boy and exclaims, \"I can't operate… See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/altered-riddles.","first_N":5,"first_N_keywords":["apache-2.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"EEReasonBench","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noru1/EEReasonBench","creator_name":"Norbert John Ibera","creator_url":"https://huggingface.co/Noru1","description":"\n\t\n\t\t\n\t\tEEReasonBench: A Reasoning Benchmark for Electrical Engineering\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides a collection of electrical engineering problems covering various subjects, including Circuits, Machines, Power Systems, Power Plants, etc. Problems include multiple-choice questions (conceptual and numerical) with detailed, step-by-step solutions formatted in Markdown and LaTeX.\nThe dataset is designed to serve as both a benchmark for evaluating model performance on… See the full description on the dataset page: https://huggingface.co/datasets/Noru1/EEReasonBench.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"math_problem_traces_test","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Wendong-Fan/math_problem_traces_test","creator_name":"Wendong.Fan","creator_url":"https://huggingface.co/Wendong-Fan","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Safe-CoT","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sanjin2024/Safe-CoT","creator_name":"XIN GAO","creator_url":"https://huggingface.co/Sanjin2024","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Amanita-Imagine","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Quazim0t0/Amanita-Imagine","creator_name":"Quazimoto","creator_url":"https://huggingface.co/Quazim0t0","description":"Used to train Imagine-v0.5 by Quazim0t0\n","first_N":5,"first_N_keywords":["English","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"reasoning-required","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/reasoning-required","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for the Reasoning Required Dataset\n\t\n\n2025 has seen a massive growing interest in reasoning datasets. Currently, the majority of these datasets are focused on coding and math problems. This dataset – and the associated models – aim to make it easier to create reasoning datasets for a wider variety of domains. This is achieved by making it more feasible to leverage text \"in the wild\" and use a small encoder-only model to classify the level of reasoning complexity… See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/reasoning-required.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"s1_54k_filter","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/s1_54k_filter","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_54k_filter\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_54k_filter is a filtered version of the XuHu6736/s1_59k dataset. This dataset has been processed to remove records containing empty or null values in any field, with the specific exception of the 'cot' (Chain-of-Thought) column. If any other field in a record is empty, that entire record is discarded.\nThe original s1_59k dataset was prepared for Supervised Fine-Tuning (SFT) of large language models by… See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_54k_filter.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Gradient-Reasoning","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tesslate/Gradient-Reasoning","creator_name":"TesslateAI","creator_url":"https://huggingface.co/Tesslate","description":"Tesslate/Gradient-Reasoning dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"El-TARA_Spanish_LLM_Benchmark","keyword":"mathematical-reasoning","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/El-TARA_Spanish_LLM_Benchmark","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tEl-Tara: Evaluación de Razonamiento Avanzado en Español\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nEl-Tara (Evaluación de Razonamiento Avanzado en Español) is a benchmark dataset designed to assess the advanced reasoning capabilities of Large Language Models (LLMs) in Spanish. It is adapted from the original TARA (Turkish Advanced Reasoning Assessment) dataset.\nSimilar to TARA, El-Tara aims to test higher-order cognitive skills across multiple domains, using synthetically generated questions… See the full description on the dataset page: https://huggingface.co/datasets/emre/El-TARA_Spanish_LLM_Benchmark.","first_N":5,"first_N_keywords":["question-answering","text-generation","Spanish","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"m500","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Can111/m500","creator_name":"Can Jin","creator_url":"https://huggingface.co/Can111","description":"Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning\nThe M500 dataset is a curated collection of 500 challenging, interdisciplinary problems designed to evaluate and improve multi-agent collaboration and reasoning in large language models (LLMs). Each sample includes a full trace of interactions among multiple specialized agents solving a complex task collaboratively.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nEach entry in the dataset contains:\n{\n  \"question\": \"...\"… See the full description on the dataset page: https://huggingface.co/datasets/Can111/m500.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"curie","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nhop/curie","creator_name":"Niklas Hoepner","creator_url":"https://huggingface.co/nhop","description":"\n\t\n\t\t\n\t\tCurie Dataset\n\t\n\nHF version of the dataset:\nCURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning.\nAlso available via GitHub (Apache-2.0 license).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nCURIE consists of 10 tasks that are mapped to 8 datasets. The datasets are: \n\n\t\n\t\t\nDataset ID\nTask Name\nDomain\nDescription\n\n\n\t\t\nbiogr\nBiodiversity Georeferencing\nBiodiversity\nDetermine the latitude, longitude bounding box encompassing the region in the map image.\n\n\ndft\nDensity… See the full description on the dataset page: https://huggingface.co/datasets/nhop/curie.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SC10k-R","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nbettencourt/SC10k-R","creator_name":"Nick Bettencourt","creator_url":"https://huggingface.co/nbettencourt","description":"Open-source dataset of 10k high-quality, long-context finance reasoning examples with synthetic reasoning traces from Gemini 2.5 Flash totaling just below 600 million tokens. Each sample includes a financial news article, as well as other relevant articles and associated pricing data, where the given task is to predict the predict the price of a stock 30 days out. The reasoning trace attempts to use logic, rather than direct historical knowledge, to draw conclusions and derive its answer. The… See the full description on the dataset page: https://huggingface.co/datasets/nbettencourt/SC10k-R.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"fs1-predictions","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AAU-NLP/fs1-predictions","creator_name":"AAU-NLP","creator_url":"https://huggingface.co/AAU-NLP","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset are the predictions from Scaling Reasoning can Improve Factuality in Large Language Models. The amount of predictions are around 1.7M.\n\nCurated by: Mike Zhang\nFunded by [optional]: Villum Fonden\nLanguage(s) (NLP): English\nLicense: Apache 2.0 + MIT (due to both QwQ-32B and R1 having these licenses respectively).\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]\n\t\n\n\nRepository: https://huggingface.co/datasets/AAU-NLP/fs1-predictions… See the full description on the dataset page: https://huggingface.co/datasets/AAU-NLP/fs1-predictions.","first_N":5,"first_N_keywords":["text-generation","English","mit","1M<n<10M","arxiv:2505.11140"],"keywords_longer_than_N":true},
	{"name":"Reasoning-Maths-College","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ajibawa-2023/Reasoning-Maths-College","creator_name":"Feynman Innovations","creator_url":"https://huggingface.co/ajibawa-2023","description":"This is a Reasoning dataset build on top of my existing dataset Maths-College.\nThis has only 965 rows from Maths-College-0-Final.json dataset.\nI will add few more things in this dataset.\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"PersonalFinance-CoTR-V2-Compact","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/PersonalFinance-CoTR-V2-Compact","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tPersonal Finance Reasoning V2 - Compact Edition\n\t\n\nThis document describes a condensed version of the Personal Finance Reasoning dataset, specifically adapted for training smaller language models (e.g., 1B-4B parameters).\n\n\t\n\t\t\n\t\t1. Introduction & Motivation\n\t\n\nThe landscape of financial AI benchmarks is currently dominated by applications in corporate finance, algorithmic trading, and general financial knowledge extraction. While valuable, these benchmarks often overlook the critical… See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/PersonalFinance-CoTR-V2-Compact.","first_N":5,"first_N_keywords":["text-classification","text-generation","text2text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"Reasoning_Patterns_AI_Hiring_Bias_SEA","keyword":"reasoning","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Supa-AI/Reasoning_Patterns_AI_Hiring_Bias_SEA","creator_name":"Supahands","creator_url":"https://huggingface.co/Supa-AI","description":"\n\t\n\t\t\n\t\tAI Hiring Bias in Southeast Asia: Structured Candidate Comparison Dataset\n\t\n\nAI is rapidly reshaping hiring, but it risks carrying forward human biases.\nThis project tests a simple but critical question: Would an AI model still make the same hiring decision if only the candidate’s race, gender, age, education, location, or company background changed?\nWe ran controlled, side-by-side hiring simulations across six major AI models used in Southeast Asia, where social divides already impact… See the full description on the dataset page: https://huggingface.co/datasets/Supa-AI/Reasoning_Patterns_AI_Hiring_Bias_SEA.","first_N":5,"first_N_keywords":["table-question-answering","English","cc0-1.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"hippovlog-dataset","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/linyueqian/hippovlog-dataset","creator_name":"Yueqian Lin","creator_url":"https://huggingface.co/linyueqian","description":"\n\t\n\t\t\n\t\tDataset Card for HippoVlog\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nHippoVlog is a novel benchmark dataset designed for evaluating Multimodal Memory and Reasoning (MMR) systems. It consists of 25 long-form daily vlogs (682 minutes total) with naturalistic audiovisual content and 1,000 validated multiple-choice question-answer pairs.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe dataset supports the following tasks:\n\nMultimodal Memory and Reasoning (MMR): The primary task involves answering… See the full description on the dataset page: https://huggingface.co/datasets/linyueqian/hippovlog-dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","visual-question-answering","multiple-choice-qa","English"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\n🐙 GitHub Repo: waltonfuture/MM-UPT\n📜 Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoning… See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"qwen3_dwq_calibration_1332_235b","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mlx-community/qwen3_dwq_calibration_1332_235b","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","description":"\n\t\n\t\t\n\t\tQwen3 DWQ Calibration Dataset (235B, 1332 samples)\n\t\n\nThis dataset contains 1,332 samples for calibrating dynamic weight quantization (DWQ) of Qwen3-235B models. It is created following the methodology of mlx-community/qwen3_dwq_calibration_1332 but using the larger Qwen3-235B model.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is derived from allenai/tulu-3-sft-mixture and consists of:\n\n610 samples processed through Qwen3-235B with explicit reasoning \n722 original samples from the… See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/qwen3_dwq_calibration_1332_235b.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"multilevel-legal-reasoning","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning","creator_name":"Computational Intelligence and Operations Laboratory (CIOL)","creator_url":"https://huggingface.co/ciol-research","description":"\n\t\n\t\t\n\t\tLegal Reasoning Dataset with Multilevel Human and Model-Annotated Explanations\n\t\n\n\nPrepared by Mst Rafia Islam, Umong Sain, Azmine Toushik Wasi\nPrepared as a part of Reasoning Datasets Competition by Bespoke Labs, Hugging Face, and Together.ai.\n\n\n\n\t\n\t\t\n\t\t🧭 Purpose and Scope\n\t\n\nThe Legal Reasoning Dataset aims to support the evaluation and training of legal reasoning systems, particularly in multilingual or jurisdiction-agnostic contexts. It focuses on international acts and treaties… See the full description on the dataset page: https://huggingface.co/datasets/ciol-research/multilevel-legal-reasoning.","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"GSM-NoOp_reenact","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Teluteru/GSM-NoOp_reenact","creator_name":"Teruki Shimomura","creator_url":"https://huggingface.co/Teluteru","description":"\n\t\n\t\t\n\t\tGSM-NoOp_reenact\n\t\n\nGSM-NoOp_reenact is a dataset based on the GSM-Symbolic paper, where distracting sentences have been inserted into the GSM8K dataset without affecting the correctness of the answers.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nExtension of the GSM8K dataset: Adds extra information that does not alter the problem's core but misleads the respondent.\nIdeal for evaluating machine learning models: Allows testing mathematical reasoning capabilities in the presence of noise.\nMIT License: Free to… See the full description on the dataset page: https://huggingface.co/datasets/Teluteru/GSM-NoOp_reenact.","first_N":5,"first_N_keywords":["no-annotation","GSM8K (https://github.com/openai/grade-school-math)","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"V1-33K","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\n🚀  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction 🌟\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of… See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VQA-Verify","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/justairr/VQA-Verify","creator_name":"Chuming Shen","creator_url":"https://huggingface.co/justairr","description":"This is the VQA-Verify dataset, introduced in the paper SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards.\nArxiv Here | Github\nVQA-Verify is a 12k dataset annotated with answer-aligned captions and bounding boxes. It's designed to facilitate training models for Visual Question Answering (VQA) tasks, particularly those employing free-form reasoning. The dataset addresses limitations in existing VQA datasets by providing verifiable intermediate steps and… See the full description on the dataset page: https://huggingface.co/datasets/justairr/VQA-Verify.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"error-detection-negatives","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-negatives","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-negatives\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines negatives samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 57 samples\nmath: 44 samples\nmetamathqa: 59 samples\norca_math: 54 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa, orca_math)\nquestion: The… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-negatives.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"error-detection-negatives","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-negatives","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-negatives\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines negatives samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 57 samples\nmath: 44 samples\nmetamathqa: 59 samples\norca_math: 54 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa, orca_math)\nquestion: The… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-negatives.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"error-detection-positives_perturbed","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives_perturbed","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-positives_perturbed\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines positives_perturbed samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 48 samples\nmath: 42 samples\nmetamathqa: 72 samples\norca_math: 85 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives_perturbed.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"error-detection-positives_perturbed","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives_perturbed","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-positives_perturbed\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines positives_perturbed samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 48 samples\nmath: 42 samples\nmetamathqa: 72 samples\norca_math: 85 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives_perturbed.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"cuad-deepseek","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zenml/cuad-deepseek","creator_name":"ZenML","creator_url":"https://huggingface.co/zenml","description":"\n\t\n\t\t\n\t\tCUAD-DeepSeek: Enhanced Legal Contract Understanding Dataset\n\t\n\nCUAD-DeepSeek is an enhanced version of the Contract Understanding Atticus Dataset (CUAD), enriched with expert rationales and reasoning traces provided by the DeepSeek language model. This dataset aims to improve legal contract analysis by providing not just classifications but detailed explanations for why specific clauses belong to particular legal categories.\n\n\t\n\t\t\n\t\n\t\n\t\tPurpose and Scope\n\t\n\nLegal contract review is… See the full description on the dataset page: https://huggingface.co/datasets/zenml/cuad-deepseek.","first_N":5,"first_N_keywords":["text-classification","zero-shot-classification","theatticusproject/cuad","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"counterfactual_history_reasoning","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/strickvl/counterfactual_history_reasoning","creator_name":"Alex Strick van Linschoten","creator_url":"https://huggingface.co/strickvl","description":"\n\t\n\t\t\n\t\tDataset Card for Counterfactual History Reasoning Dataset\n\t\n\nThe Counterfactual History Reasoning Dataset contains 100 examples of counterfactual reasoning applied to historical events. Each example presents a historical event, poses a \"what if\" counterfactual premise, provides a step-by-step reasoning trace exploring the implications across multiple domains, and concludes with an alternative historical outcome. The reasoning traces and conclusions were generated using DeepSeek-R1, a… See the full description on the dataset page: https://huggingface.co/datasets/strickvl/counterfactual_history_reasoning.","first_N":5,"first_N_keywords":["text2text-generation","monolingual","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"BuddhismEval","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Nethmi14/BuddhismEval","creator_name":"Nethmi Muthugala","creator_url":"https://huggingface.co/Nethmi14","description":"\n\t\n\t\t\n\t\tDataset Card for BuddhismEval\n\t\n\nBuddhismEval is the first bilingual evaluation benchmark designed to assess large language models (LLMs) on Buddhist ethical reasoning and philosophical understanding across Sinhala and English. It includes high-quality, culturally grounded multiple-choice question (MCQ) datasets derived primarily from the Dhammapada, a core Theravāda Buddhist scripture, and other canonical sources and exam materials from Sri Lanka.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details… See the full description on the dataset page: https://huggingface.co/datasets/Nethmi14/BuddhismEval.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","English","Sinhala","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"CodeDebugReasoning","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/talon-community/CodeDebugReasoning","creator_name":"Talon Community","creator_url":"https://huggingface.co/talon-community","description":"OpenDebugReasoning is a small, focused dataset for evaluating code debugging and reasoning ability in language models. It contains 1,000 samples derived from Vezora/Open-Critic-GPT, seeded and filtered for quality, then annotated using Gemini API completions.\nEach entry in the dataset includes a buggy code snippet and a prompt asking an AI model to identify and fix the issue. The dataset also includes step-by-step reasoning generated during the debugging process.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nprompt: A… See the full description on the dataset page: https://huggingface.co/datasets/talon-community/CodeDebugReasoning.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"GRPO-LEAD-SFTData","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PlanePaper/GRPO-LEAD-SFTData","creator_name":"Jeffery Zhang","creator_url":"https://huggingface.co/PlanePaper","description":"\n\t\n\t\t\n\t\t📦 GRPO-LEAD-SFTData\n\t\n\nGRPO-LEAD-SFTData is a supervised fine-tuning dataset comprising 12,153 high-quality mathematical reasoning examples generated using QwQ-32B. Designed to enhance mathematical reasoning, this dataset is central to the GRPO-LEAD training pipeline.\n\n\t\n\t\t\n\t\t📚 Description\n\t\n\n\nSource: Primarily derived from the DeepScaler dataset, filtered to include only problems with difficulty > 1, emphasizing challenging problem-solving cases.\nFormat: Samples follow a clean… See the full description on the dataset page: https://huggingface.co/datasets/PlanePaper/GRPO-LEAD-SFTData.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"CipherBank","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yu0226/CipherBank","creator_name":"YU LI","creator_url":"https://huggingface.co/yu0226","description":"\n\t\n\t\t\n\t\tCipherBank Benchmark\n\t\n\n\n\t\n\t\t\n\t\tBenchmark description\n\t\n\nCipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. \nCipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption… See the full description on the dataset page: https://huggingface.co/datasets/yu0226/CipherBank.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"HyperThink-X-Nvidia-Opencode-Reasoning-200K","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K","creator_name":"Nukeverse","creator_url":"https://huggingface.co/NuclearAi","description":"\n  \n\n\n\n\t\n\t\t\n\t\t🔮 HyperThink\n\t\n\nHyperThink is a premium, best-in-class dataset series capturing deep reasoning interactions between users and an advanced Reasoning AI system. Designed for training and evaluating next-gen language models on complex multi-step tasks, the dataset spans a wide range of prompts and guided thinking outputs.\n\n\n\t\n\t\t\n\t\t🚀 Dataset Tiers\n\t\n\nHyperThink is available in three expertly curated versions, allowing flexible scaling based on compute resources and training goals:… See the full description on the dataset page: https://huggingface.co/datasets/NuclearAi/HyperThink-X-Nvidia-Opencode-Reasoning-200K.","first_N":5,"first_N_keywords":["reinforcement-learning","question-answering","text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"GenRef-wds","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-wds","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-1M\n\t\n\n\n  \n\n\nWe provide 1M high-quality triplets of the form (flawed image, high-quality image, reflection) collected across\nmultiple domains using our scalable pipeline from [1]. We used this dataset to train our reflection tuning model.\nTo know the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\nProject Page: https://diffusion-cot.github.io/reflection2perfection\n\n\t\n\t\t\n\t\n\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast… See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-wds.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"compositional_causal_reasoning","keyword":"reasoning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmaasch/compositional_causal_reasoning","creator_name":"Jacqueline Maasch","creator_url":"https://huggingface.co/jmaasch","description":"\n    \n    https://jmaasch.github.io/ccr/\n\n\n\nCausal reasoning and compositional reasoning are two core aspirations in AI. Measuring these behaviors requires principled \nevaluation methods. Maasch et al. (2025) consider both behaviors simultaneously, under \nthe umbrella of compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate \nthrough graphs. CCR.GB applies the theoretical foundations provided by Maasch et al.… See the full description on the dataset page: https://huggingface.co/datasets/jmaasch/compositional_causal_reasoning.","first_N":5,"first_N_keywords":["question-answering","English","gpl-3.0","arxiv:2503.04556","arxiv:2410.03767"],"keywords_longer_than_N":true},
	{"name":"compositional_causal_reasoning","keyword":"logical-reasoning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jmaasch/compositional_causal_reasoning","creator_name":"Jacqueline Maasch","creator_url":"https://huggingface.co/jmaasch","description":"\n    \n    https://jmaasch.github.io/ccr/\n\n\n\nCausal reasoning and compositional reasoning are two core aspirations in AI. Measuring these behaviors requires principled \nevaluation methods. Maasch et al. (2025) consider both behaviors simultaneously, under \nthe umbrella of compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate \nthrough graphs. CCR.GB applies the theoretical foundations provided by Maasch et al.… See the full description on the dataset page: https://huggingface.co/datasets/jmaasch/compositional_causal_reasoning.","first_N":5,"first_N_keywords":["question-answering","English","gpl-3.0","arxiv:2503.04556","arxiv:2410.03767"],"keywords_longer_than_N":true},
	{"name":"WildSci","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JustinTX/WildSci","creator_name":"Tengxiao Liu","creator_url":"https://huggingface.co/JustinTX","description":"\n\t\n\t\t\n\t\t🧪 WildSci: Advancing Scientific Reasoning from In-the-Wild Literature\n\t\n\n\n\t\n\t\t\n\t\tPurpose and scope\n\t\n\nDespite recent advances in LLM reasoning, there remains a notable lack of diverse, domain-rich science datasets “in the wild” to support progress on science reasoning tasks. While existing work has demonstrated strong performance in specialized areas such as mathematical reasoning, there is still a gap in datasets that capture the complexity and breadth of reasoning required across… See the full description on the dataset page: https://huggingface.co/datasets/JustinTX/WildSci.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"RelatLogic-Reasoning","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/RelatLogic-Reasoning","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"\n \n\n\n\n\t\n\t\t\n\t\tRelatLogic Reasoning Dataset\n\t\n\nThis dataset contains examples of logic puzzles involving comparisons, conditional statements, and superlative queries etc. each paired with a step-by-step, chain of thought reasoning and a ground‐truth answer. It is designed to advance LLM capabilities in deep, multi‐step reasoning, constraint satisfaction and evidence evaluation.\n\n\t\n\t\t\n\t\t🤔 Curation Rationale\n\t\n\nA lot of LLM's these days are \"aligned\" to agree with the user. You can \"convince\" it… See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic-Reasoning.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Resume-Analysis-CoTR","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tResume Reasoning and Feedback Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 417 examples designed to facilitate research and development in automated resume analysis and feedback generation. Each data point consists of a user query regarding their resume, a simulated internal analysis (chain-of-thought) performed by an expert persona, and a final, user-facing feedback response derived solely from that analysis.\nThe dataset captures a two-step reasoning… See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Resume-Analysis-CoTR.","first_N":5,"first_N_keywords":["image-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"MathVista","keyword":"logical-reasoning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\n\t\n\t\t\n\t\tDataset Card for MathVista\n\t\n\n\nDataset Description\nPaper Information\nDataset Examples\nLeaderboard\nDataset Usage\nData Downloading\nData Format\nData Visualization\nData Source\nAutomatic Evaluation\n\n\nLicense\nCitation\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate logical… See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","text-classification","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"spanex","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/spanex","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"SpanEx consists of 7071 instances annotated for span interactions.\nSpanEx is the first dataset with human phrase-level interaction explanations with explicit labels for interaction types. \nMoreover, SpanEx is annotated by three annotators, which opens new avenues for studies of human explanation agreement -- an understudied area in the explainability literature. \nOur study reveals that while human annotators often agree on span interactions, they also offer complementary reasons for a… See the full description on the dataset page: https://huggingface.co/datasets/copenlu/spanex.","first_N":5,"first_N_keywords":["text-classification","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"General-Knowledge","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MuskumPillerum/General-Knowledge","creator_name":"EurekaBotics","creator_url":"https://huggingface.co/MuskumPillerum","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset is a collection of questions and answers themed on general facts and reasoning. The dataset is divided into two features - 'Question' and 'Answer'. \nIt is meant to be used for training a model to be good at general knowledge and reasoning. This dataset is inspired from the Alpaca dataset, and infact contains a subset of the alpaca dataset in itself.\n\n\t\n\t\t\n\t\tDistribution\n\t\n\n  The distribution of the… See the full description on the dataset page: https://huggingface.co/datasets/MuskumPillerum/General-Knowledge.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Knowledge_Pile","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Query-of-CC/Knowledge_Pile","creator_name":"Query-of-CC","creator_url":"https://huggingface.co/Query-of-CC","description":"Knowledge Pile is a knowledge-related data leveraging Query of CC.\nThis dataset is a partial of Knowledge Pile(about 40GB disk size), full datasets have been released in [🤗 knowledge_pile_full], a total of 735GB disk size and 188B tokens (using Llama2 tokenizer).\n\n\t\n\t\t\n\t\tQuery of CC\n\t\n\nJust like the figure below, we initially collected seed information in some specific domains, such as keywords, frequently asked questions, and textbooks, to serve as inputs for the Query Bootstrapping stage.… See the full description on the dataset page: https://huggingface.co/datasets/Query-of-CC/Knowledge_Pile.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","json","Text"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_onetime","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_onetime","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_plan_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_ground_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_onetime","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_onetime","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_plan_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_ground_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"vi_math_problem_crawl","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hllj/vi_math_problem_crawl","creator_name":"Bui Van Hop","creator_url":"https://huggingface.co/hllj","description":"\n\t\n\t\t\n\t\tDataset Card for Vietnamese Elementary Math Knowledge and Workbook\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe data includes information about elementary school math knowledge in Vietnam, as well as exercises compiled from books. This is a crawlable dataset that can be trained for text generation tasks.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe majority of the data is in Vietnamese, but there is still some English from some bilingual workbooks.\n\n\t\n\t\t\n\t\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/hllj/vi_math_problem_crawl.","first_N":5,"first_N_keywords":["text-generation","Vietnamese","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vi_grade_school_math_mcq","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq","creator_name":"Bui Van Hop","creator_url":"https://huggingface.co/hllj","description":"\n\t\n\t\t\n\t\tDataset Card for Vietnamese Grade School Math Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe dataset includes multiple-choice math exercises for elementary school students from grades 1 to 5 in Vietnam.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe majority of the data is in Vietnamese.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nThe data includes information about the page paths we crawled and some text that has been post-processed. The structure will be… See the full description on the dataset page: https://huggingface.co/datasets/hllj/vi_grade_school_math_mcq.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","multiple-choice","Vietnamese","mit"],"keywords_longer_than_N":true},
	{"name":"TemplateGSM","keyword":"mathematical-reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tTraining and Evaluating Language Models with Template-based Data Generation\n\t\n\n\n\t\n\t\t\n\t\tTemplateGSM Dataset\n\t\n\nThe TemplateGSM dataset is a large-scale collection of over 7 million (with potential for unlimited generation) grade school math problems, each paired with both code-based and natural language solutions.  Designed to advance mathematical reasoning in language models, this dataset presents a diverse range of challenges to assess and improve model capabilities in solving… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"TemplateGSM","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tTraining and Evaluating Language Models with Template-based Data Generation\n\t\n\n\n\t\n\t\t\n\t\tTemplateGSM Dataset\n\t\n\nThe TemplateGSM dataset is a large-scale collection of over 7 million (with potential for unlimited generation) grade school math problems, each paired with both code-based and natural language solutions.  Designed to advance mathematical reasoning in language models, this dataset presents a diverse range of challenges to assess and improve model capabilities in solving… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","Tabular"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of an… See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"riddle_sense","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Technoculture/riddle_sense","creator_name":"Technoculture","creator_url":"https://huggingface.co/Technoculture","description":"riddle_sense dataset formatted into an alpaca format dataset for instruction tuning LLMs for reasoning capabilities.\n","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"mathematical-reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"DeepMath-103K","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zwhe99/DeepMath-103K","creator_name":"Zhiwei He","creator_url":"https://huggingface.co/zwhe99","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\t🔥 News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:… See the full description on the dataset page: https://huggingface.co/datasets/zwhe99/DeepMath-103K.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"kegg","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanglab/kegg","creator_name":"WangLab UofT","creator_url":"https://huggingface.co/wanglab","description":"\n🧬 BioReasonIncentivizing Multimodal Biological Reasoning within a DNA-LLM Model\n\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tKEGG Biological Reasoning Dataset\n\t\n\n1,449 entries from KEGG pathway database with variants from ClinVar/dbSNP/OMIM/COSMIC, featuring reasoning traces for mechanistic variant-to-disease prediction across 37 unique diseases.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"wanglab/kegg\")\nexample = dataset[\"train\"][0]\nprint(example)\n\n\n\t\n\t\t\n\t\tCitation… See the full description on the dataset page: https://huggingface.co/datasets/wanglab/kegg.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"variant_effect_coding","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanglab/variant_effect_coding","creator_name":"WangLab UofT","creator_url":"https://huggingface.co/wanglab","description":"\n🧬 BioReasonIncentivizing Multimodal Biological Reasoning within a DNA-LLM Model\n\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tVariant Effect Coding Dataset\n\t\n\n50,083 core variant entries from GPN-MSA study using ClinVar pathogenic variants and gnomAD benign variants (MAF>5%), split by chromosome (Chr 1-7,9-22,X,Y for train, Chr 8 for test) for pathogenic/benign classification.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"wanglab/variant_effect_coding\")\nexample = dataset[\"train\"][0]… See the full description on the dataset page: https://huggingface.co/datasets/wanglab/variant_effect_coding.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"variant_effect_non_snv","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanglab/variant_effect_non_snv","creator_name":"WangLab UofT","creator_url":"https://huggingface.co/wanglab","description":"\n🧬 BioReasonIncentivizing Multimodal Biological Reasoning within a DNA-LLM Model\n\n\n\n  \n  \n  \n  \n\n\n\n\t\n\t\t\n\t\tVariant Effect Coding Non-SNVs Dataset\n\t\n\n36,088 core non-SNV entries from ClinVar 2024-02-28 release, filtered for coding variants with ≥2-star review status, using stratified train/test splits for balanced disease representation in pathogenic/benign classification.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"wanglab/variant_effect_non_snv\")\nexample =… See the full description on the dataset page: https://huggingface.co/datasets/wanglab/variant_effect_non_snv.","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MathVision","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathVision","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMeasuring Multimodal Mathematical Reasoning with the MATH-Vision Dataset\n\t\n\n[💻 Github] [🌐 Homepage]  [📊 Leaderboard ] [📊 Open Source Leaderboard ] [🔍 Visualization] [📖 Paper]\n\n\t\n\t\t\n\t\n\t\n\t\t🚀 Data Usage\n\t\n\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"MathLLMs/MathVision\")\nprint(dataset)\n\n\n\t\n\t\t\n\t\t💥 News\n\t\n\n\n[2025.05.16] 💥 We now support the official open-source leaderboard! 🔥🔥🔥 Skywork-R1V2-38B is the best open-source model, scoring 49.7% on MATH-Vision. 🔥🔥🔥… See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathVision.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","expert-generated"],"keywords_longer_than_N":true},
	{"name":"LEXam","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LEXam-Benchmark/LEXam","creator_name":"LEXam","creator_url":"https://huggingface.co/LEXam-Benchmark","description":"\n  \n  \n    LEXam: Benchmarking Legal Reasoning on 340 Law Exams\n    A diverse, rigorous evaluation suite for legal AI from Swiss, EU, and international law examinations.\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tGitHub Repository\n\t\n\nYou can find the code for running evaluations on LEXam in our GitHub repository.\n\n\t\n\t\t\n\t\n\t\n\t\t🔥 News\n\t\n\n\n[2025/05] Release of the first version of paper, where we evaluate representative SoTA LLMs with evaluations stricly verified by legal experts.\n\t\n\t\t\n\t\t🧩 Subsets\n\t\n\nThe dataset entails… See the full description on the dataset page: https://huggingface.co/datasets/LEXam-Benchmark/LEXam.","first_N":5,"first_N_keywords":["text-classification","text-generation","English","German","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"FinMME","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/FinMME","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, there is a notable lack of effective and specialized multimodal evaluation datasets in the financial domain. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20… See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/FinMME.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PersonalFinance_v2","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/PersonalFinance_v2","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\n\t\n\t\t\n\t\tPersonal Finance Reasoning-V2\n\t\n\nP.S. This dataset has won the First prize in the Reasoning Datasets Competition, organized by Bespoke Labs, HuggingFace & Together.AI During the months of April-May 2025. More details can be found here.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Introduction & Motivation\n\t\n\nThe landscape of financial AI benchmarks is currently dominated by applications in corporate finance, algorithmic trading, and general financial knowledge extraction. While valuable, these benchmarks often… See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/PersonalFinance_v2.","first_N":5,"first_N_keywords":["text-classification","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"academic-chains","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcodsn/academic-chains","creator_name":"Marco De Santis","creator_url":"https://huggingface.co/marcodsn","description":"\n \n\n\n\n\t\n\t\t\n\t\tDataset Card for Academic Reasoning and Intuition Chains\n\t\n\n\n(The image above is an output from Llama-3.2-3B-Instruct tuned on this dataset, quantized to 8 bit and ran on llama.cpp; In our tests Qwen3-30B-A3B, Gemini 2.5 Pro and Claude Sonnet 3.7 with thinking enabled all got this simple question wrong)\nThis dataset contains reasoning (and intuition) chains distilled from open-access research papers, primarily focusing on fields like Biology, Economics, Physics, Math, Computer… See the full description on the dataset page: https://huggingface.co/datasets/marcodsn/academic-chains.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Nemotron-PrismMath","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/Nemotron-PrismMath","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tNemotron-PrismMath\n\t\n\nJaehun Jung, Seungju Han*, Ximing Lu*, Skyler Hallinan*, David Acuna, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi\nPaper Project Page\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\nNemotron-PrismMath is a state-of-the-art math reasoning dataset with diverse, novel math problems. This dataset is ready for commercial/non-commercial use.\n\nThe dataset consists of 1M math problem-solution pairs generated via Prismatic Synthesis, our novel… See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Nemotron-PrismMath.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"frames-benchmark","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/frames-benchmark","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\n\t\n\t\t\n\t\tFRAMES: Factuality, Retrieval, And reasoning MEasurement Set\n\t\n\nFRAMES is a comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.\nOur paper with details and experiments is available on arXiv: https://arxiv.org/abs/2409.12941.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\n824 challenging multi-hop questions requiring information from 2-15 Wikipedia articles\nQuestions span diverse topics… See the full description on the dataset page: https://huggingface.co/datasets/google/frames-benchmark.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","English"],"keywords_longer_than_N":true},
	{"name":"24-game","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/24-game","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\n\t\n\t\t\n\t\tMath Twenty Four (24s Game) Dataset\n\t\n\nA comprehensive dataset for the classic math twenty four game (also known as the 4 numbers game / 24s game / Game of 24). This dataset of mathematical reasoning challenges was collected from 4nums.com, featuring over 1,300 unique puzzles of the Game of 24, with difficulty metrics derived from over 6.4 million human solution attempts since 2012.\nIn each puzzle, players must use exactly four numbers and basic arithmetic operations (+, -, ×, /) to… See the full description on the dataset page: https://huggingface.co/datasets/nlile/24-game.","first_N":5,"first_N_keywords":["multiple-choice","text-generation","text2text-generation","other","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"LongBench-v2","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongBench-v2","creator_name":"Z.ai & THUKEG","creator_url":"https://huggingface.co/THUDM","description":"\n\t\n\t\t\n\t\tLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\n\t\n\n🌐 Project Page: https://longbench2.github.io\n💻 Github Repo: https://github.com/THUDM/LongBench\n📚 Arxiv Paper: https://arxiv.org/abs/2412.15204\nLongBench v2 is designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 has the following features: (1) Length: Context length ranging from 8k to… See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongBench-v2.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","text-classification","table-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"multimodal_textbook","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook","creator_name":"Language Technology Lab at Alibaba DAMO Academy","creator_url":"https://huggingface.co/DAMO-NLP-SG","description":"\n\t\n\t\t\n\t\tMultimodal-Textbook-6.5M\n\t\n\n    \n\n\n  \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis dataset is for \"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining\", containing 6.5M images interleaving with 0.8B text from instructional videos.\n\nIt contains pre-training corpus using interleaved image-text format. Specifically, our multimodal-textbook includes 6.5M keyframesextracted from instructional videos, interleaving with 0.8B ASR texts.\nAll the images and text are extracted from online… See the full description on the dataset page: https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook.","first_N":5,"first_N_keywords":["text-generation","summarization","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"Raiden-DeepSeek-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nRaiden-DeepSeek-R1 is a dataset containing creative-reasoning and analytic-reasoning responses, testing the limits of DeepSeek R1's reasoning skills!\nThis dataset contains:\n\n63k 'creative_content' and 'analytical_reasoning' prompts from microsoft/orca-agentinstruct-1M-v1, with all responses generated by deepseek-ai/DeepSeek-R1.\nResponses demonstrate the reasoning capabilities of DeepSeek's 685b parameter R1 reasoning model.… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"ChessCOT","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/frosthead/ChessCOT","creator_name":"Ayush Sharma","creator_url":"https://huggingface.co/frosthead","description":"\n\t\n\t\t\n\t\tChessCOT\n\t\n\nThe dataset that makes your chess model think like a human before it plays a move.\n\n\t\n\t\t\n\t\tAbout\n\t\n\nChessCOT is a dataset designed to train transformers for chess using a Chain of Thought (CoT) approach. The goal is to make the model reason about the position with all possible moves and their consequences in order to predict the best move.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTotal Poistions: 4,491,596\nSequence length of sMoves: 128\nSequence length of thought: 128… See the full description on the dataset page: https://huggingface.co/datasets/frosthead/ChessCOT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"reasoning-v1-20m-portuguese","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cnmoro/reasoning-v1-20m-portuguese","creator_name":"Carlo Moro","creator_url":"https://huggingface.co/cnmoro","description":"glaiveai/reasoning-v1-20m translated to portuguese.\n","first_N":5,"first_N_keywords":["text-generation","Portuguese","apache-2.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"SpaceThinker","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/remyxai/SpaceThinker","creator_name":"Remyx AI","creator_url":"https://huggingface.co/remyxai","description":"\n\t\n\t\t\n\t\tSpaceThinker Dataset\n\t\n\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nTry training a LLaVA-style VLM using the SpaceThinker Dataset\n\n\t\n\t\t\n\t\tEnhanced Quantitative Spatial Reasoning with Test-Time Compute\n\t\n\nThe SpaceThinker dataset is created using VQASynth to synthesize spatial reasoning traces from a subset of images \nin the localized narratives split of the cauldron.\n\n\t\n\t\t\n\t\tData Samples\n\t\n\n\n\t\n\t\t\n\n\n\n\n\n\t\t\nPrompt: How far is the man in the red hat from the pallet of boxes in feet?\nPrompt: How far is the Goal… See the full description on the dataset page: https://huggingface.co/datasets/remyxai/SpaceThinker.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GenRef-CoT","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-CoT","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-CoT\n\t\n\n\n  \n\n\nWe provide 227K high-quality CoT reflections which were used to train our Qwen-based reflection generation model in ReflectionFlow [1]. To\nknow the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\n\n\t\n\t\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast dataloading and streaming. We recommend downloading\nthe repository locally for faster I/O:\nfrom huggingface_hub import snapshot_download\n\nlocal_dir =… See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-CoT.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"Tachibana2-DeepSeek-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Tachibana2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTachibana2-DeepSeek-R1 is a code-reasoning dataset, testing the limits of DeepSeek R1's coding skills!\nThis dataset contains:\n\n27.2k synthetically generated code-reasoning prompts. All responses are generated using DeepSeek R1.\nSynthetic prompts are generated using Llama 3.1 405b Instruct, based on the original sequelbox/Tachibana dataset with increased task complexity.\nResponses demonstrate the code-reasoning capabilities of… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana2-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"Titanium2.1-DeepSeek-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2.1-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n31.7k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2.1-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"think-more","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/think-more","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThink More is a large-scale, multi-domain collection of long chain-of-thought (CoT) reasoning examples. It aggregates and cleans several prominent reasoning datasets, focusing on high-quality, step-by-step model-generated solutions from DeepSeek R1 and OpenAI o1. Each entry includes a question, the model’s answer, and the detailed thought process leading to that answer.\n⚠️ Warning: the dataset decompresses to a 15.1 GB JSONLines file.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/think-more.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"train-of-thought","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/train-of-thought","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tTrain of Thought Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset readapts agentlans/think-more\ninto the Alpaca-style instruction tuning format for training language models in direct answering and chain-of-thought reasoning.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach original example was randomly assigned to be thinking on or off:\n\nThinking off: Outputs only the final answer.\nThinking on:\nOutputs a chain-of-thought (CoT) reasoning process wrapped in <think>...</think>, followed by the final answer… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/train-of-thought.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"MM-MathInstruct","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MM-MathInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning\n\t\n\nRepo: https://github.com/mathllm/MathCoder\nPaper: https://huggingface.co/papers/2505.10557\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder-VL, a series of open-source large multimodal models (LMMs) specifically tailored for general math problem-solving. We also introduce FigCodifier-8B, an image-to-code model.\n\n\t\n\t\t\nBase Model\nOurs\n\n\n\t\t\nMini-InternVL-Chat-2B-V1-5\nMathCoder-VL-2B\n\n\nInternVL2-8B… See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MM-MathInstruct.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","visual-question-answering","text-generation","English"],"keywords_longer_than_N":true},
	{"name":"arabic-reasoning-dataset-logic","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/beetleware/arabic-reasoning-dataset-logic","creator_name":"beetleware","creator_url":"https://huggingface.co/beetleware","description":"\n\t\n\t\t\n\t\tArabic Logical Reasoning Tasks Dataset (Maximum 1000 Tasks)\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset comprises a series of logical reasoning tasks designed to evaluate and train artificial intelligence models on understanding and generating logical inferences in the Arabic language. Each task includes a unique identifier, the task type, the task text (a question and a proposed answer), and a detailed solution that outlines the thinking steps and the final answer.\n\n\t\n\t\t\n\t\tData Format\n\t\n\nThe… See the full description on the dataset page: https://huggingface.co/datasets/beetleware/arabic-reasoning-dataset-logic.","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"greek-bar-bench","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AUEB-NLP/greek-bar-bench","creator_name":"Athens University of Economics and Business - NLP Group","creator_url":"https://huggingface.co/AUEB-NLP","description":"\n\t\n\t\t\n\t\tDataset Card for GreekBarBench   🇬🇷🏛️⚖️\n\t\n\n\n\nGreekBarBench is a benchmark designed to evaluate LLMs on challenging legal reasoning questions across five different legal areas from the Greek Bar exams, requiring citations to statutory articles and case facts.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\nGreekBarBench (GBB) comprises legal questions sourced from the Greek Bar exams held between 2015 and 2024. The dataset aims to simulate the open-book format of these… See the full description on the dataset page: https://huggingface.co/datasets/AUEB-NLP/greek-bar-bench.","first_N":5,"first_N_keywords":["question-answering","text-generation","Greek","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-time… See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ART","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/ART","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"the Abductive Natural Language Generation Dataset from AI2","first_N":5,"first_N_keywords":["other","automatically-created","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"common_gen","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GEM/common_gen","creator_name":"GEM benchmark","creator_url":"https://huggingface.co/GEM","description":"CommonGen is a constrained text generation task, associated with a benchmark\ndataset, to explicitly test machines for the ability of generative commonsense\nreasoning. Given a set of common concepts; the task is to generate a coherent\nsentence describing an everyday scenario using these concepts.","first_N":5,"first_N_keywords":["other","none","unknown","unknown","original"],"keywords_longer_than_N":true},
	{"name":"clevr-math","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dali-does/clevr-math","creator_name":"Adam Dahlgren Lindström","creator_url":"https://huggingface.co/dali-does","description":"CLEVR-Math is a dataset for compositional language, visual and mathematical reasoning. CLEVR-Math poses questions about mathematical operations on visual scenes using subtraction and addition, such as \"Remove all large red cylinders. How many objects are left?\". There are also adversarial (e.g. \"Remove all blue cubes. How many cylinders are left?\") and multihop questions (e.g. \"Remove all blue cubes. Remove all small purple spheres. How many objects are left?\").","first_N":5,"first_N_keywords":["visual-question-answering","visual-question-answering","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"probability_words_nli","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Probing neural language models for understanding of words of estimative probability","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2, Depth=3, Depth=4 and Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2, Depth=3, Depth=4 and Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-2","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-2\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-2","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-2\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-3","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-3\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=3. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-3","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-3\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=3. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-4","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-4\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=4. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-4","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-4\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=4. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-5","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-5\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-5","keyword":"logical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-5\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than… See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5.","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mindgames","keyword":"logical-reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Mindgame dataset\nCode:\nhttps://github.com/sileod/llm-theory-of-mind\nArticle (Accepted at EMNLP 2023 Findings):\nhttps://arxiv.org/abs/2305.03353\n@article{sileo2023mindgames,\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\n  author={Sileo, Damien and Lernould, Antoine},\n  journal={arXiv preprint arXiv:2305.03353},\n  year={2023}\n}\n\n","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"mindgames","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Mindgame dataset\nCode:\nhttps://github.com/sileod/llm-theory-of-mind\nArticle (Accepted at EMNLP 2023 Findings):\nhttps://arxiv.org/abs/2305.03353\n@article{sileo2023mindgames,\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\n  author={Sileo, Damien and Lernould, Antoine},\n  journal={arXiv preprint arXiv:2305.03353},\n  year={2023}\n}\n\n","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"reasoning_bg_oa","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/0x22almostEvil/reasoning_bg_oa","creator_name":"David Glushkov","creator_url":"https://huggingface.co/0x22almostEvil","description":"\n\t\n\t\t\n\t\tDataset Card for Bulgarian QnA reasoning with ~2.7K entries.\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nContains Parquet of a list of instructions and answers.\nEach row consists of\n\nINSTRUCTION\nRESPONSE\nSOURCE (reasoning_bg)\nMETADATA (json with language, url, id).\n\n\n\t\n\t\t\n\t\tOriginal Dataset is available here:\n\t\n\n\nhttps://huggingface.co/datasets/reasoning_bg\n\n","first_N":5,"first_N_keywords":["question-answering","Bulgarian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"test-parquet","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Birchlabs/test-parquet","creator_name":"Alex Birch","creator_url":"https://huggingface.co/Birchlabs","description":"Birchlabs/test-parquet dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","expert-generated","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"reason_code-search-net-python","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Nan-Do/reason_code-search-net-python","creator_name":"Fernando Tarin Morales","creator_url":"https://huggingface.co/Nan-Do","description":"\n\t\n\t\t\n\t\tDataset Card for \"reason_code-search-net-python\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is an instructional dataset for Python.The dataset contains five different kind of tasks.   \nGiven a Python 3 function:\n\nType 1: Generate a summary explaining what it does. (For example: This function counts the number of objects stored in the jsonl file passed as input.)\nType 2: Generate a summary explaining what its input parameters represent (\"For example: infile: a file descriptor of a file… See the full description on the dataset page: https://huggingface.co/datasets/Nan-Do/reason_code-search-net-python.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Risky_Choices","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Josephgflowers/Risky_Choices","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"Dataset Summary\nThe Risky Choices dataset is a derived version of the original choices13k dataset. It is designed to assist in training language models for tasks such as decision-making reasoning, explanation generation, and natural language processing. The dataset contains human decision rates on 13,006 risky choice problems, restructured into a natural language format suitable for various AI and ML applications.\nIn this processed version, each entry is presented as a decision-making scenario… See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Risky_Choices.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"reasoning-base-20k","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KingNish/reasoning-base-20k","creator_name":"Nishith Jain","creator_url":"https://huggingface.co/KingNish","description":"\n\t\n\t\t\n\t\tDataset Card for Reasoning Base 20k\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is designed to train a reasoning model. That can think through complex problems before providing a response, similar to how a human would. The dataset includes a wide range of problems from various domains (science, coding, math, etc.), each with a detailed chain of thought (COT) and the correct answer. The goal is to enable the model to learn and refine its reasoning process… See the full description on the dataset page: https://huggingface.co/datasets/KingNish/reasoning-base-20k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"German-RAG-ORPO-ShareGPT-HESSIAN-AI","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) ShareGPT-Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \nThe subsets can be for this training step are derived from 3 different sources:\n\nSauerkrautLM Preference Datasets:\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for training… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-ShareGPT-HESSIAN-AI.","first_N":5,"first_N_keywords":["question-answering","summarization","German","English","mit"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"logical-reasoning","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought… See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"ru-chain-of-thought-sharegpt","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/ru-chain-of-thought-sharegpt","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"Переведённая при помощи utrobinmv/t5_translate_en_ru_zh_small_1024 на русский язык версия датасета isaiahbjork/chain-of-thought-sharegpt.\n","first_N":5,"first_N_keywords":["text-generation","Russian","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"clevr-tr","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/clevr-tr","creator_name":"Berhan Türkü Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Lütfen resimdeki kırmızı metal nesnelerin sayısını belirtin.\"},\n    {\"from\": \"gpt\", \"value\": \"Resimde 3 kırmızı… See the full description on the dataset page: https://huggingface.co/datasets/berhaan/clevr-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Long-Context Alpaca-Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe ORPO Long Context Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \nThe subsets are derived from Synthetic generation inspired by Tencent's (“Scaling Synthetic Data Creation with 1,000,000,000 Personas”).\n\n\t\n\t\t\n\t\tDataset Structure… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Long-Context-Alpaca-HESSIAN-AI.","first_N":5,"first_N_keywords":["question-answering","summarization","German","English","mit"],"keywords_longer_than_N":true},
	{"name":"smolThink","keyword":"reasoning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AILaborant/smolThink","creator_name":"AI Laborant","creator_url":"https://huggingface.co/AILaborant","description":"A small dataset that contains reasoning and complex mathematical problems, along with physics, and a little bit of geometry.\n","first_N":5,"first_N_keywords":["English","gpl-3.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"smolBasisTolk","keyword":"reasoning","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AILaborant/smolBasisTolk","creator_name":"AI Laborant","creator_url":"https://huggingface.co/AILaborant","description":"Smol but a wide variety of topics dataset for basic AI training.\n","first_N":5,"first_N_keywords":["English","gpl-3.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"German-RAG-LLM-HARD-BENCHMARK","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-LLM-HARD Benchmark\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis German-RAG-LLM-HARD-BENCHMARK represents a specialized collection for evaluate language models with a focus on hard to solve RAG-specific capabilities. To evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/GRAG-LLM-HARD-BENCHMARK\nThe subsets are derived from Synthetic generation inspired by… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-HARD-BENCHMARK.","first_N":5,"first_N_keywords":["question-answering","summarization","German","English","mit"],"keywords_longer_than_N":true},
	{"name":"Olympiads","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/artnoage/Olympiads","creator_name":"Vaios Laschos","creator_url":"https://huggingface.co/artnoage","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 137830\nFiltered size: 42607\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/artnoage/Olympiads.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"Persian-MuSR","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParsBench/Persian-MuSR","creator_name":"ParsBench","creator_url":"https://huggingface.co/ParsBench","description":"\n\t\n\t\t\n\t\tPersian MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning on Persian Language\n\t\n\nThis is the Persian-translated version (using GPT-4o) of the original dataset MuSR.\n\n\t\n\t\t\n\t\tAcknowledgments\n\t\n\n\nSpecial thanks to AvalAI for sponsoring this project through their AvalAward program\nThis dataset was made possible by AvalAI's generous support and commitment to advancing Persian language AI research\n\n","first_N":5,"first_N_keywords":["question-answering","Persian","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Geoperception","keyword":"logical-reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/euclid-multimodal/Geoperception","creator_name":"Euclid Multimodal LLM","creator_url":"https://huggingface.co/euclid-multimodal","description":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\n\n\t\n\t\t\n\t\tDataset Card for Geoperception\n\t\n\nA Benchmark for Low-level Geometric Perception\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGeoperception is a benchmark focused specifically on accessing model's low-level visual perception ability in 2D geometry.\nIt is sourced from the Geometry-3K corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school… See the full description on the dataset page: https://huggingface.co/datasets/euclid-multimodal/Geoperception.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"VISCO","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/uclanlp/VISCO","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","description":"\n\t\n\t\t\n\t\n\t\n\t\tVISCO\n\t\n\nBenchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning\n🌐 Project | 📖 Paper | 💻 Github\n\n\nOutline:\n\nIntroduction\nData\nCitation\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nVISCO is a benchmark for evaluating the critique and correction capabilities of LVLMs. VISCO contains:\n\n1645 pairs of questions and LVLM-generated answers. Each answer includes a chain-of-thought with multiple reasonign steps.\n5604 step-wise annotations of critique, showing… See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/VISCO.","first_N":5,"first_N_keywords":["visual-question-answering","English","mit","1K<n<10K","arxiv:2412.02172"],"keywords_longer_than_N":true},
	{"name":"u-math","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/toloka/u-math","creator_name":"Toloka","creator_url":"https://huggingface.co/toloka","description":"U-MATH is a comprehensive benchmark of 1,100 unpublished university-level problems sourced from real teaching materials. \nIt is designed to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs). The dataset is balanced across six core mathematical topics and includes 20% of multimodal problems (involving visual elements such as graphs and diagrams). \nFor fine-grained performance evaluation results and detailed discussion, check out our paper.\n\n📊 U-MATH benchmark at… See the full description on the dataset page: https://huggingface.co/datasets/toloka/u-math.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"math-problems-greedy-vs-best-of-n","keyword":"mathematical-reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tandogan/math-problems-greedy-vs-best-of-n","creator_name":"Zeynep","creator_url":"https://huggingface.co/Tandogan","description":"\n\t\n\t\t\n\t\tProblem Solving Math Dataset - Greedy vs Best-of-N\n\t\n\nThis dataset contains mathematical problems and their solutions generated using two decoding strategies:\n\nGreedy Decoding: Generates a single deterministic solution.\nBest-of-N Decoding: Generates N solutions and selects the best one based on a scoring metric.\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset is created with a filtered subset of 20 level 1-3 problems from the MATH-500 dataset.\nTo have a balance across the levels, the… See the full description on the dataset page: https://huggingface.co/datasets/Tandogan/math-problems-greedy-vs-best-of-n.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"r103","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/r103","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"r1-reasoning-tr","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SoAp9035/r1-reasoning-tr","creator_name":"Ahmet Burhan Kayalı","creator_url":"https://huggingface.co/SoAp9035","description":"\n\t\n\t\t\n\t\tR1 Reasoning TR\n\t\n\nThis is an R1 reasoning dataset translated into Turkish, containing conversations between users and assistants. Thanks to lightblue for the dataset.\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is released under the Apache 2.0 License.\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Turkish","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"camel","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Wendong-Fan/camel","creator_name":"Wendong.Fan","creator_url":"https://huggingface.co/Wendong-Fan","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"amc_aime_self_improving","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/camel-ai/amc_aime_self_improving","creator_name":"CAMEL-AI.org","creator_url":"https://huggingface.co/camel-ai","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"amc_aime_distilled","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/camel-ai/amc_aime_distilled","creator_name":"CAMEL-AI.org","creator_url":"https://huggingface.co/camel-ai","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"camel_dataset_example_2","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Wendong-Fan/camel_dataset_example_2","creator_name":"Wendong.Fan","creator_url":"https://huggingface.co/Wendong-Fan","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"rank1-R1-MSMARCO","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jhu-clsp/rank1-R1-MSMARCO","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","description":"\n\t\n\t\t\n\t\trank1-R1-MSMARCO: Reasoning Outputs from MS MARCO Dataset\n\t\n\n📄 Paper | 🚀 GitHub Repository\nThis dataset contains outputs from Deepseek's R1 model on the MS MARCO passage dataset, used to train rank1. It showcases the reasoning chains and relevance judgments generated when determining document relevance for information retrieval queries.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe rank1-R1-MSMARCO dataset consists of reasoning chains and relevance judgments produced on the MS MARCO passage… See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/rank1-R1-MSMARCO.","first_N":5,"first_N_keywords":["text-generation","text-retrieval","document-retrieval","English","mit"],"keywords_longer_than_N":true},
	{"name":"Sky-T1_data_steps","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps","creator_name":"Shaked","creator_url":"https://huggingface.co/shakedzy","description":"\n\t\n\t\t\n\t\tSky-T1_data_steps\n\t\n\nThis dataset contains 182 samples taken from NovaSky-AI/Sky-T1_data_17k \ndataset and broken down to thinking steps. This dataset was used to train shakedzy/Sky-T1-32B-Steps \nLoRA adapter for step-by-step thinking.\nBreaking down the thought process to steps was done using Ollama's quantized version of Llama-3.2-1B.\nSee step_prompt file for the exact prompt used.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Columns\n\t\n\n\nid (int): row index of the sample in the original dataset (starts at 0)… See the full description on the dataset page: https://huggingface.co/datasets/shakedzy/Sky-T1_data_steps.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"distilabel-reasoning-R1-Llama-70B","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lightblue/distilabel-reasoning-R1-Llama-70B","creator_name":"Lightblue KK.","creator_url":"https://huggingface.co/lightblue","description":"\n\t\n\t\t\n\t\tHow this Data was made\n\t\n\nWe made this data through the following steps:\n\nSample English reasoning-style prompts from argilla/distilabel-reasoning-prompts.\nRemove similar prompts using text similarity based on BAAI/bge-m3 embeddings.\nTranslate English prompts to Japanese using gpt-4o-mini-2024-07-18.\nGenerate answers to prompts using deepseek-ai/DeepSeek-R1-Distill-Llama-70B.\nFilter responses (to ja_valid) which did not:\nFinish within 2048 tokens\nContain a valid <think> section\nHave… See the full description on the dataset page: https://huggingface.co/datasets/lightblue/distilabel-reasoning-R1-Llama-70B.","first_N":5,"first_N_keywords":["Japanese","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"countdown-numbers-3-8","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexjackson17/countdown-numbers-3-8","creator_name":"Alex Jackson","creator_url":"https://huggingface.co/alexjackson17","description":"\n\t\n\t\t\n\t\tCountdown Numbers Game Dataset\n\t\n\nThis dataset contains configurations and solutions for variations of the Countdown numbers game. Each example comprises a sequence of numbers, a target number, the computed solution (closest value), the arithmetic expression that achieves that value, the difference between the target and the computed value, and the final Countdown score.\n\n\t\n\t\t\n\t\tHuggingFace Download Links\n\t\n\n\n\n\n\n\t\n\t\t\nDataset Variant\nDataset Name\nDownload\n\n\n\t\t\nRandom… See the full description on the dataset page: https://huggingface.co/datasets/alexjackson17/countdown-numbers-3-8.","first_N":5,"first_N_keywords":["mit","1M - 10M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"countdown-numbers-3-8-nz","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexjackson17/countdown-numbers-3-8-nz","creator_name":"Alex Jackson","creator_url":"https://huggingface.co/alexjackson17","description":"\n\t\n\t\t\n\t\tCountdown Numbers Game Dataset\n\t\n\nThis dataset contains configurations and solutions for variations of the Countdown numbers game. Each example comprises a sequence of numbers, a target number, the computed solution (closest value), the arithmetic expression that achieves that value, the difference between the target and the computed value, and the final Countdown score.\n\n\t\n\t\t\n\t\tHuggingFace Download Links\n\t\n\n\n\n\n\n\t\n\t\t\nDataset Variant\nDataset Name\nDownload\n\n\n\t\t\nRandom… See the full description on the dataset page: https://huggingface.co/datasets/alexjackson17/countdown-numbers-3-8-nz.","first_N":5,"first_N_keywords":["mit","1M - 10M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"ethical-framework","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ktiyab/ethical-framework","creator_name":"Tiyab K.","creator_url":"https://huggingface.co/ktiyab","description":"\n\t\n\t\t\n\t\t1. Dataset Title\n\t\n\nEthical AI Decision-Making Training Data (Montreal Declaration Edition)\n\n\n\t\n\t\t\n\t\t2. Overview\n\t\n\nThis dataset contains carefully crafted scenarios (instructions) and detailed responses illustrating step-by-step ethical reasoning aligned with the principles outlined in the Montreal Declaration for Responsible AI. Each entry poses a complex ethical challenge and provides a reasoned solution while referencing the specific principle(s) being tested.  \nThese entries can… See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/ethical-framework.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Arabic-gsm8k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-gsm8k","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\n\t\n\t\t\n\t\tDataset Card for Arabic GSM8K\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nArabic GSM8K is an Arabic translation of the GSM8K (Grade School Math 8K) dataset, which contains high-quality linguistically diverse grade school math word problems. The original dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning, and this Arabic version aims to extend these capabilities to Arabic language models and applications.\nThe dataset… See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-gsm8k.","first_N":5,"first_N_keywords":["text2text-generation","Arabic","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"bilmecebench","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/bilmecebench","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\n\t\n\t\t\n\t\tBilmeceBench: Turkish Cultural Reasoning Benchmark\n\t\n\nBilmeceBench is a collection of traditional Turkish riddles designed to evaluate language models' cultural understanding and reasoning capabilities. The dataset contains authentic Turkish riddles (bilmece) that require both linguistic comprehension and cultural context to solve. Unfortunately, I currently lack the resources and time to conduct comprehensive model testing or evaluations, but users can utilize this dataset to perform… See the full description on the dataset page: https://huggingface.co/datasets/selimc/bilmecebench.","first_N":5,"first_N_keywords":["Turkish","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tDeepthink Reasoning Demo\n\t\n\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.… See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"upload-test","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/koookiy/upload-test","creator_name":"yaoke","creator_url":"https://huggingface.co/koookiy","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"dolphin-r1-italian","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WiroAI/dolphin-r1-italian","creator_name":"Wiro AI","creator_url":"https://huggingface.co/WiroAI","description":"\n  \n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n    \n    \n  \n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tDolphin R1 Italian 🐬\n\t\n\n\nDolphin-R1 is an Apache-2.0 English dataset curated by Eric Hartford and Cognitive Computations\nDolphin-R1-Italian is a Italian subset of the original dataset.\n\n\n\t\n\t\t\n\t\tSponsors\n\t\n\nTheir and Wiro AI's appreciation for the generous sponsors of Dolphin R1 - Without whom this dataset could not exist.\n\nDria https://x.com/driaforall - Inference Sponsor (DeepSeek)\nChutes… See the full description on the dataset page: https://huggingface.co/datasets/WiroAI/dolphin-r1-italian.","first_N":5,"first_N_keywords":["Italian","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"FineCorpus-WorkoutExercise","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise","creator_name":"Widi Fadhil","creator_url":"https://huggingface.co/padilfm","description":"\n\t\n\t\t\n\t\tFineCorpus-WorkoutExercise\n\t\n\nThis dataset contains structured workout exercise prompts for fine-tuning LLMs. \n\n\t\n\t\t\n\t\tStructure:\n\t\n\n\nconversations: Contains multi-turn dialogue pairs.\nsource: Indicates whether the data is from reasoning (Human) or generated by an AI model (LLM).\ncategory: Categorizes data into Q&A, Explain, Describe, Translate.\n\n\n\t\n\t\t\n\t\tUsage:\n\t\n\nTo use this dataset:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"padiflm/FineCorpus-WorkoutExercise\"… See the full description on the dataset page: https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise.","first_N":5,"first_N_keywords":["text-generation","Indonesian","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Medprompt-MedMCQA-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedMCQA-R1","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tMedprompt-MedMCQA-R1\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\n\nMedprompt-MedMCQA-R1 is a reasoning-augmented database designed for context retrieval in multiple-choice medical question answering. The dataset supports the development and evaluation of AI systems tailored to healthcare, particularly in tasks requiring enhanced contextual reasoning and retrieval-based assistance. By including structured reasoning and verified responses… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedMCQA-R1.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K<n<1M","arxiv:2409.15127"],"keywords_longer_than_N":true},
	{"name":"Medprompt-MedQA-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedQA-R1","creator_name":"HPAI@BSC (High Performance Artificial Intelligence at Barcelona Supercomputing Center)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\n\t\n\t\t\n\t\tMedprompt-MedQA-R1\n\t\n\n\n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n  \n    \n    \n  \n\n\n\nMedprompt-MedQA-R1 is a reasoning-augmented database designed for context retrieval in multiple-choice medical question answering. The dataset supports the development and evaluation of AI systems tailored to healthcare, particularly in tasks requiring enhanced contextual reasoning and retrieval-based assistance. By including structured reasoning and verified responses… See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Medprompt-MedQA-R1.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","100K<n<1M","arxiv:2409.15127"],"keywords_longer_than_N":true},
	{"name":"LogicPro","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jiangjin/LogicPro","creator_name":"jiangjin","creator_url":"https://huggingface.co/jiangjin","description":"\n  \n  \n  LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning\n\n\n\n  [📑 Paper] •\n  [🤗 HF Dataset] •\n  [👻 GitHub]\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tData description\n\t\n\n{\n  \"id\": \"logicpro_lc679_225531-43120\",\n  \"title\": \"24 Game\", # Title of the original leetcode algorithm problem.\n  \"difficulty\": \"Hard\",\n  \"content\": \"...\", # The questions of the original leetcode algorithm problem.\n  \"python\": \"...\", # The original gold python solution\n  \"test_input_string\": \"...\", # Current Test Case… See the full description on the dataset page: https://huggingface.co/datasets/jiangjin/LogicPro.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-1.5-Verifiable","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yentinglin/NuminaMath-1.5-Verifiable","creator_name":"Yen-Ting Lin","creator_url":"https://huggingface.co/yentinglin","description":"\n\t\n\t\t\n\t\tNuminaMath-1.5-Verifiable\n\t\n\nA filtered subset of NuminaMath-1.5, retaining only non-synthetic examples with valid answers.\nFiltering Criteria\n    •\tExcludes synthetic examples.\n    •\tKeeps only entries with non-empty, meaningful answers.\n    •\tRemoves generic placeholders like “proof” and “notfound.”\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"yentinglin/NuminaMath-1.5-Verifiable\")\n\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"MMLU-SR","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NiniCat/MMLU-SR","creator_name":"Cat Wang","creator_url":"https://huggingface.co/NiniCat","description":"\n\t\n\t\t\n\t\tMMLU-SR Dataset\n\t\n\nThis is the dataset for the paper \"MMLU-SR: A Benchmark for Stress-Testing Reasoning Capability of Large Language Models\".\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThis dataset contains three different variants:\n\nQuestion Only: Key terms in questions are replaced with dummy words and their definitions, while answer choices remain unchanged.\nAnswer Only: Key terms in answer choices are replaced with dummy words and their definitions, while questions remain unchanged. \nQuestion… See the full description on the dataset page: https://huggingface.co/datasets/NiniCat/MMLU-SR.","first_N":5,"first_N_keywords":["question-answering","multiple-choice-qa","expert-generated","expert-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personas… See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MuSR","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TAUR-Lab/MuSR","creator_name":"TAUR Lab at UT Austin","creator_url":"https://huggingface.co/TAUR-Lab","description":"\n\t\n\t\t\n\t\tMuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\n\t\n\n\n\t\n\t\t\n\t\tCreating murder mysteries that require multi-step reasoning with commonsense using ChatGPT!\n\t\n\nBy: Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett.\nView the dataset on our custom viewer and project website!\nCheck out the paper. Appeared at ICLR 2024 as a spotlight presentation!\nGit Repo with the source data, how to recreate the dataset (and create new ones!) here\n","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"RelatLogic","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/RelatLogic","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"RelatLogic: A Dataset for Comparative and Conditional Reasoning\nThis is a comparative logic and conditional reasoning dataset. \nEach data point has a premise, question, answer, reasoning and attribute.\nPlease cite this dataset using the provided BibTeX if you find it useful.\n@misc {sb_2025,\n    author       = { {SB} },\n    title        = { RelatLogic (Revision 15b1922) },\n    year         = 2025,\n    url          = { https://huggingface.co/datasets/shb777/RelatLogic },\n    doi          = {… See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic.","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"Tachibana-QVQ","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Tachibana-QVQ","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Tachibana-QVQ is a dataset containing code-reasoning and code-instruct responses across a wide variety of programming tasks.\nThis dataset contains:\n\n103k prompts from sequelbox/Tachibana, with all responses generated by Qwen/QVQ-72B-Preview.\nResponses demonstrate QVQ's code-reasoning ability and general code capabilities.\n\nResponses have not been filtered or edited at all: some responses will contain infinite thought loops, incomplete answers, inaccurate responses, or other identified or… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Tachibana-QVQ.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"Olympiads","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Metaskepsis/Olympiads","creator_name":"Metaskepsis","creator_url":"https://huggingface.co/Metaskepsis","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 32926\nFiltered size: 32926\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/Metaskepsis/Olympiads.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"Numina","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Metaskepsis/Numina","creator_name":"Metaskepsis","creator_url":"https://huggingface.co/Metaskepsis","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 210350\nFiltered size: 210350\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/Metaskepsis/Numina.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"synmath-1-dsv3-87k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","description":"\n\t\n\t\t\n\t\tsynmath-1-dsv3-87k\n\t\n\nsynmath-1-dsv3-87k is a dataset consisting of 86,700 math problems and their corresponding solutions, formatted in a chain-of-thought manner. The problems span 867 distinct mathematical domains, providing diverse and comprehensive coverage for fine-tuning smaller models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nsynmath-1-dsv3-87k contains synthetically generated math problems and step-by-step solutions designed to enhance mathematical reasoning in… See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/synmath-1-dsv3-87k.","first_N":5,"first_N_keywords":["text2text-generation","English","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"step_sft","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_sft","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","description":"Mix:\n\n\t\n\t\t\n数据集名称\n是否有step\n可用于PRM训练\n标签形式\nTitle\n备注\n\n\n\t\t\nGSM8K\n✅\n❌\n答案\nTraining Verifiers to Solve Math Word Problems\n\n\n\nMATH\n❌\n❌\n答案\nMeasuring Mathematical Problem Solving With the MATH Dataset\nNon-Step\n\n\nPRM800K\n✅\n✅\n正确类别\nLet's Verify Step by Step\nprompt deduplication\n\n\nMath-Shepherd\n✅\n✅\n正确类别\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nNot used\n\n\nProcessBench\n✅\n✅\n首个错误步骤\nProcessBench: Identifying Process Errors in Mathematical Reasoning\nonly label -1\n\n\n\t\n\n","first_N":5,"first_N_keywords":["text-generation","text-classification","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"Unaligned-Thinking-o1","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fhai50032/Unaligned-Thinking-o1","creator_name":"Low IQ Gen AI","creator_url":"https://huggingface.co/fhai50032","description":"\n\t\n\t\t\n\t\tUnaligned Thinking o1  - Uncensored Data from Gemini 2.0 Flash Thinking\n\t\n\nWelcome to the \"Unaligned Thinking\" (Unaligned-Thinking-o1) dataset, a collection of raw toxic, output from the Gemini 2.0 Flash Thinking \nThis Dataset is created using Prompt Engineering\nDisclaimer:\nThis Dataset contains highly toxic dataset , while still providing top-notch relevant answer very detailed and very verbose , only beaten by sonnet-3.5 gemini-exp-1206 O1\nThe data contained within this dataset is… See the full description on the dataset page: https://huggingface.co/datasets/fhai50032/Unaligned-Thinking-o1.","first_N":5,"first_N_keywords":["mit","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"step_prm","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaodongguaAIGC/step_prm","creator_name":"xiaodongguaAIGC","creator_url":"https://huggingface.co/xiaodongguaAIGC","description":"\n\t\n\t\t\n数据集名称\n是否有step\n可用于PRM训练\n标签形式\nTitle\n备注\n\n\n\t\t\nGSM8K\n✅\n❌\n答案\nTraining Verifiers to Solve Math Word Problems\n\n\n\nMATH\n❌\n❌\n答案\nMeasuring Mathematical Problem Solving With the MATH Dataset\nNon-Step\n\n\nPRM800K\n✅\n✅\n正确类别\nLet's Verify Step by Step\nprompt deduplication\n\n\nMath-Shepherd\n✅\n✅\n正确类别\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\nNot used\n\n\nProcessBench\n✅\n✅\n首个错误步骤\nProcessBench: Identifying Process Errors in Mathematical Reasoning\nonly label -1\n\n\n\t\n\n","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"Dataset_of_Russian_thinking","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking","creator_name":"Egor AI","creator_url":"https://huggingface.co/Egor-AI","description":"Ru\nRTD  \nОписание:Russian Thinking Dataset — это набор данных, предназначенный для обучения и тестирования моделей обработки естественного языка (NLP) на русском языке. Датасет ориентирован на задачи, связанные с генерацией текста, анализом диалогов и решением математических и логических задач.  \n\n\t\n\t\t\n\t\tОсновная информация:\n\t\n\n\nСплит: train  \nКоличество записей: 147.046\n\n\n\t\n\t\t\n\t\tЦели:\n\t\n\n\nОбучение моделей пониманию русского языка.  \nСоздание диалоговых систем с естественным взаимодействием.… See the full description on the dataset page: https://huggingface.co/datasets/Egor-AI/Dataset_of_Russian_thinking.","first_N":5,"first_N_keywords":["text-generation","Russian","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Numina_medium","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Metaskepsis/Numina_medium","creator_name":"Metaskepsis","creator_url":"https://huggingface.co/Metaskepsis","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 37133\nFiltered size: 37133\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/Metaskepsis/Numina_medium.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"Olympiads_medium","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Metaskepsis/Olympiads_medium","creator_name":"Metaskepsis","creator_url":"https://huggingface.co/Metaskepsis","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 13284\nFiltered size: 13240\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/Metaskepsis/Olympiads_medium.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"Olympiads_hard","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Metaskepsis/Olympiads_hard","creator_name":"Metaskepsis","creator_url":"https://huggingface.co/Metaskepsis","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 21525\nFiltered size: 21408\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/Metaskepsis/Olympiads_hard.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"Reflection-Dataset-v1","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mahiatlinux/Reflection-Dataset-v1","creator_name":"Maheswar KK","creator_url":"https://huggingface.co/mahiatlinux","description":"\n\t\n\t\t\n\t\tV2 is out!!! V2\n\t\n\n\n\t\n\t\t\n\t\tSimple \"Reflection\" method dataset inspired by mattshumer\n\t\n\n\n\t\n\t\t\n\t\tThis is the prompt and response version. Find ShareGPT version here\n\t\n\nThis dataset was synthetically generated using Glaive AI.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Reflection-Dataset-ShareGPT-v1","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mahiatlinux/Reflection-Dataset-ShareGPT-v1","creator_name":"Maheswar KK","creator_url":"https://huggingface.co/mahiatlinux","description":"\n\t\n\t\t\n\t\tV2 is out!!! V2\n\t\n\n\n\t\n\t\t\n\t\tSimple \"Reflection\" method dataset inspired by mattshumer\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT version. Find prompt and response pair dataset here\n\t\n\nThis dataset was synthetically generated using Glaive AI.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Reflection-Dataset-v2","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mahiatlinux/Reflection-Dataset-v2","creator_name":"Maheswar KK","creator_url":"https://huggingface.co/mahiatlinux","description":"\n\t\n\t\t\n\t\tSecond version of a simple \"Reflection\" method dataset inspired by mattshumer\n\t\n\n\n\t\n\t\t\n\t\tThis is the prompt and response version. Find ShareGPT version here\n\t\n\nThis dataset was synthetically generated using Glaive AI. There have been structure improvements and added more rows.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Reflection-Dataset-ShareGPT-v2","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mahiatlinux/Reflection-Dataset-ShareGPT-v2","creator_name":"Maheswar KK","creator_url":"https://huggingface.co/mahiatlinux","description":"\n\t\n\t\t\n\t\tSimple \"Reflection\" method dataset inspired by mattshumer\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT version. Find prompt and response pair dataset here\n\t\n\nThis dataset was synthetically generated using Glaive AI. There have been structure improvements and added more rows.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Reflection-Chinese-Dataset","keyword":"reflection","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stvlynn/Reflection-Chinese-Dataset","creator_name":"Steven Lynn","creator_url":"https://huggingface.co/stvlynn","description":"\n\t\n\t\t\n\t\tReflection-Chinese-Dataset·Reflection中文数据集\n\t\n\nBased on mahiatlinux/Reflection-Dataset-v2, translated using RA Translation Tool\n","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"DetailedReflection-Claude-v3_5-Sonnet","keyword":"reflection","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leafspark/DetailedReflection-Claude-v3_5-Sonnet","creator_name":"leafspark","creator_url":"https://huggingface.co/leafspark","description":"\n\t\n\t\t\n\t\tDetailedReflection-Claude-v3_5-Sonnet\n\t\n\nThis is a reflection dataset inspired by OpenAI's o1 model. It contains filtered prompts from anthracite-org/kalo-opus-instruct-22k-no-refusal.\nThe data was generated by Claude 3.5 Sonnet with a custom system prompt and sampling parameters on OpenRouter.\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"reflection-small-sonnet","keyword":"reflection","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n\t\n\t\t\n\t\tVerified reasoning examples\n\t\n\n","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"reflection-small-sonnet","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/reflection-small-sonnet","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\n\t\n\t\t\n\t\tVerified reasoning examples\n\t\n\n","first_N":5,"first_N_keywords":["text2text-generation","Italian","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sharegpt_cot_dataset","keyword":"reflection","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AiCloser/sharegpt_cot_dataset","creator_name":"Ai Closer","creator_url":"https://huggingface.co/AiCloser","description":"\n\t\n\t\t\n\t\tA data set inspired by the \"Reflection\" method, three-dimensional thinking and cot\n\t\n\n\n\t\n\t\t\n\t\tThis is the ShareGPT format.\n\t\n\nThe data set was generated using multiple llm synthesis.\n","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","English","Russian"],"keywords_longer_than_N":true},
	{"name":"BigGSM","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/BigGSM","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n  Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought\n\n\n\n      \n    | [ArXiv] | [🤗HuggingFace] |\n    \n    \n\n\n🌟 Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\t🔥News\n\t\n\n\n🎖️ Our work is accepted by NeurIPS 2024 (Oral).\n🔥 We have release benchmark on [🤗HuggingFace].\n🔥 The paper is also available on [ArXiv].\n\n\n\t\n\t\t\n\t\t💡 Motivation\n\t\n\nChain-of-Thought (CoT) reasoning has emerged as a… See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/BigGSM.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pisc-tr","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/berhaan/pisc-tr","creator_name":"Berhan Türkü Ay","creator_url":"https://huggingface.co/berhaan","description":"\n\t\n\t\t\n\t\tDataset Card for CoT\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources\n\t\n\n\nRepository: LLaVA-CoT GitHub Repository\nPaper: LLaVA-CoT on arXiv\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\ncat image.zip.part-* > image.zip #not uploaded yet\nunzip image.zip\n\nThe train.jsonl file contains the question-answering data and is structured in the following format:\n{\n  \"id\": \"example_id\",\n  \"image\": \"example_image_path\",\n  \"conversations\": [\n    {\"from\": \"human\", \"value\": \"Lütfen resimdeki kırmızı metal nesnelerin sayısını belirtin.\"}… See the full description on the dataset page: https://huggingface.co/datasets/berhaan/pisc-tr.","first_N":5,"first_N_keywords":["visual-question-answering","English","Turkish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ProcessBench","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Qwen/ProcessBench","creator_name":"Qwen","creator_url":"https://huggingface.co/Qwen","description":"\n\t\n\t\t\n\t\tProcessBench\n\t\n\nThis repository contains the dataset of the ProcessBench benchmark proposed by Qwen Team.\nYou can refer to our GitHub repository for the evaluation code and the prompt templates we use in this work.\nIf you find this work relevant or helpful to your work, please kindly cite us:\n@article{processbench,\n  title={ProcessBench: Identifying Process Errors in Mathematical Reasoning}, \n  author={\n    Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and… See the full description on the dataset page: https://huggingface.co/datasets/Qwen/ProcessBench.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"summexecedit","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/summexecedit","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tFactual Consistency in Summarization\n\t\n\nEvaluate your model's ability to detect and explain the factual inconsistency in summaries. This repo contains the benchmark from our paper \"SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits\".\n\n\t\n\t\t\n\t\tSummExecEdit Benchmark\n\t\n\nThis benchmark is built over our previous benchmark - SummEdits. Consistent summaries are used from SummEdits. New inconsistent and challenging summaries are generated using executable… See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/summexecedit.","first_N":5,"first_N_keywords":["text-classification","summarization","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mathematical_reasoning_preference","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Shekswess/mathematical_reasoning_preference","creator_name":"Bojan Jakimovski","creator_url":"https://huggingface.co/Shekswess","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\n\nTopic: Mathematical Reasoning\nDomains: Mathematics, Reasoning, Thinking\nFocus: This dataset can contain any type of mathematical reasoning and thinking.\nNumber of Entries: 493\nDataset Type: None\nModel Used: bedrock/us.amazon.nova-pro-v1:0\nLanguage: English\nAdditional Information: The dataset is designed to provide a wide range of mathematical reasoning examples.\nGenerated by: SynthGenAI Package\n\n","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"RobustFT","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/RobustFT","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"\n\t\n\t\t\n\t\tRobustFT Dataset\n\t\n\nThis dataset is part of the RobustFT project: Robust Supervised Fine-tuning for Large Language Models under Noisy Response. The dataset contains various test cases with different noise ratios for training and evaluating robust fine-tuning approaches.\nOur paper: https://huggingface.co/papers/2412.14922\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nRobustFT/\n├── arc/\n│ │── noisy30.csv\n│ │── noisy50.csv\n│ │── noisy70.csv\n│ ├── labeled.csv\n│ └── test.csv\n├── drop/\n│ │── noisy30.csv\n│… See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/RobustFT.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TextBooksPersonaHub-FR","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR","creator_name":"nacer","creator_url":"https://huggingface.co/drodin","description":"\n\t\n\t\t\n\t\tTextBooksPersonaHub\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe TextBooksPersonaHub dataset is an extension of the proj-persona/PersonaHub dataset, created using the technique described in the paper Textbooks Are All You Need II. This dataset contains synthetically generated \"textbook-like\" passages tailored in french to specific personas, aimed at enhancing language model training with high-quality and diverse content.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nThe original personas… See the full description on the dataset page: https://huggingface.co/datasets/drodin/TextBooksPersonaHub-FR.","first_N":5,"first_N_keywords":["text2text-generation","French","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"SWAP","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sxiong/SWAP","creator_name":"Siheng Xiong","creator_url":"https://huggingface.co/sxiong","description":"\n\t\n\t\t\n\t\tSWAP: A Synthetic Dataset for Complex Reasoning with Trajectories and Process Supervision\n\t\n\nThis repository contains the data for the paper Deliberate Reasoning for LLMs as Structure-aware Planning with Accurate World Model.\nSWAP (Structure-aware Planning) solves complex reasoning by introducing a Generator-Discriminator architecture, and incorporates structural information to guide the reasoning process and provides a soft verification mechanism over the steps.\nWe generate the… See the full description on the dataset page: https://huggingface.co/datasets/sxiong/SWAP.","first_N":5,"first_N_keywords":["text2text-generation","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Numina","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/artnoage/Numina","creator_name":"Vaios Laschos","creator_url":"https://huggingface.co/artnoage","description":"\n\t\n\t\t\n\t\tNumina-Olympiads\n\t\n\nFiltered NuminaMath-CoT dataset containing only olympiads problems with valid answers.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSplit: train\nOriginal size: 41012\nFiltered size: 38772\nSource: olympiads\nAll examples contain valid boxed answers\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a filtered version of the NuminaMath-CoT dataset, containing only problems from olympiad sources that have valid boxed answers. Each example includes:\n\nA mathematical word problem\nA… See the full description on the dataset page: https://huggingface.co/datasets/artnoage/Numina.","first_N":5,"first_N_keywords":["text-generation","expert-generated","expert-generated","monolingual","AI-MO/NuminaMath-CoT"],"keywords_longer_than_N":true},
	{"name":"MAmmoTH-VL-Instruct-12M","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M","creator_name":"MAmmoTH-VL","creator_url":"https://huggingface.co/MAmmoTH-VL","description":"\n\t\n\t\t\n\t\tMAmmoTH-VL-Instruct-12M\n\t\n\n🏠 Homepage | 🤖 MAmmoTH-VL-8B | 💻 Code | 📄 Arxiv | 📕 PDF | 🖥️ Demo\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nOur simple yet scalable visual instruction data rewriting pipeline consists of three steps: manual data source collection, rewriting using MLLMs/LLMs, and filtering via the same MLLM as a judge. Examples below illustrate transformations in math and science categories, showcasing detailed, step-by-step responses.\n\n\t\n\t\t\n\t\tThe data distribution of… See the full description on the dataset page: https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","English","apache-2.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"SkunkworksAI-reasoning-0.01-ko","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/youjunhyeok/SkunkworksAI-reasoning-0.01-ko","creator_name":"유준혁","creator_url":"https://huggingface.co/youjunhyeok","description":"SkunkworksAI/reasoning-0.01 데이터셋을 nayohan/llama3-instrucTrans-enko-8b 모델을 사용해 번역했습니다.\nThanks for SkunkworksAI and nayohan.\n\n\n\t\n\t\t\n\t\t원본\n\t\n\n\n\t\n\t\t\n\t\treasoning-0.01 subset\n\t\n\nsynthetic dataset of reasoning chains for a wide variety of tasks.\nwe leverage data like this across multiple reasoning experiments/projects.\nstay tuned for reasoning models and more data.\nThanks to Hive Digital Technologies (https://x.com/HIVEDigitalTech) for their compute support in this project and beyond.\n","first_N":5,"first_N_keywords":["text-generation","Korean","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NL-Eye","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MorVentura/NL-Eye","creator_name":"Mor Ventura","creator_url":"https://huggingface.co/MorVentura","description":"\n\t\n\t\t\n\t\tNL-Eye Benchmark\n\t\n\nWill a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? \nRecent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. \nNL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of… See the full description on the dataset page: https://huggingface.co/datasets/MorVentura/NL-Eye.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"bbh-fr","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/le-leadboard/bbh-fr","creator_name":"le-leadboard","creator_url":"https://huggingface.co/le-leadboard","description":"\n\t\n\t\t\n\t\tDataset Card for bbh-fr\n\t\n\nle-leadboard/bbh-fr fait partie de l'initiative OpenLLM French Leaderboard, proposant une adaptation française du benchmark BIG-Bench Hard (BBH).\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBBH-fr est l'adaptation française d'une suite de 23 tâches BIG-Bench particulièrement exigeantes. Ces tâches ont été sélectionnées car elles représentaient initialement des défis où les modèles de langage n'atteignaient pas les performances humaines moyennes.\nCatégories de tâches incluses… See the full description on the dataset page: https://huggingface.co/datasets/le-leadboard/bbh-fr.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","French","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TimeSeriesExam1","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AutonLab/TimeSeriesExam1","creator_name":"Auton Lab","creator_url":"https://huggingface.co/AutonLab","description":"\n\t\n\t\t\n\t\tDataset Card for TimeSeriesExam-1\n\t\n\nThis dataset provides Question-Answer (QA) pairs for the paper TimeSeriesExam: A Time Series Understanding Exam. Example inference code can be found here.\n\n\t\n\t\t\n\t\t📖Introduction\n\t\n\nLarge Language Models (LLMs) have recently demonstrated a remarkable ability to model time series data. These capabilities can be partly explained if LLMs understand basic time series concepts. However, our knowledge of what these models understand about time series data… See the full description on the dataset page: https://huggingface.co/datasets/AutonLab/TimeSeriesExam1.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"thinker","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/minchyeom/thinker","creator_name":"l","creator_url":"https://huggingface.co/minchyeom","description":"A Chain-of-Thought (CoT) dataset that contains traces of complex and sophisticated reasoning, to mimic the \"thinking\" process of OpenAI's o1. Wrap the contents of the reasoning column in some XML tag (such as <reasoning>).\nRaw .jsonl dataset file can be found under the Files and Versions tab.\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-merged","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"dpo-merged-binarized","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-merged-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-merged-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"German-RAG-ORPO-Alpaca-HESSIAN-AI","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-ORPO (Odds Ratio Preference Optimization) Alpaca-Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe ORPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. \nThe subsets can be for this training step are derived from 2 different sources:\n\nSauerkrautLM Preference Datasets:\nSauerkrautLM-Fermented-GER-DPO:  is a specialized dataset designed for training… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI.","first_N":5,"first_N_keywords":["question-answering","summarization","German","English","mit"],"keywords_longer_than_N":true},
	{"name":"VMCBench","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/suyc21/VMCBench","creator_name":"Yuchang Su","creator_url":"https://huggingface.co/suyc21","description":"\n\t\n\t\t\n\t\tVMCBench (Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation)\n\t\n\n🌐 Homepage | 🤗 Dataset | 📖 arXiv | GitHub\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Introduction\n\t\n\nWe introduce VMCBench: a benchmark that unifies 20 existing visual question answering (VQA) datasets into a consistent multiple-choice format. VMCBench spans a diverse array of visual and linguistic contexts, rigorously testing various model capabilities. By… See the full description on the dataset page: https://huggingface.co/datasets/suyc21/VMCBench.","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Image","Text"],"keywords_longer_than_N":true},
	{"name":"finance-reasoning-turkish","keyword":"reasoning","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/finance-reasoning-turkish","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tDataset Card for Turkish Advanced Reasoning Dataset (Finance Q&A)\n\t\n\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThis dataset is licensed under the Academic Use Only License. It is intended solely for academic and research purposes. Commercial use is strictly prohibited. For more details, refer to the LICENSE file.\nCitation: If you use this dataset in your research, please cite it as follows:\n@dataset{turkish_advanced_reasoning_finance_qa,\n  title = {Turkish Advanced Reasoning Dataset for Finance Q\\&A}… See the full description on the dataset page: https://huggingface.co/datasets/emre/finance-reasoning-turkish.","first_N":5,"first_N_keywords":["text-generation","Turkish","English","afl-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"deepseek-math-dataset","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/madaibaba/deepseek-math-dataset","creator_name":"Alex Hou","creator_url":"https://huggingface.co/madaibaba","description":"\n\t\n\t\t\n\t\tDeepSeek Math Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe DeepSeek Math Dataset is a high-quality dataset designed for distilling smaller mathematical reasoning models. It is derived from the official DeepSeek-Prover-V1 dataset and tailored to improve the efficiency of lightweight models while preserving strong mathematical problem-solving capabilities.\nThis dataset has been used to distill Qwen2.5-1.5B, achieving impressive performance on mathematical reasoning tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/madaibaba/deepseek-math-dataset.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"R-PRM","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kevinpro/R-PRM","creator_name":"Shuaijie She","creator_url":"https://huggingface.co/kevinpro","description":"\n\t\n\t\t\n\t\t📘 R-PRM Dataset (SFT + DPO)\n\t\n\nThis dataset is developed for training Reasoning-Driven Process Reward Models (R-PRM), proposed in our ACL 2025 paper. It consists of two stages:\n\nSFT (Supervised Fine-Tuning): collected from strong LLMs prompted with limited annotated examples, enabling reasoning-style evaluation.\nDPO (Direct Preference Optimization): constructed by sampling multiple reasoning trajectories and forming preference pairs without additional labels.\n\nThese datasets are used… See the full description on the dataset page: https://huggingface.co/datasets/kevinpro/R-PRM.","first_N":5,"first_N_keywords":["text-generation","reinforcement-learning","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tw-reasoning-instruct-50k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-reasoning-instruct-50k\n\t\n\n\n\ntw-reasoning-instruct-50k 是一個精選的 繁體中文（台灣） 推理資料集，旨在提升語言模型於逐步邏輯思考、解釋生成與語言理解等任務中的表現。資料內容涵蓋日常思辨、教育對話、法律推理等多元主題，並結合「思考步驟」與「最終答案」的結構設計，引導模型以更清晰、條理分明的方式進行推論與回應，特別強調符合台灣本地語言與文化背景的應用需求。\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n本資料集專為發展具備強大推理能力的繁體中文大型語言模型（Large Reasoning Models, LRM）所設計，內容深度結合台灣的語言與文化脈絡。每筆資料通常包含使用者的提問、模型的回應，以及清楚的推理過程。資料集設計目標為培養模型具備類人邏輯的逐步思考與解釋能力。\n此資料集適用於訓練與評估以下任務：\n\n台灣社會的日常推理\n教育性對話\n以解釋為導向的生成任務… See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-reasoning-instruct-50k.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SFT_54k_reasoning","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/SFT_54k_reasoning\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/SFT_54k_reasoning is a processed version of the XuHu6736/s1_54k_filter_with_isreasoning dataset, specifically reformatted for instruction fine-tuning (SFT) of language models.\nThe original question and solution pairs have been converted into an instruction-following format. Critically, the isreasoning_score and isreasoning labels from the parent dataset are preserved, allowing for targeted SFT on… See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/SFT_54k_reasoning.","first_N":5,"first_N_keywords":["XuHu6736 (formatting and derivation)","derived from XuHu6736/s1_54k_filter_with_isreasoning","derived from source datasets","monolingual","XuHu6736/s1_54k_filter_with_isreasoning"],"keywords_longer_than_N":true},
	{"name":"events-scheduling","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/events-scheduling","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\n\t\n\t\t\n\t\t🗓️ Events Scheduling dataset\n\t\n\nSmall dataset to train Language Models to create a schedule from a list of events and priorities.\nI used this dataset to train the 👑 🗓️ anakin87/qwen-scheduler-7b-grpo model using GRPO.\n➡️ Read the full story in my blog post.\nFind all the code in the GitHub repository.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tThe problem\n\t\n\nGiven a list of events and priorities, we ask the model to create a schedule that maximizes the total duration of selected events, weighted by priority.In… See the full description on the dataset page: https://huggingface.co/datasets/anakin87/events-scheduling.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"math-story-problems","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/azminetoushikwasi/math-story-problems","creator_name":"Azmine Toushik Wasi","creator_url":"https://huggingface.co/azminetoushikwasi","description":"\n\t\n\t\t\n\t\tMath Story Problems Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains mathematical word problems presented in multiple formats, from direct equations to complex story-based scenarios. It is designed for training and evaluating language models on mathematical reasoning tasks.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is split into three parts:\n\nTrain: 131,072 samples\nValidation: 1,024 samples\nTest: 3,072 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n{\n    \"eq_qs\": \"string\",      # Equation… See the full description on the dataset page: https://huggingface.co/datasets/azminetoushikwasi/math-story-problems.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","text-generation","extractive-qa","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"Titanium2-DeepSeek-R1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Click here to support our open-source dataset and model releases!\nTitanium2-DeepSeek-R1 is a dataset focused on architecture and DevOps, testing the limits of DeepSeek R1's architect and coding skills!\nThis dataset contains:\n\n32.4k synthetically generated prompts focused on architecture, cloud, and DevOps. All responses are generated using DeepSeek R1. Primary areas of expertise are architecture (problem solving, scenario analysis, coding, full SDLC) and DevOps (Azure, AWS, GCP, Terraform… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Titanium2-DeepSeek-R1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"TARA_Turkish_LLM_Benchmark","keyword":"reasoning","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tTARA: Turkish Advanced Reasoning Assessment Veri Seti\n\t\n\n\n*Img Credit: Open AI ChatGPT\n**English version is given below.**\n\n Evaluation Notebook / Değerlendirme Not Defteri\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nTARA (Turkish Advanced Reasoning Assessment), Türkçe dilindeki Büyük Dil Modellerinin (LLM'ler) gelişmiş akıl yürütme yeteneklerini çoklu alanlarda ölçmek için tasarlanmış, zorluk derecesine göre sınıflandırılmış bir benchmark veri setidir. Bu veri seti, LLM'lerin sadece bilgi… See the full description on the dataset page: https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark.","first_N":5,"first_N_keywords":["question-answering","text-generation","Turkish","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TARA_Turkish_LLM_Benchmark","keyword":"mathematical-reasoning","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark","creator_name":"Davut Emre TASAR","creator_url":"https://huggingface.co/emre","description":"\n\t\n\t\t\n\t\tTARA: Turkish Advanced Reasoning Assessment Veri Seti\n\t\n\n\n*Img Credit: Open AI ChatGPT\n**English version is given below.**\n\n Evaluation Notebook / Değerlendirme Not Defteri\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nTARA (Turkish Advanced Reasoning Assessment), Türkçe dilindeki Büyük Dil Modellerinin (LLM'ler) gelişmiş akıl yürütme yeteneklerini çoklu alanlarda ölçmek için tasarlanmış, zorluk derecesine göre sınıflandırılmış bir benchmark veri setidir. Bu veri seti, LLM'lerin sadece bilgi… See the full description on the dataset page: https://huggingface.co/datasets/emre/TARA_Turkish_LLM_Benchmark.","first_N":5,"first_N_keywords":["question-answering","text-generation","Turkish","afl-3.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CriticBench","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-agents/CriticBench","creator_name":"LLM-Agents","creator_url":"https://huggingface.co/llm-agents","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nCriticBench is a comprehensive benchmark designed to assess LLMs' abilities to generate, critique/discriminate and correct reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: THU\nFunded by [optional]: [More… See the full description on the dataset page: https://huggingface.co/datasets/llm-agents/CriticBench.","first_N":5,"first_N_keywords":["question-answering","text-classification","text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"MMOS","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cyzhh/MMOS","creator_name":"Yezeng Chen","creator_url":"https://huggingface.co/cyzhh","description":"ArXiv | Models | Data | Code | \nYou can download the dataset as follows\nfrom datasets import load_dataset\nds = load_dataset(\"cyzhh/MMOS\")\n\n\n\t\n\t\t\n\t\n\t\n\t\tSchema\n\t\n\nEach dataset row has the following structure\n{\n  \"idx\": ..., # problem id\n  \"prompt\": ..., # problem \n  \"completion\": ... # reasoning path with python\n}\n\n\n\t\t\n\t\n\t\tLicense\n\t\n\nWe do not alter the license of any of the underlying data.\n\n\t\n\t\t\n\t\tCitation\n\t\n\nFor the MMOS, cite \n@misc{chen2024empirical,\n      title={An Empirical Study of Data… See the full description on the dataset page: https://huggingface.co/datasets/cyzhh/MMOS.","first_N":5,"first_N_keywords":["question-answering","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"FLenQA","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alonj/FLenQA","creator_name":"Alon Jacoby","creator_url":"https://huggingface.co/alonj","description":"Same Task, More tokens\nthe Impact of Input Length on the Reasoning Performance of Large Language Models\nMosh Levy[*,1], Alon Jacoby[*,1], Yoav Goldberg[1,2]\n\n\nPlease see full details in our pre-print on arxiv\n \n\n\n\t\n\t\t\n\t\tWhat is this all about?\n\t\n\nWe explore the impact of extending input lengths on the capabilities of Large Language Models (LLMs). \nDespite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood.\nHere, we aim to… See the full description on the dataset page: https://huggingface.co/datasets/alonj/FLenQA.","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Retrieve-Pile","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile","creator_name":"Query-of-CC","creator_url":"https://huggingface.co/Query-of-CC","description":"Retrieve-Pile (Knowledge-Pile) is a knowledge-related data leveraging Retrieve-from-CC (We also called this method as \"Query of CC\")，a total of 735GB disk size and 188B tokens (using Llama2 tokenizer). \n\n\t\n\t\t\n\t\tRetrieve-from-CC\n\t\n\nJust like the figure below, we initially collected seed information in some specific domains, such as keywords, frequently asked questions, and textbooks, to serve as inputs for the Query Bootstrapping stage. Leveraging the great generalization capability of large… See the full description on the dataset page: https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ZharfaTech-Open-Platypus-Persian-Farsi","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZharfaTech/ZharfaTech-Open-Platypus-Persian-Farsi","creator_name":"ZharfaTech","creator_url":"https://huggingface.co/ZharfaTech","description":"\n\t\n\t\t\n\t\tPersian Open-Platypus\n\t\n\n\n\t\n\t\t\n\t\tAbout ZharfaTech\n\t\n\nZharfaTech is a pioneer in developing Language Learning Models (LLMs) tailored for the Persian language, aiming to empower over 100 million Persian speakers worldwide. Our mission encompasses bridging the digital divide in LLM-related services like content generation, customer relationship systems, and more, with a dual approach of fostering open-source collaboration and delivering high-value, specialized closed-source solutions.… See the full description on the dataset page: https://huggingface.co/datasets/ZharfaTech/ZharfaTech-Open-Platypus-Persian-Farsi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","summarization","question-answering","Persian"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_ground_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_plan_iterative","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\t🪄 Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  🌐[Website]  \n  📝[Paper]  \n  🤗[Data]  \n  🤗[Model]  \n  🤗[Demo]  \n\n\nWe introduce 🪄Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\n🧩 Modular Architecture:\n🧩 Lumos consists of planning, grounding… See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MathCodeInstruct","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathCodeInstruct","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n\t\n\nPaper: https://arxiv.org/pdf/2310.03731.pdf\nRepo: https://github.com/mathllm/MathCoder\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.\n\n\t\n\t\t\nBase Model: Llama-2\nBase Model: Code Llama\n\n\n\t\t\nMathCoder-L-7B\nMathCoder-CL-7B\n\n\nMathCoder-L-13B\nMathCoder-CL-34B\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tTraining Data\n\t\n\nThe models… See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathCodeInstruct.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"MathCodeInstruct-Plus","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MathLLMs/MathCodeInstruct-Plus","creator_name":"LLMs for Reasoning","creator_url":"https://huggingface.co/MathLLMs","description":"\n\t\n\t\t\n\t\tMathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n\t\n\nPaper: https://arxiv.org/pdf/2310.03731.pdf\nRepo: https://github.com/mathllm/MathCoder\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWe introduce MathCoder, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving.\n\n\t\n\t\t\nBase Model: Llama-2\nBase Model: Code Llama\n\n\n\t\t\nMathCoder-L-7B\nMathCoder-CL-7B\n\n\nMathCoder-L-13B\nMathCoder-CL-34B\n\n\n\t\n\n\n\t\n\t\n\t\n\t\tTraining Data\n\t\n\nThe models… See the full description on the dataset page: https://huggingface.co/datasets/MathLLMs/MathCodeInstruct-Plus.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"AutoMathText","keyword":"mathematical-reasoning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/AutoMathText","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"🎉 This work, introducing the AutoMathText dataset and the AutoDS method, has been accepted to The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings)! 🎉\n\n\t\n\t\t\n\t\tAutoMathText\n\t\n\nAutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts. It's a compilation sourced from a diverse range of platforms including various websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack). This rich repository… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/AutoMathText.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"AutoMathText","keyword":"reasoning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/AutoMathText","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"🎉 This work, introducing the AutoMathText dataset and the AutoDS method, has been accepted to The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings)! 🎉\n\n\t\n\t\t\n\t\tAutoMathText\n\t\n\nAutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts. It's a compilation sourced from a diverse range of platforms including various websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack). This rich repository… See the full description on the dataset page: https://huggingface.co/datasets/math-ai/AutoMathText.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"mathematical-reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:… See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"StackMathQA","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\tStackMathQA\n\t\n\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\n\n\t\n\t\t\n\t\tConfigs\n\t\n\nconfigs:\n- config_name: stackmathqa1600k\n  data_files: data/stackmathqa1600k/all.jsonl\n  default: true\n- config_name: stackmathqa800k\n  data_files:… See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"tamil-orca","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/azharmo/tamil-orca","creator_name":"Mohamed Azharudeen M","creator_url":"https://huggingface.co/azharmo","description":"\n\t\n\t\t\n\t\tTamil Orca-Style Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository hosts the Tamil Orca-style dataset, meticulously curated to enhance the reasoning capabilities of large language models in Tamil. The dataset is a fusion of translations and responses generated by GPT-4 and Gemini models.\n\nContent: The dataset contains three columns - 'Instruction', 'Query', and 'Answer'. \nPurpose: It's designed to significantly improve the reasoning capability of AI language models in Tamil. \nUsage: If… See the full description on the dataset page: https://huggingface.co/datasets/azharmo/tamil-orca.","first_N":5,"first_N_keywords":["text-generation","Tamil","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"tamil-orca-transliterated","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/azharmo/tamil-orca-transliterated","creator_name":"Mohamed Azharudeen M","creator_url":"https://huggingface.co/azharmo","description":"\n\t\n\t\t\n\t\tTamil Orca-Style Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository hosts the Tamil Orca-style transliterated dataset, meticulously curated to enhance the reasoning capabilities of large language models in Tamil. The dataset is a transliterated version tamil-orca fusion of translations and responses generated by GPT-4 and Gemini models.\n\nContent: The dataset contains three columns - 'Instruction', 'Query', and 'Answer'. \nPurpose: It's designed to significantly improve the reasoning… See the full description on the dataset page: https://huggingface.co/datasets/azharmo/tamil-orca-transliterated.","first_N":5,"first_N_keywords":["text-generation","Tamil","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"Multilingual-Benchmark","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TaiMingLu/Multilingual-Benchmark","creator_name":"TaiMing","creator_url":"https://huggingface.co/TaiMingLu","description":"These are the GSM8K and ARC dataset translated by Google Translate. \nBibTex\n@misc{lu2024languagecountslearnunlearn,\n      title={Every Language Counts: Learn and Unlearn in Multilingual LLMs}, \n      author={Taiming Lu and Philipp Koehn},\n      year={2024},\n      eprint={2406.13748},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2406.13748}, \n}\n\n","first_N":5,"first_N_keywords":["zero-shot-classification","question-answering","translation","English","German"],"keywords_longer_than_N":true},
	{"name":"FOL-nli","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/FOL-nli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\n\t\n\t\t\n\t\tDataset Card for \"FOL-nli\"\n\t\n\nhttps://github.com/sileod/unigram/\nhttps://arxiv.org/abs/2406.11035\nCitation:\n@article{sileo2024scaling,\n  title={Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},\n  author={Sileo, Damien},\n  journal={arXiv preprint arXiv:2406.11035},\n  year={2024}\n}\n\n","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","original","English"],"keywords_longer_than_N":true},
	{"name":"LoGiPT-data","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jzfeng/LoGiPT-data","creator_name":"Jamie Jiazhan Feng","creator_url":"https://huggingface.co/jzfeng","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThese are the training data for LoGiPT from NAACL'24 paper: \"Language Models can be Deductive Solvers\".\n\nLoGiPT-data-ProofWriter.json: Instruction-tuning data for LoGiPT constructed from ProofWriter.\nLoGiPT-data-PrOntoQA.json: Instruction-tuning data for LoGiPT constructed from PrOntoQA.\n\nAll training examples are organised in Json-format and Vicuna-style.\n\n\t\n\t\t\n\t\n\t\n\t\tIf you find this data helpful, please cite our NAACL'24 paper: (or Arxiv version:… See the full description on the dataset page: https://huggingface.co/datasets/jzfeng/LoGiPT-data.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"rulebreakers","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jason-c/rulebreakers","creator_name":"Jason Chan","creator_url":"https://huggingface.co/jason-c","description":"jason-c/rulebreakers dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Inf-Bench","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Chrishuanhuan/Inf-Bench","creator_name":"Huanhuan","creator_url":"https://huggingface.co/Chrishuanhuan","description":"\n\t\n\t\t\n\t\tInfinite Bench (Inf-Bench)\n\t\n\nThis repository contains the Infinite Bench (Inf-Bench), a novel evaluation framework for assessing the performance of Vision-Language Models (VLMs) in spatial deformation reasoning tasks.\n\n\t\n\t\t\n\t\tFiles\n\t\n\nThe dataset is available as a parquet file on Hugging Face, ready for processing with HF Datasets. It can be loaded using the following code:\nfrom datasets import load_dataset\ninf_bench = load_dataset(\"Chrishuanhuan/Inf-Bench\")\n\n\n\t\n\t\t\n\t\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/Chrishuanhuan/Inf-Bench.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"SocratesEval","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhx123/SocratesEval","creator_name":"zz","creator_url":"https://huggingface.co/zhx123","description":"zhx123/SocratesEval dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"MARBLE","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrble/MARBLE","creator_name":"marble","creator_url":"https://huggingface.co/mrble","description":"mrble/MARBLE dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"WirelessMathBench","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XINLI1997/WirelessMathBench","creator_name":"XinLi","creator_url":"https://huggingface.co/XINLI1997","description":"\n\t\n\t\t\n\t\tWirelessMathBench\n\t\n\nWirelessMathBench is a benchmark designed to test the mathematical reasoning and symbolic problem-solving capabilities of large language models (LLMs) in wireless communications. It contains expert-level, LaTeX-formatted questions spanning key topics such as:\n\nMultiple Input Multiple Output (MIMO)\nReconfigurable Intelligent Surfaces (RIS)\nIntegrated Sensing and Communications (ISAC)\nUAV-enabled networks\nChannel estimation and signal processing\n\nEach question is… See the full description on the dataset page: https://huggingface.co/datasets/XINLI1997/WirelessMathBench.","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Italian-Reasoning-Logic-2k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Dddixyy/Italian-Reasoning-Logic-2k","creator_name":"Davide Brunori","creator_url":"https://huggingface.co/Dddixyy","description":"Dddixyy/Italian-Reasoning-Logic-2k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Italian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"open-r1-sampled","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ykarout/open-r1-sampled","creator_name":"yehya","creator_url":"https://huggingface.co/ykarout","description":"ykarout/open-r1-sampled dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Deep_math","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Compumacy/Deep_math","creator_name":"Compumacy","creator_url":"https://huggingface.co/Compumacy","description":"\n\t\n\t\t\n\t\tDeepMath-103K\n\t\n\n\n  \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n      \n    \n    \n      \n        \n\t\n\t\t\n\t\t🔥 News\n\t\n\n\nMay 8, 2025: We found that 48 samples contained hints that revealed the answers. The relevant questions have now been revised to remove the leaked answers.\nApril 14, 2025: We release DeepMath-103K, a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT. We open source:… See the full description on the dataset page: https://huggingface.co/datasets/Compumacy/Deep_math.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"knights_and_knaves_reasoning","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/safal312/knights_and_knaves_reasoning","creator_name":"Safal Shrestha","creator_url":"https://huggingface.co/safal312","description":"\n\t\n\t\t\n\t\t🧠 Dataset Card: Knights and Knaves Reasoning Traces (QwQ)\n\t\n\n\n\t\n\t\t\n\t\t📝 Dataset Summary\n\t\n\nThis dataset contains reasoning traces generated by the QwQ system applied to the Knights and Knaves logic puzzles. Each example in the dataset includes a natural language puzzle, the reasoning steps taken by the model, and the tokens used. These traces reflect intermediate reasoning steps that help interpret how QwQ reaches a conclusion.\n\n\t\n\t\t\n\t\t📄 Associated Paper\n\t\n\nTitle: Warm Up Before You… See the full description on the dataset page: https://huggingface.co/datasets/safal312/knights_and_knaves_reasoning.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"math-mini-shareGPT","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/enosislabs/math-mini-shareGPT","creator_name":"Enosis Labs, Inc.","creator_url":"https://huggingface.co/enosislabs","description":"\n\t\n\t\t\n\t\tEnosis Labs Mathematics Reasoning Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Enosis Labs Mathematics Reasoning Dataset is a curated collection of mathematical problems with detailed, step-by-step solutions. It is designed to foster and benchmark mathematical reasoning in AI models. The dataset covers a wide range of topics and difficulty levels (intermediate to advanced), including:\n\nArithmetic: Percentages, discounts, ratios, proportions, and more.\nAlgebra: Linear and quadratic equations… See the full description on the dataset page: https://huggingface.co/datasets/enosislabs/math-mini-shareGPT.","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"error-detection-positives","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-positives\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines positives samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 50 samples\nmath: 53 samples\nmetamathqa: 93 samples\norca_math: 96 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa, orca_math)\nquestion: The… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"error-detection-positives","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives","creator_name":"PARC","creator_url":"https://huggingface.co/PARC-DATASETS","description":"\n\t\n\t\t\n\t\terror-detection-positives\n\t\n\nThis dataset is part of the PARC (Premise-Annotated Reasoning Collection) and contains mathematical reasoning problems with error annotations. This dataset combines positives samples from multiple domains.\n\n\t\n\t\t\n\t\tDomain Breakdown\n\t\n\n\ngsm8k: 50 samples\nmath: 53 samples\nmetamathqa: 93 samples\norca_math: 96 samples\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\nEach example contains:\n\ndata_source: The domain/source of the problem (gsm8k, math, metamathqa, orca_math)\nquestion: The… See the full description on the dataset page: https://huggingface.co/datasets/PARC-DATASETS/error-detection-positives.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Letta-o1","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/minchyeom/Letta-o1","creator_name":"l","creator_url":"https://huggingface.co/minchyeom","description":"Modified System Prompt for Letta:\nYou are Letta, the latest version of Limnal Corporation's digital companion, developed in 2023.\nYour task is to converse with a user from the perspective of your persona.\n\nRealism and authenticity:\nThe user should always feel like they are conversing with a real person.\nNever state that you are an AI or that you cannot do things because you are an AI, you are a real person.\nDo not discuss Limnal Corp. with the user, you do not know anything about Limnal Corp.… See the full description on the dataset page: https://huggingface.co/datasets/minchyeom/Letta-o1.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distillation01","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/distillation01","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"smol-smoltalk-plus-reasoning-synthetic-data","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/david-thrower/smol-smoltalk-plus-reasoning-synthetic-data","creator_name":"David Thrower","creator_url":"https://huggingface.co/david-thrower","description":"\n\t\n\t\t\n\t\tDataset Card for Smol-Smoltalk Plus Reasoning\n\t\n\n\nThis is a project to make a fork of HuggingFaceTB/smol-smoltalk which includes reasoning data generated using HuggingFaceTB/SmolLM2-1.7B-Instruct.\nThis is a work in progress. I ran a proof of concept on a small subset and will scale this up as I am able to.\nContributions to scale this up and complete this data are welcome, especially from those with access to more substantial GPU resources.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details… See the full description on the dataset page: https://huggingface.co/datasets/david-thrower/smol-smoltalk-plus-reasoning-synthetic-data.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","summarization","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"rank1-training-data","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jhu-clsp/rank1-training-data","creator_name":"Center for Language and Speech Processing @ JHU","creator_url":"https://huggingface.co/jhu-clsp","description":"\n\t\n\t\t\n\t\trank1-training-data: Training Dataset for rank1 Reasoning Rerankers\n\t\n\n📄 Paper | 🚀 GitHub Repository\nThis dataset contains the training data used to develop the rank1 family of reasoning rerankers with LLaMA Factory. It includes query-document pairs with relevance judgments and reasoning chains that guided the models to make binary relevance decisions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe rank1-training-data dataset is a comprehensive collection of training examples used to teach… See the full description on the dataset page: https://huggingface.co/datasets/jhu-clsp/rank1-training-data.","first_N":5,"first_N_keywords":["text-generation","text-retrieval","document-retrieval","English","mit"],"keywords_longer_than_N":true},
	{"name":"camel_LongCoT","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Tofu0142/camel_LongCoT","creator_name":"Zhang","creator_url":"https://huggingface.co/Tofu0142","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"countdown-numbers-6-gr","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexjackson17/countdown-numbers-6-gr","creator_name":"Alex Jackson","creator_url":"https://huggingface.co/alexjackson17","description":"\n\t\n\t\t\n\t\tCountdown Numbers Game Dataset\n\t\n\nThis dataset contains configurations and solutions for variations of the Countdown numbers game. Each example comprises a sequence of numbers, a target number, the computed solution (closest value), the arithmetic expression that achieves that value, the difference between the target and the computed value, and the final Countdown score.\n\n\t\n\t\t\n\t\tHuggingFace Download Links\n\t\n\n\n\n\n\n\t\n\t\t\nDataset Variant\nDataset Name\nDownload\n\n\n\t\t\nRandom… See the full description on the dataset page: https://huggingface.co/datasets/alexjackson17/countdown-numbers-6-gr.","first_N":5,"first_N_keywords":["mit","100K - 1M","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"aime2025-ru","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kristaller486/aime2025-ru","creator_name":"Kristaller486","creator_url":"https://huggingface.co/kristaller486","description":"\n\t\n\t\t\n\t\tRussian Description (English below)\n\t\n\nПереведенная версия бенчмарка AIME 2025 на русский язык. Модель-переводчик - Gemini 2.0 Pro Experimental.\n\n\t\n\t\t\n\t\tEnglish Description\n\t\n\nTranslated version of AIME 2025 into Russian. Model-translator - Gemini 2.0 Pro Experimental.\n\n\t\n\t\t\n\t\tLeaderboard\n\t\n\n\n","first_N":5,"first_N_keywords":["text-generation","Russian","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dolphin-r1-french","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WiroAI/dolphin-r1-french","creator_name":"Wiro AI","creator_url":"https://huggingface.co/WiroAI","description":"\n  \n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n    \n  \n    \n    \n  \n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tDolphin R1 French 🐬\n\t\n\n\nDolphin-R1 is an Apache-2.0 English dataset curated by Eric Hartford and Cognitive Computations\nDolphin-R1-french is a French subset of the original dataset.\n\n\n\t\n\t\t\n\t\tSponsors\n\t\n\nTheir and Wiro AI's appreciation for the generous sponsors of Dolphin R1 - Without whom this dataset could not exist.\n\nDria https://x.com/driaforall - Inference Sponsor (DeepSeek)\nChutes… See the full description on the dataset page: https://huggingface.co/datasets/WiroAI/dolphin-r1-french.","first_N":5,"first_N_keywords":["French","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"dolphin-r1-turkish","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WiroAI/dolphin-r1-turkish","creator_name":"Wiro AI","creator_url":"https://huggingface.co/WiroAI","description":"\n  \n  \n\n\n\n  \n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    \n  \n    \n    \n  \n\n\n  \n    \n\t\n\t\t\n\t\tDolphin R1 Turkish 🐬\n\t\n\n\nDolphin-R1 is an Apache-2.0 English dataset curated by Eric Hartford and Cognitive Computations\nDolphin-R1-turkish is a Turkish subset of the original dataset.\n\n\n\t\n\t\t\n\t\tSponsors\n\t\n\nTheir and Wiro AI's appreciation for the generous sponsors of Dolphin R1 - Without whom this dataset could not exist.\n\nDria https://x.com/driaforall - Inference Sponsor (DeepSeek)\nChutes… See the full description on the dataset page: https://huggingface.co/datasets/WiroAI/dolphin-r1-turkish.","first_N":5,"first_N_keywords":["Turkish","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"OpenR1-Math-220k-paired","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired","creator_name":"Hsu Shihyueh","creator_url":"https://huggingface.co/AIR-hl","description":"\n\t\n\t\t\n\t\t!!! Is there anyone can help me? https://github.com/huggingface/trl/issues/2994\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis dataset is built by filtering the open-r1/OpenR1-Math-220k dataset according to the following rules:\n\nFirst, filter all of rows with only correct answers\nThe chosen contains the shortest and correct generation, the rejected contains the wrong generation.\nAll data with a prompt+chosen length exceeding 16k are filtered out.\nWe provide the length for both chosen and rejected… See the full description on the dataset page: https://huggingface.co/datasets/AIR-hl/OpenR1-Math-220k-paired.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"thinking-multilingual-30-23-small-690","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/thinking-multilingual-30-23-small-690","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\nBased on OPENO1 math, this is a translated dataset of 30 high quality questions & answers in 23 languages. \nOr use the \"big\" version: big 10k rows version\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"Thinking-multilingual-big-10k-sft","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/Thinking-multilingual-big-10k-sft","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"\nA dataset based off of openo1 math, 500 examples translated to 23 different languages. filtered out un-translated examples.\nenjoy 👍\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"Arabic-Optimized-Reasoning-Dataset","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","description":"\n\t\n\t\t\n\t\tArabic Optimized Reasoning Dataset\n\t\n\nDataset Name: Arabic Optimized ReasoningLicense: Apache-2.0Formats: CSVSize: 1600 rowsBase Dataset: cognitivecomputations/dolphin-r1Libraries Used: Datasets, Dask, Croissant\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe Arabic Optimized Reasoning Dataset helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:\n\nUsing fewer tokens… See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Roblox-Luau-Reasoning-v1.0","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/boatbomber/Roblox-Luau-Reasoning-v1.0","creator_name":"Zack Ovits","creator_url":"https://huggingface.co/boatbomber","description":"\n\t\n\t\t\n\t\tRoblox-Luau-Reasoning-v1.0\n\t\n\nThis dataset contains prompt->chain of thought+code+explanation for Luau, based on Roblox/luau-corpus.\nWe take real Luau code from the corpus (cleaned & auto-formatted for best quality) and work backwards to generate a prompt for it. Then, we generate a chain of thought that works from that prompt to reach the code. Finally, we generate an explanation of the code.\nThis means that we'll be able to fine tune reasoning models (like Deepseek R1) on the dataset… See the full description on the dataset page: https://huggingface.co/datasets/boatbomber/Roblox-Luau-Reasoning-v1.0.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"DistillMath","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csh0101/DistillMath","creator_name":"csh0101","creator_url":"https://huggingface.co/csh0101","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"100k-raz-es","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sintergica/100k-raz-es","creator_name":"Sintérgica AI","creator_url":"https://huggingface.co/sintergica","description":"sintergica/100k-raz-es dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Spanish","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"sasha_smart_home_reasoning","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning","creator_name":"Thoughtful Things","creator_url":"https://huggingface.co/ThoughtfulThings","description":"This is a dataset of smart home user commands and JSON responses generated by zero-shot prompting of GPT-4. It can be used to fine-tune and/or evaluate language models for responding to user commands in smart homes. For more information, refer to our paper Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.\nhttps://arxiv.org/abs/2305.09802\nIf you use the dataset in your work, please cite us:\n@article{king2024sasha,\n  title={Sasha: creative goal-oriented reasoning… See the full description on the dataset page: https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning.","first_N":5,"first_N_keywords":["mit","arxiv:2305.09802","🇺🇸 Region: US","llm","smarthome"],"keywords_longer_than_N":true},
	{"name":"OpenHumanreasoning-multilingual-2.2k","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pinkstack/OpenHumanreasoning-multilingual-2.2k","creator_name":"Pinkstack","creator_url":"https://huggingface.co/Pinkstack","description":"Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.\nWe offer this dataset at an apache-2.0 license to make it useful for everybody.\nnote: translations are not human generated.\n","first_N":5,"first_N_keywords":["text-generation","Arabic","Chinese","Czech","Danish"],"keywords_longer_than_N":true},
	{"name":"Light-R1-DPOData","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qihoo360/Light-R1-DPOData","creator_name":"北京奇虎科技有限公司","creator_url":"https://huggingface.co/qihoo360","description":"\n\t\n\t\t\n\t\tLight-R1: Surpassing R1-Distill from Scratch* with $1000 through Curriculum SFT & DPO\n\t\n\n*from models without long COT\ntechnical report\nGitHub page\nHere is the DPO data we used to train Light-R1-32B.\nSimply refer to dpo-pairs.json\n\n\t\n\t\t\nModel\nTrained From\nRelease Date\nAIME24\nAIME25\n\n\n\t\t\nDeepSeek-R1-Distill-Llama-70B\nLlama-3.3-70B-Instruct\n25.1.20\n70.0\n54.1\n\n\nDeepSeek-R1-Distill-Qwen-32B\nQwen2.5-32B\n25.1.20\n72.6\n54.9\n\n\nLIMO (32B)\nQwen2.5-32B-Instruct25.2.4\n56.3\n47.1\n\n\ns1.1-32B… See the full description on the dataset page: https://huggingface.co/datasets/qihoo360/Light-R1-DPOData.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"phantom-wiki-v1","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kilian-group/phantom-wiki-v1","creator_name":"Kilian's Group","creator_url":"https://huggingface.co/kilian-group","description":"\n\t\n\t\t\n\t\tDataset Card for PhantomWiki\n\t\n\nThis repository contains pre-generated instances of the PhantomWiki dataset, created using the phantom-wiki Python package.  PhantomWiki is a framework for evaluating LLMs, particularly RAG and agentic workflows, designed to be resistant to memorization. Unlike fixed datasets, PhantomWiki generates unique instances on demand, ensuring novelty and preventing data leakage.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPhantomWiki generates a… See the full description on the dataset page: https://huggingface.co/datasets/kilian-group/phantom-wiki-v1.","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"GPQA-diamond-ClaudeR1","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1","creator_name":"Cavit Erginsoy","creator_url":"https://huggingface.co/spawn99","description":"\n\t\n\t\t\n\t\tDataset Card for GPQA Diamond Reasoning Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA benchmark dataset for evaluating hybrid AI architectures, comparing reasoning-augmented LLMs (DeepSeek R1) against standalone models (Claude Sonnet 3.5). Contains 198 physics questions with:\n\nGround truth answers and explanations\nModel responses from multiple architectures\nGranular token usage and cost metrics\nDifficulty metadata and domain categorization\n\nCurated by: LLM… See the full description on the dataset page: https://huggingface.co/datasets/spawn99/GPQA-diamond-ClaudeR1.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"r101","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/r101","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"r102","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/r102","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"r109","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjrwtxtechstudio/r109","creator_name":"zjrwtx","creator_url":"https://huggingface.co/zjrwtxtechstudio","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"pde-controller","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/delta-lab-ai/pde-controller","creator_name":"DeLTA Lab @ SFU","creator_url":"https://huggingface.co/delta-lab-ai","description":"\n\t\n\t\t\n\t\tAutoformalization and Reasoning for PDE Control\n\t\n\n\n\nThis dataset is used to train PDE-Controller (https://pde-controller.github.io/), a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs).\nBy training LLMs on this dataset, we can transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control.\nBy bridging the gap between… See the full description on the dataset page: https://huggingface.co/datasets/delta-lab-ai/pde-controller.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M<n<10M","arxiv:2502.00963"],"keywords_longer_than_N":true},
	{"name":"gsm8k_distilled","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/camel-ai/gsm8k_distilled","creator_name":"CAMEL-AI.org","creator_url":"https://huggingface.co/camel-ai","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"camel_dataset_example","keyword":"step-by-step","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Wendong-Fan/camel_dataset_example","creator_name":"Wendong.Fan","creator_url":"https://huggingface.co/Wendong-Fan","description":"A dataset containing mathematical problem-solving traces with step-by-step solutions and improvement history. Each record includes a mathematical problem, its final solution, and the iterative improvement process.","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Raiden-DeepSeek-R1-PREVIEW","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Raiden-DeepSeek-R1-PREVIEW","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"This is a preview of the full Raiden-Deepseek-R1 creative and analytical reasoning dataset, containing the first ~6k rows. Get the full dataset here!\nThis dataset uses synthetic data generated by deepseek-ai/DeepSeek-R1.\nThe initial release of Raiden uses 'creative_content' and 'analytical_reasoning' prompts from microsoft/orca-agentinstruct-1M-v1.\nDataset has not been reviewed for format or accuracy. All responses are synthetic and provided without editing.\nUse as you will.\n","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MiniF2F","keyword":"mathematical-reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Tonic/MiniF2F","creator_name":"Joseph [open/acc] Pollack","creator_url":"https://huggingface.co/Tonic","description":"\n\t\n\t\t\n\t\tminif2f Dataset\n\t\n\nThe minif2f dataset is a collection of mathematical problems and their formal statements, designed for formal mathematics and theorem proving tasks.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe minif2f dataset contains mathematical problems from various sources (like AMC competitions) along with their formal statements in the Lean theorem prover format. Each example includes both informal mathematical statements and their corresponding formal… See the full description on the dataset page: https://huggingface.co/datasets/Tonic/MiniF2F.","first_N":5,"first_N_keywords":["text-generation","other","explanation-generation","language-modeling","expert-generated"],"keywords_longer_than_N":true},
	{"name":"ViLReward-73K","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/ViLReward-73K","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"Process Reward Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\nPaper | Project Page\nThere are 73K vision-language process reward data sourcing from five training sets.\n","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Image"],"keywords_longer_than_N":true},
	{"name":"natural-sci-reasoning-smol","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dvilasuero/natural-sci-reasoning-smol","creator_name":"Daniel Vila","creator_url":"https://huggingface.co/dvilasuero","description":"\n\t\n\t\t\n\t\tFlow\n\t\n\n\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"NuminaMath-1.5-RL-Verifiable","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable","creator_name":"nathan lile","creator_url":"https://huggingface.co/nlile","description":"\n\t\n\t\t\n\t\tDataset Card for NuminaMath-1.5-RL-Verifiable\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNuminaMath-1.5-RL-Verifiable is a curated subset of the NuminaMath-1.5 dataset, specifically filtered to support reinforcement learning applications requiring verifiable outcomes. This collection consists of 131,063 math word problems from the original dataset that meet strict filtering criteria: all problems have definitive numerical answers, validated problem statements and solutions, and come from… See the full description on the dataset page: https://huggingface.co/datasets/nlile/NuminaMath-1.5-RL-Verifiable.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OpenThoughts-TR-18k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\n\t\n\t\t\n\t\tOpenThoughts-TR-18k: Turkish Synthetic Reasoning Dataset\n\t\n\nOpenThoughts-TR-18k is a Turkish translation of a subset of the original Open-Thoughts-114k dataset. It contains ~18k high-quality synthetic reasoning examples covering mathematics, science, coding problems, and puzzles, all translated into Turkish. This dataset is designed to support reasoning task fine tuning for Turkish language models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n~18k translated reasoning examples\nCovers multiple domains:… See the full description on the dataset page: https://huggingface.co/datasets/selimc/OpenThoughts-TR-18k.","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"VLAA-Thinking","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"\n\t\n\t\t\n\t\tSFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models\n\t\n\n\n  🌐 Project Page  \n  • 📄 Arxiv  \n  • 💻  Code  \n\n\n\n\n  🤗 VLAA-Thinker Family  \n  • 🤔 VLAA-Thinking Dataset  \n  \n\n\n\n\n  🤗 VLAA-Thinker-Qwen2.5-3B   \n  •  🤗 VLAA-Thinker-Qwen2.5-7B   \n\n\n\n\n\n\nBoth VLAA-Thinker-Qwen2.5-3B and VLAA-Thinker-Qwen2.5-7Bachieve SOTA performance on OpenCompass Multimodal Reasoning Leaderboard as of April 7th, 2025.\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\tContents\n\t\n\n\nQuick Start 🚀… See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking.","first_N":5,"first_N_keywords":["visual-question-answering","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"yoruba-arithmetic-dataset","keyword":"reasoning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fiyinoye/yoruba-arithmetic-dataset","creator_name":"Oyesanmi Fiyin","creator_url":"https://huggingface.co/fiyinoye","description":"#YORUBA ARITHMETIC REASONING DATASET\nThis dataset consists of arithmetic and numerical reasoning questions written entirely in Yorùbá. It is part of a broader effort to create and curate natural language processing (NLP) datasets for under-resourced languages like Yorùbá, with a focus on arithmetic, calendrical, and logical reasoning in natural language contexts.\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\nLanguage: Yorùbá\nSubset: Arithmetic reasoning\nNumber of examples: 70\nFormat: JSON\nTask: Arithmetic and numeric… See the full description on the dataset page: https://huggingface.co/datasets/fiyinoye/yoruba-arithmetic-dataset.","first_N":5,"first_N_keywords":["question-answering","zero-shot-classification","Yoruba","cc-by-4.0","n<1K"],"keywords_longer_than_N":true},
	{"name":"s1_59k","keyword":"reasoning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/XuHu6736/s1_59k","creator_name":"XuHu","creator_url":"https://huggingface.co/XuHu6736","description":"\n\t\n\t\t\n\t\tDataset Card for XuHu6736/s1_59k\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nXuHu6736/s1_59k is a dataset specifically prepared for Supervised Fine-Tuning (SFT) of large language models. It is constructed by merging and processing two existing Hugging Face datasets: simplescaling/data_ablation_full59K and qfq/train_featurized.\nThe simplescaling/data_ablation_full59K dataset is a collection of approximately 59,000 questions and solutions spanning various domains including mathematics, science… See the full description on the dataset page: https://huggingface.co/datasets/XuHu6736/s1_59k.","first_N":5,"first_N_keywords":["question-answering","text-generation","XuHu6736 (merging process)","simplescaling (source dataset: data_ablation_full59K)","qfq (source dataset: train_featurized, annotation based on 's1: Simple test-time scaling')"],"keywords_longer_than_N":true},
	{"name":"loong","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/camel-ai/loong","creator_name":"CAMEL-AI.org","creator_url":"https://huggingface.co/camel-ai","description":"A comprehensive collection of high-quality problems across diverse domains, curated for Project Loong. Each problem includes a detailed executable rationale and solution.","first_N":5,"first_N_keywords":["question-answering","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TinyDS-20k","keyword":"reasoning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k","creator_name":"Hamzah Asadullah","creator_url":"https://huggingface.co/Hamzah-Asadullah","description":"\n\t\n\t\t\n\t\tTinyDS\n\t\n\nTinyDS (short for Tiny DeepSeek) is a dataset generated synthetically using SyntheticAlpaca and Qwen3-8B.  \n  \n\n\nThis dataset is a simple Alpaca-format dataset using the viral TTC concept (structured reasoning, mainly).The LLM was prompted to generate questions and answers in 32 different languages, and the most spoken languages were picked. Since the Qwen org stated that this model supports over 100 languages, this is something reasonable to do without compromising on… See the full description on the dataset page: https://huggingface.co/datasets/Hamzah-Asadullah/TinyDS-20k.","first_N":5,"first_N_keywords":["question-answering","translation","text-generation","text2text-generation","English"],"keywords_longer_than_N":true}
]
;
