const data_for_modality_embeddings = 
[
	{"name":"glove.6B.50d.umap.2d","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis dataset is a UMAP 2D-projection of the glove.6B.50d embeddings from Stanford. It is intended as a fast reference for visualizing embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe embeddings have a vocabulary of 400k tokens with 2 dimensions each token.\\nCurated by: Mario Tormo Romero\\nLicense: cc0-1.0\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/glove.6B.50d.umap.2d."},
	{"name":"embedded_movies_small","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/acloudfan/embedded_movies_small","creator_name":"raj","creator_url":"https://huggingface.co/acloudfan","description":"This dataset was created from the HuggingFace dataset AIatMongoDB/embedded_movies\\nWhy was it needed?\\n\\nThe original dataset is close to 25 GB, for learning and experiments it is an overkill\\nData in the dataset needs to be cleaned up e.g., some features are Null that requires extra care\\nSome of the embeddings are missing\\n\\nHow to use?\\n\\nUse for sentiment analysis\\nText similarity (plot)\\nEmbeddings : ready to use with vector DB & search libraries\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdataset_info:\\n  features:\\n  - name:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/acloudfan/embedded_movies_small."},
	{"name":"jina-embeddings-v2-base-en-5192024-xqq9-webapp","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjina-embeddings-v2-base-en-5192024-xqq9-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"machine learning data generation\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jina-embeddings-v2-base-en-5192024-xqq9-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-5192024-xqq9-webapp."},
	{"name":"supreme-court-of-pak-judgments","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments","creator_name":"Ibtehaj Khan","creator_url":"https://huggingface.co/Ibtehaj10","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Pak-Law Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset contains legal documents related to Pakistani law. It includes text data, case details, embeddings generated using the mixedbread-ai/mxbai-embed-large-v1 model, and citation numbers.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset has the following features:\\n\\ntext: The text of the legal documents.\\ncase_details: Details about the legal cases.\\nembeddings: Embeddings of the text, generated using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ibtehaj10/supreme-court-of-pak-judgments."},
	{"name":"Arabic-finanical-rag-embedding-dataset","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset","creator_name":"Omartificial Intelligence Space","creator_url":"https://huggingface.co/Omartificial-Intelligence-Space","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArabic Version of The Finanical Rag Embedding Dataset\\n\\t\\n\\n\\nThis dataset is tailored for fine-tuning embedding models in Retrieval-Augmented Generation (RAG) setups. It consists of 7,000 question-context pairs translated into Arabic, sourced from NVIDIA's 2023 SEC Filing Report. \\nThe dataset is designed to improve the performance of embedding models by providing positive samples for financial question-answering tasks in Arabic.\\nThis dataset is the Arabic version of the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic-finanical-rag-embedding-dataset."},
	{"name":"Chunked-Indian-Supreme-Court-Judgements","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vihaannnn/Chunked-Indian-Supreme-Court-Judgements","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIndian Supreme Court Judgements Chunked\\n\\t\\n\\n"},
	{"name":"Core-S2RGB-DINOv2","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2RGB-DINOv2","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCore-S2RGB-DINOv2 üî¥üü¢üîµ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nDataset\\nModality\\nNumber of Embeddings\\nSensing Type\\nTotal Comments\\nSource Dataset\\nSource Model\\nSize\\n\\n\\n\\t\\t\\nCore-S2RGB-SigLIP\\nSentinel-2 Level 2A (RGB)\\n56,147,150\\nTrue Colour (RGB)\\nGeneral-Purpose Global\\nCore-S2L2A\\nDINOv2\\n223.1 GB\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nField\\nType\\nDescription\\n\\n\\n\\t\\t\\nunique_id\\nstring\\nhash generated from geometry, time, product_id, and embedding model\\n\\n\\nembedding\\narray\\nraw embedding array\\n\\n\\ngrid_cell\\nstring\\nMajor TOM cell‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2RGB-DINOv2."},
	{"name":"DS569k","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/donnyb/DS569k","creator_name":"Donny Bertucci","creator_url":"https://huggingface.co/donnyb","description":"Want to analyze some proteins, but lack embeddings? Want to perform vector similarity search? Want a context of known proteins embeddings? Look no further!\\nThis repository is a dataset of Reviewed Swiss-Prot Proteins. Each protein I compute the embeddings for ESM2 (6 layer model) and ProteinCLIP. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpecs\\n\\t\\n\\nSee the data viewer for all information. Most of the metadata on each protein and the sequences themself come from Reviewed Swiss-Prot Proteins.\\nImportant columns\\n\\naccession: the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/donnyb/DS569k."},
	{"name":"Glove-Embedding","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liandarizkia/Glove-Embedding","creator_name":"annisa rizki liliandari","creator_url":"https://huggingface.co/liandarizkia","description":"liandarizkia/Glove-Embedding dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"sa-data","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phalanx80/sa-data","creator_name":"Paolo De Gasperis","creator_url":"https://huggingface.co/phalanx80","description":"\\n\\t\\n\\t\\t\\n\\t\\tStoria dell'Arte Dataset (SA-Data)\\n\\t\\n\\n \\n\\n\\t\\n\\t\\t\\n\\t\\tüìå Descrizione del Dataset\\n\\t\\n\\nIl dataset SA-Data √® una raccolta strutturata di articoli della rivista Storia dell'Arte (https://www.storiadellarterivista.it/) digitalizzati e arricchiti con metadati dettagliati e rappresentazioni semantiche. √à stato creato per supportare la ricerca accademica e le applicazioni di elaborazione del linguaggio naturale.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîç Contenuto\\n\\t\\n\\nIl dataset include:\\n\\n1050 articoli pubblicati tra il 1969 e‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phalanx80/sa-data."},
	{"name":"Core-S1RTC-SSL4EO","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S1RTC-SSL4EO","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCore-S1RTC-SSL4EO üì°‚ö°üõ∞Ô∏è\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nDataset\\nModality\\nNumber of Embeddings\\nSensing Type\\nTotal Comments\\nSource Dataset\\nSource Model\\nSize\\n\\n\\n\\t\\t\\nCore-S1RTC-SSL4EO\\nSentinel-1 RTC\\n36,748,875\\nSAR\\nGeneral-Purpose Global\\nCore-S1RTC\\nSSL4EO-ResNet50-MOCO\\n332.5 GB\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nField\\nType\\nDescription\\n\\n\\n\\t\\t\\nunique_id\\nstring\\nhash generated from geometry, time, product_id, and embedding model\\n\\n\\nembedding\\narray\\nraw embedding array\\n\\n\\ngrid_cell\\nstring\\nMajor TOM cell\\n\\n\\ngrid_row_u\\nint‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S1RTC-SSL4EO."},
	{"name":"Core-S2L1C-SSL4EO","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2L1C-SSL4EO","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCore-S2L1C-SSL4EO  üü•üü©üü¶üüßüü®üü™ üõ∞Ô∏è\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nDataset\\nModality\\nNumber of Embeddings\\nSensing Type\\nTotal Comments\\nSource Dataset\\nSource Model\\nSize\\n\\n\\n\\t\\t\\nCore-S2L1C-SSL4EO\\nSentinel-2 (Level 1C)\\n56,147,150\\nMulti-Spectral\\nGeneral-Purpose Global\\nCore-S2L1C\\nSSL4EO-ResNet50-DINO\\n252.9 GB\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nField\\nType\\nDescription\\n\\n\\n\\t\\t\\nunique_id\\nstring\\nhash generated from geometry, time, product_id, and embedding model\\n\\n\\nembedding\\narray\\nraw embedding array\\n\\n\\ngrid_cell\\nstring‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2L1C-SSL4EO."},
	{"name":"USCode-QAPairs-Finetuning","keyword":"embeddings","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUSCode-QueryPairs Dataset\\n\\t\\n\\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\nSource: United States Code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning."},
	{"name":"commonvoice-12.0-arabic-voice-converted-speakers","keyword":"embedding","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmodar/commonvoice-12.0-arabic-voice-converted-speakers","creator_name":"Modar M. Alfadly","creator_url":"https://huggingface.co/xmodar","description":"These are the speakers' embeddings and information for the Voice Converted Arabic Common Voice 12.0 dataset.\\n"},
	{"name":"Core-S1RTC-DeCUR","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S1RTC-DeCUR","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCore-S1RTC-DeCUR üì°‚ö°üõ∞Ô∏è\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nDataset\\nModality\\nNumber of Embeddings\\nSensing Type\\nTotal Comments\\nSource Dataset\\nSource Model\\nSize\\n\\n\\n\\t\\t\\nCore-S1RTC-SSL4EO\\nSentinel-1 RTC\\n36,748,875\\nSAR\\nGeneral-Purpose Global\\nCore-S1RTC\\nDeCUR\\nGB\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nField\\nType\\nDescription\\n\\n\\n\\t\\t\\nunique_id\\nstring\\nhash generated from geometry, time, product_id, and embedding model\\n\\n\\nembedding\\narray\\nraw embedding array\\n\\n\\ngrid_cell\\nstring\\nMajor TOM cell\\n\\n\\ngrid_row_u\\nint\\nMajor TOM cell row‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S1RTC-DeCUR."},
	{"name":"datacomp-small-clip","keyword":"embeddings","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fondant-ai/datacomp-small-clip","creator_name":"Fondant","creator_url":"https://huggingface.co/fondant-ai","description":"\\n    \\n        \\n    \\n\\n\\n\\n    \\n        Production-ready \\n        data processing made \\n        easy \\n        and \\n        shareable\\n    \\n    \\n    Explore the Fondant docs ¬ª\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for fondant-ai/datacomp-small-clip\\n\\t\\n\\n\\n\\nThis is a dataset containing image urls and their CLIP embeddings, based on the datacomp_small dataset, and processed with fondant.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nLarge (image) datasets are often unwieldy to use due to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fondant-ai/datacomp-small-clip."},
	{"name":"korean_parallel_sentences_v1.1","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1","creator_name":"Lemon Mint","creator_url":"https://huggingface.co/lemon-mint","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Korean Parallel Sentences Ver 1.1\\n\\t\\n\\nThis dataset card provides information about the Korean Parallel Sentences Ver 1.1 dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Korean Parallel Sentences Ver 1.1 dataset is a collection of parallel sentences in Korean and English.\\nAlthough the factual accuracy of the data is not guaranteed, it has been designed to ensure accurate and consistent translation style between English and Korean.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lemon-mint/korean_parallel_sentences_v1.1."},
	{"name":"MMEB-train","keyword":"embedding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/MMEB-train","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\\n\\t\\n\\t\\t\\n\\t\\tMassive Multimodal Embedding Benchmark\\n\\t\\n\\nThe training data split used for training VLM2Vec models in the paper VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks (ICLR 2025).\\nMMEB benchmark covers 4 meta tasks and 36 datasets meticulously selected for evaluating capabilities of multimodal embedding models.\\nDuring training, we utilize 20 out of the 36 datasets.\\nFor evaluation, we assess performance on the 20 in-domain (IND) datasets and the remaining 16‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/MMEB-train."},
	{"name":"Indian-Supreme-Court-Judgements-Chunked","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked","creator_name":"Vihaan Nama","creator_url":"https://huggingface.co/vihaannnn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIndian Supreme Court Judgements Chunked\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExecutive Summary\\n\\t\\n\\nThe dataset aims to address the chronic backlog in the Indian judiciary system, particularly in the Supreme Court, by creating a dataset optimized for legal language models (LLMs). The dataset will consist of pre-processed, chunked, and embedded textual data derived from the Supreme Court's judgment PDFs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProblem and Importance - Motivation\\n\\t\\n\\nIndian courts are overwhelmed with pending cases‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vihaannnn/Indian-Supreme-Court-Judgements-Chunked."},
	{"name":"Core-S2RGB-SigLIP","keyword":"embeddings","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Major-TOM/Core-S2RGB-SigLIP","creator_name":"Major TOM","creator_url":"https://huggingface.co/Major-TOM","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCore-S2RGB-SigLIP üî¥üü¢üîµ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nModality\\nNumber of Embeddings\\nSensing Type\\nComments\\nSource Dataset\\nSource Model\\nSize\\n\\n\\n\\t\\t\\nSentinel-2 Level 2A (RGB)\\n20,212,974\\nTrue Colour\\nVision-Language Global\\nCore-S2L2A\\nSigLIP-SO400M-384\\n41.3 GB\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nField\\nType\\nDescription\\n\\n\\n\\t\\t\\nunique_id\\nstring\\nhash generated from geometry, time, product_id, and embedding model\\n\\n\\nembedding\\narray\\nraw embedding array\\n\\n\\ngrid_cell\\nstring\\nMajor TOM cell\\n\\n\\ngrid_row_u\\nint\\nMajor TOM cell‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Major-TOM/Core-S2RGB-SigLIP."},
	{"name":"local-emoji-search-gte","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pszemraj/local-emoji-search-gte","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tlocal emoji semantic search\\n\\t\\n\\nEmoji, their text descriptions and precomputed text embeddings with Alibaba-NLP/gte-large-en-v1.5 for use in emoji semantic search. \\nThis work is largely inspired by the original emoji-semantic-search repo and aims to provide the data for fully local use, as the demo is not working as of a few days ago.\\n\\nThis repo only contains a precomputed embedding \\\"database\\\", equivalent to server/emoji-embeddings.jsonl.gz in the original repo, to be used as the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pszemraj/local-emoji-search-gte."},
	{"name":"mmE5-synthetic","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-synthetic","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\\n\\t\\n\\t\\t\\n\\t\\tmmE5 Synthetic Data\\n\\t\\n\\nThis dataset contains synthetic datasets used for the finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\\n\\nClassification\\nRetrieval\\nVQA\\n\\nGithub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImage Preparation\\n\\t\\n\\nFirst, you should prepare the images used for training:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImage Downloads\\n\\t\\n\\n\\nDownload mmE5-synthetic Images:\\n\\nRun the following command to download and extract the images only in this dataset.\\nmkdir -p images && cd images\\nwget‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-synthetic."},
	{"name":"oscar-en-minilm-2m","keyword":"embeddings","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jamescalam/oscar-en-minilm-2m","creator_name":"James Briggs","creator_url":"https://huggingface.co/jamescalam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOscar EN 2M Embeddings\\n\\t\\n\\nThis dataset contains 2M sentences extracted from the English subset of the OSCAR dataset, and encoded into sentence embeddings using the sentence-transformers/all-MiniLM-L6-v2 model.\\n"},
	{"name":"pwesuite-eval","keyword":"embedding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zouharvi/pwesuite-eval","creator_name":"Vil√©m Zouhar","creator_url":"https://huggingface.co/zouharvi","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\tPWESuite-Eval\\n\\t\\n\\nDataset composed of multiple smaller datasets used for the evaluation of phonetic word embeddings.\\nSee code for evaluation here.\\nIf you use this dataset/evaluation, please cite the paper at LREC-COLING 2024:\\n@inproceedings{zouhar-etal-2024-pwesuite,\\n    title = \\\"{PWES}uite: Phonetic Word Embeddings and Tasks They Facilitate\\\",\\n    author = \\\"Zouhar, Vil{\\\\'e}m  and\\n      Chang, Kalvin  and\\n      Cui, Chenxuan  and\\n      Carlson, Nate B.  and\\n      Robinson, Nathaniel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zouharvi/pwesuite-eval."},
	{"name":"cifar100-enriched","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/renumics/cifar100-enriched","creator_name":"Renumics","creator_url":"https://huggingface.co/renumics","description":"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses.\\nThere are two labels per image - fine label (actual class) and coarse label (superclass)."},
	{"name":"blbooks-parquet-embedded","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/blbooks-parquet-embedded","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"blbooks-parquet-embedded\\\"\\n\\t\\n\\nMore Information needed\\n"},
	{"name":"maltese_embeddings","keyword":"embeddings","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DGurgurov/maltese_embeddings","creator_name":"Daniil Gurgurov","creator_url":"https://huggingface.co/DGurgurov","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains three distinct datasets focusing on Maltese word embeddings:\\n\\nGloVe Maltese Word Embeddings\\nEmbeddings generated using GloVe on the \\\"korpus_malti\\\" dataset, the largest Maltese corpus available.\\n\\nWord2Vec Maltese Word Embeddings\\nWord embeddings for Maltese obtained using Word2Vec trained on the \\\"korpus_malti\\\" dataset.\\n\\nPPMI Maltese Word Embeddings\\nPointwise Mutual Information (PPMI) based word embeddings generated from ConceptNet data via‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DGurgurov/maltese_embeddings."},
	{"name":"movie_descriptors_small","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors_small","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for some movies from the original dataset.\\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\\nThis dataset has a bigger version here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset has 28655 rows and 3 columns:\\n\\n'name':‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors_small."},
	{"name":"movie_descriptors","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mt0rm0/movie_descriptors","creator_name":"Mario Tormo Romero","creator_url":"https://huggingface.co/mt0rm0","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis dataset is a subset from Kaggle's The Movie Dataset that contains only name, release year and overview for every film in the original dataset that has that information complete.\\nIt is intended as a toy dataset for learning about embeddings in a workshop from the AI Service Center Berlin-Brandenburg at the Hasso Plattner Institute.\\nThis dataset has a smaller version here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset has 44435‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mt0rm0/movie_descriptors."},
	{"name":"openalex-multilingual-embeddings","keyword":"embeddings","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings","creator_name":"Global Campus","creator_url":"https://huggingface.co/GlobalCampus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAlex Multilingual Embeddings\\n\\t\\n\\nThis dataset contains multilingual text embeddings of all records in OpenAlex with a title or an abstract from the snapshot of 2023-10-20.\\nThe dataset was created for the FORAS project to investigate the efficacy of \\ndifferent methods of searching in databases of academic publications. All scripts will be available in a GitHub repository. \\nThe project is supported by a grant from the Dutch Research Council (grant no. 406.22.GO.048)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GlobalCampus/openalex-multilingual-embeddings."},
	{"name":"XTD-10","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\\n\\t\\n\\t\\t\\n\\t\\tXTD Multimodal Multilingual Data With Instruction\\n\\t\\n\\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\\n\\nit, es, ru, zh, pl, tr, ko\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Usage\\n\\t\\n\\n\\nThe instruction on the query side is: \\\"Retrieve an image of this caption.\\\"\\nThe instruction on the document side is: \\\"Represent the given image.\\\"\\nEach example contains a query and a set of targets. The first one in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10."},
	{"name":"mmE5-MMEB-hardneg","keyword":"embedding","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg","creator_name":"Liang Wang","creator_url":"https://huggingface.co/intfloat","description":"\\n\\t\\n\\t\\t\\n\\t\\tmmE5 Labeled Data\\n\\t\\n\\nThis dataset contains datasets used for the supervised finetuning of mmE5 (mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data):\\n\\nMMEB (with hard negative)\\nInfoSeek (from M-BEIR)\\nTAT-DQA\\nArxivQA\\n\\nGithub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImage Preparation\\n\\t\\n\\nFirst, you should prepare the images used for training:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImage Downloads\\n\\t\\n\\n\\nDownload All Images Used in mmE5:\\n\\nYou can use the script provided in our source code to download all images used‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/intfloat/mmE5-MMEB-hardneg."}
]
;
