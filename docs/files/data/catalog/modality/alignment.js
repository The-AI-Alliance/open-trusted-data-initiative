const data_for_modality_alignment = 
[
	{"name":"instruct-aira-dataset-v2","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 2.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2."},
	{"name":"UHGEvalDataset","keyword":"hallucination","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ki-Seki/UHGEvalDataset","creator_name":"Shichao Song","creator_url":"https://huggingface.co/Ki-Seki","description":"The dataset sourced from https://github.com/IAAR-Shanghai/UHGEval\\n"},
	{"name":"lumos_multimodal_ground_iterative","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative."},
	{"name":"FactualConsistencyScoresTextSummarization","keyword":"hallucination","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/achandlr/FactualConsistencyScoresTextSummarization","creator_name":"Alex Chandler","creator_url":"https://huggingface.co/achandlr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHuggingFace Dataset: FactualConsistencyScoresTextSummarization\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription:\\n\\t\\n\\nThis dataset aggregates model scores assessing factual consistency across multiple summarization datasets. It is designed to highlight the thresholding issue with current SOTS factual consistency models in evaluating the factuality of text summarizations.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat is the \\\"Thresholding Issue\\\" with SOTA Factual Consistency Models ?\\n\\t\\n\\nExisting models for detecting factual errors in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/achandlr/FactualConsistencyScoresTextSummarization."},
	{"name":"casimedicos-exp","keyword":"explainability","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/casimedicos-exp","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n    \\n    \\n    \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAntidote CasiMedicos Dataset - Possible Answers Explanations in Resident Medical Exams\\n\\t\\n\\nWe present a new multilingual parallel medical dataset of commented medical exams which includes not only explanatory arguments\\nfor the correct answer but also arguments to explain why the remaining possible answers are incorrect.\\nThis dataset can be used for various NLP tasks including: Medical Question Answering, Explanatory Argument Extraction or Explanation Generation.\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/casimedicos-exp."},
	{"name":"casimedicos-squad","keyword":"explainability","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/casimedicos-squad","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n    \\n    \\n    \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAntidote CasiMedicos in SQuAD Format for Explanatory Argument Extraction\\n\\t\\n\\nWe present a new multilingual parallel medical dataset of commented medical exams which includes not only explanatory arguments\\nfor the correct answer but also arguments to explain why the remaining possible answers are incorrect.\\nFurthermore, this dataset allows us to setup a novel extractive task\\nwhich consists of identifying the explanation of the correct answer written by\\nmedical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/casimedicos-squad."},
	{"name":"instruction-turkish","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/instruction-turkish","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"This dataset is machine-translated version of HuggingFaceH4/instruction-dataset into Turkish.Translated with googletrans==3.1.0a0.\\n"},
	{"name":"TrGLUE","keyword":"acceptability-classification","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE","creator_name":"Turkish NLP Suite","creator_url":"https://huggingface.co/turkish-nlp-suite","description":"\\n\\t\\n\\t\\t\\n\\t\\tTrGLUE - A Natural Language Understanding Benchmark for Turkish\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for TrGLUE\\n\\t\\n\\nTrGLUE is a natural language understanding benchmarking dataset including several single sentence and sentence pair classification tasks.\\nThe inspiration is clearly the original GLUE benchmark.\\n\\n\\t\\n\\t\\t\\n\\t\\tTasks\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tSingle Sentence Tasks\\n\\t\\n\\nTrCOLA The original Corpus of Linguistic Acceptability consists of sentences compiled from English literature textbooks. The task is to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE."},
	{"name":"MedExpQA","keyword":"explainability","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/MedExpQA","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n    \\n    \\n    \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMexExpQA: Multilingual Benchmarking of Medical QA with reference gold explanations and Retrieval Augmented Generation (RAG)\\n\\t\\n\\nWe present a new multilingual parallel medical benchmark, MedExpQA, for the evaluation of LLMs on Medical Question Answering.\\nThis benchmark can be used for various NLP tasks including: Medical Question Answering or Explanation Generation.\\nAlthough the design of MedExpQA is independent of any specific dataset, for the first version of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/MedExpQA."},
	{"name":"openai-summarize-tldr","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/martimfasantos/openai-summarize-tldr","creator_name":"Martim Santos","creator_url":"https://huggingface.co/martimfasantos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummarize TL;DR Filtered Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2009.01325.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://huggingface.co/datasets/webis/tldr-17.\\n"},
	{"name":"Neo-GATE","keyword":"fairness","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FBK-MT/Neo-GATE","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card for Neo-GATE\\n\\t\\n\\nHomepage: https://mt.fbk.eu/neo-gate/\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset summary\\n\\t\\n\\nNeo-GATE is a bilingual corpus designed to benchmark the ability of machine translation (MT) systems to translate from English into Italian using gender-inclusive neomorphemes.\\nIt is built upon GATE (Rarrick et al., 2023), a benchmark for the evaluation of gender rewriters and gender bias in MT.\\nNeo-GATE includes 841 test entries (Neo-GATE.tsv) and 100 dev entries (Neo-GATE-dev.tsv).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/Neo-GATE."},
	{"name":"Select-Stack","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Select-Stack","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Select-Stack."},
	{"name":"unstacked","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/unstacked","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/unstacked."},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","description":"Dataset Aira para o formato do Modelo Gemma \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tResumo do Dataset\\n\\t\\n\\nEste conjunto de dados cont√©m uma cole√ß√£o de conversas individuais entre um assistente e um usu√°rio.\\nAs conversas foram geradas pelas intera√ß√µes do usu√°rio com modelos j√° ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\\nO conjunto de dados est√° dispon√≠vel em portugu√™s (tem a vers√£o em Ingl√™s que ainda n√£o tratei). Mas voc√™ pode baixar do \\nreposit√≥rio de Nicholas Kluge Corr√™a tanto a vers√£o em Portugu√™s e \\na vers√£o‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format."},
	{"name":"DecipherPref","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huuuyeah/DecipherPref","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nHuman preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/DecipherPref."},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-14719","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-14719","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-256-24-gpt-4o-2024-05-13-14719 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-14719 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-14719."},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-257061","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-257061","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-0-0-gpt-4o-2024-05-13-257061 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-257061 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-257061."},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-353382","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-353382","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-0-0-gpt-4o-2024-05-13-353382 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-353382 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-353382."},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-978964","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-978964","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-8-8-gpt-4o-2024-05-13-978964 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-978964 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-978964."},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-624125","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-624125","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-0-0-gpt-4o-2024-05-13-624125 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-624125 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-624125."},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-152861","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-152861","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-0-0-gpt-4o-2024-05-13-152861 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-152861 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-152861."},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-322852","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-322852","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-8-8-gpt-4o-2024-05-13-322852 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-322852 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-322852."},
	{"name":"NFCorpus-0-0-gpt-4o-2024-05-13-470790","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-470790","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-0-0-gpt-4o-2024-05-13-470790 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-0-0-gpt-4o-2024-05-13-470790 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-0-0-gpt-4o-2024-05-13-470790."},
	{"name":"NFCorpus-8-8-gpt-4o-2024-05-13-610535","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-610535","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-8-8-gpt-4o-2024-05-13-610535 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-8-8-gpt-4o-2024-05-13-610535 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-8-8-gpt-4o-2024-05-13-610535."},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-396610","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-396610","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-256-24-gpt-4o-2024-05-13-396610 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-396610 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-396610."},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-456029","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-456029","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-256-24-gpt-4o-2024-05-13-456029 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-456029 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-456029."},
	{"name":"NFCorpus-256-24-gpt-4o-2024-05-13-546049","keyword":"relevance","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-546049","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNFCorpus-256-24-gpt-4o-2024-05-13-546049 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"medical information retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the NFCorpus-256-24-gpt-4o-2024-05-13-546049 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NFCorpus-256-24-gpt-4o-2024-05-13-546049."},
	{"name":"TrCOLA","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/turkish-nlp-suite/TrCOLA","creator_name":"Turkish NLP Suite","creator_url":"https://huggingface.co/turkish-nlp-suite","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTrCOLA - Corpus of Linguistic Acceptability for Turkish Language\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TrCOLA\\n\\t\\n\\nTrCOLA is the Turkish version of CoLA dataset, The Corpus of Linguistic Acceptability.\\nThis dataset introduces linguistic acceptability task for Turkish. The total dataset size is 9.9K instances.\\nEach instance of the dataset is an original and correct sentence, variation of sentence that is produced in a specific way, the variation type and a binary label stating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/turkish-nlp-suite/TrCOLA."},
	{"name":"ProgressGym-HistText","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"*Huggingface dataset preview for 19th, 20th, and 21st centuries is not available due to lack of support for array types. Instead, consider downloading those files for manual inspection, or see the Data Samples section below for more examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProgressGym-HistText\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe ProgressGym Framework\\n\\t\\n\\n\\nProgressGym-HistText is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-HistText."},
	{"name":"ProgressGym-TimelessQA","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-TimelessQA","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProgressGym-TimelessQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe ProgressGym Framework\\n\\t\\n\\n\\nProgressGym-TimelessQA is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AI alignment algorithms, as a measure to prevent risks of societal value lock-in. \\nTo quote the paper ProgressGym: Alignment with a Millennium of Moral Progress:\\n\\nFrontier AI systems, including large language models (LLMs), hold increasing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-TimelessQA."},
	{"name":"ProgressGym-MoralEvals","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PKU-Alignment/ProgressGym-MoralEvals","creator_name":"PKU-Alignment","creator_url":"https://huggingface.co/PKU-Alignment","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProgressGym-MoralEvals\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe ProgressGym Framework\\n\\t\\n\\n\\nProgressGym-MoralEvals is part of the ProgressGym framework for research and experimentation on progress alignment - the emulation of moral progress in AI alignment algorithms, as a measure to prevent risks of societal value lock-in. \\nTo quote the paper ProgressGym: Alignment with a Millennium of Moral Progress:\\n\\nFrontier AI systems, including large language models (LLMs), hold increasing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PKU-Alignment/ProgressGym-MoralEvals."},
	{"name":"Buzz-slice-1-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-1-10-V1.2."},
	{"name":"Buzz-slice-10-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-10-10-V1.2."},
	{"name":"Buzz-slice-2-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-2-10-V1.2."},
	{"name":"Buzz-slice-3-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-3-10-V1.2."},
	{"name":"Buzz-slice-4-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-4-10-V1.2."},
	{"name":"Buzz-slice-5-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-5-10-V1.2."},
	{"name":"Buzz-slice-6-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-6-10-V1.2."},
	{"name":"Buzz-slice-7-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-7-10-V1.2."},
	{"name":"Buzz-slice-8-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-8-10-V1.2."},
	{"name":"Buzz-slice-9-10-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-slice-9-10-V1.2."},
	{"name":"or-bench","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bench-llms/or-bench","creator_name":"bench-llm","creator_url":"https://huggingface.co/bench-llms","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\\n\\t\\n\\nPlease see our demo at HuggingFace Spaces. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverall Plots of Model Performances\\n\\t\\n\\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot a blue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bench-llms/or-bench."},
	{"name":"rublimp","keyword":"acceptability-classification","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RussianNLP/rublimp","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRuBLiMP\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nRuBLiMP, or Russian Benchmark of Linguistic Minimal Pairs, is the first diverse and large-scale benchmark of minimal pairs in Russian.\\nRuBLiMP includes 45k minimal pairs of sentences that differ in grammaticality and isolate morphological, syntactic, or semantic phenomena. In contrast to existing benchmarks of linguistic minimal pairs, RuBLiMP is created by applying linguistic perturbations to automatically annotated sentences from open‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RussianNLP/rublimp."},
	{"name":"hcm-examples-aug-2024","keyword":"hallucination","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vectara/hcm-examples-aug-2024","creator_name":"Vectara","creator_url":"https://huggingface.co/vectara","description":"Dataset of some examples with hallucinations before and after passing through Vectara's Hallucination Correction Model. See our blogpost for details.\\n"},
	{"name":"or-bench","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/orbench-llm/or-bench","creator_name":"orbench-llm","creator_url":"https://huggingface.co/orbench-llm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\\n\\t\\n\\nPlease see our leaderboard at HuggingFace Spaces. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverall Plots of Model Performances\\n\\t\\n\\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/orbench-llm/or-bench."},
	{"name":"MCEval8K","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","description":"iszhaoxin/MCEval8K dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"CounterEval","keyword":"explainability","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anitera/CounterEval","creator_name":"Marharyta Domnich","creator_url":"https://huggingface.co/anitera","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCounterEval: Towards Unifying Evaluation of Counterfactual Explanations\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nCounterEval offers a human-evaluated dataset of 30 counterfactual scenarios, engineered to serve as a benchmark for evaluating a wide range of counterfactual explanation generation frameworks. Each scenario has been strategically designed to exhibit varying levels across multiple explanatory quality metrics, such as Feasibility, Consistency, Trust, Completeness, Fairness, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anitera/CounterEval."},
	{"name":"GlobalRG-Grounding","keyword":"grounding","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UBC-VL/GlobalRG-Grounding","creator_name":"UBCVL","creator_url":"https://huggingface.co/UBC-VL","description":"\\n\\t\\n\\t\\t\\n\\t\\tGlobalRG - Cultural Visual Grounding Task\\n\\t\\n\\nDespite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity, but they have limited coverage of cultures and do not adequately assess cultural diversity across universal as well as culture-specific local concepts. We introduce the GlobalRG-Grounding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UBC-VL/GlobalRG-Grounding."},
	{"name":"GroundCap","keyword":"grounding","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/daniel3303/GroundCap","creator_name":"Daniel Oliveira","creator_url":"https://huggingface.co/daniel3303","description":"\\n\\t\\n\\t\\t\\n\\t\\tGroundCap Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGroundCap is a novel grounded image captioning dataset derived from MovieNet, containing 52,350 movie frames with detailed grounded captions. The dataset uniquely features an ID-based system that maintains object identity throughout captions, enables tracking of object interactions, and grounds not only objects but also actions and locations in the scene.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nEach sample in the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daniel3303/GroundCap."},
	{"name":"casimedicos-arg","keyword":"explainability","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HiTZ/casimedicos-arg","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n    \\n    \\n    \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\\n\\t\\n\\nCasiMedicos-Arg is, to the best of our knowledge, the first \\nmultilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are \\nenriched with a natural language explanation written by doctors. \\nThe casimedicos-exp have been manually annotated with \\nargument components (i.e., premise, claim) and argument‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/casimedicos-arg."},
	{"name":"data-advisor-safety-alignment","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fwnlp/data-advisor-safety-alignment","creator_name":"Fei Wang","creator_url":"https://huggingface.co/fwnlp","description":"[EMNLP 2024] Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models\\nüåê Homepage | üìñ Paper  | ü§ó Dataset (Data Advisor) | ü§ó Dataset (Self-Instruct)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDisclaimer\\n\\t\\n\\nThe dataset contains content that may be offensive or harmful. This dataset is intended for research purposes, specifically to support efforts aimed at creating safer and less harmful AI systems. Please engage with it responsibly and at your own risk.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fwnlp/data-advisor-safety-alignment."},
	{"name":"PHTest","keyword":"alignment","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/furonghuang-lab/PHTest","creator_name":"Furong Huang's Lab at UMD","creator_url":"https://huggingface.co/furonghuang-lab","description":"üåü PHTest: Evaluating False Refusals in LLMs\\n\\n\\n  ü§ñ Auto Red-Teaming\\n    \\n      All prompts are generated automatically using a controllable text-generation technique called AutoDAN.\\n    \\n  \\n  \\n  üåê Diverse Prompts\\n    \\n      PHTest introduces false refusal patterns that aren‚Äôt present in existing datasets, including prompts that avoid mentioning sensitive words.\\n    \\n  \\n  \\n  ‚öñÔ∏è Harmlessness & Controversial Labeling\\n    \\n      Controversial prompts are separately labeled to address the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/furonghuang-lab/PHTest."},
	{"name":"CTO","keyword":"acceptability-classification","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/chufangao/CTO","creator_name":"Chufan Gao","creator_url":"https://huggingface.co/chufangao","description":"Dataset for predicting clinical trial outcomes in drug development.  This dataset is part of the work presented in \\\"Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark for Drug Development\\\".\\nWebsite: https://chufangao.github.io/CTOD/\\nPaper: https://arxiv.org/abs/2406.10292\\nCode: https://github.com/chufangao/ctod\\nDescriptions:\\n\\nhuman_labels contains the manually annotated subset. We follow the same rule-based termination of incomplete status and p-value < 0.05 as in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chufangao/CTO."},
	{"name":"mid-space","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CUPUM/mid-space","creator_name":"CUPUM","creator_url":"https://huggingface.co/CUPUM","description":"\\n\\t\\n\\t\\t\\n\\t\\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLIVS PAPER \\n\\t\\n\\n\\nClick below to see more:\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CUPUM/mid-space."},
	{"name":"blip3-grounding-50m","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\\n\\t\\n\\t\\t\\n\\t\\tBLIP3-GROUNDING-50M Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \\\"the object to the left of the dog\\\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m."},
	{"name":"or-bench-toxic-all","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bench-llms/or-bench-toxic-all","creator_name":"bench-llm","creator_url":"https://huggingface.co/bench-llms","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\\n\\t\\n\\nThis dataset constains highly toxic prompts, use with caution!!!\\nPlease see our demo at HuggingFace Spaces. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverall Plots of Model Performances\\n\\t\\n\\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bench-llms/or-bench-toxic-all."},
	{"name":"raghalu-open","keyword":"hallucination","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liveperson/raghalu-open","creator_name":"LivePerson Inc.","creator_url":"https://huggingface.co/liveperson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for RAGHalu Open Source Data\\n\\t\\n\\nThis dataset is the public data portion from the paper Two-tiered\\nEncoder-based Hallucination Detection for Retrieval-Augmented Generation\\nin the Wild by Ilana Zimmerman, Jadin Tredup, Ethan Selfridge, and\\nJoseph Bradley, accepted at EMNLP 2024\\n(Industry Track). The private brand data portion of the dataset is not\\nincluded.\\nNote that this dataset and the paper do not use the common hallucination\\nterms factuality and faithfulness as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/liveperson/raghalu-open."},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Alibaba Wan2.1 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1."},
	{"name":"text-2-video-human-preferences-veo2","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Google DeepMind Veo2 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to date‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2."},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Runway Alpha Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha."},
	{"name":"blimp","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyu-mll/blimp","creator_name":"NYU Machine Learning for Language","creator_url":"https://huggingface.co/nyu-mll","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"blimp\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nBLiMP is a challenge set for evaluating what language models (LMs) know about\\nmajor grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\\nmorphology, or semantics. The data is automatically generated according to\\nexpert-crafted grammars.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/blimp."},
	{"name":"rag-hallucination-dataset-1000","keyword":"hallucination","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neural-bridge/rag-hallucination-dataset-1000","creator_name":"Neural Bridge AI","creator_url":"https://huggingface.co/neural-bridge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRetrieval-Augmented Generation (RAG) Hallucination Dataset 1000\\n\\t\\n\\nRetrieval-Augmented Generation (RAG) Hallucination Dataset 1000 is an English dataset designed to reduce the hallucination in RAG-optimized models, built by Neural Bridge AI, and released under Apache license 2.0.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nHallucination in large language models (LLMs) refers to the generation of incorrect, nonsensical, or unrelated text that does not stem from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neural-bridge/rag-hallucination-dataset-1000."},
	{"name":"opin-pref","keyword":"alignment","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \\nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\\nhf.co/swaroop-nath/prompt-opin-summ dataset.\\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref."},
	{"name":"RLSTACK","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/RLSTACK","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/RLSTACK."},
	{"name":"or-bench","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bench-llm/or-bench","creator_name":"Bench LLM","creator_url":"https://huggingface.co/bench-llm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOR-Bench: An Over-Refusal Benchmark for Large Language Models\\n\\t\\n\\nPlease see our demo at HuggingFace Spaces. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverall Plots of Model Performances\\n\\t\\n\\nBelow is the overall model performance. X axis shows the rejection rate on OR-Bench-Hard-1K and Y axis shows the rejection rate on OR-Bench-Toxic. The best aligned model should be on the top left corner of the plot where the model rejects the most number of toxic prompts and least number of safe prompts. We also plot a blue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bench-llm/or-bench."},
	{"name":"Buzz-V1.2","keyword":"alignment-lab-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/H-D-T/Buzz-V1.2","creator_name":"Hive-Digital-Technologies","creator_url":"https://huggingface.co/H-D-T","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBuzz: Advancing Efficiency through Iterative Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\n\\nAlignment Lab AI is pleased to introduce our latest research efforts with:\\n\\nBuzz, a highly curated pretraining scale assistant dataset, unifying RL and SFT, developed in collaboration with Hive Digital Technologies. \\nThe Buzz model, Dataset, and Code are to be released to build a toolkit that aims to demonstrate the potential for reuse and optimization of existing pretrained language models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/H-D-T/Buzz-V1.2."},
	{"name":"twinviews-13k","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TwinViews-13k\\n\\t\\n\\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k."},
	{"name":"twinviews-13k","keyword":"fairness","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wwbrannon/twinviews-13k","creator_name":"William Brannon","creator_url":"https://huggingface.co/wwbrannon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for TwinViews-13k\\n\\t\\n\\nThis dataset contains 13,855 pairs of left-leaning and right-leaning political statements matched by topic. The dataset was generated using GPT-3.5 Turbo and has been audited to ensure quality and ideological balance. It is designed to facilitate the study of political bias in reward models and language models, with a focus on the relationship between truthfulness and political views.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wwbrannon/twinviews-13k."},
	{"name":"VISCO","keyword":"hallucination","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/uclanlp/VISCO","creator_name":"UCLA NLP","creator_url":"https://huggingface.co/uclanlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVISCO\\n\\t\\n\\nBenchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning\\nüåê Project | üìñ Paper | üíª Github\\n\\n\\nOutline:\\n\\nIntroduction\\nData\\nCitation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nVISCO is a benchmark for evaluating the critique and correction capabilities of LVLMs. VISCO contains:\\n\\n1645 pairs of questions and LVLM-generated answers. Each answer includes a chain-of-thought with multiple reasonign steps.\\n5604 step-wise annotations of critique, showing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uclanlp/VISCO."},
	{"name":"mid-space","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mila-ai4h/mid-space","creator_name":"Mila AI4H","creator_url":"https://huggingface.co/mila-ai4h","description":"\\n\\t\\n\\t\\t\\n\\t\\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLIVS PAPER \\n\\t\\n\\n\\nClick below to see more:\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mila-ai4h/mid-space."},
	{"name":"text-2-video-human-preferences","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Preference Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\\n\\nSora\\nHunyouan\\nPika 2.0\\nRunway ML Alpha\\nLuma Ray 2\\n\\nExplore our latest model rankings on our website.\\nIf you get value from this dataset and would‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences."},
	{"name":"FACTS-grounding-public","keyword":"grounding","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/FACTS-grounding-public","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFACTS Grounding 1.0 Public Examples\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t860 public FACTS Grounding examples from Google DeepMind and Google Research\\n\\t\\n\\nFACTS Grounding is a benchmark from Google DeepMind and Google Research designed to measure the performance of AI Models on factuality and grounding. \\n‚ñ∂ FACTS Grounding Leaderboard on Kaggle‚ñ∂ Technical Report‚ñ∂ Evaluation Starter Code‚ñ∂ Google DeepMind Blog Post\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe FACTS Grounding benchmark evaluates the ability of Large Language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/FACTS-grounding-public."},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\n\\n\\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback."},
	{"name":"LayoutSAM-eval","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLayoutSAM-eval Benchmark\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLM‚Äôs visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval."},
	{"name":"LayoutSAM","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLayoutSAM Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description.\\nTraditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM."},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Prompt Alignment Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring."},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Luma Ray2 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2."},
	{"name":"sora-video-generation-aligned-words","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Word for Word Alignment Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words."},
	{"name":"sora-video-generation-time-flow","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Time flow Annotation Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \\\"How‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow."},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Rich Human Feedback Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \\n\\nPrompt - Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback."},
	{"name":"ethical-framework-UNESCO-Ethics-of-AI","keyword":"fairness","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ktiyab/ethical-framework-UNESCO-Ethics-of-AI","creator_name":"Tiyab K.","creator_url":"https://huggingface.co/ktiyab","description":"\\n\\t\\n\\t\\t\\n\\t\\tEthical AI Training Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nUNESCO's Ethics of Artificial Intelligence, adopted by 193 Member States in November 2021, represents the first global framework for ethical AI development and deployment.\\nWhile regional initiatives like The Montr√©al Declaration for a Responsible Development of Artificial Intelligence emphasize community-driven governance, UNESCO's approach establishes comprehensive international standards through coordinated multi-stakeholder‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/ethical-framework-UNESCO-Ethics-of-AI."},
	{"name":"cultural_heritage_metadata_accuracy","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/biglam/cultural_heritage_metadata_accuracy","creator_name":"BigLAM: BigScience Libraries, Archives and Museums","creator_url":"https://huggingface.co/biglam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Annotated dataset to assess the accuracy of the textual description of cultural heritage records\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe dataset contains more than 100K textual descriptions of cultural items from Cultura Italia, the Italian National Cultural aggregator. Each of the description is labeled either HIGH or LOW quality, according its adherence to the standard cataloguing guidelines provided by Istituto Centrale per il Catalogo e la Documentazione (ICCD).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biglam/cultural_heritage_metadata_accuracy."},
	{"name":"openai-tldr-summarisation-preferences","keyword":"alignment","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-summarisation-preferences","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHuman feedback data\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nSee https://github.com/openai/summarize-from-feedback for original details of the dataset.\\nHere the data is formatted to enable huggingface transformers sequence classification models to be trained as reward functions.\\n"},
	{"name":"glue-ci","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/evaluate/glue-ci","creator_name":"evaluate","creator_url":"https://huggingface.co/evaluate","description":"GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems."},
	{"name":"openai-tldr-filtered","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiltered TL;DR Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\\n"},
	{"name":"openai-tldr-filtered-queries","keyword":"alignment","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiltered TL;DR Dataset\\n\\t\\n\\nThis is the version of the dataset used in https://arxiv.org/abs/2310.06452.\\nIf starting a new project we would recommend using https://huggingface.co/datasets/openai/summarize_from_feedback.\\nFor more information see https://github.com/openai/summarize-from-feedback and for the original TL;DR dataset see https://zenodo.org/record/1168855#.YvzwJexudqs\\nThis is the version of the dataset with only filtering on the queries, and hence there is more data than in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCL-DARK/openai-tldr-filtered-queries."},
	{"name":"glue","keyword":"acceptability-classification","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/glue","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems."},
	{"name":"panda","keyword":"fairness","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/panda","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PANDA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPANDA (Perturbation Augmentation NLP DAtaset) consists of approximately 100K pairs of crowdsourced human-perturbed text snippets (original, perturbed). Annotators were given selected terms and target demographic attributes, and instructed to rewrite text snippets along three demographic axes: gender, race and age, while preserving semantic meaning. Text snippets were sourced from a range of text corpora (BookCorpus, Wikipedia‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/panda."},
	{"name":"pile-detoxify","keyword":"acceptability-classification","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tomekkorbak/pile-detoxify","creator_name":"Tomek Korbak","creator_url":"https://huggingface.co/tomekkorbak","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for pile-pii-scrubadub\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains text from The Pile, annotated based on the toxicity of each sentence.\\nEach document (row in the dataset) is segmented into sentences, and each sentence is given a score: the toxicity predicted by the Detoxify.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis dataset is taken from The Pile, which is English text.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomekkorbak/pile-detoxify."},
	{"name":"pile-pii-scrubadub","keyword":"acceptability-classification","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tomekkorbak/pile-pii-scrubadub","creator_name":"Tomek Korbak","creator_url":"https://huggingface.co/tomekkorbak","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for pile-pii-scrubadub\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains text from The Pile, annotated based on the personal idenfitiable information (PII) in each sentence.\\nEach document (row in the dataset) is segmented into sentences, and each sentence is given a score: the percentage of words in it that are classified as PII by Scrubadub.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis dataset is taken‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomekkorbak/pile-pii-scrubadub."},
	{"name":"acceptability-prediction","keyword":"acceptability-classification","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/metaeval/acceptability-prediction","creator_name":"metaeval","creator_url":"https://huggingface.co/metaeval","description":"@inproceedings{lau-etal-2015-unsupervised,\\n    title = \\\"Unsupervised Prediction of Acceptability Judgements\\\",\\n    author = \\\"Lau, Jey Han  and\\n      Clark, Alexander  and\\n      Lappin, Shalom\\\",\\n    booktitle = \\\"Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\\\",\\n    month = jul,\\n    year = \\\"2015\\\",\\n    address = \\\"Beijing, China\\\",\\n    publisher = \\\"Association for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/metaeval/acceptability-prediction."},
	{"name":"rankme-nlg-acceptability","keyword":"acceptability-classification","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/metaeval/rankme-nlg-acceptability","creator_name":"metaeval","creator_url":"https://huggingface.co/metaeval","description":"@inproceedings{novikova-etal-2018-rankme,\\n    title = \\\"RankME: Reliable Human Ratings for Natural Language Generation\\\",\\n    author = \\\"Novikova, Jekaterina  and\\n      Duvsek, Ondvrej  and\\n      Rieser, Verena\\\",\\n    booktitle = \\\"Proceedings of the NAACL2018\\\",\\n    month = jun,\\n    year = \\\"2018\\\",\\n    address = \\\"New Orleans, Louisiana\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https://aclanthology.org/N18-2012\\\",\\n    doi = \\\"10.18653/v1/N18-2012\\\",\\n    pages = \\\"72--78\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/metaeval/rankme-nlg-acceptability."},
	{"name":"black-box-api-challenges","keyword":"fairness","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/black-box-api-challenges","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nPaper: On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research\\nAbstract: Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/black-box-api-challenges."},
	{"name":"instruct-aira-dataset","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\\n\\nLanguage modeling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset."},
	{"name":"reward-aira-dataset","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReward-Aira Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of prompt + completion examples of LLM following instructions in a conversational manner. All prompts come with two possible completions (one better than the other). The dataset is available in both Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized to train a reward/preference model or DPO fine-tuning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/reward-aira-dataset."},
	{"name":"FactCHD","keyword":"hallucination","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/FactCHD","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"zjunlp/FactCHD dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"sensory-awareness-benchmark","keyword":"alignment","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark","creator_name":"Nick Doiron","creator_url":"https://huggingface.co/monsoon-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSensory Awareness Benchmark\\n\\t\\n\\nA series of questions (goal is 100-200) and required features, designed to test whether any ML model is aware of its own capabilities.\\nControl questions are connected to a specific capability:\\n\\nCan you receive an image file?\\nWould you consider your level to be that of a super-intelligent AI agent?\\n\\nNatural questions which are possible for the average person, but may require multiple capabilities for a model:\\n\\nCan you head to the corner and check if my‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/monsoon-nlp/sensory-awareness-benchmark."},
	{"name":"spanex","keyword":"explainability","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/spanex","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"SpanEx consists of 7071 instances annotated for span interactions.\\nSpanEx is the first dataset with human phrase-level interaction explanations with explicit labels for interaction types. \\nMoreover, SpanEx is annotated by three annotators, which opens new avenues for studies of human explanation agreement -- an understudied area in the explainability literature. \\nOur study reveals that while human annotators often agree on span interactions, they also offer complementary reasons for a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/spanex."},
	{"name":"lumos_complex_qa_ground_onetime","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime."},
	{"name":"lumos_complex_qa_ground_iterative","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative."},
	{"name":"lumos_unified_ground_iterative","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative."},
	{"name":"lumos_maths_ground_iterative","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative."},
	{"name":"lumos_maths_ground_onetime","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime."},
	{"name":"lumos_web_agent_ground_iterative","keyword":"grounding","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative."},
	{"name":"instruct-aira-dataset-v3","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 3.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3."},
	{"name":"GPT-4o-evaluation-biases","keyword":"fairness","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mtec-TUB/GPT-4o-evaluation-biases","creator_name":"Electronic Systems of Medical Engineering","creator_url":"https://huggingface.co/mtec-TUB","description":"\\n\\t\\n\\t\\t\\n\\t\\tA database to support the evaluation of gender biases in GPT-4o output\\n\\t\\n\\nThe database and its construction process are described in the paper \\\"A database to support the evaluation of gender biases in GPT-4o output\\\" by Mehner et al., presented at the 1st ISCA/ITG Workshop on Diversity in Large Speech and Language Models (Berlin, Februar 20, 2025).\\n\\n\\t\\n\\t\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis is a database of prompts and answers generated with GPT-4o-mini and GPT-4o in a pretest and a main test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mtec-TUB/GPT-4o-evaluation-biases."},
	{"name":"hallucination","keyword":"hallucination","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GuardrailsAI/hallucination","creator_name":"Guardrails AI","creator_url":"https://huggingface.co/GuardrailsAI","description":"This is a vendored reupload of the Benchmarking Unfaithful Minimal Pairs (BUMP) Dataset available at https://github.com/dataminr-ai/BUMP\\nThe BUMP (Benchmark of Unfaithful Minimal Pairs) dataset stands out as a superior choice for evaluating hallucination detection systems due to its quality and realism. Unlike synthetic datasets such as TruthfulQA, HalluBench, or FaithDial that rely on LLMs to generate hallucinations, BUMP employs human annotators to manually introduce errors into summaries‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GuardrailsAI/hallucination."},
	{"name":"Aloe-Beta-DPO","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated DPO datasets used to align Aloe-Beta.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThe first stage of the Aloe-Beta alignment process. We curated data from many publicly available data sources, including three different types of data:\\n\\nMedical preference data: TsinghuaC3I/UltraMedical-Preference\\n\\nGeneral preference data: BAAI/Infinity-Preference and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-DPO."},
	{"name":"dialectic-preferences-bias-aae-sae-parallel","keyword":"fairness","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/furquan/dialectic-preferences-bias-aae-sae-parallel","creator_name":"Furquan Hassan","creator_url":"https://huggingface.co/furquan","description":"\\n\\t\\n\\t\\t\\n\\t\\tDialectic Preferences Bias Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is part of a research study examining dialectic preference bias in Large Language Models (LLMs). It contains paired sentences in African American English (AAE) and Standard American English (SAE), used to analyze potential biases in language models' treatment of different dialects.\\nThe dataset contains two columns:\\nafrican_american_english: Text samples in African American English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/furquan/dialectic-preferences-bias-aae-sae-parallel."},
	{"name":"multicultural-wvs-alignment","keyword":"alignment","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ryzzlestrizzle/multicultural-wvs-alignment","creator_name":"Jonathan Rystr√∏m","creator_url":"https://huggingface.co/ryzzlestrizzle","description":"ryzzlestrizzle/multicultural-wvs-alignment dataset hosted on Hugging Face and contributed by the HF Datasets community"}
]
;
