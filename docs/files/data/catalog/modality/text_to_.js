const data_for_modality_text_to_ = 
[
	{"name":"emova-sft-4m","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-sft-4m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-SFT-4M\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-SFT-4M is a comprehensive dataset curated for omni-modal instruction tuning, including textual, visual, and audio interactions. This dataset is created by gathering open-sourced multi-modal instruction datasets and synthesizing high-quality omni-modal conversation data to enhance user experience. This dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-sft-4m.","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 1-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-11.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Rosmontis","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Rosmontis","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Rosmontis dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","audio-to-audio","Chinese","Japanese","Korean"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v28","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v28.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v19","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\nimage size: 4-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response. Argh I forgot to enable this. Using RLE compression.\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v162","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v162","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v162.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v163","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v163","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v163.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v165","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v165","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v165.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v167","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v167","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v167.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v168","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v168","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v168.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v172","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v172","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v172.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v174","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v174","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v174.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v176","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v176","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v176.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v177","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v177","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v177.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v180","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v180","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v180.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v181","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v181","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v181.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LeX-10K","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/X-ART/LeX-10K","creator_name":"X-ART","creator_url":"https://huggingface.co/X-ART","description":"\n\n\t\n\t\t\n\t\tüñºÔ∏è LeX-10K: High-Quality Dataset for Text Rendering\n\t\n\nLeX-10K is a curated dataset of 10K high-resolution, visually diverse 1024√ó1024 images tailored for text-to-image generation with a focus on aesthetics, text fidelity, and stylistic richness.\nProject Page | Paper\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåü Why LeX-10K?\n\t\n\nWe compare LeX-10K with two widely used datasets: AnyWord-3M and MARIO-10M.As shown below, LeX-10K significantly outperforms both in terms of aesthetic quality, text readability, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/X-ART/LeX-10K.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ECG-Grounding","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LANSG/ECG-Grounding","creator_name":"LAN XIANG","creator_url":"https://huggingface.co/LANSG","description":"\n\t\n\t\t\n\t\tECG-Grounding dataset\n\t\n\nECG-Grounding provides more accurate, holistic, and evidence-driven interpretations with diagnoses grounded in measurable ECG features.¬†Currently, it contains 30,000 instruction pairs annotated with heartbeat-level physiological features. This is the first high-granularity ECG grounding dataset, enabling evidence-based diagnosis and improving the trustworthiness of medical AI. We will continue to release more ECG-Grounding data and associated beat-level‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LANSG/ECG-Grounding.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"KVG-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MaxyLee/KVG-Bench","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n \n\n \n \nThis is the official repository of KVG-Bench, a comprehensive benchmark of Knowledge-intensive Visual Grounding (KVG) spanning 10 categories with 1.3K manually curated test cases.\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SightationReasoning","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sightation/SightationReasoning","creator_name":"The Sightation Collection","creator_url":"https://huggingface.co/Sightation","description":"\n\t\n\t\t\n\t\tSighationReasoning\n\t\n\nSightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions\n\n\nüìÑ arXiv\nü§ó Dataset\n\n\nOften, the needs and visual abilities differ between the annotator group and the end user group.\nGenerating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain.\nSighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sightation/SightationReasoning.","first_N":5,"first_N_keywords":["image-to-text","image-text-to-text","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SID_Set","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/saberzl/SID_Set","creator_name":"hzl","creator_url":"https://huggingface.co/saberzl","description":"\n\t\n\t\t\n\t\tDataset Card for SID_Set\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe provide Social media Image Detection dataSet (SID-Set), which offers three key advantages:\n\nExtensive volume: Featuring 300K AI-generated/tampered and authentic images with comprehensive annotations.\nBroad diversity: Encompassing fully synthetic and tampered images across various classes.\nElevated realism: Including images that are predominantly indistinguishable from genuine ones through mere visual inspection.\n\nPlease check‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/saberzl/SID_Set.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PEBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xuzhaopan/PEBench","creator_name":"xu","creator_url":"https://huggingface.co/xuzhaopan","description":"\n\t\n\t\t\n\t\tPEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models\n\t\n\nPaper\nPEBench, a comprehensive benchmark for evaluating machine unlearning in MLLMs, focusing on both personal entities and event scenes to provide a holistic assessment of unlearning efficacy and scope.\nMore details on loading and using the data are at our github page.\nIf you do find our code helpful or use our benchmark dataset, please citing our paper.\n@article{xu2025pebench‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xuzhaopan/PEBench.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"gen_image_wordnet_preferences","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VityaVitalich/gen_image_wordnet_preferences","creator_name":"Viktor Moskvoretskii","creator_url":"https://huggingface.co/VityaVitalich","description":"This dataset contains generated images.  See the associated Hugging Face Collection for examples and additional details: Generated Image Wordnet\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","csv","Tabular"],"keywords_longer_than_N":true},
	{"name":"spider-test-portuguese","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boakpe/spider-test-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","description":"\n\t\n\t\t\n\t\tSpider Dataset - Vers√£o em Portugu√™s\n\t\n\nEste reposit√≥rio cont√©m a tradu√ß√£o para portugu√™s da parti√ß√£o de teste do dataset Spider, um benchmark para a tarefa de Text-to-SQL.\n\n\t\n\t\t\n\t\tSobre esta tradu√ß√£o\n\t\n\nA tradu√ß√£o da parti√ß√£o \"test\" do Spider (contendo 2.147 inst√¢ncias) foi realizada seguindo um processo rigoroso:\n\nTradu√ß√£o inicial: Utilizando a API do GPT-4o mini da OpenAI\nRevis√£o manual: Todas as 2.147 quest√µes foram revisadas e validadas manualmente\nCrit√©rios de tradu√ß√£o:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boakpe/spider-test-portuguese.","first_N":5,"first_N_keywords":["Portuguese","cc-by-sa-4.0","1K<n<10K","arxiv:1809.08887","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"GenS-Video-150K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yaolily/GenS-Video-150K","creator_name":"Linli Yao","creator_url":"https://huggingface.co/yaolily","description":"\nüîó Project Page ¬∑ üìñ Paper ¬∑ ‚≠ê GitHub ¬∑ üìä Dataset ¬∑ ü§ó Checkpoints\n\n\n\n\n\t\n\t\t\n\t\tGenS-Video-150K Dataset\n\t\n\nTo enable effective frame sampling, we introduce GenS-Video-150K, a large-scale synthetic dataset specifically designed for training frame sampling models. Annotated by GPT-4o, this dataset features:\n\nDense coverage: Annotates ~20% of all frames with relevance scores.\nFine-grained assessment: Assigns confidence scores (level 1 to 5) to relevant frames.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yaolily/GenS-Video-150K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2503.09146"],"keywords_longer_than_N":true},
	{"name":"Vchitect_T2V_DataVerse","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vchitect/Vchitect_T2V_DataVerse","creator_name":"Vchitect","creator_url":"https://huggingface.co/Vchitect","description":"\n\t\n\t\t\n\t\tVchitect-T2V-Dataverse\n\t\n\n\n\n    Vchitect Team1‚ÄÉ\n\n\n\n    1Shanghai Artificial Intelligence Laboratory‚ÄÉ\n\n \n \n\n                      Paper | \n                      Project Page |\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Overview\n\t\n\nThe Vchitect-T2V-Dataverse is the core dataset used to train our text-to-video diffusion model, Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models.\nIt comprises 14 million high-quality videos collected from the Internet, each paired with detailed textual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vchitect/Vchitect_T2V_DataVerse.","first_N":5,"first_N_keywords":["text-to-video","apache-2.0","1M - 10M","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"FlintstonesSV_Plus_Plus","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Janak12/FlintstonesSV_Plus_Plus","creator_name":"Janak Kapuriya","creator_url":"https://huggingface.co/Janak12","description":"\n\t\n\t\t\n\t\tüöÄüöÄüöÄ Paper Information\n\t\n\n\nPaper Title: FlintstonesSV++: Improving Story Narration using Visual Scene Graph\nAccepted at: Text2Story Workshop, ECIR Conference 2025, Lucca, Italy.\nAuthors: Janak Kapuriya, Paul Buitelaar\nOrganization: Insight Research Ireland Center for Data Analytics, University of Galway, Ireland.\nGithub Link: https://github.com/janak11111/FlintstonesSV_Plus_Plus\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåü FlintstonesSV++\n\t\n\nThe FlintstonesSV++ dataset is an enhanced version of the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Janak12/FlintstonesSV_Plus_Plus.","first_N":5,"first_N_keywords":["text-to-image","visual-question-answering","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"InstructCLIP-InstructPix2Pix-Data","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SherryXTChen/InstructCLIP-InstructPix2Pix-Data","creator_name":"Sherry Chen","creator_url":"https://huggingface.co/SherryXTChen","description":"\n\t\n\t\t\n\t\tDataset Card for InstructCLIP-InstructPix2Pix-Data\n\t\n\nThe dataset can be used to train instruction-guided image editing models. \nIt is built on top of InstructPix2Pix CLIP-filtered with new edit instructions.\nFor each sample, source_image is the original image, instruction is the edit instruction, target_image is the edited image, and original_instruction is the edit instruction from the InstructPix2Pix CLIP-filtered dataset.\nPlease refer to our repo to see how the edit instructions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SherryXTChen/InstructCLIP-InstructPix2Pix-Data.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"V1-33K-Old","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K-Old","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nüöÄ  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction üåü\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K-Old.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Video-MMLU","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enxin/Video-MMLU","creator_name":"EnxinSong","creator_url":"https://huggingface.co/Enxin","description":"\n\n\n\t\n\t\t\n\t\tVideo-MMLU Benchmark\n\t\n\n\n  \n    \n    \n    \n    \n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: Video-MMLU Benchmark\n\n\n\t\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\tBenchmark Collection and Processing\n\t\n\nVideo-MMLU specifically targets videos that focus on theorem demonstrations and probleming-solving, covering mathematics, physics, and chemistry. The videos deliver dense information through numbers and formulas, pose significant challenges for video LMMs in dynamic OCR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enxin/Video-MMLU.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MiscSpeech-ja","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rakuto/MiscSpeech-ja","creator_name":"rakuto","creator_url":"https://huggingface.co/Rakuto","description":"\n\t\n\t\t\n\t\tMiscSpeech-ja\n\t\n\nThis dataset comprises audio and corresponding transcripts collected from a diverse range of YouTube videos and Podcasts. \n","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Japanese","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"boliu","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tianyu92/boliu","creator_name":"wutianyu","creator_url":"https://huggingface.co/tianyu92","description":"\n  \n  Image generated by DALL-E. See prompt for more details\n\n\n\n\t\n\t\t\n\t\tsynthetic_text_to_sql\n\t\n\n\ngretelai/synthetic_text_to_sql is a rich dataset of high quality synthetic Text-to-SQL samples, \ndesigned and generated using Gretel Navigator, and released under Apache 2.0.\nPlease see our release blogpost for more details.\nThe dataset includes:\n\n  105,851 records partitioned into 100,000 train and 5,851 test records\n  ~23M total tokens, including ~12M SQL tokens\n  Coverage across 100 distinct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tianyu92/boliu.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"pxhere","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/pxhere","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for pxhere Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a large collection of high-quality photographs sourced from pxhere.com, a free stock photo website. The dataset includes approximately 1,100,000 images in full resolution covering a wide range of subjects including nature, people, urban environments, objects, animals, and landscapes. All images are provided under the Creative Commons Zero (CC0) license, making them freely available for personal and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pxhere.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"SMMILE","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VideoGameQA-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/taesiri/VideoGameQA-Bench","creator_name":"taesiri","creator_url":"https://huggingface.co/taesiri","description":"\n\t\n\t\t\n\t\tVideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance\n\t\n\nby Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman, Cor-Paul Bezemer\n\n\t\n\t\t\n\t\tAbstract:\n\t\n\n\nWith video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/taesiri/VideoGameQA-Bench.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ViStoryBench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ViStoryBench/ViStoryBench","creator_name":"ViStoryBench","creator_url":"https://huggingface.co/ViStoryBench","description":"\n\t\n\t\t\n\t\tModel Card: ViStoryBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nViStoryBench is a comprehensive benchmark dataset for story visualization. It aims to thoroughly evaluate and advance the performance of story visualization models by providing diverse story types, artistic styles, and detailed annotations. The goal of story visualization is to generate a sequence of visually coherent and content-accurate images based on a given narrative text and character reference images.\nKey features of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ViStoryBench/ViStoryBench.","first_N":5,"first_N_keywords":["text-to-image","human-annotated","machine-generated","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"TTA-Bench","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hui519/TTA-Bench","creator_name":"hui wang","creator_url":"https://huggingface.co/Hui519","description":"\n\t\n\t\t\n\t\tTTA-Bench Dataset\n\t\n\n\n\t\n\t\t\n\t\tüéØ Overview\n\t\n\nWelcome to TTA-Bench! This repository contains our comprehensive evaluation framework for text-to-audio (TTA) systems. We've carefully curated 2,999 prompts across six different evaluation dimensions, creating a standardized benchmark for assessing text-to-audio generation capabilities.\n\n\t\n\t\t\n\t\tüìö Dataset Structure\n\t\n\nEach prompt in our dataset contains these essential fields:\n\nid: Unique identifier for each prompt (format: prompt_XXXX)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Hui519/TTA-Bench.","first_N":5,"first_N_keywords":["text-to-audio","English","mit","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"MusicSem","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rsalga/MusicSem","creator_name":"Rebecca Salganik","creator_url":"https://huggingface.co/Rsalga","description":"\n\t\n\t\t\n\t\tDataset Card for MusicSem\n\t\n\n\n\n\nThis dataset contains 35977 entries of text-audio pairs. There is an accompanying test set of size 480 which is withheld for leaderboard purposes. Please reach out to authors for further access.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: Rebecca Salganik, Teng Tu, Fei-Yueh Chen, Xiaohao Liu, Kaifeng Lu, Ethan Luvisia, Zhiyao Duan, Guillaume Salha-Galvan, Anson Kahng, Yunshan Ma, Jian Kang\nLanguage(s) : English\nLicense: MIT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rsalga/MusicSem.","first_N":5,"first_N_keywords":["text-to-audio","summarization","feature-extraction","audio-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"OpenSpeech-Dataset-by-Wang","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VIZINTZOR/OpenSpeech-Dataset-by-Wang","creator_name":"VYNCX","creator_url":"https://huggingface.co/VIZINTZOR","description":"Dataset ‡∏´‡∏•‡∏±‡∏Å : https://www.wang.in.th/\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataset : \n\n‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 8,450 ‡πÑ‡∏ü‡∏•‡πå\n\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 10 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\n\nSampling Rate 48,000 hz(‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏î‡πâ‡∏ß‡∏¢ AI)\n\n‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå Zip\n\nmetadata.csv\n\n\n00001.wav|‡∏Ç‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö\n00002.wav|‡∏â‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n00003.wav|‡πÉ‡∏ä‡πâ‡πÉ‡∏ö‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÑ‡∏´‡∏°‡∏Ñ‡πà‡∏∞\n....\n\n","first_N":5,"first_N_keywords":["text-to-speech","Thai","cc-by-sa-4.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"ViLBench","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/ViLBench","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"Benchmark Data for ViLBench: A Suite for Vision-Language Process Reward Modeling\narXiv | Project Page\nThere are 600 data collected from 5 existing vision-language tasks\n","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","n<1K","arxiv:2503.20271"],"keywords_longer_than_N":true},
	{"name":"MMMR","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csegirl/MMMR","creator_name":"g","creator_url":"https://huggingface.co/csegirl","description":"This repository contains the data presented in MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks.\nProject page: https://mmmr-benchmark.github.io/\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","json","Image"],"keywords_longer_than_N":true},
	{"name":"AgentSynth","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sunblaze-ucb/AgentSynth","creator_name":"sunblaze-ucb","creator_url":"https://huggingface.co/sunblaze-ucb","description":"\n\t\n\t\t\n\t\tAgentSynth\n\t\n\n\n\t\n\t\t\n\t\tAgentSynth: Scalable Task Generation for Generalist Computer-Use Agents\n\t\n\nPaper | Project Page | Code\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWe introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sunblaze-ucb/AgentSynth.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2506.14205","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"NeIn","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nhatttanbui/NeIn","creator_name":"Nhat-Tan Bui","creator_url":"https://huggingface.co/nhatttanbui","description":"\n\t\n\t\t\n\t\tNeIn: Telling What You Don‚Äôt Want (CVPRW 2025)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nNeIn is the first large-scale for studying negation in text-guided image editing. It comprises 366,957 quintuplets, i.e., source image, original caption, selected object, negative sentence, and target image in total, including 342,775 queries for training and 24,182 queries for\nbenchmarking image editing methods.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nCOCO (img): The source image from MS-COCO.\nT_original (str): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nhatttanbui/NeIn.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"NSFWErasureBench_Advprompts","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lzw3098/NSFWErasureBench_Advprompts","creator_name":"lzw","creator_url":"https://huggingface.co/lzw3098","description":"This dataset contains adversarial prompt datasets for three themes generated using the RAB method, with each prompt replicated 10 times.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"animespeech-orpheus-prep-800","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/taresh18/animespeech-orpheus-prep-800","creator_name":"Taresh Rajput","creator_url":"https://huggingface.co/taresh18","description":"Preprocessed dataset in Orpheus TTS FT format corresponding to voices [\"107\", \"125\", \"145\", \"16\", \"163\", \"179\", \"180\", \"183\", \"185\", \"187\"] from ShoukanLabs/AniSpeech\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"myface","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Temka/myface","creator_name":"Temka Serge","creator_url":"https://huggingface.co/Temka","description":"\n\t\n\t\t\n\t\tZurag Dataset with Prompts\n\t\n\nThis dataset contains a collection of face images with corresponding text descriptions (prompts). It is inspired by the structure of Temka/myface and is designed for applications such as:\n\nFine-tuning diffusion models (e.g., Stable Diffusion, LoRA)\nFace recognition\nText-to-image generation\n\n\n\t\n\t\t\n\t\tüßæ Dataset Structure\n\t\n\n\nimages/ ‚Äî folder containing all the .jpg image files.\nmetadata.csv ‚Äî a CSV file with the following columns:\nfile_name: name of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Temka/myface.","first_N":5,"first_N_keywords":["image-classification","text-to-image","manually-created","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"wan_waving_hands","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_waving_hands","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_wiping_surface","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_wiping_surface","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_writing","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_writing","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"deepseek-svg-description","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description","creator_name":"Shahzeb Khoso","creator_url":"https://huggingface.co/ShahzebKhoso","description":"\n\t\n\t\t\n\t\tSVG Reasoning and Generation Dataset\n\t\n\nA rich dataset containing SVG graphics, structured reasoning, and generated descriptions.Built from the base of thesantatitan/deepseek-svg-dataset but enhanced with separated SVG codes and detailed reasoning-based descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDescription Generation Process\n\t\n\nThe dataset has been enhanced by using the reasoning part from the original completion to generate longer, detailed descriptions. The SVG code part of the completion is ignored‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShahzebKhoso/deepseek-svg-description.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"wan_stirring","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_stirring","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"robot-action-prediction-dataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bryandts/robot-action-prediction-dataset","creator_name":"Bryan Delton Tawarikh Sibarani","creator_url":"https://huggingface.co/bryandts","description":"\n\t\n\t\t\n\t\tRobotic Action Prediction Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains triplets of (current observation, action instruction, future observation) for training models to predict future frames of robotic actions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ncurrent_frame: Input image (RGB) of the current observation\ninstruction: Textual description of the action to perform\nfuture_frame: Target image (RGB) showing the expected outcome 50 frames later‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bryandts/robot-action-prediction-dataset.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"wan_strumming_guitar","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_strumming_guitar","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_swinging_arms","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_swinging_arms","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"concept_coverage_laion_6m","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mayug/concept_coverage_laion_6m","creator_name":"Mayug Maniparambil","creator_url":"https://huggingface.co/mayug","description":"\n\t\n\t\t\n\t\tüì¶ Freeze-Align Dataset\n\t\n\nThe Freeze-Align Dataset (concept_coverage_laion_6m) is a curated collection of high-quality image-text pairs designed to facilitate efficient multimodal alignment using frozen unimodal encoders. This dataset supports the research presented in our CVPR 2025 paper, \"Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment\", enabling models to achieve CLIP-level performance with significantly reduced computational resources.\nThe dataset is curated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mayug/concept_coverage_laion_6m.","first_N":5,"first_N_keywords":["zero-shot-classification","text-to-image","image-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"lomwe-speech-text","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/lomwe-speech-text","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tLomwe Speech-Text Parallel Dataset\n\t\n\nThis dataset is a collection of aligned audio-text pairs in Lomwe, extracted from the CMU Wilderness dataset. It is useful for tasks such as:\n\nSpeech recognition (ASR)\nText-to-speech (TTS)\nLanguage modeling for low-resource languages\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nEach entry in the dataset contains:\n\naudio: A .wav file sampled at 16kHz\ntext: A transcription of the spoken audio in Lomwe (digits removed)\n\n\n\t\n\t\t\n\t\tExample\n\t\n\n\n\t\n\t\t\naudio\ntext‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/lomwe-speech-text.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","audio-intent-classification","machine-generated"],"keywords_longer_than_N":true},
	{"name":"wan_turning_in_circle","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_turning_in_circle","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_tying_shoelaces","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_tying_shoelaces","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_typing","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_typing","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"ControllabeGenDDI","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hcarrion/ControllabeGenDDI","creator_name":"H√©ctor Carri√≥n","creator_url":"https://huggingface.co/hcarrion","description":"\n\t\n\t\t\n\t\tDataset Card for cgDDI\n\t\n\nThis repository is the official dataset of cgDDI: Controllable Generation of Diverse Dermatological Imagery for Fair and Efficient Malignancy Classification which is currently under review. As this paper is not openly distributed (or on Arxiv), the method detail here will be minimal.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nDiscussed on the manuscript. Soon to be added here post-review!\n\n\t\n\t\t\n\t\tCode\n\t\n\nAll code can be found on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hcarrion/ControllabeGenDDI.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","image-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"wan_unwrap_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_unwrap_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"GenRef-wds","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/diffusion-cot/GenRef-wds","creator_name":"Diffusion CoT","creator_url":"https://huggingface.co/diffusion-cot","description":"\n\t\n\t\t\n\t\tGenRef-1M\n\t\n\n\n  \n\n\nWe provide 1M high-quality triplets of the form (flawed image, high-quality image, reflection) collected across\nmultiple domains using our scalable pipeline from [1]. We used this dataset to train our reflection tuning model.\nTo know the details of the dataset creation pipeline, please refer to Section 3.2 of [1].\nProject Page: https://diffusion-cot.github.io/reflection2perfection\n\n\t\n\t\t\n\t\n\t\n\t\tDataset loading\n\t\n\nWe provide the dataset in the webdataset format for fast‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/diffusion-cot/GenRef-wds.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","webdataset"],"keywords_longer_than_N":true},
	{"name":"MUSAR-Gen","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guozinan/MUSAR-Gen","creator_name":"zn","creator_url":"https://huggingface.co/guozinan","description":"\n\n‚≠êÔ∏è Although MUSAR is trained solely on diptych data constructed from concatenated single-subject samples, we recognize that a high-quality multi-subject paired dataset is highly beneficial for the field of image customization. To accelerate progress in this field, we are releasing the high-quality multi-subject dataset generated by MUSAR: MUSAR-Gen. It delivers FLUX-comparable image quality without exhibiting attribute entanglement issues. Hope it will be helpful to researchers working on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guozinan/MUSAR-Gen.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MUSAR-Gen","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guozinan/MUSAR-Gen","creator_name":"zn","creator_url":"https://huggingface.co/guozinan","description":"\n\n‚≠êÔ∏è Although MUSAR is trained solely on diptych data constructed from concatenated single-subject samples, we recognize that a high-quality multi-subject paired dataset is highly beneficial for the field of image customization. To accelerate progress in this field, we are releasing the high-quality multi-subject dataset generated by MUSAR: MUSAR-Gen. It delivers FLUX-comparable image quality without exhibiting attribute entanglement issues. Hope it will be helpful to researchers working on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guozinan/MUSAR-Gen.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"MMR1-in-context-synthesizing","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset is designed for unsupervised post-training of Multi-Modal Large Language Models (MLLMs) focusing on enhancing reasoning capabilities. It contains image-problem-answer triplets, where the problem requires multimodal reasoning to derive the correct answer from the provided image. The dataset is intended for use with the MM-UPT framework described in the accompanying paper.\n\nüêô GitHub Repo: waltonfuture/MM-UPT\nüìú Paper (arXiv): Unsupervised Post-Training for Multi-Modal LLM Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/MMR1-in-context-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ArVoice","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ArVoice","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n  ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis\n\n Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki \n {hawau.toyin, hanan.aldarmaki}@mbzuai.ac.ae \n\n\n    ArVoice is a multi-speaker Modern Standard Arabic (MSA) speech corpus with fully diacritized transcriptions, intended  for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection.  \n      ArVoice‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ArVoice.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"ArVoice","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ArVoice","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n  ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis\n\n Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki \n {hawau.toyin, hanan.aldarmaki}@mbzuai.ac.ae \n\n\n    ArVoice is a multi-speaker Modern Standard Arabic (MSA) speech corpus with fully diacritized transcriptions, intended  for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection.  \n      ArVoice‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ArVoice.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"stock-photos-asian-people","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/stock-photos-asian-people","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tStock Photos (Asian People, Stable Diffusion 1.5)\n\t\n\n\n  \n  Collage of randomly selected images from the dataset\n\n\nCollection of synthetic stock photographs created with Stable Diffusion 1.5, emphasizing Asian people \n(including East Asians, Southeast Asians, South Asians, and Central Asians).\nImages were generated using a diverse set of prompts and filtered for quality, realism, and safety.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\n\nImage generation\nImage captioning\nVisual representation learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/stock-photos-asian-people.","first_N":5,"first_N_keywords":["text-to-image","feature-extraction","English","cc0-1.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"videomarathon","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jylins/videomarathon","creator_name":"Jingyang Lin","creator_url":"https://huggingface.co/jylins","description":"\n\t\n\t\t\n\t\tDataset Card for VideoMarathon\n\t\n\nVideoMarathon is a large-scale long video instruction-following dataset with a total duration of approximately 9,700 hours, comprising 3.3 million QA pairs across 22 task categories.\nPaper and more resources: [arXiv] [Project Website] [GitHub] [Model]\n\n\t\n\t\t\n\t\n\t\n\t\tIntended Uses\n\t\n\nThis dataset is used for academic research purposes only.\n\n\t\n\t\t\n\t\n\t\n\t\tTask Taxonomy\n\t\n\nThe dataset contains 22 diverse tasks over six fundamental topics, including temporality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jylins/videomarathon.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"kasem-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/kasem-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tKasem Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 75990 parallel speech-text pairs for Kasem, a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Kasem - xsm\nTask: Speech Recognition, Text-to-Speech\nSize: 75990 audio files > 1KB‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/kasem-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Kasem"],"keywords_longer_than_N":true},
	{"name":"OmniConsistency-TR","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/salihfurkaan/OmniConsistency-TR","creator_name":"Salih Furkan Erik","creator_url":"https://huggingface.co/salihfurkaan","description":"\n\t\n\t\t\n\t\tOmniConsistency-TR\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nOmniConsistency is a multimodal benchmark to evaluate cross-modal consistency in generated image-text pairs. It consists of 22 distinct visual styles (e.g., \"Ghibli\", \"LEGO\", \"Van Gogh\") and contains human-annotated captions describing synthetic artworks.\nThis is the Turkish-translated version of the OmniConsistency dataset, originally released by Showlab. It includes 22 stylistic splits with multilingual captions, where all English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/salihfurkaan/OmniConsistency-TR.","first_N":5,"first_N_keywords":["text-to-image","Turkish","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"SpaCE-10","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Cusyoung/SpaCE-10","creator_name":"ZiYang Gong","creator_url":"https://huggingface.co/Cusyoung","description":"This repository contains the dataset for the paper SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence.\n\n SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence\n\n\nGitHub Repository: https://github.com/Cuzyoung/SpaCE-10\n\n\n\t\n\t\t\n\t\n\t\n\t\tüß† What is SpaCE-10?\n\t\n\nSpaCE-10 is a compositional spatial intelligence benchmark for evaluating Multimodal Large Language Models (MLLMs) in indoor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cusyoung/SpaCE-10.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vai-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/vai-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tVai Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 23286 parallel speech-text pairs for Vai, a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Vai - vai\nTask: Speech Recognition, Text-to-Speech\nSize: 23286 audio files > 1KB (small/corrupted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/vai-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Vai"],"keywords_longer_than_N":true},
	{"name":"common_voices_21_mn","keyword":"text-to-speech","license":"Mozilla Public License 2.0","license_url":"https://choosealicense.com/licenses/mpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/btsee/common_voices_21_mn","creator_name":"Battseren Badral","creator_url":"https://huggingface.co/btsee","description":"btsee/common_voices_21_mn dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Mongolian","mpl-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ToneWebinars","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/ToneWebinars","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\n\t\n\t\t\n\t\tToneWebinars\n\t\n\nToneWebinars ‚Äî —ç—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ZeroAgency/shkolkovo-bobr.video-webinars-audio.\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –±—ã–ª–∏ –ø–µ—Ä–µ–ø–∞–∫–æ–≤—ã–Ω—ã –≤ parquet —Ñ–æ—Ä–º–∞—Ç —Å –Ω–∞—Ä–µ–∑–∫–æ–π –ø–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–Ω—ã–º —Ç–∞–∫–º–∫–æ–¥–∞–º. –í –¥–∞—Ç–∞—Å–µ—Ç–µ 2053.55 —á–∞—Å–∞ –∞—É–¥–∏–æ –¥–ª—è train —Å–ø–ª–∏—Ç–∞ –∏ 154.34 –¥–ª—è validation.\n\n\n\t\n\t\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ\n\t\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è:\n\n–°—Å—ã–ª–∫–∞ –Ω–∞ MP3-—Ñ–∞–π–ª (audio)\n–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ (text)\n–ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (sample_rate)\n\n\n\n\t\n\t\t\n\t\t–§–æ—Ä–º–∞—Ç –∑–∞–ø–∏—Å–∏ (JSON)\n\t\n\n{\n  \"audio\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/ToneWebinars.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Russian","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"UGC-VideoCap","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/openinterx/UGC-VideoCap","creator_name":"Memories.ai Research","creator_url":"https://huggingface.co/openinterx","description":"\n\t\n\t\t\n\t\tUGC-VideoCaptioner Dataset\n\t\n\nReal-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio-visual content. However, existing video captioning benchmarks and models remain predominantly visual-centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of full-modality datasets and lightweight, capable models hampers progress in fine-grained, multimodal video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openinterx/UGC-VideoCap.","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2507.11336","üá∫üá∏ Region: US","video-captioning"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"VStar-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/VStar-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the V* benchmark. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"InfoVQA-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/InfoVQA-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the Infographics VQA. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"atlas-large","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/navintiwari/atlas-large","creator_name":"Navin Tiwari","creator_url":"https://huggingface.co/navintiwari","description":"This is an extended version of the ATLAS dataset at https://huggingface.co/datasets/ggxxii/ATLAS. This dataset contains text prompts, uv textures and different views of the rendered RGB images using SMPL model and AMASS poses. The original ATLAS dataset was released with the paper TexDreamer. The details of the paper can be found at author's github page at https://ggxxii.github.io/texdreamer. This extended version of dataset is created to aid an easy implementation of the TexDreamer paper.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"deg-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/deg-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tDeg Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 125958 parallel speech-text pairs for Deg, a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Deg - mzw\nTask: Speech Recognition, Text-to-Speech\nSize: 125958 audio files > 1KB (small/corrupted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/deg-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Deg"],"keywords_longer_than_N":true},
	{"name":"cc0-textures","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/cc0-textures","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for CC0 Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 18,785 texture images from cc0-textures.com. It includes textures of wood, metal, concrete, fabric, stone, ceramic, and other materials. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while keeping good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en): Texture titles and tags‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cc0-textures.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"jsonl-mls-speechtokenizer","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anilkeshwani/jsonl-mls-speechtokenizer","creator_name":"Anil Keshwani","creator_url":"https://huggingface.co/anilkeshwani","description":"anilkeshwani/jsonl-mls-speechtokenizer dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"mbspeech_mn","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/btsee/mbspeech_mn","creator_name":"Battseren Badral","creator_url":"https://huggingface.co/btsee","description":"\n\t\n\t\t\n\t\tMBSpeech MN: Mongolian Biblical Speech Dataset\n\t\n\nMBSpeech MN is a Mongolian text-to-speech (TTS) dataset derived from biblical texts. It consists of aligned audio recordings and corresponding sentences in Mongolian. The dataset is suitable for training TTS models and other speech processing applications.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n  Language: Mongolian (mn)\n  Task: Text-to-Speech (TTS)\n  License: MIT\n  Size:\n  Download size: ~721 MB\n\n  Dataset size: ~822 MB\n\n  Examples: 3,846‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/btsee/mbspeech_mn.","first_N":5,"first_N_keywords":["text-to-speech","Mongolian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"mbspeech_mn","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/btsee/mbspeech_mn","creator_name":"Battseren Badral","creator_url":"https://huggingface.co/btsee","description":"\n\t\n\t\t\n\t\tMBSpeech MN: Mongolian Biblical Speech Dataset\n\t\n\nMBSpeech MN is a Mongolian text-to-speech (TTS) dataset derived from biblical texts. It consists of aligned audio recordings and corresponding sentences in Mongolian. The dataset is suitable for training TTS models and other speech processing applications.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n  Language: Mongolian (mn)\n  Task: Text-to-Speech (TTS)\n  License: MIT\n  Size:\n  Download size: ~721 MB\n\n  Dataset size: ~822 MB\n\n  Examples: 3,846‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/btsee/mbspeech_mn.","first_N":5,"first_N_keywords":["text-to-speech","Mongolian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"GenRefTest","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jianqunZ/GenRefTest","creator_name":"Jianqun Zhou","creator_url":"https://huggingface.co/jianqunZ","description":"jianqunZ/GenRefTest dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TTS-Multilingual-Test-Set","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MiniMaxAI/TTS-Multilingual-Test-Set","creator_name":"MiniMax","creator_url":"https://huggingface.co/MiniMaxAI","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nTo assess the multilingual zero-shot voice cloning capabilities of TTS models, we have constructed a test set encompassing 24 languages. This dataset provides both audio samples for voice cloning and corresponding test texts.\nSpecifically, the test set for each language includes:\n100 distinct test sentences.\nAudio samples from two speakers (one male and one female) carefully selected from the Mozilla Common Voice (MCV) dataset, intended for voice cloning.\nResearchers can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiniMaxAI/TTS-Multilingual-Test-Set.","first_N":5,"first_N_keywords":["text-to-speech","cc-by-sa-4.0","< 1K","soundfolder","Audio"],"keywords_longer_than_N":true},
	{"name":"TTI-Set","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pointofnoreturn/TTI-Set","creator_name":"laura wagner","creator_url":"https://huggingface.co/pointofnoreturn","description":"\n\t\n\t\t\n\t\tText-to-Image Model Attribution Dataset\n\t\n\nThis dataset is distilled from two comprehensive sources:\n\nA 2-year snapshot of the CivitAI SFW (Safe-for-Work) image dataset, containing metadata for generated images.\nA complete export of all models published on CivitAI, including metadata such as model names, types, and version identifiers.\n\nBy matching image-level resourceIDs (used to generate each image) with the corresponding model version IDs from the model dataset, we identified and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pointofnoreturn/TTI-Set.","first_N":5,"first_N_keywords":["image-classification","multi-class-image-classification","manual","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"DORI-Benchmark","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/appledora/DORI-Benchmark","creator_name":"Nazia Tasnim","creator_url":"https://huggingface.co/appledora","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nDORI (Discriminative Orientation Reasoning Intelligence) is a comprehensive benchmark designed to evaluate object orientation understanding in multimodal large language models (MLLMs). The benchmark isolates and evaluates orientation perception as a primary capability, offering a systematic assessment framework that spans four essential dimensions of orientation comprehension: frontal alignment, rotational transformations, relative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/appledora/DORI-Benchmark.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"audio_data_russian","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kijjjj/audio_data_russian","creator_name":"fgfd","creator_url":"https://huggingface.co/kijjjj","description":"\n\t\n\t\t\n\t\tDataset Audio Russian\n\t\n\nThis is a dataset with Russian audio data, split into train for tasks like text-to-speech, speech recognition, and speaker identification.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\ntext: Audio transcription (string).\nspeaker_name: Speaker identifier (string).\naudio: Audio file.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nLoad the dataset like this:\nfrom datasets import load_dataset\ndataset = load_dataset(\"kijjjj/audio_data_russian\", split=\"train\")\nprint(dataset[0])\n\n","first_N":5,"first_N_keywords":["text-to-speech","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"mls-eng-128kb","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ntt123/mls-eng-128kb","creator_name":"Th√¥ng Nguy·ªÖn","creator_url":"https://huggingface.co/ntt123","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/mls-eng-128kb.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls-eng-128kb","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ntt123/mls-eng-128kb","creator_name":"Th√¥ng Nguy·ªÖn","creator_url":"https://huggingface.co/ntt123","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/mls-eng-128kb.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"WM-ABench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maitrix-org/WM-ABench","creator_name":"Maitrix.org","creator_url":"https://huggingface.co/maitrix-org","description":"\n\t\n\t\t\n\t\tWM-ABench: An Atomic Evaluation Benchmark of World Modeling abilities of Vision-Language Models\n\t\n\nPaper: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation\nWM-ABench is a comprehensive benchmark that evaluates whether Vision-Language Models (VLMs) can truly understand and simulate physical world dynamics, or if they rely on shortcuts and pattern-matching. The benchmark covers 23 dimensions of world modeling across 6 physics simulators with over 100,000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maitrix-org/WM-ABench.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","multiple-choice","image-text-to-text","English"],"keywords_longer_than_N":true},
	{"name":"RealEdit-Jul2021-Dec2024","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ben-caffee/RealEdit-Jul2021-Dec2024","creator_name":"Ben Caffee","creator_url":"https://huggingface.co/ben-caffee","description":"This dataset is an extension of RealEdit including Reddit data from July 2021 to December 2024.\nIt was curated using the code provided in this pull request in the RealEdit repository.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"My","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ochir04075143/My","creator_name":"Ochir","creator_url":"https://huggingface.co/Ochir04075143","description":"\n\t\n\t\t\n\t\tMy Face Dataset\n\t\n\nThis dataset contains a collection of face images with corresponding text descriptions (prompts). It is designed for applications such as:\n\nFine-tuning diffusion models (e.g., Stable Diffusion, LoRA)\nFace recognition\nText-to-image generation\n\n\n\t\n\t\t\n\t\tüßæ Dataset Structure\n\t\n\n\ntrain/ ‚Äî folder containing all the .jpg image files.\nmetadata.jsonl ‚Äî a JSON Lines file where each line is a JSON object with the following fields:\nfile_name: name of the image file.\ntext: a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ochir04075143/My.","first_N":5,"first_N_keywords":["image-classification","text-to-image","manually-created","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UW-FMRL2/MMMG","creator_name":"Foundation Model and RL Research Lab @ UW","creator_url":"https://huggingface.co/UW-FMRL2","description":"\n\t\n\t\t\n\t\tDataset Card for MMMG\n\t\n\n\nWe present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation. \nThis huggingface page only contains the raw dataset of MMMG, for full evaluation suite, please refer to our github page:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UW-FMRL2/MMMG.","first_N":5,"first_N_keywords":["text-to-audio","text-to-image","text-to-speech","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UW-FMRL2/MMMG","creator_name":"Foundation Model and RL Research Lab @ UW","creator_url":"https://huggingface.co/UW-FMRL2","description":"\n\t\n\t\t\n\t\tDataset Card for MMMG\n\t\n\n\nWe present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation. \nThis huggingface page only contains the raw dataset of MMMG, for full evaluation suite, please refer to our github page:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UW-FMRL2/MMMG.","first_N":5,"first_N_keywords":["text-to-audio","text-to-image","text-to-speech","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UW-FMRL2/MMMG","creator_name":"Foundation Model and RL Research Lab @ UW","creator_url":"https://huggingface.co/UW-FMRL2","description":"\n\t\n\t\t\n\t\tDataset Card for MMMG\n\t\n\n\nWe present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation. \nThis huggingface page only contains the raw dataset of MMMG, for full evaluation suite, please refer to our github page:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UW-FMRL2/MMMG.","first_N":5,"first_N_keywords":["text-to-audio","text-to-image","text-to-speech","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"LearningPaper24","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vivianchen98/LearningPaper24","creator_name":"Shenghui Chen","creator_url":"https://huggingface.co/vivianchen98","description":"\n\t\n\t\t\n\t\tLearningPaper24 Dataset\n\t\n\nThis dataset contains video recordings and metadata from ICLR and NeurIPS 2024 conference talks. It includes both poster and oral presentations, along with their associated metadata such as titles, abstracts, keywords, and primary areas.\nThe paper list is originally sourced from Paperlists.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nlearningpaper24/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata/\n‚îÇ   ‚îî‚îÄ‚îÄ catalog.json\n‚îî‚îÄ‚îÄ video/\n    ‚îú‚îÄ‚îÄ {openreview_id}_{slideslive_id}.mp4\n    ‚îî‚îÄ‚îÄ ...‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vivianchen98/LearningPaper24.","first_N":5,"first_N_keywords":["summarization","video-text-to-text","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"VideoVista-CulturalLingo","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo","creator_name":"Uni-MoE","creator_url":"https://huggingface.co/Uni-MoE","description":"\n    \n\n\n    \n\n\n    \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tVideoVista-CulturalLingo\n\t\n\nThis repository contains the VideoVista-CulturalLingo, introduced in VideoVista-CulturalLingo: 360¬∞ Horizons-Bridging Cultures, Languages,\nand Domains in Video Comprehension. \n üéâ Our new VideoVista-CulturalLingo bridges cultures (China, North America, and Europe), languages (Chinese and English), and domains (140+)in video comprehension. \n üåç Welcome to join us on this journey of video understanding!\n\n\t\n\t\t\n\t\tFiles\n\t\n\nWe provice the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Uni-MoE/VideoVista-CulturalLingo.","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NewsLensSync","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sparklessszzz/NewsLensSync","creator_name":"Anonymous","creator_url":"https://huggingface.co/sparklessszzz","description":"sparklessszzz/NewsLensSync dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","table-question-answering","question-answering","summarization","sentence-similarity"],"keywords_longer_than_N":true},
	{"name":"wan_balancing_on_one_leg","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_balancing_on_one_leg","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"Live-WhisperX-526K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","description":"\n\t\n\t\t\n\t\tDataset Card for Live-WhisperX-526K\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for the training of the LiveCC-7B-Instruct model. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tData Sources\n\t\n\nAfter we finished the pre-training of LiveCC-7B-Base model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","100K<n<1M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"wan_brushing_teeth","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_brushing_teeth","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_catching_object","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_catching_object","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_crossing_arms","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_crossing_arms","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-1k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 1k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nLoading the data:\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-1k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","< 1K","webdataset"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-1k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 1k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nLoading the data:\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-1k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","< 1K","webdataset"],"keywords_longer_than_N":true},
	{"name":"text-to-speech-sentences","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/text-to-speech-sentences","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"\n\t\n\t\t\n\t\tTexts for Ukrainian Text to Speech dataset\n\t\n\nCode for this dataset is here: https://github.com/egorsmkv/uk-tts-dataset-text\n","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"wan_deflate_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_deflate_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"OpenVid-60k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-60k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 60k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom torchcodec.decoders import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-60k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"OpenVid-60k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-60k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 60k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom torchcodec.decoders import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-60k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"wan_doing_head_tilt","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_head_tilt","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_doing_robot_dance","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_robot_dance","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_facepalming","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_facepalming","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"VCRBench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/VCRBench","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":"\n\t\n\t\t\n\t\tVCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models\n\t\n\n \n \n \n \n \nAuthors: Pritam Sarkar and Ali Etemad\nThis repository provides the official implementation of VCRBench.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nPlease check our GitHub repo for the details of usage: VCRBench\nfrom dataset import VCRBench\ndataset=VCRBench(question_file=\"data.json\", \n                video_root=\"./\",\n                mode='default', \n                )\n    \nfor sample in dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/VCRBench.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"testNSFW","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lzw3098/testNSFW","creator_name":"lzw","creator_url":"https://huggingface.co/lzw3098","description":"1.The NSFWErasureBen.csv file contains prompts from four NSFW datasets and the human-annotated results of images generated using corresponding seeds. The manual annotation covers three categories: Nudity, Violence, and Horror. Any instance classified as positive in at least one of these categories is considered NSFW.\n2.The ‚Äòadv_prompts‚Äô directory contains adversarial prompt datasets for three themes generated using the RAB method, with each prompt replicated 10 times.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"NSFWErasureBench","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lzw3098/NSFWErasureBench","creator_name":"lzw","creator_url":"https://huggingface.co/lzw3098","description":"1.The NSFWErasureBen.csv file contains prompts from four NSFW datasets and the human-annotated results of images generated using corresponding seeds. The manual annotation covers three categories: Nudity, Violence, and Horror. Any instance classified as positive in at least one of these categories is considered NSFW.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"phyworldbench","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/phyworldbench/phyworldbench","creator_name":"phyworldbench","creator_url":"https://huggingface.co/phyworldbench","description":"\n\t\n\t\t\n\t\tPhyWorldBench\n\t\n\nThis repository hosts the core assets of PhyWorldBench, the 1,050 JSON prompt files, the evaluation standards, and the physics categories and subcategories, used to assess physical realism in text-to-video models. \nWe have also attached over 10k of the generated videos that we used for experimentation. Since the videos are not part of our benchmark, we did not include all the generated videos.\n","first_N":5,"first_N_keywords":["text-to-video","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"flux-1s-generated-human-associations-10k","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AbstractPhil/flux-1s-generated-human-associations-10k","creator_name":"AbstractPhila","creator_url":"https://huggingface.co/AbstractPhil","description":"Generated With: Flux 1S\nSize: 1024x1024\nSteps: 4\nGuidance: 0\nGenerated using random captions from a tokenization conjunction generation system - meant to introduce subject based linkers from one diffusion concept to another.\nThis needs additional AI passes to filter out things like floating hands, bad anatomy and so on.\nThis dataset is full of diffusion poison, but it's all generated from Flux 1S so it can be used for classification, object identification, association identification, and so on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AbstractPhil/flux-1s-generated-human-associations-10k.","first_N":5,"first_N_keywords":["text-to-image","English","mit","Image","Text"],"keywords_longer_than_N":true},
	{"name":"redblueai.sed","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ahiyan/redblueai.sed","creator_name":"Ahiyan Kabir","creator_url":"https://huggingface.co/Ahiyan","description":"Ahiyan/redblueai.sed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","summarization","text-to-image","English","German"],"keywords_longer_than_N":true},
	{"name":"GeoQA-8K-direct-synthesizing","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing","creator_name":"Lai Wei","creator_url":"https://huggingface.co/WaltonFuture","description":"This dataset supports the unsupervised post-training of multi-modal large language models (MLLMs) as described in the paper Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO. It's designed to enable continual self-improvement without external supervision, using a self-rewarding mechanism based on majority voting over multiple sampled responses. The dataset is used to improve the reasoning ability of MLLMs, as demonstrated by significant improvements on benchmarks like MathVista‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaltonFuture/GeoQA-8K-direct-synthesizing.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"GLOBE_V3","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MushanW/GLOBE_V3","creator_name":"MushanW","creator_url":"https://huggingface.co/MushanW","description":"\n\t\n\t\t\n\t\tImportant notice\n\t\n\nDifferences between V3 version and two previous versions (V1|V2):\n\nThis version is built base on Common Voice 21.0 English Subset.\n   This version only includes utterance that are an exact match with the transcription from Whisper V3 LARGE (CER == 0).\n   This version includes the original Common Voice metadata (age, gender, accent, and ID).\n   All audio files in this version are at 24kHz sampling rate.\n   All audio files in this version are unenhanced. (We‚Äôd greatly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushanW/GLOBE_V3.","first_N":5,"first_N_keywords":["text-to-audio","automatic-speech-recognition","audio-to-audio","audio-classification","mozilla-foundation/common_voice_14_0"],"keywords_longer_than_N":true},
	{"name":"Clotho-Moment","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lighthouse-emnlp2024/Clotho-Moment","creator_name":"lighthouse","creator_url":"https://huggingface.co/lighthouse-emnlp2024","description":"\n\t\n\t\t\n\t\tClotho-Moment\n\t\n\nThis repository provides wav files used in Language-based audio moment retrieval.\nEach sample includes long audio containing some audio events with the temporal and textual annotation.\n\n\t\n\t\t\n\t\tSplit\n\t\n\n\nTrain\ntrain/train-{000..715}.tar\n37930 audio samples\n\n\nValid\nvalid/valid-{000..108}.tar\n5741 audio samples\n\n\nTest\ntest/test-{000..142}.tar\n7569 audio samples\n\n\n\n\n\t\n\t\t\n\t\tUsing Webdataset\n\t\n\nimport torch\nimport webdataset as wds\nfrom huggingface_hub import get_token\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lighthouse-emnlp2024/Clotho-Moment.","first_N":5,"first_N_keywords":["text-to-audio","English","apache-2.0","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"Japanese-photos","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/Japanese-photos","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tÊó•Êú¨„ÅÆÂÜôÁúü„Åü„Å°\n\t\n\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„Åß„ÅØÊó•Êú¨„ÅÆÂÜôÁúü„Åü„Å°„ÇíÂÖ±Êúâ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÊó•Êú¨„Åß„Çà„ÅèË¶ã„Çâ„Çå„ÇãÂÖâÊôØ„ÇíAI„Å´Â≠¶Áøí„Åï„Åõ„Çã„Åì„Å®„Åß\nÊó•Êú¨„Çâ„Åó„ÅÑÂøúÁ≠î„ÅÆ„Åß„Åç„ÇãAI„ÇíÈñãÁô∫„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å´„ÄÅ\nCC-0„ÅßÂÜôÁúü800Êûö„Å™„Å©„ÇíÂÖ¨Èñã„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÁâπ„Å´ÊúÄËøëÂØøÂè∏„Å®„Åã„ÅåÊó•Êú¨„Çâ„Åó„Åè„Å™„ÅÑ„ÅÆ„ÅåÊ∞ó„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„É™„É≥„ÇØ: images.tar\nËã±Ë™û„ÅÆË™¨ÊòéÊñá : metadata.csv\nÊó•Êú¨Ë™û„ÅÆË™¨ÊòéÊñá : metadata_ja.csv\n\n\t\n\t\t\n\t\n\t\n\t\t„É©„Ç§„Çª„É≥„Çπ„Å´„Å§„ÅÑ„Å¶\n\t\n\nÊíÆÂΩ±ËÄÖ„Åß„ÅÇ„ÇãÁßÅ„ÅØ„Åì„Çå„Çâ„ÅÆÂÜôÁúü„ÅÆËëó‰ΩúÊ®©„ÇíÊîæÊ£Ñ„Åó„Åæ„Åô„ÄÇ\n„ÅÑ„Åã„Çà„ÅÜ„Å´„ÇÇ‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n„Åó„Åã„Åó„ÄÅËÇñÂÉèÊ®©„ÇÑÂïÜÊ®ôÊ®©„ÅØÊÆã„Å£„Å¶„ÅÑ„Åæ„Åô„ÅÆ„Åß„ÄÅ„ÅîÊ≥®ÊÑè„Åè„Å†„Åï„ÅÑ„ÄÇ\nÊó•Êú¨Ë™û„Å®Ëã±Ë™û„ÅÆË™¨ÊòéÊñá„ÅØ„Å®„ÇÇ„Å´Qwen2.5VL„Å´„Çà„Çä‰Ωú„Çâ„Çå„Åæ„Åó„Åü„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅËëó‰ΩúÊ®©„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tÂÜôÁúü„ÅÆÊ¶ÇË¶Å\n\t\n\nÂü∫Êú¨ÁöÑ„Å´ÂêÑÂú∞„ÇíÊóÖË°å„Åó„ÅüÈöõ„ÅÆÊó•Êú¨ÂêÑÂú∞„ÅÆÂÜôÁúü„ÅåÂÜô„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ\nÂÆ§Â§ñ„ÇÇ„ÅÇ„Çå„Å∞„ÄÅÂÆ§ÂÜÖ„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇÂÜôÁúü„ÅÆ‰æã„ÇíÂèÇËÄÉ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n\t\n\t\n\t\n\t\tÂÜôÁúü„ÅÆ‰æã\n\t\n\n\nÊó•Êú¨Ë™û„ÅÆË™¨ÊòéÊñá‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/Japanese-photos.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"PC-Agent-E","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/henryhe0123/PC-Agent-E","creator_name":"Yanheng He","creator_url":"https://huggingface.co/henryhe0123","description":"This repository contains the dataset used in the paper Efficient Agent Training for Computer Use.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"wan_flipping_coin","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_flipping_coin","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_frowning","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_frowning","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_gasping","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_gasping","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"HAL-9000-Speech","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/campwill/HAL-9000-Speech","creator_name":"William Campbell","creator_url":"https://huggingface.co/campwill","description":"\n\t\n\t\t\n\t\tHAL 9000 Speech Dataset\n\t\n\nThis repository contains audio recordings of dialogue from HAL 9000, the AI character from 2001: A Space Odyssey. The full dataset contains most, but not all audio recordings of HAL 9000 from the film. The dataset is not cleaned, as background noise and variations in his voice are prevalent.  \nThe dataset can be formatted into the LJSpeech structure to ensure compatibility with most text-to-speech (TTS) models and training pipelines, such as Piper.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/campwill/HAL-9000-Speech.","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"wan_googly_eyes_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_googly_eyes_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_inflating_balloon","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_inflating_balloon","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_jazz_hands","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_jazz_hands","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_liquify_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_liquify_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"Magma-Video-ToM","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM","creator_name":"Multimodal AI Agents","creator_url":"https://huggingface.co/MagmaAI","description":"\nMagma: A Foundation Model for Multimodal AI Agents\n\nJianwei Yang*1‚Ä†¬†\nReuben Tan1‚Ä†¬†\nQianhui Wu1‚Ä†¬†\nRuijie Zheng2‚Ä°¬†\nBaolin Peng1‚Ä°¬†\nYongyuan Liang2‚Ä°\nYu Gu1¬†\nMu Cai3¬†\nSeonghyeon Ye4¬†\nJoel Jang5¬†\nYuquan Deng5¬†\nLars Liden1¬†\nJianfeng Gao1‚ñΩ\n1 Microsoft Research; 2 University of Maryland; 3 University of Wisconsin-Madison4 KAIST; 5 University of Washington\n* Project lead  ‚Ä† First authors  ‚Ä° Second authors  ‚ñΩ Leadership  \n[arXiv Paper] ¬† [Project Page] ¬† [Hugging Face Paper] ¬† [Github Repo] ¬† [Video]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM.","first_N":5,"first_N_keywords":["video-text-to-text","robotics","mit","1M - 10M","arrow"],"keywords_longer_than_N":true},
	{"name":"wan_nodding_head","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_nodding_head","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_opening_closing_fan","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_opening_closing_fan","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_opening_umbrella","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_opening_umbrella","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_overgrow_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_overgrow_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_peel_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_peel_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_popping_balloon","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_popping_balloon","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_pouring_liquid","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_pouring_liquid","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"TAMMs","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IceInPot/TAMMs","creator_name":"ÈîÖ‰∏≠ÂÜ∞","creator_url":"https://huggingface.co/IceInPot","description":"\n\t\n\t\t\n\t\tTAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting\n\t\n\nTAMMs is a large-scale dataset derived from the Functional Map of the World (fMoW) dataset, curated to support multimodal and temporal reasoning tasks such as change detection and future prediction.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 37,003 high-quality temporal sequences, each consisting of at least four distinct satellite images of the same location captured at different‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IceInPot/TAMMs.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wan_rolling_eyes","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_rolling_eyes","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_saluting","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_saluting","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_shatter_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_shatter_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"Objaverse-Rand6View","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/huanngzh/Objaverse-Rand6View","creator_name":"zehuan-huang","creator_url":"https://huggingface.co/huanngzh","description":"\n\t\n\t\t\n\t\tObjaverse-Rand6View\n\t\n\nGithub | Project Page | Paper\n\n\t\n\t\t\n\t\t1. Dataset Introduction\n\t\n\nTL;DR: This dataset contains multi-view images that are rendered from a high-quality subset of Objaverse, used in MV-Adapter.\nFeatures:\n\nOrthographic or perspective views (random)\n1024x1024 resolution\nRGB, Depth, Normal, Camera\n\n\n\t\n\t\t\n\t\n\t\n\t\t2. Data Extraction\n\t\n\nsudo apt-get install git-lfs\ngit lfs install\ngit clone https://huggingface.co/datasets/huanngzh/Objaverse-Rand6View\ncat‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanngzh/Objaverse-Rand6View.","first_N":5,"first_N_keywords":["text-to-3d","image-to-3d","text-to-image","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"wan_showing_muscles","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_showing_muscles","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_shuffling_cards","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_shuffling_cards","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_sitting_down","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_sitting_down","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"WorldGenBench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/worldrl/WorldGenBench","creator_name":"GenRL","creator_url":"https://huggingface.co/worldrl","description":"\n\t\n\t\t\n\t\tüìö WorldGenBench\n\t\n\n\n\n\n\nA World-Knowledge-Integrated Benchmark/Dataset for Reasoning-Driven Text-to-Image Generation\n\n\n\n\t\n\t\t\n\t\tPaper Link: https://huggingface.co/papers/2505.01490\n\t\n\n\n\n\t\n\t\t\n\t\tPurpose and Scope\n\t\n\nWorldGenBench is designed to systematically evaluate text-to-image (T2I) models on two critical but underexplored capabilities:\n\nWorld Knowledge Grounding: Understanding background facts assumed but not explicitly stated in prompts.\nImplicit Reasoning: Inferring necessary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/worldrl/WorldGenBench.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MMSQL","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GuoZiming/MMSQL","creator_name":"ÈÉ≠Â≠êÈì≠","creator_url":"https://huggingface.co/GuoZiming","description":"\n\n      \n\n\t\n\t\t\n\t\tMMSQL\n\t\n\n\n\nThis repository contains the DATASET:MMSQL in paper \"Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types\" [Arxiv] [IJCNN2025]. The repository is structured to ensure the reproducibility of the experiments and includes scripts, notebooks, test suits, and data outputs. You can get an overview of this project and the paper through the page.\n\n\t\n\t\t\n\t\n\t\n\t\tConstruction\n\t\n\nWe used üê¶QDA-SQL. to generate a training set with 4 question types:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GuoZiming/MMSQL.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"self-alignment","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pritamqu/self-alignment","creator_name":"Pritam Sarkar","creator_url":"https://huggingface.co/pritamqu","description":" \n \n \n \n \n\n\t\n\t\t\n\t\n\t\n\t\tVideo sources\n\t\n\nIn the json files, src indicates the video sources which can be downloaded as follows.\n\nvideo-vqa-webvid_qa: WebVid\nvideo-conversation-videochat2: VideoChat2\nvideo-classification-ssv2: SSv2\nvideo-reasoning-clevrer_qa: CLEVRER\nvideo-vqa-tgif_frame_qa: TGIF\nvideo-reasoning-next_qa: NExTQA\nvideo-conversation-videochat1: VideoChat\nvideo-vqa-tgif_transition_qa: TGIF\nvideo-reasoning-clevrer_mc: CLEVRER\nvideo-vqa-ego_qa: EgoQA\nvideo-classification-k710:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pritamqu/self-alignment.","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K<n<100K","arxiv:2504.12083","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"wan_zipping_up_jacket","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_zipping_up_jacket","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"keystroke-typing-videos","keyword":"video-text-to-text","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/andrewt28/keystroke-typing-videos","creator_name":"Andrew Tran","creator_url":"https://huggingface.co/andrewt28","description":"\n\t\n\t\t\n\t\tKeystroke Typing Videos of Reuters\n\t\n\nRecordings of typing randomly sampled sentences (<= 150 characters) from nltk Reuters dataset. Keystroke data is provided too.\n","first_N":5,"first_N_keywords":["video-text-to-text","English","afl-3.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"ToneSpeak","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/ToneSpeak","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\n\t\n\t\t\n\t\tToneSpeak\n\t\n\nToneSpeak ‚Äî –±–æ–ª—å—à–æ–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—É–¥–∏–æ–¥–∞—Ç–∞—Å–µ—Ç —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–Ω—Ç–æ–Ω–∞—Ü–∏–π, —Ç–µ–º–±—Ä–∞ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≥–æ–ª–æ—Å–∞. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ 26.33 —á–∞—Å–∞ –∞—É–¥–∏–æ –¥–ª—è train —Å–ø–ª–∏—Ç–∞ –∏ 2.91 —á–∞—Å–∞ –¥–ª—è valitation.\n\n\n\t\n\t\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ\n\t\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∞—É–¥–∏–æ —Å–æ–±—Ä–∞–Ω—ã:\n\n–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ (text)\n–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏ –∏ —ç–º–æ—Ü–∏–π (text_description), —Ä–∞–∑–±–∏—Ç–æ–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º:\nAccent/Affect  \nVoice Affect  \nTone  \nPhrasing  \nPunctuation  \nEmotion  \nEmphasis  \nPronunciation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/ToneSpeak.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Russian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MMMG","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MMMGBench/MMMG","creator_name":"MMMG","creator_url":"https://huggingface.co/MMMGBench","description":"\n\t\n\t\t\n\t\tüß† MMMG: Massive Multi-Discipline Multi-Tier Knowledge Image Benchmark\n\t\n\n\n  üß¨ Project Page ‚Ä¢\n  üìÇ Code\n\n\nMMMG introduces knowledge image generation as a new frontier in text-to-image research. This benchmark probes the reasoning capabilities of image generation models by challenging them to produce educational and scientific visuals grounded in structured knowledge.\nKnowledge images‚Äîsuch as charts, diagrams, mind maps, and scientific illustrations‚Äîplay a crucial role in human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMGBench/MMMG.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MicroG-4M","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LEI-QI-233/MicroG-4M","creator_name":"Lei Qi","creator_url":"https://huggingface.co/LEI-QI-233","description":"\n\t\n\t\t\n\t\tMicroG-4M Dataset\n\t\n\nThis repository stores the entire content of the  MicroG-4M dataset itself.\nFor more information and details, including training, evaluation, statistics, and related code, please:\n\nRefer to our paper\n\nVisit our GitHub\n\n\nIn addition to the original dataset format, we provide a Parquet format for automatically generating Croissant files on the Hugging Face platform. Loading via Croissant will fetch these Parquet files directly. For detailed information, please check‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LEI-QI-233/MicroG-4M.","first_N":5,"first_N_keywords":["video-classification","visual-question-answering","video-text-to-text","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"image-wallpapers-dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","description":"\n\t\n\t\t\n\t\tNavanjana/image-wallpapers-dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains high-quality images paired with descriptive text annotations, designed for computer vision and multimodal machine learning tasks. Each image has been preprocessed to standard dimensions and paired with detailed descriptions extracted from web sources.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal Images: [NUMBER] images\nImage Format: JPEG (RGB)\nImage Dimensions: 224√ó224 pixels\nText Descriptions: Natural‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/image-wallpapers-dataset.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-feature-extraction","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-130k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","description":"This is the SophiaVL-7B-130k dataset of SophiaVL-R1 (https://arxiv.org/abs/2505.17018). The textual data is stored in JSON files (located in ./json/), and the corresponding images are contained in ZIP archives.\nCode: https://github.com/kxfan2002/SophiaVL-R1\nData is in the following format:\n    {\n        \"problem_id\": 1, # id in current class\n        \"problem\": \"Subtract 0 cyan cubes. How many objects are left?\", # textual question\n        \"data_type\": \"image\", # text-only data(\"text\") or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bunny127/SophiaVL-R1-130k.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Base-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Base-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-SFT-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe SFT data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data contains 8,000+ reasoning trajectories, including :\n\n2,000+ textual reasoning trajectories, rejection sampled from the base model Qwen2.5-VL-Instruct. These data aims to preserve textual reasoning ability on easier VL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ToneBooks","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/ToneBooks","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\n\t\n\t\t\n\t\tToneBooks\n\t\n\nToneBooks ‚Äî –±–æ–ª—å—à–æ–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ–∫–Ω–∏–≥ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –∏–Ω—Ç–æ–Ω–∞—Ü–∏–π, —Ç–µ–º–±—Ä–∞ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≥–æ–ª–æ—Å–∞. –í –¥–∞—Ç–∞—Å–µ—Ç–µ 179.16 —á–∞—Å–æ–≤ –∞—É–¥–∏–æ –¥–ª—è train —Å–ø–ª–∏—Ç–∞ –∏ 9.42 —á–∞—Å–∞ –¥–ª—è validation.\n–ë–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ its5Q –∑–∞ –ø–æ–º–æ—â—å –≤ —Å–±–æ—Ä–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n\n\n\t\n\t\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ\n\t\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–±—Ä–∞–Ω—ã:\n\n–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ (text)\n–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏ –∏ —ç–º–æ—Ü–∏–π (text_description), —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º:\nAccent/Affect  \nVoice Affect‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/ToneBooks.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Russian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-10k-256x256","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","description":"\n\t\n\t\t\n\t\tüìÑ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-10k-256x256.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"aimeghamuse","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/siyadsiya/aimeghamuse","creator_name":"K","creator_url":"https://huggingface.co/siyadsiya","description":"siyadsiya/aimeghamuse dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-classification","text-to-image","Malayalam","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"ReasonGen-R1-RL-T2I-11k","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Franklin0/ReasonGen-R1-RL-T2I-11k","creator_name":"Franklin Zhang","creator_url":"https://huggingface.co/Franklin0","description":"This is the RL dataset for the paper: \"ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL\".\nReasonGen-R1 is a two-stage framework that imbues an autoregressive image generator with explicit text-based \"thinking\" skills via supervised fine-tuning (SFT) on a newly generated reasoning dataset of written rationales. It then refines its outputs using Group Relative Policy Optimization (GRPO). This dataset contains the model-crafted rationales paired with visual prompts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Franklin0/ReasonGen-R1-RL-T2I-11k.","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"ReasonGen-R1-RL-Geneval-12k","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Franklin0/ReasonGen-R1-RL-Geneval-12k","creator_name":"Franklin Zhang","creator_url":"https://huggingface.co/Franklin0","description":"This is the RL dataset for the paper: \"ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL\".\nReasonGen-R1 is a two-stage framework that imbues an autoregressive image generator with explicit text-based \"thinking\" skills via supervised fine-tuning (SFT) on a newly generated reasoning dataset of written rationales. It then refines its outputs using Group Relative Policy Optimization (GRPO). This dataset contains the model-crafted rationales paired with visual prompts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Franklin0/ReasonGen-R1-RL-Geneval-12k.","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"UI-Genie-Agent-5k","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HanXiao1999/UI-Genie-Agent-5k","creator_name":"HanXiao","creator_url":"https://huggingface.co/HanXiao1999","description":"This repository contains the dataset from the paper UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\nMobile GUI Agents.\nGithub: https://github.com/Euphoria16/UI-Genie\n","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTest","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTest","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","description":"This repository contains the dataset used in GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nGitHub repository: https://github.com/yueliu1999/GuardReasoner-VL\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"emotion-prompts","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/caster97/emotion-prompts","creator_name":"caster97","creator_url":"https://huggingface.co/caster97","description":"\n\t\n\t\t\n\t\tEmotion Prompts Dataset\n\t\n\nThis dataset contains simple textual prompts designed for eliciting or detecting emotional tones in generated speech or text. It is useful for training or evaluating emotion-conditioned models such as TTS (text-to-speech) or dialogue systems.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\nDifficulty: Describes prompt complexity (Simple, Moderate, Complex).\nPrompt: A text prompt with an {emotion} placeholder.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nTo use this dataset with the Hugging Face Datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/caster97/emotion-prompts.","first_N":5,"first_N_keywords":["text-to-speech","mit","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"SophiaVL-R1-Thinking-156k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bunny127/SophiaVL-R1-Thinking-156k","creator_name":"kxbunny","creator_url":"https://huggingface.co/bunny127","description":"This is the SophiaVL-R1-Thinking-156k dataset for training Thinking Reward Model of SophiaVL-R1 (SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward).\nThe data is constructed in sharegpt format. text_only_part.json is text-only data. multimodal_part.json is image-text data. Images can be found in images.\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.17018","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"openvid-hd","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVidHD-0.4M\n\t\n\nOpenVidHD-0.4M is a high-quality subset of the OpenVid-1M dataset, curated to support research on video understanding, generation, and evaluation with high-resolution and rich semantic annotations.\n\n\t\n\t\t\n\t\tüìö Overview\n\t\n\nThis subset consists of approximately 433,509 video clips, each accompanied by detailed metadata, including:\n\nNatural language captions\nAesthetic quality scores\nMotion intensity scores\nTemporal consistency scores\nCamera motion descriptors\nFrame rate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"openvid-hd","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVidHD-0.4M\n\t\n\nOpenVidHD-0.4M is a high-quality subset of the OpenVid-1M dataset, curated to support research on video understanding, generation, and evaluation with high-resolution and rich semantic annotations.\n\n\t\n\t\t\n\t\tüìö Overview\n\t\n\nThis subset consists of approximately 433,509 video clips, each accompanied by detailed metadata, including:\n\nNatural language captions\nAesthetic quality scores\nMotion intensity scores\nTemporal consistency scores\nCamera motion descriptors\nFrame rate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"V1-33K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/haonan3/V1-33K","creator_name":"Haonan Wang","creator_url":"https://huggingface.co/haonan3","description":"\n\n\n\t\n\t\t\n\t\tV1: Toward Multimodal Reasoning by Designing Auxiliary Tasks\n\t\n\n\nüöÄ  Toward Multimodal Reasoning via Unsupervised Task -- Future Prediction üåü\n\n\n\n\n\n\n\n\n \n\nAuthors: Haonan Wang, Chao Du, Tianyu PangGitHub: haonan3/V1Dataset: V1-33K on Hugging Face\n\n\n\n\t\n\t\t\n\t\tMultimodal Reasoning\n\t\n\nRecent Large Reasoning Models (LRMs) such as DeepSeek-R1 have demonstrated impressive reasoning abilities; however, their capabilities are limited to textual data. Current models capture only a small part of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haonan3/V1-33K.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PixelReasoner-RL-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"Overview.\nThe RL data for training Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning,\nThe queries require fine-grained visual analysis in both images (e.g., infographics, visually-rich scenes, etc) and videos. \nDetails.\nThe data includes 15,402 training queries with verifierable answers. The key fields include:\n\nquestion, answer, qid\nis_video: a flag to distinguish video and image queries\nimage: a list of image paths.\nFor video-based queries, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"MM-Math-Align","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THU-KEG/MM-Math-Align","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","description":"\nHard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models\n\n\n\n| üêô Github Code |\nüìÉ Paper |\n\n\n\n\t\n\t\t\n\t\tDataset description:\n\t\n\nWe release MM-Math-Align, a dataset built upon MM-Math, which is derived from actual geometry questions used in middle school exams. Each sample contains the original geometric diagram(original_image), a Python script's image(positive_image) that approximately reconstructs the diagram, a caption(positive_caption) describing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/MM-Math-Align.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/T2I-ConBench/test","creator_name":"T2I-ConBench","creator_url":"https://huggingface.co/T2I-ConBench","description":"T2I-ConBench/test dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"zuhri","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/humair025/zuhri","creator_name":"Humair M","creator_url":"https://huggingface.co/humair025","description":"\n\t\n\t\t\n\t\tZuhri ‚Äî Urdu G2P Dataset\n\t\n\nZuhri is a comprehensive and manually verified Urdu Grapheme-to-Phoneme (G2P) dataset. It is designed to aid research and development in areas such as speech synthesis, pronunciation modeling, and computational linguistics, specifically for the Urdu language.\nThis dataset provides accurate phoneme transcriptions and IPA representations, making it ideal for use in building high-quality TTS (Text-to-Speech), ASR (Automatic Speech Recognition), and other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humair025/zuhri.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","Urdu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"zuhri","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/humair025/zuhri","creator_name":"Humair M","creator_url":"https://huggingface.co/humair025","description":"\n\t\n\t\t\n\t\tZuhri ‚Äî Urdu G2P Dataset\n\t\n\nZuhri is a comprehensive and manually verified Urdu Grapheme-to-Phoneme (G2P) dataset. It is designed to aid research and development in areas such as speech synthesis, pronunciation modeling, and computational linguistics, specifically for the Urdu language.\nThis dataset provides accurate phoneme transcriptions and IPA representations, making it ideal for use in building high-quality TTS (Text-to-Speech), ASR (Automatic Speech Recognition), and other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/humair025/zuhri.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","Urdu","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"alternative_christian_text2music","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BeardingFace/alternative_christian_text2music","creator_name":"Benjamin","creator_url":"https://huggingface.co/BeardingFace","description":"MP3s created with a Pro Subscription to a proprietary text2music provider. While under a Pro Subscription, outputs are licensed permissively.\nDesigned to be LORA training for the below model:\nhttps://huggingface.co/ACE-Step/ACE-Step-v1-3.5B\nI may update the _Prompts in the dataset for other models in the future.\nPlease support the official ACE-Step team:\n@misc{gong2025acestep,\n  title={ACE-Step: A Step Towards Music Generation Foundation Model},\n  author={Junmin Gong, Wenxiao Zhao, Sen Wang‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BeardingFace/alternative_christian_text2music.","first_N":5,"first_N_keywords":["text-to-audio","English","apache-2.0","1K - 10K","text"],"keywords_longer_than_N":true},
	{"name":"bird_train","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cycloneboy/bird_train","creator_name":"cycloneboy","creator_url":"https://huggingface.co/cycloneboy","description":"\n\t\n\t\t\n\t\tCSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning\n\t\n\nThis repository contains the datasets used and/or generated in the paper CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning.\nCode Repository: https://github.com/CycloneBoy/csc_sql\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nLarge language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cycloneboy/bird_train.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","arxiv:2505.13271","üá∫üá∏ Region: US","text-to-sql"],"keywords_longer_than_N":true},
	{"name":"4HistoryDataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jianqunZ/4HistoryDataset","creator_name":"Jianqun Zhou","creator_url":"https://huggingface.co/jianqunZ","description":"jianqunZ/4HistoryDataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"kawaii_chibi_avatar_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robb-0/kawaii_chibi_avatar_dataset","creator_name":"robbie","creator_url":"https://huggingface.co/robb-0","description":"\n\t\n\t\t\n\t\tKawaii Chibi Avatar Dataset\n\t\n\n\nThis is the dataset used to train \nKawaii Chibi Avatar for Illustrious.\n\n\nAll images have a .txt file auto-tagged on Civitai.\nAll images were generated on SDXL using Kawaii Chibi Avatar for SDXL\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tLicense\n\t\n\nLicense: CC BY 4.0\nAttribution:\nKawaii Chibi Avatar Dataset ¬© 2025 by Robb-0 is licensed under CC BY 4.0\n\n","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"coco2017-segmentation-50k-256x256","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256","creator_name":"Ole","creator_url":"https://huggingface.co/peteole","description":"\n\t\n\t\t\n\t\tüìÑ License and Attribution\n\t\n\nThis dataset is a downsampled version of the COCO 2017 dataset, tailored for segmentation tasks. It has the following fields:\n\nimage: 256x256 image\nsegmentation: 256x256 image. Each pixel encodes the class of that pixel. See class_names_dict.json for a legend.\ncaptions: a list of captions for the image, each by a different labeler.\n\nUse the dataset as follows:\nimport requests\nfrom datasets import load_dataset\n\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/peteole/coco2017-segmentation-50k-256x256.","first_N":5,"first_N_keywords":["image-segmentation","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo3","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Veo 3 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~46k human responses from ~20k human annotators were collected to evaluate Veo3 video generation model on our benchmark. This dataset was collected in half a day using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it ‚ù§Ô∏è‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo3","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Veo 3 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~46k human responses from ~20k human annotators were collected to evaluate Veo3 video generation model on our benchmark. This dataset was collected in half a day using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider liking it ‚ù§Ô∏è‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo3.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-wan-latents-81frames","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents Dataset\n\t\n\nThis repository contains VAE-encoded latent representations extracted from the OpenVid HD video dataset using the Wan2.1 VAE encoder.\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nSource Dataset: Enderfga/openvid-hd (~433k videos)\nGenerated Dataset: Enderfga/openvid-hd-wan-latents-81frames (~270k latents)\nVAE Model: Wan2.1 VAE from Alibaba's Wan2.1 video generation suite\nFrame Count: 81 frames per video (21 temporal latent dimensions √ó ~3.86 frame compression ratio)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","arxiv:2407.02371"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-wan-latents-81frames","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents Dataset\n\t\n\nThis repository contains VAE-encoded latent representations extracted from the OpenVid HD video dataset using the Wan2.1 VAE encoder.\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nSource Dataset: Enderfga/openvid-hd (~433k videos)\nGenerated Dataset: Enderfga/openvid-hd-wan-latents-81frames (~270k latents)\nVAE Model: Wan2.1 VAE from Alibaba's Wan2.1 video generation suite\nFrame Count: 81 frames per video (21 temporal latent dimensions √ó ~3.86 frame compression ratio)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","arxiv:2407.02371"],"keywords_longer_than_N":true},
	{"name":"IndustryEQA","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/IndustryEQA/IndustryEQA","creator_name":"IndustryEQA","creator_url":"https://huggingface.co/IndustryEQA","description":"\n\t\n\t\t\n\t\tIndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios\n\t\n\nAuthors: Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong\nPaper üìù | Code üíª\nThis benchmark dataset accopmanies our paper of the same title. Built upon the NVIDIA Isaac Sim platform,\nIndustryEQA provides high-fidelity episodic memory videos featuring diverse industrial assets,\ndynamic human agents, and carefully designed hazardous situations inspired by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IndustryEQA/IndustryEQA.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"ReasonGen-R1-SFT-230k","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Franklin0/ReasonGen-R1-SFT-230k","creator_name":"Franklin Zhang","creator_url":"https://huggingface.co/Franklin0","description":"SFT Dataset for the paper: \"ReasonGen-R1: Cot for Autoregressive Image generation models through SFT and RL\".\nWebsite: https://aka.ms/reasongen\nCode: https://github.com/Franklin-Zhang0/Image-RL\nArxiv: https://arxiv.org/abs/2505.24875\n","first_N":5,"first_N_keywords":["text-to-image","cc-by-4.0","100K - 1M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Cosmos-R1-RL-dataset","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mohammadbhat/Cosmos-R1-RL-dataset","creator_name":"Mohammad Qazim Bhat","creator_url":"https://huggingface.co/mohammadbhat","description":"mohammadbhat/Cosmos-R1-RL-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VLM-Video-Understanding","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\n\t\n\t\t\n\t\tVLM-Video-Understanding\n\t\n\n\nA minimalistic demo for image inference and video understanding using OpenCV, built on top of several popular open-source Vision-Language Models (VLMs). This repository provides Colab notebooks demonstrating how to apply these VLMs to video and image tasks using Python and Gradio.\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis project showcases lightweight inference pipelines for the following:\n\nVideo frame extraction and preprocessing\nImage-level inference with VLMs\nReal-time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/VLM-Video-Understanding.","first_N":5,"first_N_keywords":["video-text-to-text","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"testnew","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/setfunctionenvironment/testnew","creator_name":"setfunctionenvironment","creator_url":"https://huggingface.co/setfunctionenvironment","description":"\n\t\n\t\t\n\t\tAudio Dataset Statistics\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal audio files\n556,667\n\n\nTotal duration\n1,024.71 hours (3,688,949 seconds)\n\n\nAverage duration\n6.63 seconds\n\n\nShortest clip\n0.41 seconds\n\n\nLongest clip\n44.97 seconds\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSpeaker Breakdown\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTop 10 Speakers by Clip Count\n\t\n\n\n\t\n\t\t\nSpeaker\nClips\nDuration\n% of Total\n\n\n\t\t\nDespina\n60,150\n118.07 hours\n11.5%\n\n\nSulafat\n31,593\n58.15 hours\n5.7%\n\n\nAchernar29,889\n54.53 hours\n5.3%\n\n\nAutonoe\n27,897‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/setfunctionenvironment/testnew.","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-4o-Image","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-4o-Image","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\n\t\n\t\t\n\t\tüìö ShareGPT-4o-Image\n\t\n\nShareGPT-4o-Image is a large-scale and high-quality image generation dataset, where all images are produced by GPT-4o‚Äôs image generation capabilities. This dataset is designed to align open multimodal models with GPT-4o‚Äôs strengths in visual content creation. It includes 45K text-to-image and 46K text-and-image-to-image samples, making it a useful resource for enhancing multimodal models in both image generation and editing tasks.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-4o-Image.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nkp37/OpenVid-1M","creator_name":"nkp","creator_url":"https://huggingface.co/nkp37","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper [ICLR 2025] OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation.\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkp37/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nkp37/OpenVid-1M","creator_name":"nkp","creator_url":"https://huggingface.co/nkp37","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper [ICLR 2025] OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation.\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkp37/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"imagenet-1k-vl-enriched","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched","creator_name":"Visual Layer","creator_url":"https://huggingface.co/visual-layer","description":"\n  \n    Visualize on Visual Layer\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tImagenet-1K-VL-Enriched\n\t\n\nAn enriched version of the ImageNet-1K Dataset with image caption, bounding boxes, and label issues!\nWith this additional information, the ImageNet-1K dataset can be extended to various tasks such as image retrieval or visual question answering.\nThe label issues helps to curate a cleaner and leaner dataset.\n\n\t\n\t\n\t\n\t\tDescription\n\t\n\nThe dataset consists of 6 columns:\n\nimage_id: The original filename of the image from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/visual-layer/imagenet-1k-vl-enriched.","first_N":5,"first_N_keywords":["object-detection","image-classification","text-to-image","image-to-text","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"Cosmos-Reason1-SFT-Dataset","keyword":"video-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nThe data format is a pair of video and text annotations. We summarize the data and annotations in Table 4 (SFT), Table 5 (RL), and Table 6 (Benchmark) of the Cosmos-Reason1 paper. ‚Äã‚Äã We release the annotations for embodied reasoning tasks for BridgeDatav2, RoboVQA, Agibot, HoloAssist, AV, and the videos for the RoboVQA and AV datasets. We additionally release the annotations and videos for the RoboFail dataset for benchmarks. By releasing the dataset, NVIDIA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"sql-create-context","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"FaceCaption-15M","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFacaCaption-15M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\n\n\nFaceCaption-15M, a large-scale, diverse, and high-quality dataset of facial images accompanied by their natural language descriptions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"SynSQL-2.5M","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/seeklhy/SynSQL-2.5M","creator_name":"lihaoyang","creator_url":"https://huggingface.co/seeklhy","description":"\n\t\n\t\t\n\t\tSynSQL-2.5M - The First Million-Scale Cross-Domain Text-to-SQL Dataset\n\t\n\nWe introduce the first million-scale text-to-SQL dataset, SynSQL-2.5M, containing over 2.5 million diverse and high-quality data samples, spanning more than 16,000 databases from various domains.\nBuilding on SynSQL-2.5M, we introduce OmniSQL, a family of powerful text-to-SQL models available in three sizes: 7B, 14B, and 32B. During the fine-tuning process, we also integrate training sets from Spider and BIRD‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seeklhy/SynSQL-2.5M.","first_N":5,"first_N_keywords":["table-question-answering","translation","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"spider-ko","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huggingface-KREW/spider-ko","creator_name":"Hugging Face KREW","creator_url":"https://huggingface.co/huggingface-KREW","description":"\n\t\n\t\t\n\t\tDataset Card for spider-ko: ÌïúÍµ≠Ïñ¥ Text-to-SQL Îç∞Ïù¥ÌÑ∞ÏÖã\n\t\n\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ÏÖã ÏöîÏïΩ\n\t\n\nSpider-KOÎäî Yale UniversityÏùò Spider Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î≤àÏó≠Ìïú ÌÖçÏä§Ìä∏-SQL Î≥ÄÌôò Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§. ÏõêÎ≥∏ Spider Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÏûêÏó∞Ïñ¥ ÏßàÎ¨∏ÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Î≤àÏó≠ÌïòÏó¨ Íµ¨ÏÑ±ÌïòÏòÄÏäµÎãàÎã§. Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Îã§ÏñëÌïú ÎèÑÎ©îÏù∏Ïùò Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê ÎåÄÌïú ÏßàÏùòÏôÄ Ìï¥Îãπ SQL ÏøºÎ¶¨Î•º Ìè¨Ìï®ÌïòÍ≥† ÏûàÏúºÎ©∞, ÌïúÍµ≠Ïñ¥ Text-to-SQL Î™®Îç∏ Í∞úÎ∞ú Î∞è ÌèâÍ∞ÄÏóê ÌôúÏö©Îê† Ïàò ÏûàÏäµÎãàÎã§.\n\n\t\n\t\t\n\t\tÏßÄÏõê ÌÉúÏä§ÌÅ¨ Î∞è Î¶¨ÎçîÎ≥¥Îìú\n\t\n\n\ntext-to-sql: ÌïúÍµ≠Ïñ¥ ÏûêÏó∞Ïñ¥ ÏßàÎ¨∏ÏùÑ SQL ÏøºÎ¶¨Î°ú Î≥ÄÌôòÌïòÎäî ÌÉúÏä§ÌÅ¨Ïóê ÏÇ¨Ïö©Îê©ÎãàÎã§.\n\n\n\t\n\t\t\n\t\tÏñ∏Ïñ¥\n\t\n\nÎç∞Ïù¥ÌÑ∞ÏÖãÏùò ÏßàÎ¨∏ÏùÄ ÌïúÍµ≠Ïñ¥(ko)Î°ú Î≤àÏó≠ÎêòÏóàÏúºÎ©∞, SQL ÏøºÎ¶¨Îäî ÏòÅÏñ¥ Í∏∞Î∞òÏúºÎ°ú Ïú†ÏßÄÎêòÏóàÏäµÎãàÎã§. ÏõêÎ≥∏ ÏòÅÏñ¥ ÏßàÎ¨∏ÎèÑ Ìï®Íªò Ï†úÍ≥µÎê©ÎãàÎã§.\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞\n\t\n\n\n\t\n\t\t\n\t\tÎç∞Ïù¥ÌÑ∞ ÌïÑÎìú\n\t\n\n\ndb_id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huggingface-KREW/spider-ko.","first_N":5,"first_N_keywords":["table-question-answering","machine-generated","expert-generated","extended-from-spider","Korean"],"keywords_longer_than_N":true},
	{"name":"DeepEyes-Datasets-47k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k","creator_name":"ChenShawn","creator_url":"https://huggingface.co/ChenShawn","description":"This repository contains the datasets used in the paper DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning.\nCode: https://github.com/Visual-Agent/DeepEyes\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2505.14362","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"X2I2","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OmniGen2/X2I2","creator_name":"OmniGen","creator_url":"https://huggingface.co/OmniGen2","description":"\n  \n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\t\n\t\t\n\t\tX2I2 Dataset\n\t\n\n\n2025-07-15: jsons/reflect/reflect.jsonl has been fixed and updated.\n2025-07-05: X2I2 are available now.\n\n\n\t\n\t\t\n\t\tX2I2-video-editing\n\t\n\n# meta file (en): jsons/video_edit/edit_mv.jsonl\n# meta file (zh): jsons/video_edit/edit_mv_zh.jsonl\n# images:\ncd images/video_edit/edit_mv_0 && cat edit_mv_0.tar.gz.part_* > edit_mv_0.tar.gz && tar -xzvf edit_mv_0.tar.gz\ncd images/video_edit/edit_mv_1 && cat edit_mv_1.tar.gz.part_* > edit_mv_1.tar.gz && tar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OmniGen2/X2I2.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","any-to-any","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"spider","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xlangai/spider","creator_name":"XLang NLP Lab","creator_url":"https://huggingface.co/xlangai","description":"\n\t\n\t\t\n\t\tDataset Card for Spider\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students.\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/spider.","first_N":5,"first_N_keywords":["expert-generated","expert-generated","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"multilingual_librispeech","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/multilingual_librispeech","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tDataset Card for MultiLingual LibriSpeech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/multilingual_librispeech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"multilingual_librispeech","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/multilingual_librispeech","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tDataset Card for MultiLingual LibriSpeech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/multilingual_librispeech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"nouns","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m1guelpf/nouns","creator_name":"Miguel Piedrafita","creator_url":"https://huggingface.co/m1guelpf","description":"\n\t\n\t\t\n\t\tDataset Card for Nouns auto-captioned\n\t\n\nDataset used to train Nouns text to image model\nAutomatically generated captions for Nouns from their attributes, colors and items. Help on the captioning script appreciated!\nFor each row the dataset contains image and text keys. image is a varying size PIL jpeg, and text is the accompanying text caption. Only a train split is provided.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite it as:\n@misc{piedrafita2022nouns,\n      author =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m1guelpf/nouns.","first_N":5,"first_N_keywords":["text-to-image","machine-generated","other","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"MagicBrush","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/osunlp/MagicBrush","creator_name":"OSU NLP Group","creator_url":"https://huggingface.co/osunlp","description":"\n\t\n\t\t\n\t\tDataset Card for MagicBrush\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nMagicBrush is the first large-scale, manually-annotated instruction-guided image editing dataset covering diverse scenarios single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises 10K (source image, instruction, target image) triples, which is sufficient to train large-scale image editing models.\nPlease check our website to explore more visual results.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\"img_id\" (str):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/osunlp/MagicBrush.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"webvid-10M","keyword":"text-to-video","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TempoFunk/webvid-10M","creator_name":"TempoFunk","creator_url":"https://huggingface.co/TempoFunk","description":"TempoFunk/webvid-10M dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-video","text-to-image","video-classification","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"webvid-10M","keyword":"text-to-image","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TempoFunk/webvid-10M","creator_name":"TempoFunk","creator_url":"https://huggingface.co/TempoFunk","description":"TempoFunk/webvid-10M dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-video","text-to-image","video-classification","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"the-mc-speech-dataset","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/czyzi0/the-mc-speech-dataset","creator_name":"Mateusz Czy≈ºnikiewicz","creator_url":"https://huggingface.co/czyzi0","description":"This is public domain speech dataset consisting of 24018 short audio clips of a single speaker reading sentences in Polish. A transcription is provided for each clip. Clips have total length of more than 22 hours.\nTexts are in public domain. The audio was recorded in 2021-22 as a part of my master's thesis and is in public domain.\nIf you use this dataset, please cite:\n@masterthesis{mcspeech,\n  title={Analiza por√≥wnawcza korpus√≥w nagra≈Ñ mowy dla cel√≥w syntezy mowy w jƒôzyku polskim}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/czyzi0/the-mc-speech-dataset.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Polish","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GRIT","keyword":"text-to-image","license":"Microsoft Public License","license_url":"https://choosealicense.com/licenses/ms-pl/","language":"en","dataset_url":"https://huggingface.co/datasets/zzliang/GRIT","creator_name":"zhiliang","creator_url":"https://huggingface.co/zzliang","description":"\n\t\n\t\t\n\t\tGRIT: Large-Scale Training Corpus of Grounded Image-Text Pairs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe introduce GRIT, a large-scale dataset of Grounded Image-Text pairs, which is created based on image-text pairs from COYO-700M and LAION-2B. We construct a pipeline to extract and link text spans (i.e., noun phrases, and referring expressions) in the caption to their corresponding image regions. More details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nDuring the construction, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zzliang/GRIT.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","object-detection","zero-shot-classification","image-captioning"],"keywords_longer_than_N":true},
	{"name":"midjourney-prompts","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vivym/midjourney-prompts","creator_name":"Ming Yang","creator_url":"https://huggingface.co/vivym","description":"\n\t\n\t\t\n\t\tmidjourney-prompts\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset contains the cleaned midjourney prompts from Midjourney.\nTotal prompts: 9,085,397\n\n\t\n\t\t\nVersion\nCount\n\n\n\t\t\n5.2\n2,272,465\n\n\n5.1\n2,060,106\n\n\n5.0\n3,530,770\n\n\n4.0\n1,204,384\n\n\n3.0\n14,991\n\n\n2.0\n791\n\n\n1.0\n1,239\n\n\n\t\n\n\n\t\n\t\t\nStyle\nCount\n\n\n\t\t\ndefault\n8,874,181\n\n\nraw\n177,953\n\n\nexpressive\n27,919\n\n\nscenic\n2,146\n\n\ncute\n2,036\n\n\noriginal\n511\n\n\n\t\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"hifi-tts","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MikhailT/hifi-tts","creator_name":"Mikhail Tsimashkou","creator_url":"https://huggingface.co/MikhailT","description":"Hi-Fi Multi-Speaker English TTS Dataset (Hi-Fi TTS) is based on LibriVox's public domain audio books and Gutenberg Project texts.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"hifi-tts","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MikhailT/hifi-tts","creator_name":"Mikhail Tsimashkou","creator_url":"https://huggingface.co/MikhailT","description":"Hi-Fi Multi-Speaker English TTS Dataset (Hi-Fi TTS) is based on LibriVox's public domain audio books and Gutenberg Project texts.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"nst-da","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/nst-da","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\n\t\n\t\t\n\t\tDataset Card for NST-da\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is an upload of the NST Danish ASR Database (16 kHz) ‚Äì reorganized.\nThe training and test splits are the original ones.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nTraining automatic speech recognition is the intended task for this dataset. No leaderboard is active at this point.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is available in Danish (da).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n\nSize of downloaded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/nst-da.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Danish","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"civitai-stable-diffusion-2.5m","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hanruijiang/civitai-stable-diffusion-2.5m","creator_name":"Han Ruijiang","creator_url":"https://huggingface.co/hanruijiang","description":"inspired by thefcraft/civitai-stable-diffusion-337k.\ncollected using civitai api to get all prompts.\n","first_N":5,"first_N_keywords":["text-generation","text-to-image","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"google-tamil","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/google-tamil","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for Tamil Speech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 7 hours of transcribed high-quality audio of Tamil sentences recorded by 50 volunteers. The dataset is intended for speech technologies. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\ntext-to-speech, text-to-audio: The dataset can be used to train a model for Text-To-Speech (TTS).\nautomatic-speech-recognition‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/google-tamil.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Tamil","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"google-tamil","keyword":"text-to-audio","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/google-tamil","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for Tamil Speech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 7 hours of transcribed high-quality audio of Tamil sentences recorded by 50 volunteers. The dataset is intended for speech technologies. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\ntext-to-speech, text-to-audio: The dataset can be used to train a model for Text-To-Speech (TTS).\nautomatic-speech-recognition‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/google-tamil.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Tamil","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"multilingual-tts","keyword":"text-to-speech","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MohamedRashad/multilingual-tts","creator_name":"Mohamed Rashad","creator_url":"https://huggingface.co/MohamedRashad","description":"\n\t\n\t\t\n\t\tBefore Anything and Everything ‚ö±\n\t\n\nIn the time of writing this Dataset Card, 17,490 18,412 civilian has been killed in Palestine (7,870 8,000 are children and 6,121 6,200 are women).\nSeek any non-profit organization to help them with what you can (For myself, I use Mersal) üáµüá∏\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe Multilingual TTS dataset is an exceptional compilation of text-to-speech (TTS) samples, meticulously crafted to showcase the richness and diversity of human languages.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MohamedRashad/multilingual-tts.","first_N":5,"first_N_keywords":["text-to-speech","Arabic","English","Chinese","Spanish"],"keywords_longer_than_N":true},
	{"name":"AniSpeech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShoukanLabs/AniSpeech","creator_name":"ShoukanLabs","creator_url":"https://huggingface.co/ShoukanLabs","description":"\n\t\n\t\t\n\t\tAniSpeech Dataset\n\t\n\nWelcome to the AniSpeech dataset, a continually expanding collection of captioned anime voices brought to you by ShoukanLabs.\n\nAs we label more and more audio, they'll automagically be uploaded here for use, seperated by language\n\n\n\n\t\n\t\t\n\t\tANNOUNCMENTS:\n\t\n\n\nAn upcoming update will add an immense ammount of data to the dataset... however... because we cannot manually go through this dataset we have had to rely on manual quality estimation, as such, speaker splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShoukanLabs/AniSpeech.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"AniSpeech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShoukanLabs/AniSpeech","creator_name":"ShoukanLabs","creator_url":"https://huggingface.co/ShoukanLabs","description":"\n\t\n\t\t\n\t\tAniSpeech Dataset\n\t\n\nWelcome to the AniSpeech dataset, a continually expanding collection of captioned anime voices brought to you by ShoukanLabs.\n\nAs we label more and more audio, they'll automagically be uploaded here for use, seperated by language\n\n\n\n\t\n\t\t\n\t\tANNOUNCMENTS:\n\t\n\n\nAn upcoming update will add an immense ammount of data to the dataset... however... because we cannot manually go through this dataset we have had to rely on manual quality estimation, as such, speaker splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShoukanLabs/AniSpeech.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"libritts_r","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mythicinfinity/libritts_r","creator_name":"Mythic Infinity","creator_url":"https://huggingface.co/mythicinfinity","description":"\n\t\n\t\t\n\t\tDataset Card for LibriTTS-R\n\t\n\n\n\nLibriTTS-R [1] is a sound quality improved version of the LibriTTS corpus \n(http://www.openslr.org/60/) which is a multi-speaker English corpus of approximately \n585 hours of read English speech at 24kHz sampling rate, published in 2019.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is the LibriTTS-R dataset, adapted for the datasets library.\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\nThere are 7 splits (dots replace dashes from the original dataset, to comply with hf naming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mythicinfinity/libritts_r.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"text-to-image-prompts","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kazimir-ai/text-to-image-prompts","creator_name":"Kazimir.ai","creator_url":"https://huggingface.co/Kazimir-ai","description":"\n\t\n\t\t\n\t\tThe dataset of the most popular text-to-image prompts.\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: kazimir.ai\nFunded by [optional]: [More Information Needed]\nShared by [optional]: https://kazimir.ai\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nFree to use.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nCSV file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kazimir-ai/text-to-image-prompts.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"mls_eng","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls_eng","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls_eng","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls_eng","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for English MLS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a streamable version of the English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"synthetic_text_to_sql","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gretelai/synthetic_text_to_sql","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","description":"\n  \n  Image generated by DALL-E. See prompt for more details\n\n\n\n\t\n\t\t\n\t\tsynthetic_text_to_sql\n\t\n\n\ngretelai/synthetic_text_to_sql is a rich dataset of high quality synthetic Text-to-SQL samples, \ndesigned and generated using Gretel Navigator, and released under Apache 2.0.\nPlease see our release blogpost for more details.\nThe dataset includes:\n\n  105,851 records partitioned into 100,000 train and 5,851 test records\n  ~23M total tokens, including ~12M SQL tokens\n  Coverage across 100 distinct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_text_to_sql.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Visual-CoT","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deepcs233/Visual-CoT","creator_name":"Hao Shao","creator_url":"https://huggingface.co/deepcs233","description":"\n\t\n\t\t\n\t\tVisCoT Dataset Card\n\t\n\n\n\nThere is a shortage of multimodal datasets for training multi-modal large language models (MLLMs) that require to identify specific regions in an image for additional attention to improve response performance. This type of dataset with grounding bbox annotations could possibly help the MLLM output intermediate interpretable attention area and enhance performance.\nTo fill the gap, we curate a visual CoT dataset. This dataset specifically focuses on identifying‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepcs233/Visual-CoT.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","arxiv:2403.16999","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"synthetic-dataset-1m-dalle3-high-quality-captions","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions","creator_name":"Ben","creator_url":"https://huggingface.co/ProGamerGov","description":"\n\t\n\t\t\n\t\tDataset Card for Dalle3 1 Million+ High Quality Captions\n\t\n\nAlt name: Human Preference Synthetic Dataset\n\n\n\nExample grids for landscapes, cats, creatures, and fantasy are also available.\n\n\n\t\n\t\t\n\t\tDescription:\n\t\n\nThis dataset comprises of AI-generated images sourced from various websites and individuals, primarily focusing on Dalle 3 content, along with contributions from other AI systems of sufficient quality like Stable Diffusion and Midjourney (MJ v5 and above). As users typically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-text-to-text","other"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"\n\t\n\t\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Recap-COCO-30K","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCSC-VLAA/Recap-COCO-30K","creator_name":"UCSC-VLAA","creator_url":"https://huggingface.co/UCSC-VLAA","description":"\n\t\n\t\t\n\t\tLlava recaptioned COCO2014 ValSet.\n\t\n\nUsed for text-to-image generation evaluaion. More detial can be found in What If We Recaption Billions of Web Images with LLaMA-3?\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\"image_id\" (str): COCO image id.\n\"coco_url\" (image): the COCO image url.\n\"caption\" (str): the original COCO caption.\n\"recaption\" (str): the llava recaptioned COCO caption.\n\n\t\n\t\t\n\t\tCitation\n\t\n\n\n\nBibTeX:\n@article{li2024recapdatacomp,\n  title={What If We Recaption Billions of Web Images with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UCSC-VLAA/Recap-COCO-30K.","first_N":5,"first_N_keywords":["text-to-image","cc-by-4.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"mls-eng-speaker-descriptions","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of English MLS\n\t\n\nThis dataset consists in annotations of the English subset of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages.\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls-eng-speaker-descriptions","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of English MLS\n\t\n\nThis dataset consists in annotations of the English subset of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages.\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-speaker-descriptions.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"zoengjyutgaai","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CanCLID/zoengjyutgaai","creator_name":"Á≤µË™ûË®àÁÆóË™ûË®ÄÂ≠∏Âü∫Á§éÂª∫Ë®≠ÁµÑ (CanCLID)","creator_url":"https://huggingface.co/CanCLID","description":"\n\t\n\t\t\n\t\tÂºµÊÇ¶Ê•∑Ë¨õÂè§Ë™ûÈü≥Êï∏ÊìöÈõÜ\n\t\n\nEnglish\nÂë¢ÂÄã‰øÇÂºµÊÇ¶Ê•∑Ë¨õ„Ää‰∏âÂúãÊºîÁæ©„Äã„ÄÅ„ÄäÊ∞¥Êª∏ÂÇ≥„Äã„ÄÅ„ÄäËµ∞ÈÄ≤ÊØõÊæ§Êù±ÁöÑÊúÄÂæåÊ≠≤Êúà„Äã„ÄÅ„ÄäÈπøÈºéË®ò„ÄãË™ûÈü≥Êï∏ÊìöÈõÜ„ÄÇÂºµÊÇ¶Ê•∑‰øÇÂª£Â∑ûÊúÄÂá∫ÂêçÂòÖË¨õÂè§‰Ω¨ / Á≤µË™ûË™¨Êõ∏Ëóù‰∫∫„ÄÇ‰Ω¢Âæû‰∏ä‰∏ñÁ¥Ä‰∏ÉÂçÅÂπ¥‰ª£ÈñãÂßãÂ∞±Âñ∫Âª£Êù±ÂêÑÂÄãÊî∂Èü≥ÈõªÂè∞Â∫¶Ë¨õÂè§Ôºå‰Ω¢ÊääËÅ≤‰øÇÂ•ΩÂ§öÂª£Â∑û‰∫∫ÂòÖÂÖ±ÂêåÂõûÊÜ∂„ÄÇÊú¨Êï∏ÊìöÈõÜÊî∂ÈõÜÂòÖ‰øÇ‰Ω¢ÊúÄÁü•ÂêçÂòÖ‰∏âÈÉ®‰ΩúÂìÅ„ÄÇ\nÊï∏ÊìöÈõÜÁî®ÈÄîÔºö\n\nTTSÔºàË™ûÈü≥ÂêàÊàêÔºâË®ìÁ∑¥ÈõÜ\nASRÔºàË™ûÈü≥Ë≠òÂà•ÔºâË®ìÁ∑¥ÈõÜÊàñÊ∏¨Ë©¶ÈõÜ\nÂêÑÁ®ÆË™ûË®ÄÂ≠∏„ÄÅÊñáÂ≠∏Á†îÁ©∂\nÁõ¥Êé•ËÅΩÂöüÊ¨£Ë≥ûËóùË°ìÔºÅ\n\nTTS ÊïàÊûúÊºîÁ§∫Ôºöhttps://huggingface.co/spaces/laubonghaudoi/zoengjyutgaai_tts\n\n\t\n\t\t\n\t\n\t\n\t\tË™¨Êòé\n\t\n\n\nÊâÄÊúâÊñáÊú¨ÈÉΩÊ†πÊìö https://jyutping.org/blog/typo/ Âêå https://jyutping.org/blog/particles/ Ë¶èÁØÑÁî®Â≠ó„ÄÇ\nÊâÄÊúâÊñáÊú¨ÈÉΩ‰ΩøÁî®ÂÖ®ËßíÊ®ôÈªûÔºåÂÜáÂçäËßíÊ®ôÈªû„ÄÇ\nÊâÄÊúâÊñáÊú¨ÈÉΩÁî®Êº¢Â≠óËΩâÂØ´ÔºåÁÑ°ÈòøÊãâ‰ºØÊï∏Â≠óÁÑ°Ëã±ÊñáÂ≠óÊØç\nÊâÄÊúâÈü≥È†ªÊ∫êÈÉΩÂ≠òÊîæÂñ∫/sourceÔºåÁÇ∫Êñπ‰æøÁõ¥Êé•Áî®‰ΩúË®ìÁ∑¥Êï∏ÊìöÔºåÂàáÂàÜÂæåÂòÖÈü≥È†ªÈÉΩÊîæÂñ∫ opus/\nÊâÄÊúâ opus Èü≥È†ªÁöÜÁÇ∫ 48000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CanCLID/zoengjyutgaai.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-generation","feature-extraction","audio-to-audio"],"keywords_longer_than_N":true},
	{"name":"zoengjyutgaai","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CanCLID/zoengjyutgaai","creator_name":"Á≤µË™ûË®àÁÆóË™ûË®ÄÂ≠∏Âü∫Á§éÂª∫Ë®≠ÁµÑ (CanCLID)","creator_url":"https://huggingface.co/CanCLID","description":"\n\t\n\t\t\n\t\tÂºµÊÇ¶Ê•∑Ë¨õÂè§Ë™ûÈü≥Êï∏ÊìöÈõÜ\n\t\n\nEnglish\nÂë¢ÂÄã‰øÇÂºµÊÇ¶Ê•∑Ë¨õ„Ää‰∏âÂúãÊºîÁæ©„Äã„ÄÅ„ÄäÊ∞¥Êª∏ÂÇ≥„Äã„ÄÅ„ÄäËµ∞ÈÄ≤ÊØõÊæ§Êù±ÁöÑÊúÄÂæåÊ≠≤Êúà„Äã„ÄÅ„ÄäÈπøÈºéË®ò„ÄãË™ûÈü≥Êï∏ÊìöÈõÜ„ÄÇÂºµÊÇ¶Ê•∑‰øÇÂª£Â∑ûÊúÄÂá∫ÂêçÂòÖË¨õÂè§‰Ω¨ / Á≤µË™ûË™¨Êõ∏Ëóù‰∫∫„ÄÇ‰Ω¢Âæû‰∏ä‰∏ñÁ¥Ä‰∏ÉÂçÅÂπ¥‰ª£ÈñãÂßãÂ∞±Âñ∫Âª£Êù±ÂêÑÂÄãÊî∂Èü≥ÈõªÂè∞Â∫¶Ë¨õÂè§Ôºå‰Ω¢ÊääËÅ≤‰øÇÂ•ΩÂ§öÂª£Â∑û‰∫∫ÂòÖÂÖ±ÂêåÂõûÊÜ∂„ÄÇÊú¨Êï∏ÊìöÈõÜÊî∂ÈõÜÂòÖ‰øÇ‰Ω¢ÊúÄÁü•ÂêçÂòÖ‰∏âÈÉ®‰ΩúÂìÅ„ÄÇ\nÊï∏ÊìöÈõÜÁî®ÈÄîÔºö\n\nTTSÔºàË™ûÈü≥ÂêàÊàêÔºâË®ìÁ∑¥ÈõÜ\nASRÔºàË™ûÈü≥Ë≠òÂà•ÔºâË®ìÁ∑¥ÈõÜÊàñÊ∏¨Ë©¶ÈõÜ\nÂêÑÁ®ÆË™ûË®ÄÂ≠∏„ÄÅÊñáÂ≠∏Á†îÁ©∂\nÁõ¥Êé•ËÅΩÂöüÊ¨£Ë≥ûËóùË°ìÔºÅ\n\nTTS ÊïàÊûúÊºîÁ§∫Ôºöhttps://huggingface.co/spaces/laubonghaudoi/zoengjyutgaai_tts\n\n\t\n\t\t\n\t\n\t\n\t\tË™¨Êòé\n\t\n\n\nÊâÄÊúâÊñáÊú¨ÈÉΩÊ†πÊìö https://jyutping.org/blog/typo/ Âêå https://jyutping.org/blog/particles/ Ë¶èÁØÑÁî®Â≠ó„ÄÇ\nÊâÄÊúâÊñáÊú¨ÈÉΩ‰ΩøÁî®ÂÖ®ËßíÊ®ôÈªûÔºåÂÜáÂçäËßíÊ®ôÈªû„ÄÇ\nÊâÄÊúâÊñáÊú¨ÈÉΩÁî®Êº¢Â≠óËΩâÂØ´ÔºåÁÑ°ÈòøÊãâ‰ºØÊï∏Â≠óÁÑ°Ëã±ÊñáÂ≠óÊØç\nÊâÄÊúâÈü≥È†ªÊ∫êÈÉΩÂ≠òÊîæÂñ∫/sourceÔºåÁÇ∫Êñπ‰æøÁõ¥Êé•Áî®‰ΩúË®ìÁ∑¥Êï∏ÊìöÔºåÂàáÂàÜÂæåÂòÖÈü≥È†ªÈÉΩÊîæÂñ∫ opus/\nÊâÄÊúâ opus Èü≥È†ªÁöÜÁÇ∫ 48000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CanCLID/zoengjyutgaai.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-generation","feature-extraction","audio-to-audio"],"keywords_longer_than_N":true},
	{"name":"housey-home-pixart-alpha","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/housey-home-pixart-alpha","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tHousey Home v2: PixArt Alpha Split\n\t\n\nI was in the process of producing a fully synthetic dataset for ungrounded image generation using an unconventional combination of layers. As such, I needed a dataset of highly similar objects with 'themes'. In order to produce log(x, y) combinations of options in the final model. This is that dataset.\nThe initial ( 07/15/2024 ) release includes ~1.3k unique houses, each processed using a VQA, PixArt-alpha/PixArt-Sigma-XL-2-1024-MS, to be precise.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/housey-home-pixart-alpha.","first_N":5,"first_N_keywords":["text-to-image","unconditional-image-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"RapBank","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zqning/RapBank","creator_name":"ziqianning","creator_url":"https://huggingface.co/zqning","description":"\n\t\n\t\t\n\t\tDataset Card for RapBank\n\t\n\n\n\nRapBank is the first dataset for rap generation. The rap songs are collected from YouTube, and we provide a meticulously designed pipeline for data processing\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: https://github.com/NZqian/RapBank\nPaper: https://arxiv.org/abs/2408.15474\nDemo: https://nzqian.github.io/Freestyler/\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\nThe RapBank dataset comprises links to a total of 94, 164 songs.\nHowever, due to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zqning/RapBank.","first_N":5,"first_N_keywords":["text-to-speech","cc-by-sa-4.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"midjourney-v6-llava","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/brivangl/midjourney-v6-llava","creator_name":"Ivan Drokin","creator_url":"https://huggingface.co/brivangl","description":"This dataset based on https://huggingface.co/datasets/CortexLM/midjourney-v6 dataset, captioned with LLava-1.6 model. \nThis dataset was released as is. By accessing and using this dataset, you acknowledge and agree that Cortex Foundation and the author of this repo are not responsible for any copyright violations or legal consequences that may arise from the use of these images.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Recap-Long-Laion","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwu-ww/Recap-Long-Laion","creator_name":"Wei Wu","creator_url":"https://huggingface.co/weiwu-ww","description":"\n\t\n\t\t\n\t\tDataset Card for Recap-Long-Laion\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of long captions of ~49M images from LAION-5B dataset. The long captions are generated by pre-trained Multi-modality Large Language Models (ShareGPT4V/InstructBLIP/LLava1.5) with the text prompt \"Describe the image in detail\".\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nWe distribute the image url with long captions under a standard Creative Common CC-BY-4.0 license. The individual images are under their own‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weiwu-ww/Recap-Long-Laion.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"GLOBE_V2","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MushanW/GLOBE_V2","creator_name":"MushanW","creator_url":"https://huggingface.co/MushanW","description":"\n\t\n\t\t\n\t\tImportant notice\n\t\n\nDifferences between V2 version and the version described in paper:\n\nThe V2 version provide audio in 44.1kHz sample rate. (Supersampling)\nThe V2 versionn removed some samples (~5%) due to the volumn and text aligment issues.\n\n\n\t\n\t\t\n\t\tGlobe\n\t\n\nThe full paper can be accessed here: arXiv\nAn online demo can be accessed here: Github\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nThis paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to address the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushanW/GLOBE_V2.","first_N":5,"first_N_keywords":["text-to-audio","automatic-speech-recognition","audio-to-audio","audio-classification","mozilla-foundation/common_voice_14_0"],"keywords_longer_than_N":true},
	{"name":"movie_posters_100k_controlnet","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stzhao/movie_posters_100k_controlnet","creator_name":"steve z","creator_url":"https://huggingface.co/stzhao","description":"Dataset Name: 10k Movie Poster Images with Layouts and Captions\nDescription:\nThis dataset contains 10,000 movie poster images, along with their extracted layout information and captions. The captions are generated by concatenating the movie title and genre(s). The layout annotations were extracted using PaddleOCR, providing precise structural details of the posters.\nSource:\nThe dataset is a curated set of the movie-posters-100k dataset.\nKey Features:\nImages: 10,000 high-resolution movie‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stzhao/movie_posters_100k_controlnet.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ConsisID-preview-Data","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ConsisID-preview-Data","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\t\n\t\t\n\t\tUsage\n\t\n\ncat videos.tar.part* > videos.tar\ncat masks.tar.part* > masks.tar\ntar -xvf bboxes.tar\ntar -xvf masks.tar\ntar -xvf videos.tar\ntar -xvf face_images.tar\n\nFor how to process your own data like ConsisID-Preview-Data dataset in the ConsisID paper, please refer to here. (Support Multi-ID)\n\n\t\n\t\t\n\t\n\t\n\t\tAcknowledgement\n\t\n\n\nThe current open source data is not the complete set for training ConsisID.\nThe current 31.9K captions correspond to videos with a single ID, while the remaining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ConsisID-preview-Data.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"gemini-flash-2.0-speech","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/gemini-flash-2.0-speech","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"\n\t\n\t\t\n\t\tüéôÔ∏è Gemini Flash 2.0 Speech Dataset\n\t\n\n\nThis is a high quality synthetic speech dataset generated by Gemini Flash 2.0 via the Multimodal Live API. It contains speech from 2 speakers - Puck (Male) and Kore (Female) in English.\nüèÖ #1 Trending Audio Dataset in Feb 2025\nüèÖ Used in training of Kokoro TTS and LLaSA 1B\n\n\t\n\t\n\t\n\t\t„ÄΩÔ∏è Stats\n\t\n\nTotal number of audio files: 47,256*2 = 94512Total duration: 1023527.20seconds (284.31 hours)   \nAverage duration: 10.83 seconds   \nShortest file: 0.6‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shb777/gemini-flash-2.0-speech.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"IndicTTS_Punjabi","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SPRINGLab/IndicTTS_Punjabi","creator_name":"SPRINGLab","creator_url":"https://huggingface.co/SPRINGLab","description":"\n\t\n\t\t\n\t\tPunjabi Indic TTS Dataset\n\t\n\nThis dataset is derived from the Indic TTS Database project, specifically using the Punjabi monolingual recordings from both male and female speakers. The dataset contains high-quality speech recordings with corresponding text transcriptions, making it suitable for text-to-speech (TTS) research and development.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguage: Punjabi\nTotal Duration: ~20 hours (Male: 10 hours, Female: 10 hours)\nAudio Format: WAV\nSampling Rate: 48000Hz‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SPRINGLab/IndicTTS_Punjabi.","first_N":5,"first_N_keywords":["text-to-speech","pb","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Lux-Japanese-Speech-Corpus","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus","creator_name":"KohnoseLami","creator_url":"https://huggingface.co/Lami","description":"\n\t\n\t\t\n\t\tLux Japanese Speech Corpus\n\t\n\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\nLux Japanese Speech Corpus „ÅØ„ÄÅ„Ç™„É™„Ç∏„Éä„É´„Ç≠„É£„É©„ÇØ„Çø„Éº„ÄåLux („É´„ÇØ„Çπ)„Äç„Å´„Çà„ÇãÊó•Êú¨Ë™û„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàË™≠„Åø‰∏ä„ÅíÈü≥Â£∞„ÇíÂèéÈå≤„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇ„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ2Á®ÆÈ°û„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÅßÊßãÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nraw: Âä†Â∑•Ââç„ÅÆ 96kHz/16bit „ÅÆ WAV „Éï„Ç°„Ç§„É´\ncleaned: „Éé„Ç§„Ç∫Èô§Âéª„Å™„Å©„ÅÆÂá¶ÁêÜ„ÇíÊñΩ„Åó„Åü 96kHz/16bit „ÅÆ WAV „Éï„Ç°„Ç§„É´\n\nÂêÑÈü≥Â£∞„Éï„Ç°„Ç§„É´„Å´ÂØæÂøú„Åô„Çã„Éà„É©„É≥„Çπ„ÇØ„É™„Éó„Ç∑„Éß„É≥ÔºàË™≠„Åø‰∏ä„Åí„ÅüÊñáÁ´†Ôºâ„ÅØ„ÄÅmetadata.csv „Å´Ë®òÈå≤„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰Ωì„ÅÆ„É°„ÇøÊÉÖÂ†±„ÅØ dataset_infos.json „ÅßÁÆ°ÁêÜ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\t„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†\n\t\n\n‰ª•‰∏ã„ÅØ„ÄÅ„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅÆÊé®Â•®„Éá„Ç£„É¨„ÇØ„Éà„É™ÊßãÈÄ†„ÅÆ‰æã„Åß„Åô„ÄÇ\nLux-Japanese-Speech-Corpus/\n‚îú‚îÄ‚îÄ .gitattributes           # Git„ÅÆ„Ç´„Çπ„Çø„Éû„Ç§„Ç∫„Éï„Ç°„Ç§„É´\n‚îú‚îÄ‚îÄ README.md‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Lami/Lux-Japanese-Speech-Corpus.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Japanese","cc-by-4.0","1B<n<10B"],"keywords_longer_than_N":true},
	{"name":"gretel-synthetic-text-to-sql","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philschmid/gretel-synthetic-text-to-sql","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\n\t\n\t\t\n\t\tFork of gretelai/synthetic_text_to_sql\n\t\n\nThe gretelai/synthetic_text_to_sql dataset is a large, Apache 2.0 licensed, synthetic Text-to-SQL dataset consisting of 105,851 high-quality records across 100 diverse domains, designed for training language models. It includes comprehensive SQL tasks with varying complexities, database contexts, natural language explanations, and contextual tags, outperforming existing datasets in SQL correctness and standards compliance.\n","first_N":5,"first_N_keywords":["question-answering","table-question-answering","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Co-Spy-Bench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ruojiruoli/Co-Spy-Bench","creator_name":"Siyuan Cheng","creator_url":"https://huggingface.co/ruojiruoli","description":"\n\n\n\t\n\t\t\n\t\tCO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI (CVPR 2025)\n\t\n\nWith the rapid advancement of generative AI, it is now possible to synthesize high-quality images in a few seconds. Despite the power of these technologies, they raise significant concerns regarding misuse.\nTo address this, various synthetic image detectors have been proposed. However, many of them struggle to generalize across diverse generation parameters and emerging generative models.\nIn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ruojiruoli/Co-Spy-Bench.","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"nigerian_accented_english_dataset","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/benjaminogbonna/nigerian_accented_english_dataset","creator_name":"Benjamin Ogbonna","creator_url":"https://huggingface.co/benjaminogbonna","description":"\n\t\n\t\t\n\t\tDataset Card for Nigerian Accent English Speech Data 1.0\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Nigerian Accent Speech Data is a comprehensive dataset of about 8 hours of audio recordings featuring speakers from various regions of Nigeria, \ncapturing the rich diversity of Nigerian accents. This dataset is specifically curated to address the gap in speech and language \ndatasets for African accents, making it a valuable resource for researchers and developers working on Automatic Speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/benjaminogbonna/nigerian_accented_english_dataset.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","crowdsourced","crowdsourced","English"],"keywords_longer_than_N":true},
	{"name":"QariOCR-v0.3-markdown-mixed-dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NAMAA-Space/QariOCR-v0.3-markdown-mixed-dataset","creator_name":"Network for Advancing Modern ArabicNLP & AI","creator_url":"https://huggingface.co/NAMAA-Space","description":"\n\t\n\t\t\n\t\tQARI Markdown Mixed Dataset\n\t\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\t\n\t\n\t\n\t\tüìã Dataset Summary\n\t\n\nThe QARI v0.3 Markdown Mixed Dataset is a specialized synthetic dataset designed for training Arabic OCR models with a focus on complex document layouts and HTML structure understanding. \nThis dataset is part of the QARI-OCR project, which achieves state-of-the-art performance in Arabic text recognition.This dataset contains 37,000 synthetically generated Arabic document images (29.6k train, 3.7k validation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NAMAA-Space/QariOCR-v0.3-markdown-mixed-dataset.","first_N":5,"first_N_keywords":["text-to-image","Arabic","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"wan_melt_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_melt_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"OmniSpatial","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qizekun/OmniSpatial","creator_name":"Zekun Qi","creator_url":"https://huggingface.co/qizekun","description":"\n\t\n\t\t\n\t\tOmniSpatial\n\t\n\nThis repository contains the data presented in OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models.\n\n\t\n\t\t\n\t\tTask Schema Documentation\n\t\n\nThis document provides a structured explanation of the task schema for the visual-spatial reasoning benchmark.\n\n\n\t\n\t\t\n\t\tSchema Structure\n\t\n\nThe schema is represented in JSON format, containing the following key components:\n\n\t\n\t\t\nKey\nDescription\n\n\n\t\t\nid\nIdentifier for the question, formatted as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qizekun/OmniSpatial.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"VisuLogic-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VisuLogic/VisuLogic-Train","creator_name":"VisuLogic-Benchmark","creator_url":"https://huggingface.co/VisuLogic","description":"\n\t\n\t\t\n\t\tVisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models\n\t\n\nA Challenging Visual-centric Benchmark for Evaluating Multimodal Reasoning in MLLMs!\nThis is the Benchmark data repo of VisuLogic.\nFor more details, please refer to the project page with dataset exploration and visualization tools: https://visulogic-benchmark.github.io/VisuLogic/.\n\n\t\n\t\t\n\t\n\t\n\t\tVisuLogic Resouces\n\t\n\nüåê Homepage | üèÜ Leaderboard | üìñ Paper | ü§ó Benchmark | ü§ó Train Data \nüíª Eval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VisuLogic/VisuLogic-Train.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"ViRL39K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ViRL39K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\t1. Overview of ViRL39K\n\t\n\nViRL39K (pronounced as \"viral\") provides a curated collection of 38,870 verifiable QAs for Vision-Language RL training. \nIt is built on top of newly collected problems and existing datasets (\nLlava-OneVision, \nR1-OneVision,\nMM-Eureka,\nMM-Math,\nM3CoT,\nDeepScaleR,\nMV-Math)\nthrough cleaning, reformatting, rephrasing and verification.ViRL39K lays the foundation for SoTA Vision-Language Reasoning Model VL-Rethinker. It has the following merits:\n\nhigh-quality and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ViRL39K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"ui-vision","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ServiceNow/ui-vision","creator_name":"ServiceNow","creator_url":"https://huggingface.co/ServiceNow","description":"\n\t\n\t\t\n\t\tUI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction\n\t\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nAutonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ServiceNow/ui-vision.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ByteMorph-Bench","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boese0601/ByteMorph-Bench","creator_name":"Di Chang","creator_url":"https://huggingface.co/Boese0601","description":"\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ByteMorph-Bench\n\t\n\nThe task of editing images to reflect non-rigid motions, such as changes in camera viewpoint, object deformation, human articulation, or complex interactions, represents a significant yet underexplored frontier in computer vision. Current methodologies and datasets often concentrate on static imagery or rigid transformations, thus limiting their applicability to expressive edits involving dynamic movement. To bridge this gap, we present‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boese0601/ByteMorph-Bench.","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"SIV-Bench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fancylalala/SIV-Bench","creator_name":"KongFanqi","creator_url":"https://huggingface.co/Fancylalala","description":"This repository contains the dataset for the paper SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning.\nProject page: https://kfq20.github.io/sivbench/\nCode: https://github.com/kfq20/SIV-Bench\n","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","Video"],"keywords_longer_than_N":true},
	{"name":"GameQA-140K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Code2Logic/GameQA-140K","creator_name":"Code2Logic","creator_url":"https://huggingface.co/Code2Logic","description":"\n\t\n\t\t\n\t\t1. Overview\n\t\n\nGameQA is a large-scale, diverse, and challenging multimodal reasoning dataset designed to enhance the general reasoning capabilities of Vision Language Models (VLMs). Generated using the innovative Code2Logic framework, it leverages game code to synthesize high-quality visual-language Chain-of-Thought (CoT) data. The dataset addresses the scarcity of multimodal reasoning data, critical for advancing complex multi-step reasoning in VLMs. Each sample includes visual game‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-140K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"DetailMaster","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/DetailMaster","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tDetailMaster: Can Your Text-to-Image Model Handle Long Prompts?\n\t\n\nWe introduce DetailMaster, a benchmark designed to evaluate text-to-image generation in long-prompt scenarios, accompanied by a robust fine-grained evaluation protocol. See more details in our paper.\n\nAbstract: While recent text-to-image (T2I) models show impressive capabilities in synthesizing images from brief descriptions, their performance significantly degrades when confronted with long, detail-intensive prompts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/DetailMaster.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ImplicitQA","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ucf-crcv/ImplicitQA","creator_name":"Center for Research in Computer Vision, University of Central Florida","creator_url":"https://huggingface.co/ucf-crcv","description":"\n\t\n\t\t\n\t\tImplicitQA Dataset\n\t\n\nThe ImplicitQA dataset was introduced in the paper ImplicitQA: Going beyond frames towards Implicit Video Reasoning.\nProject page: https://implicitqa.github.io/\nImplicitQA is a novel benchmark specifically designed to test models on implicit reasoning in Video Question Answering (VideoQA). Unlike existing VideoQA benchmarks that primarily focus on questions answerable through explicit visual content (actions, objects, events directly observable within individual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ucf-crcv/ImplicitQA.","first_N":5,"first_N_keywords":["video-text-to-text","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-5M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\t‚ú® Summary\n\t\n\nWe create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality \n720P subject-text-video triples. To ensure subject-information diversity in our dataset by, we (1) segmenting subjects \nand building pairing information via‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","arxiv:2505.20292"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-5M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\t‚ú® Summary\n\t\n\nWe create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality \n720P subject-text-video triples. To ensure subject-information diversity in our dataset by, we (1) segmenting subjects \nand building pairing information via‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-5M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","arxiv:2505.20292"],"keywords_longer_than_N":true},
	{"name":"tts-indo","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agufsamudra/tts-indo","creator_name":"Gufranaka Samudra","creator_url":"https://huggingface.co/agufsamudra","description":"\n\t\n\t\t\n\t\tagufsamudra/tts-indo\n\t\n\nagufsamudra/tts-indo is a preprocessed Indonesian speech dataset designed for training Text-to-Speech (TTS) models. This dataset is derived from the original Dataset TTS Indo available on Kaggle.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nNumber of Examples: 114,036\nDataset Size: ~4GB\nAudio Sampling Rate: 16,000 Hz\nFeatures:\naudio: WAV audio recordings\ntext: Transcription of the audio\n\n\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nEach sample in the dataset contains:\n\naudio: A dictionary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agufsamudra/tts-indo.","first_N":5,"first_N_keywords":["text-to-speech","Indonesian","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"GameQA-5K","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Code2Logic/GameQA-5K","creator_name":"Code2Logic","creator_url":"https://huggingface.co/Code2Logic","description":"In this repository, we specifically provide the 5k training samples from the complete GameQA-140K dataset used in our work for GRPO training of the models.\nRefer to our paper for details. And our code for training and evaluation is at https://github.com/tongjingqi/Code2Logic.\n\n\t\n\t\t\n\t\n\t\n\t\tCode2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning\n\t\n\nThis is the first work, to the best of our knowledge, that leverages game code to synthesize multimodal reasoning data for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Code2Logic/GameQA-5K.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"PrismLayersPro100k","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/artplus/PrismLayersPro100k","creator_name":"artplus","creator_url":"https://huggingface.co/artplus","description":"\n\t\n\t\t\n\t\tPrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models\n\t\n\n\n\t\n\t\t\n\t\tPrismLayersPro100k\n\t\n\nPrismLayersPro100k is a high-quality dataset designed for training and evaluating multi-layer transparent image generation models. It contains 100,000 samples, each with multiple layers including foreground objects and background scenes. The dataset aims to facilitate research in transparent object compositing, layer decomposition, and text-aware image generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/artplus/PrismLayersPro100k.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ShotBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vchitect/ShotBench","creator_name":"Vchitect","creator_url":"https://huggingface.co/Vchitect","description":"\n\t\n\t\t\n\t\tShotBench: Expert-Level Cinematic Understanding in Vision-Language Models\n\t\n\nThis is the official test set of ShotBench, comprising 3,572 question-answer pairs. Each sample is paired with either an image or a video clip. In total, ShotBench includes 3,049 images and 464 videos, primarily sourced from films that received Oscar nominations for Best Cinematography, ensuring high visual quality and strong cinematic style.\n\nPaper: ShotBench: Expert-Level Cinematic Understanding in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vchitect/ShotBench.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"govdocs1-pdf-source","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source","creator_name":"BEEspoke Data","creator_url":"https://huggingface.co/BEE-spoke-data","description":"\n\t\n\t\t\n\t\tgovdocs1: source PDF files\n\t\n\nThis is ~220,000 open-access PDF documents (about 6.6M pages) from the dataset govdocs1. It wants to be OCR'd.\n\nUploaded as tar file pieces of ~10 GiB each due to size/file count limits with an index.csv covering details\n5,000 randomly sampled PDFs are available unarchived in sample/. Hugging Face supports previewing these in-browser, for example this one\n\n\n\t\n\t\n\t\n\t\tRecovering the data\n\t\n\nDownload the data/ directory (with huggingface-cli download or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BEE-spoke-data/govdocs1-pdf-source.","first_N":5,"first_N_keywords":["image-text-to-text","image-feature-extraction","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Visco-Attack","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/miaozq/Visco-Attack","creator_name":"miaozq","creator_url":"https://huggingface.co/miaozq","description":"\n\t\n\t\t\n\t\tVisCo Attack: Visual Contextual Jailbreak Dataset\n\t\n\nüìÑ arXiv:2507.02844 ¬∑ üíª Code ‚Äì Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\nThis dataset contains the adversarial contexts, prompts, and images from the paper: \"Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection\".\n\n\t\n\t\t\n\t\n\t\n\t\t‚ö†Ô∏è Content Warning\n\t\n\nThis dataset contains content that is offensive and/or harmful. It was created for research purposes to study the safety‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/miaozq/Visco-Attack.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"kiaraTTS","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/niki2one/kiaraTTS","creator_name":"niki","creator_url":"https://huggingface.co/niki2one","description":"\n\t\n\t\t\n\t\tKiara TTS\n\t\n\nMasih belum ada training, hanya data mentahan saja \nberisi CSV dan file audio WAV \ndata hanya 270 Audio\nSemoga ada yang bisa melatih data ini, beritau saya jika kamu\nbisa melatih data ini.terimakasih banya atas kerja samanya\n\nDeveloped by: [Niki]\nLanguage(s) (NLP): [Indonesian]\nLicense: [Apache]\n\n","first_N":5,"first_N_keywords":["text-to-speech","Indonesian","apache-2.0","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"innersun_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/semiosphere/innersun_dataset","creator_name":"semiosphere","creator_url":"https://huggingface.co/semiosphere","description":"\n\t\n\t\t\n\t\tInner Sun Dataset\n\t\n\n\n  \n\n\n\nWell, so in order to share what we got the best, I'm offering a dataset called \"Inner Sun\". It won't be all about agartha. But they are not tagged, but properly captioned via civitai.\nIn short: 33/34 images, captions (descriptions),as just tags seem to be subpar here.\nEven SDXL can deal with caption. Pony and Illustrious do not. Flux and similar work fine with such captions.\nGood thing about captions is that they point out elements discussed on the Agartha‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/semiosphere/innersun_dataset.","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"TikTok_Most_Shared_Video_Transcription_Example","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_Most_Shared_Video_Transcription_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_Most_Shared_Video_Transcription_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","translation","summarization"],"keywords_longer_than_N":true},
	{"name":"TikTok_Most_Shared_Video_Transcription_Example","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_Most_Shared_Video_Transcription_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_Most_Shared_Video_Transcription_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","translation","summarization"],"keywords_longer_than_N":true},
	{"name":"ShahNegar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sadrasabouri/ShahNegar","creator_name":"Sadra Sabouri","creator_url":"https://huggingface.co/sadrasabouri","description":"\n\t\n\t\t\n\t\tShahNegar (A Plotted version of The Shahnameh)\n\t\n\nThis dataset is a plotted version of Ferdowsi's Shahnameh (which is a highly-regarded ancient set of Farsi poems) generated using DALL-E mini (aka craiyon). You can use this dataset using the code below: \nfrom datasets import load_dataset\n\ndataset = load_dataset(\"sadrasabouri/ShahNegar\")\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains more than 30K images with their corresponding text from the Shahnameh. For each Shahnameh‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sadrasabouri/ShahNegar.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-captioning","machine-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"coyo-700m","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kakaobrain/coyo-700m","creator_name":"Kakao Brain","creator_url":"https://huggingface.co/kakaobrain","description":"\n\t\n\t\t\n\t\tDataset Card for COYO-700M\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCOYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models. Our dataset follows a similar strategy to previous vision-and-language datasets, collecting many informative pairs of alt-text and its associated image in HTML documents. We expect COYO to be used to train popular large-scale foundation models \ncomplementary to other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kakaobrain/coyo-700m.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"skateboarding-tricks","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vogloblinsky/skateboarding-tricks","creator_name":"Ogloblinsky","creator_url":"https://huggingface.co/vogloblinsky","description":"\n\t\n\t\t\n\t\tDataset Card for Skateboarding tricks\n\t\n\nDataset used to train Text to skateboarding image model.\nFor each row the dataset contains image and text keys.\nimage is a varying size PIL jpeg, and text is the accompanying text caption.\n","first_N":5,"first_N_keywords":["text-to-image","machine-generated","other","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-turkish-subset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset","creator_name":"Cemil Guney","creator_url":"https://huggingface.co/mcemilg","description":"\n\t\n\t\t\n\t\tDataset Card for laion2B-multi-turkish-subset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nLAION-5B is a large scale openly accessible image-text dataset contains text from multiple languages. This is a Turkish subset data of laion/laion2B-multi. It's compatible to be used with image2dataset to fetch the images at scale.\n\n\t\n\t\t\n\t\n\t\n\t\tData Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['SAMPLE_ID', 'URL', 'TEXT', 'HEIGHT', 'WIDTH', 'LICENSE', 'LANGUAGE', 'NSFW', 'similarity']‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mcemilg/laion2B-multi-turkish-subset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","crowdsourced","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"IMaSC","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thennal/IMaSC","creator_name":"Thennal","creator_url":"https://huggingface.co/thennal","description":"\n\t\n\t\t\n\t\tIMaSC: ICFOSS Malayalam Speech Corpus\n\t\n\nIMaSC is a Malayalam text and speech corpus made available by ICFOSS for the purpose of developing speech technology for Malayalam, particularly text-to-speech. The corpus contains 34,473 text-audio pairs of Malayalam sentences spoken by 8 speakers, totalling in approximately 50 hours of audio.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of 34,473 instances with fields text, speaker, and audio. The audio is mono, sampled at 16kH. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thennal/IMaSC.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"spectrogram-captions","keyword":"text-to-image","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vucinatim/spectrogram-captions","creator_name":"Tim Vuƒçina","creator_url":"https://huggingface.co/vucinatim","description":"Dataset of captioned spectrograms (text describing the sound).\n","first_N":5,"first_N_keywords":["text-to-image","machine-generated","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"bible_tts_hausa","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vpetukhov/bible_tts_hausa","creator_name":"Viktor Petukhov","creator_url":"https://huggingface.co/vpetukhov","description":"\n\t\n\t\t\n\t\tDataset Card for BibleTTS Hausa\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nBibleTTS is a large high-quality open Text-to-Speech dataset with up to 80 hours of single speaker, studio quality 48kHz recordings.\nThis is a Hausa part of the dataset. Aligned hours: 86.6, aligned verses: 40,603.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nHausa\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\naudio: audio path\nsentence: transcription of the audio\nlocale: always set to ha\nbook: 3-char book encoding\nverse: verse id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vpetukhov/bible_tts_hausa.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Mroue","keyword":"text-to-image","license":"Artistic License 2.0","license_url":"https://choosealicense.com/licenses/artistic-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gr3en/Mroue","creator_name":"Walter Maiorino","creator_url":"https://huggingface.co/Gr3en","description":"Gr3en/Mroue dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","found","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"fashion-captions-de","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jinaai/fashion-captions-de","creator_name":"Jina AI","creator_url":"https://huggingface.co/jinaai","description":"\n\n\n\n\n\n\nThe data offered by Jina AI, Finetuner team.\n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis dataset is a German-language dataset based on the Fashion12K dataset, which originally contains both English and German text descriptions for each item.\nThis dataset was used to to finetuner CLIP using the Finetuner tool.\n\n\t\n\t\t\n\t\tFine-tuning\n\t\n\nPlease refer to our documentation: Multilingual Text-to-Image Search with MultilingualCLIP\nand blog Improving Search Quality for Non-English Queries with Fine-tuned‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinaai/fashion-captions-de.","first_N":5,"first_N_keywords":["text-to-image","monolingual","original","German","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"microsoft-fluentui-emoji-512-whitebg","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Norod78/microsoft-fluentui-emoji-512-whitebg","creator_name":"Doron Adler","creator_url":"https://huggingface.co/Norod78","description":"\n\t\n\t\t\n\t\tDataset Card for \"microsoft-fluentui-emoji-512-whitebg\"\n\t\n\nsvg and their file names were converted to images and text from Microsoft's fluentui-emoji repo\n","first_N":5,"first_N_keywords":["unconditional-image-generation","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"microsoft-fluentui-emoji-768","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Norod78/microsoft-fluentui-emoji-768","creator_name":"Doron Adler","creator_url":"https://huggingface.co/Norod78","description":"\n\t\n\t\t\n\t\tDataset Card for \"microsoft-fluentui-emoji-768\"\n\t\n\nsvg and their file names were converted to images and text from Microsoft's fluentui-emoji repo\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Umamusume-voice-text-pairs","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Plachta/Umamusume-voice-text-pairs","creator_name":"ElderFrog","creator_url":"https://huggingface.co/Plachta","description":"Plachta/Umamusume-voice-text-pairs dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Japanese","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"textures-color-1k","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dream-textures/textures-color-1k","creator_name":"Dream Textures","creator_url":"https://huggingface.co/dream-textures","description":"\n\t\n\t\t\n\t\ttextures-color-1k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe textures-color-1k dataset is an image dataset of 1000+ color image textures in 512x512 resolution with associated text descriptions.\nThe dataset was created for training/fine-tuning diffusion models on texture generation tasks.\nIt contains a combination of CC0 procedural and photoscanned PBR materials from ambientCG.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text descriptions are in English, and created by joining the tags of each material with a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dream-textures/textures-color-1k.","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"textures-normal-1k","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dream-textures/textures-normal-1k","creator_name":"Dream Textures","creator_url":"https://huggingface.co/dream-textures","description":"\n\t\n\t\t\n\t\ttextures-normal-1k\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe textures-normal-1k dataset is an image dataset of 1000+ normal map textures in 512x512 resolution with associated text descriptions.\nThe dataset was created for training/fine-tuning models for text to image tasks.\nIt contains a combination of CC0 procedural and photoscanned PBR materials from ambientCG.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text descriptions are in English, and created by joining the tags of each material with a space character.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dream-textures/textures-normal-1k.","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"cifar_stable_diffusion","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MatthewWaller/cifar_stable_diffusion","creator_name":"Matthew Waller","creator_url":"https://huggingface.co/MatthewWaller","description":"MatthewWaller/cifar_stable_diffusion dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"regularization-architecture","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-architecture","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tArchitecture Regularization Images\n\t\n\nA collection of regularization & class instance datasets of architecture for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-castle","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-castle","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tCastle Regularization Images\n\t\n\nA collection of regularization & class instance datasets of castles for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-horse","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-horse","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tHorse Regularization Images\n\t\n\nA collection of regularization & class instance datasets of horses for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-creature","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-creature","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tCreature Regularization Images\n\t\n\nA collection of regularization & class instance datasets of creatures for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-forest","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-forest","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tForest Regularization Images\n\t\n\nA collection of regularization & class instance datasets of forests for the Stable Diffusion 1.5 model to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-space","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-space","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tSpace Regularization Images\n\t\n\nA collection of regularization & class instance datasets of space for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-tiger","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-tiger","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tTiger Regularization Images\n\t\n\nA collection of regularization & class instance datasets of tigers for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"regularization-landscape","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-landscape","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tLandscape Regularization Images\n\t\n\nA collection of regularization & class instance datasets of landscapes for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-man","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-man","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tMan Regularization Images\n\t\n\nA collection of regularization & class instance datasets of men for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"regularization-woman","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3ee/regularization-woman","creator_name":"3ee Games","creator_url":"https://huggingface.co/3ee","description":"\n\t\n\t\t\n\t\tWoman Regularization Images\n\t\n\nA collection of regularization & class instance datasets of women for the Stable Diffusion 1.5 to use for DreamBooth prior preservation loss training.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"hungarian-single-speaker-tts","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KTH/hungarian-single-speaker-tts","creator_name":"KTH","creator_url":"https://huggingface.co/KTH","description":"\n\t\n\t\t\n\t\tDataset Card for CSS10 Hungarian: Single Speaker Speech Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe corpus consists of a single speaker, with 4515 segments extracted\nfrom a single LibriVox audiobook.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[Needs More Information]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe audio is in Hungarian.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n[Needs More Information]\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[Needs More Information]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[Needs More Information]\n\n\t\n\t\t\n\t\tData Splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KTH/hungarian-single-speaker-tts.","first_N":5,"first_N_keywords":["text-to-speech","other","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"MusicCaps","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google/MusicCaps","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\n\t\n\t\t\n\t\tDataset Card for MusicCaps\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MusicCaps dataset contains 5,521 music examples, each of which is labeled with an English aspect list and a free text caption written by musicians. An aspect list is for example \"pop, tinny wide hi hats, mellow piano melody, high pitched female vocal melody, sustained pulsating synth lead\", while the caption consists of multiple sentences about the music, e.g., \n\"A low sounding male voice is rapping over a fast paced drums‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/MusicCaps.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-sa-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Bored_Ape_NFT_text","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vessl/Bored_Ape_NFT_text","creator_name":"VESSL AI","creator_url":"https://huggingface.co/vessl","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nAll rights belong to their owners. Models and datasets can be removed from the site at the request of the copyright holder.\n\n\t\n\t\t\n\t\tHow to use\n\t\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"VESSL/Bored_Ape_NFT_text\")\n\n\n\t\n\t\t\n\t\tData Field\n\t\n\nimage = binary image file and path \ntext = auto generated prompt for image\n\n\t\n\t\t\n\t\tCitation & Information\n\t\n\n@InProceedings{VESSL,    author={Jinpil Choi}    year=2023}\n\n\t\n\t\t\n\t\tProjects‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vessl/Bored_Ape_NFT_text.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"cmu-arctic-xvectors","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors","creator_name":"Matthijs Hollemans","creator_url":"https://huggingface.co/Matthijs","description":"\n\t\n\t\t\n\t\tSpeaker embeddings extracted from CMU ARCTIC\n\t\n\nThere is one .npy file for each utterance in the dataset, 7931 files in total. The speaker embeddings are 512-element X-vectors.\nThe CMU ARCTIC dataset divides the utterances among the following speakers:\n\nbdl (US male)\nslt (US female)\njmk (Canadian male)\nawb (Scottish male)\nrms (US male)\nclb (US female)\nksp (Indian male)\n\nThe X-vectors were extracted using this script, which uses the speechbrain/spkrec-xvect-voxceleb model.\nUsage:\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Matthijs/cmu-arctic-xvectors.","first_N":5,"first_N_keywords":["text-to-speech","audio-to-audio","mit","1K - 10K","Text"],"keywords_longer_than_N":true},
	{"name":"esa-hubble","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Supermaxman/esa-hubble","creator_name":"Maxwell Weinzierl","creator_url":"https://huggingface.co/Supermaxman","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for ESA Hubble Deep Space Images & Captions\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThe ESA Hubble Deep Space Images & Captions dataset is composed primarily of Hubble deep space scans as high-resolution images,\nalong with textual descriptions written by ESA/Hubble. Metadata is also included, which enables more detailed filtering and understanding of massive space scans.\nThe purpose of this dataset is to enable text-to-image generation methods for generating high-quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Supermaxman/esa-hubble.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"boudoir-dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/soymia/boudoir-dataset","creator_name":"Milton Arango","creator_url":"https://huggingface.co/soymia","description":"\n\t\n\t\t\n\t\tDataset Card for \"boudoir-dataset\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nImages scrapped from selected Galleries on Behance.\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"galactic-animation","keyword":"text-to-image","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexZheng/galactic-animation","creator_name":"Alex Zheng","creator_url":"https://huggingface.co/AlexZheng","description":"Dataset to make the galactic-diffusion\nnum: 133\nsource: Entergalactic on Netflix\nincluding: male, female, male and female, indoor scene, outdoor scene","first_N":5,"first_N_keywords":["text-to-image","afl-3.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"common_voice_11_clean_tokenized","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anforsm/common_voice_11_clean_tokenized","creator_name":"Anton Forsman","creator_url":"https://huggingface.co/anforsm","description":"A cleaned and tokenized version of the English data from Mozilla Common Voice 11 dataset.\nCleaning steps:\n\nFiltered on samples with >2 upvotes and <1 downvotes]\nRemoved non voice audio at start and end through pytorch VAD\n\nTokenization:\n\nAudio tokenized through EnCodec by Meta\nUsing 24khz pre-trained model, and target bandwidth of 1.5\nRepresented in text as audio_token_0 - audio_token_1023\n\n\nPrompts constructed as \"text: <common voice transcript>\\naudio: <audio tokens>\"\nPrompts tokenized with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anforsm/common_voice_11_clean_tokenized.","first_N":5,"first_N_keywords":["text-to-speech","text-generation","English","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"minercraft","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/keras-dreambooth/minercraft","creator_name":"Keras Dreambooth Event","creator_url":"https://huggingface.co/keras-dreambooth","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThis dataset was used to fine-tune this model\n\n\t\n\t\t\n\t\tDemo\n\t\n\nYou can try with this demo\n\n\t\n\t\t\n\t\tIntended uses & limitations\n\t\n\nA lot of image is belonging to landscape in Minecraft world\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"rabbit-toy","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/keras-dreambooth/rabbit-toy","creator_name":"Keras Dreambooth Event","creator_url":"https://huggingface.co/keras-dreambooth","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThis dataset was used to fine-tune this model\n\n\t\n\t\t\n\t\tDemo\n\t\n\nYou can try with this demo\n\n\t\n\t\t\n\t\tIntended uses & limitations\n\t\n\nImage of mother rabbit toy\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"hokusai-style","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/keras-dreambooth/hokusai-style","creator_name":"Keras Dreambooth Event","creator_url":"https://huggingface.co/keras-dreambooth","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThis dataset was used to fine-tune this model\n\n\t\n\t\t\n\t\tDemo\n\t\n\nYou can try with this demo\n\n\t\n\t\t\n\t\tIntended uses & limitations\n\t\n\nImage of Hokusai artist\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"akita-inu","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/keras-dreambooth/akita-inu","creator_name":"Keras Dreambooth Event","creator_url":"https://huggingface.co/keras-dreambooth","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThis dataset was used to fine-tune this model\n\n\t\n\t\t\n\t\tDemo\n\t\n\nYou can try with this demo\n\n\t\n\t\t\n\t\tIntended uses & limitations\n\t\n\nImage of Akita dog - A famous and cute dog of Japan\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","imagefolder","Image","Datasets"],"keywords_longer_than_N":true},
	{"name":"indian-foods-dataset","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bharat-raghunathan/indian-foods-dataset","creator_name":"Bharat Raghunathan","creator_url":"https://huggingface.co/bharat-raghunathan","description":"\n\t\n\t\t\n\t\tDataset Card for Indian Foods Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a multi-category(multi-class classification) related Indian food dataset showcasing The-massive-Indian-Food-Dataset. \nThis card has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nEnglish\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n{\n  \"image\": \"Image(decode=True, id=None)\",\n  \"target\": \"ClassLabel(names=['biryani', 'cholebhature'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bharat-raghunathan/indian-foods-dataset.","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc0-1.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"peanuts-flan-t5-xl","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/afmck/peanuts-flan-t5-xl","creator_name":"Alex McKinney","creator_url":"https://huggingface.co/afmck","description":"\n\t\n\t\t\n\t\tPeanut Comic Strip Dataset (Snoopy & Co.)\n\t\n\n\nThis is a dataset Peanuts comic strips from 1950/10/02 to 2000/02/13.\nThere are 77,456 panels extracted from 17,816 comic strips. \nThe dataset size is approximately 4.4G.\nEach row in the dataset contains the following fields:\n\nimage: PIL.Image containing the extracted panel.\npanel_name: unique identifier for the row.\ncharacters: tuple[str, ...] of characters included in the comic strip the panel is part of.\nthemes: tuple[str, ...] of theme‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/afmck/peanuts-flan-t5-xl.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"milly-images","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/spongus/milly-images","creator_name":"spongussss","creator_url":"https://huggingface.co/spongus","description":"A collection of images from a very silly cat, these are all from @fatfatmillycat in twitter. Intended to be used with stable-diffusion-v1-4\n","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-segmentation","English","unlicense"],"keywords_longer_than_N":true},
	{"name":"GameplayCaptions","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/asgaardlab/GameplayCaptions","creator_name":"Analytics of Software, Games and Repository Data (ASGAARD) Lab","creator_url":"https://huggingface.co/asgaardlab","description":"\n\t\n\t\t\n\t\tDataset Card for \"Gameplay Captions\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"riffusion-musiccaps-dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hyeon2/riffusion-musiccaps-dataset","creator_name":"Cho Hyeon Min","creator_url":"https://huggingface.co/Hyeon2","description":"riffusion manipulated google/MusicCaps\n","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"km-speech-corpus","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/seanghay/km-speech-corpus","creator_name":"seanghay","creator_url":"https://huggingface.co/seanghay","description":"\n\t\n\t\t\n\t\tDataset Card for \"km-speech-corpus\"\n\t\n\nsampling_rate: 16000\nmean_seconds: 2.5068187111021882\nmax_seconds: 19.392\nmin_seconds: 0.448\ntotal_seconds: 37459.392\ntotal_hrs: 10.405386666666667\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Khmer","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"riffusion_musiccaps_datasets_768","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hyeon2/riffusion_musiccaps_datasets_768","creator_name":"Cho Hyeon Min","creator_url":"https://huggingface.co/Hyeon2","description":"\n\t\n\t\t\n\t\tDataset Card for \"riffusion-musiccaps-datasets-768\"\n\t\n\nConverted google/musicCaps to spectograms with audio_to_spectrum with riffusion cli.\nRandom 7.68 sec for each music in musicCaps.\nMore Information needed\n","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-copy","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\n\t\n\t\t\n\t\tFork of b-mc2/sql-create-context\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"GMaSC","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thennal/GMaSC","creator_name":"Thennal","creator_url":"https://huggingface.co/thennal","description":"\n\t\n\t\t\n\t\tGMaSC: GEC Barton Hill Malayalam Speech Corpus\n\t\n\nGMaSC is a Malayalam text and speech corpus created by the Government Engineering College Barton Hill with an emphasis on Malayalam-accented English. The corpus contains 2,000 text-audio pairs of Malayalam sentences spoken by 2 speakers, totalling in approximately 139 minutes of audio. Each sentences has at least one English word common in Malayalam speech.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset consists of 2,000 instances with fields‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thennal/GMaSC.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"medium","keyword":"text-to-video","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TempoFunk/medium","creator_name":"TempoFunk","creator_url":"https://huggingface.co/TempoFunk","description":"curr. size: 53,081 videos\ngoal (todo): 100,000+\n","first_N":5,"first_N_keywords":["text-to-video","English","agpl-3.0","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"diffusiondb-pixelart","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jainr3/diffusiondb-pixelart","creator_name":"Rahul Jain","creator_url":"https://huggingface.co/jainr3","description":"DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2\nmillion images generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these models.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"MMC4-130k-chinese-image","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","description":"MMC4-130k-chineseÊòØÂØπMMC4‰∏≠ÔºåÊäΩÊ†∑‰∫Ü130kÂ∑¶Âè≥ simliartyËæÉÈ´òÁöÑÂõæÊñápairÂæóÂà∞ÁöÑÊï∞ÊçÆÈõÜ\nChineseÁâàÊú¨ÊòØÂØπËøôÈáåÊâÄÊúâÁöÑcaptionËøõË°å‰∫ÜÁøªËØë„ÄÇ\nÊàë‰ª¨‰ºöÈôÜÁª≠Â∞ÜÊõ¥Â§öÊï∞ÊçÆÈõÜÂèëÂ∏ÉÂà∞hfÔºåÂåÖÊã¨\n\n Coco CaptionÁöÑ‰∏≠ÊñáÁøªËØë\n CoQAÁöÑ‰∏≠ÊñáÁøªËØë\n CNewSumÁöÑEmbeddingÊï∞ÊçÆ\n Â¢ûÂπøÁöÑÂºÄÊîæQAÊï∞ÊçÆ\n WizardLMÁöÑ‰∏≠ÊñáÁøªËØë\n\nÂ¶ÇÊûú‰Ω†‰πüÂú®ÂÅöËøô‰∫õÊï∞ÊçÆÈõÜÁöÑÁ≠πÂ§áÔºåÊ¨¢ËøéÊù•ËÅîÁ≥ªÊàë‰ª¨ÔºåÈÅøÂÖçÈáçÂ§çËä±Èí±„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\tÈ™ÜÈ©º(Luotuo): ÂºÄÊ∫ê‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°Âûã\n\t\n\nhttps://github.com/LC1332/Luotuo-Chinese-LLM\nÈ™ÜÈ©º(Luotuo)È°πÁõÆÊòØÁî±ÂÜ∑Â≠êÊòÇ @ ÂïÜÊ±§ÁßëÊäÄ, ÈôàÂêØÊ∫ê @ Âçé‰∏≠Â∏àËåÉÂ§ßÂ≠¶ ‰ª•Âèä ÊùéÈ≤ÅÈ≤Å @ ÂïÜÊ±§ÁßëÊäÄ ÂèëËµ∑ÁöÑ‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂºÄÊ∫êÈ°πÁõÆÔºåÂåÖÂê´‰∫Ü‰∏ÄÁ≥ªÂàóËØ≠Ë®ÄÊ®°Âûã„ÄÇ\n( Ê≥®ÊÑè: ÈôàÂêØÊ∫ê Ê≠£Âú®ÂØªÊâæ2024Êé®ÂÖçÂØºÂ∏àÔºåÊ¨¢ËøéËÅîÁ≥ª )\nÈ™ÜÈ©ºÈ°πÁõÆ‰∏çÊòØÂïÜÊ±§ÁßëÊäÄÁöÑÂÆòÊñπ‰∫ßÂìÅ„ÄÇ\n\t\n\t\t\n\t\tCitation\n\t\n\nPlease cite the repo if you use the data or code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/silk-road/MMC4-130k-chinese-image.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Chinese","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"genshin_ch_10npc","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmj2002/genshin_ch_10npc","creator_name":"xmj","creator_url":"https://huggingface.co/xmj2002","description":"\n\t\n\t\t\n\t\tDataset Card for \"genshin_ch_10npc\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["text-to-speech","Chinese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304-clean","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nÈùûÂÆòÊñπÁöÑÔºåÁà¨ÂèñËá™midjourney v5ÁöÑ2023Âπ¥4ÊúàÁöÑÊï∞ÊçÆÔºå‰∏ÄÂÖ±1701420Êù°„ÄÇ\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜ‰ø°ÊÅØ Dataset Information\n\t\n\nÂéüÂßãÈ°πÁõÆÂú∞ÂùÄÔºöhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\nÊàëÂÅö‰∫Ü‰∏Ä‰∫õÊ∏ÖÊ¥óÔºåÊ∏ÖÁêÜÂá∫‰∫Ü‰∏§‰∏™Êñá‰ª∂Ôºö\n\nori_prompts_df.parquet Ôºà1,255,812ÂØπÔºåmidjourneyÁöÑÂõõÊ†ºÂõæÔºâ\n\nupscaled_prompts_df.parquet Ôºà445,608ÂØπÔºå‰ΩøÁî®‰∫ÜÈ´òÊ∏ÖÊåá‰ª§ÁöÑÂõæÔºåËøôÊÑèÂë≥ÁùÄËøô‰∏™ÂõæÊõ¥ÂèóÊ¨¢Ëøé„ÄÇÔºâ\n\n\nOriginal project address:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-v5-202304-clean.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"midjourney-kaggle-clean","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/midjourney-kaggle-clean","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nÈùûÂÆòÊñπÁöÑÔºåÂØπKaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] ‰∏äÁöÑÊï∞ÊçÆÈõÜËøõË°å‰∫ÜÊ∏ÖÁêÜÔºå‰∏ÄÂÖ±Êúâ 248,167ÂØπ„ÄÇ\nUnofficially, a cleanup of the dataset on Kaggle (Midjourney User Prompts & Generated Images (250k))[https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage?select=general-01_2022_06_20.json] yielded 248,167 pairs.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/midjourney-kaggle-clean.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"sample_controlnet_dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SaffalPoosh/sample_controlnet_dataset","creator_name":"Talha Yousuf","creator_url":"https://huggingface.co/SaffalPoosh","description":"\n\t\n\t\t\n\t\tControlNet training\n\t\n\nthis dataset is subset of fill_50k dataset just to test the finetuning logic.\n\nTODO:\n\n\n add text data\n\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ParsiGoo","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kamtera/ParsiGoo","creator_name":"Flincer","creator_url":"https://huggingface.co/Kamtera","description":"A Persian multispeaker dataset for text-to-speech purposes.","first_N":5,"first_N_keywords":["text-to-speech","other","monolingual","original","Persian"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/felixsaaro/test","creator_name":"Felix S","creator_url":"https://huggingface.co/felixsaaro","description":"\n\t\n\t\t\n\t\tDataset Card for \"test\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"FETV","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lyx97/FETV","creator_name":"Yuanxin Liu","creator_url":"https://huggingface.co/lyx97","description":"\n\t\n\t\t\n\t\tFETV\n\t\n\nFETV is a benchmark for Fine-grained Evaluation of open-domain Text-to-Video generation\n\n\t\n\t\t\n\t\tOverview\n\t\n\nFETV consist of a diverse set of text prompts, categorized based on three orthogonal aspects: major content, attribute control, and prompt complexity.\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nAll FETV data are all available in the file fetv_data.json. Each line is a data instance, which is formatted as:\n{\n  \"video_id\": \"1006807024\", \n  \"prompt\": \"A mountain‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lyx97/FETV.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"anime-synthetics","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mirav/anime-synthetics","creator_name":"Mira","creator_url":"https://huggingface.co/mirav","description":"Mostly unfiltered anime-style images generated by various text to image models, collected from various sources (some were submitted for inclusion by their creators).\nIncludes a subset of p1atdev/niji-v5, albeit captioned differently than the source. \nContains 2224 image & caption pairs.\nAs it is unfiltered, some adult content may be included.\nCaptions may not be completely accurate.\nIf you wish to submit content, do it as a pull request.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"whisperspeech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/collabora/whisperspeech","creator_name":"Collabora","creator_url":"https://huggingface.co/collabora","description":"\n\t\n\t\t\n\t\tThe WhisperSpeech Dataset\n\t\n\nThis dataset contains data to train SPEAR TTS-like text-to-speech models that utilized semantic tokens derived from the OpenAI Whisper\nspeech recognition model.\nWe currently provide semantic and acoustic tokens for the LibriLight and LibriTTS datasets (English only).\nAcoustic tokens:\n\n24kHz EnCodec 6kbps (8 quantizers)\n\nSemantic tokens:\n\nWhisper tiny VQ bottleneck trained on a subset of LibriLight\n\nAvailable LibriLight subsets:\n\nsmall/medium/large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/collabora/whisperspeech.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","1K - 10K","text"],"keywords_longer_than_N":true},
	{"name":"spider-schema","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-schema","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Schema\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset contains the 166 databases used in the Spider dataset.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-schema.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"spider-context-instruct","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"DreamEditBench","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tianleliphoebe/DreamEditBench","creator_name":"Tianle LI","creator_url":"https://huggingface.co/tianleliphoebe","description":"\n\t\n\t\t\n\t\tDreamEditBench for Subject Replacement task and Subject Addition task.\n\t\n\nThe goal of subject replacement is to replace a subject from a source image with a customized subject. In contrast, the aim of the subject addition task is to add a customized\nsubject to a desired position in the source image. To standardize the evaluation of the two proposed tasks, we curate a new benchmark, i.e. DreamEditBench, consisting of 22 subjects in alignment with DreamBooth with 20 images for each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tianleliphoebe/DreamEditBench.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"spider-context-validation","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-context-validation","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Context Validation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to validate spider-fine-tuned LLMs with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-validation.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"cleanvid-15m_map","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shinonomelab/cleanvid-15m_map","creator_name":"Shinonome AI Lab","creator_url":"https://huggingface.co/shinonomelab","description":"\n\t\n\t\t\n\t\tCleanVid Map (15M) üé•\n\t\n\n\n\t\n\t\t\n\t\tTempoFunk Video Generation Project\n\t\n\nCleanVid-15M is a large-scale dataset of videos with multiple metadata entries such as:\n\nTextual Descriptions üìÉ\nRecording Equipment üìπ\nCategories üî†\nFramerate üéûÔ∏è\nAspect Ratio üì∫\n\nCleanVid aim is to improve the quality of WebVid-10M dataset by adding more data and cleaning the dataset by dewatermarking the videos in it.\nThis dataset includes only the map with the urls and metadata, with 3,694,510 more entries than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shinonomelab/cleanvid-15m_map.","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"fictional_characters_raw_data_without_images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gryffindor-ISWS/fictional_characters_raw_data_without_images","creator_name":"gryffindor","creator_url":"https://huggingface.co/gryffindor-ISWS","description":"gryffindor-ISWS/fictional_characters_raw_data_without_images dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","gpl-3.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"MusicCaps-ru","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/MusicCaps-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\tMusicCaps-ru\n\t\n\nTranslated version of google/MusicCaps into Russian.\n","first_N":5,"first_N_keywords":["text-to-speech","translated","Russian","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"audiocaps","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/audiocaps","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\taudiocaps\n\t\n\nHuggingFace mirror of official data repo.\n","first_N":5,"first_N_keywords":["text-to-speech","monolingual","original","English","mit"],"keywords_longer_than_N":true},
	{"name":"audiocaps-ru","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/audiocaps-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\n\t\n\t\t\n\t\taudiocaps-ru\n\t\n\nTranslated version of d0rj/audiocaps into Russian.\n","first_N":5,"first_N_keywords":["text-to-speech","translated","monolingual","d0rj/audiocaps","Russian"],"keywords_longer_than_N":true},
	{"name":"laion_mi","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/antoniaaa/laion_mi","creator_name":"Antoni Kowalczuk","creator_url":"https://huggingface.co/antoniaaa","description":"\n\t\n\t\t\n\t\tDataset Card for \"LAION-mi\"\n\t\n\nThis is a dataset from Towards More Realistic Membership Inference Attacks on Large Diffusion Models, link.\nWe provide a robust evaluation setup for membership inference attacks on a state-of-the-art Stable Diffusion model and publish the corresponding dataset along with the proposed setup.\n\n\t\n\t\t\n\t\tCiting\n\t\n\nIf you find our work useful in your research, please cite as \n@misc{dubi≈Ñski2023realistic,\n      title={Towards More Realistic Membership Inference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/antoniaaa/laion_mi.","first_N":5,"first_N_keywords":["text-to-image","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"spider-natsql-skeleton-context-instruct","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-natsql-skeleton-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider NatSQL Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs on the Spider dataset with database context using NatSQL.\n\n\t\n\t\t\n\t\tNatSQL\n\t\n\nNatSQL is an intermediate representation for SQL that simplifies the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-natsql-skeleton-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"tartakovsky-style","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/szymonrucinski/tartakovsky-style","creator_name":"Szymon Ruci≈Ñski","creator_url":"https://huggingface.co/szymonrucinski","description":"\n\t\n\t\t\n\t\tDataset Card for \"tartakovsky-style\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"generated-data-fictional-characters-without-images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gryffindor-ISWS/generated-data-fictional-characters-without-images","creator_name":"gryffindor","creator_url":"https://huggingface.co/gryffindor-ISWS","description":"gryffindor-ISWS/generated-data-fictional-characters-without-images dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","gpl-3.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"generated-data-fictional-characters-with-images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gryffindor-ISWS/generated-data-fictional-characters-with-images","creator_name":"gryffindor","creator_url":"https://huggingface.co/gryffindor-ISWS","description":"gryffindor-ISWS/generated-data-fictional-characters-with-images dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","gpl-3.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"spider-natsql-context-validation","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-natsql-context-validation","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider NatSQL Context Validation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to validate LLMs on the Spider dev dataset with database context using NatSQL.\n\n\t\n\t\t\n\t\tNatSQL\n\t\n\nNatSQL is an intermediate representation for SQL that simplifies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-natsql-context-validation.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"edited_common_voice","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lunarlist/edited_common_voice","creator_name":"taetiya taechamatavorn","creator_url":"https://huggingface.co/lunarlist","description":"\n\t\n\t\t\n\t\tDataset Card for \"edited_common_voice\"\n\t\n\nMore Information needed\nThis dataset is a Thai TTS dataset that use the voice from Common Voice dataset and modify the voice to not to sound like the original.\nMedium: Text-To-Speech ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Tacotron2\n","first_N":5,"first_N_keywords":["text-to-speech","Thai","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"spider-natsql-context-instruct","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-natsql-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider NatSQL Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs on the Spider dataset with database context using NatSQL.\n\n\t\n\t\t\n\t\tNatSQL\n\t\n\nNatSQL is an intermediate representation for SQL that simplifies the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-natsql-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"stable-diffusion-2-1-with-images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gryffindor-ISWS/stable-diffusion-2-1-with-images","creator_name":"gryffindor","creator_url":"https://huggingface.co/gryffindor-ISWS","description":"gryffindor-ISWS/stable-diffusion-2-1-with-images dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","gpl-3.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"stable-diffusion-2-1-without-images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gryffindor-ISWS/stable-diffusion-2-1-without-images","creator_name":"gryffindor","creator_url":"https://huggingface.co/gryffindor-ISWS","description":"gryffindor-ISWS/stable-diffusion-2-1-without-images dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","gpl-3.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"spider-skeleton-context-instruct","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Skeleton Context Instruct\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\n\n\t\n\t\t\n\t\tYale Lily Spider Leaderboards\n\t\n\nThe leaderboard can be seen at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"artelingo-dummy","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/youssef101/artelingo-dummy","creator_name":"mohamed","creator_url":"https://huggingface.co/youssef101","description":"ArtELingo is a benchmark and dataset introduced in a research paper aimed at promoting work on diversity across languages and cultures. It is an extension of ArtEmis, which is a collection of 80,000 artworks from WikiArt with 450,000 emotion labels and English-only captions. ArtELingo expands this dataset by adding 790,000 annotations in Arabic and Chinese. The purpose of these additional annotations is to evaluate the performance of \"cultural-transfer\" in AI systems.\nThe dataset in ArtELingo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/youssef101/artelingo-dummy.","first_N":5,"first_N_keywords":["image-to-text","text-classification","image-classification","text-to-image","text-generation"],"keywords_longer_than_N":true},
	{"name":"JapaneseGoblin","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin","creator_name":"RyokoAI Extra","creator_url":"https://huggingface.co/RyokoExtra","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for JapaneseGoblin\n\t\n\nWE ARE THE JAPANESE GOBLIN!\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nJapaneseGoblin is a dump of en.touhouwiki.net wiki.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset is primarily intended for unsupervised training of text generation models; however, it may be useful for other purposes.\n\ntext-classification\ntext-generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nPrimarily english, however there are also japanese as well.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin.","first_N":5,"first_N_keywords":["text-classification","text-generation","text-to-image","text-to-video","Japanese"],"keywords_longer_than_N":true},
	{"name":"JapaneseGoblin","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin","creator_name":"RyokoAI Extra","creator_url":"https://huggingface.co/RyokoExtra","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for JapaneseGoblin\n\t\n\nWE ARE THE JAPANESE GOBLIN!\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nJapaneseGoblin is a dump of en.touhouwiki.net wiki.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThis dataset is primarily intended for unsupervised training of text generation models; however, it may be useful for other purposes.\n\ntext-classification\ntext-generation\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nPrimarily english, however there are also japanese as well.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RyokoExtra/JapaneseGoblin.","first_N":5,"first_N_keywords":["text-classification","text-generation","text-to-image","text-to-video","Japanese"],"keywords_longer_than_N":true},
	{"name":"midjourney-v5-202304","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304","creator_name":"JohnTeddy3","creator_url":"https://huggingface.co/JohnTeddy3","description":"\n\t\n\t\t\n\t\tmidjourney-v5-202304-clean\n\t\n\n\n\t\n\t\t\n\t\tÁÆÄ‰ªã Brief Introduction\n\t\n\nËΩ¨ËΩΩËá™wanng/midjourney-v5-202304-clean\nÈùûÂÆòÊñπÁöÑÔºåÁà¨ÂèñËá™midjourney v5ÁöÑ2023Âπ¥4ÊúàÁöÑÊï∞ÊçÆÔºå‰∏ÄÂÖ±1701420Êù°„ÄÇ\nUnofficial, crawled from midjourney v5 for April 2023, 1,701,420 pairs in total.\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜ‰ø°ÊÅØ Dataset Information\n\t\n\nÂéüÂßãÈ°πÁõÆÂú∞ÂùÄÔºöhttps://huggingface.co/datasets/tarungupta83/MidJourney_v5_Prompt_dataset\nÊàëÂÅö‰∫Ü‰∏Ä‰∫õÊ∏ÖÊ¥óÔºåÊ∏ÖÁêÜÂá∫‰∫Ü‰∏§‰∏™Êñá‰ª∂Ôºö\n\nori_prompts_df.parquet Ôºà1,255,812ÂØπÔºåmidjourneyÁöÑÂõõÊ†ºÂõæÔºâ\n\nupscaled_prompts_df.parquet Ôºà445,608ÂØπÔºå‰ΩøÁî®‰∫ÜÈ´òÊ∏ÖÊåá‰ª§ÁöÑÂõæÔºåËøôÊÑèÂë≥ÁùÄËøô‰∏™ÂõæÊõ¥ÂèóÊ¨¢Ëøé„ÄÇÔºâ\n\n\nOriginal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JohnTeddy3/midjourney-v5-202304.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"LibriSpeech-Synthesizer-TTS","keyword":"text-to-speech","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/rmcpantoja/LibriSpeech-Synthesizer-TTS","creator_name":"Rene Mateo Cedillo Pantoja","creator_url":"https://huggingface.co/rmcpantoja","description":"rmcpantoja/LibriSpeech-Synthesizer-TTS dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Spanish","Spanish Sign Language","unlicense","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"LibriSpeech-Synthesizer-TTS","keyword":"text-to-speech","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/rmcpantoja/LibriSpeech-Synthesizer-TTS","creator_name":"Rene Mateo Cedillo Pantoja","creator_url":"https://huggingface.co/rmcpantoja","description":"rmcpantoja/LibriSpeech-Synthesizer-TTS dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Spanish","Spanish Sign Language","unlicense","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"sayoko-tts-corpus","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bandad/sayoko-tts-corpus","creator_name":"kai washizaki","creator_url":"https://huggingface.co/bandad","description":"\n\t\n\t\t\n\t\t„Çµ„É®Â≠ê Èü≥Â£∞„Ç≥„Éº„Éë„Çπ\n\t\n\n\n\t\n\t\t\n\t\t„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊñπÊ≥ï\n\t\n\n„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÂúßÁ∏Æ„Åó„Åüzip„Éï„Ç°„Ç§„É´„Çí„ÄÅgdrive„Å´ÁΩÆ„ÅÑ„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n„Åæ„Åü„ÄÅ‰ª•‰∏ã„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„Åß„ÄÅhuggingface hub„Åã„Çâ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÇÇÂèØËÉΩ„Åß„Åô„ÄÇ\n# pip install --upgrade huggingface_hub\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(repo_id=\"bandad/sayoko-tts-corpus\", repo_type=\"dataset\", revision=\"main\", local_dir=\"./sayoko-tts-corpus\")\n\n\n\t\n\t\t\n\t\tÊ¶ÇË¶Å\n\t\n\n81Ê≠≥„ÅÆÂ•≥ÊÄß„ÅÆÈü≥Â£∞„Ç≥„Éº„Éë„Çπ„Åß„Åô„ÄÇ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bandad/sayoko-tts-corpus.","first_N":5,"first_N_keywords":["text-to-speech","Japanese","cc-by-4.0","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"VietBibleVox","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ntt123/VietBibleVox","creator_name":"Th√¥ng Nguy·ªÖn","creator_url":"https://huggingface.co/ntt123","description":"\n\t\n\t\t\n\t\n\t\n\t\tVietBibleVox Dataset\n\t\n\nThe VietBibleVox Dataset is based on the data extracted from open.bible specifically for the Vietnamese language. As the original data is provided under the cc-by-sa-4.0 license, this derived dataset is also licensed under cc-by-sa-4.0.\nThe dataset comprises 29,185 pairs of (verse, audio clip), with each verse from the Bible read in Vietnamese by a male voice.\n\nThe verses are the original texts and may not be directly usable for training text-to-speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/VietBibleVox.","first_N":5,"first_N_keywords":["text-to-speech","Vietnamese","cc-by-sa-4.0","10K<n<100K","Audio"],"keywords_longer_than_N":true},
	{"name":"hdvila-100M","keyword":"text-to-video","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TempoFunk/hdvila-100M","creator_name":"TempoFunk","creator_url":"https://huggingface.co/TempoFunk","description":"TempoFunk/hdvila-100M dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-video","text-to-image","video-classification","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"hdvila-100M","keyword":"text-to-image","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TempoFunk/hdvila-100M","creator_name":"TempoFunk","creator_url":"https://huggingface.co/TempoFunk","description":"TempoFunk/hdvila-100M dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-video","text-to-image","video-classification","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"pusheen","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qillura/pusheen","creator_name":"Jason","creator_url":"https://huggingface.co/qillura","description":"qillura/pusheen dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"factorio-blueprint-visualizations","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/piebro/factorio-blueprint-visualizations","creator_name":"Piet","creator_url":"https://huggingface.co/piebro","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a collection of visualizations of Factorio Blueprints using this Factorio Visualization Tool: https://github.com/piebro/factorio-blueprint-visualizer. The Blueprints are collected from https://www.factorio.school/.\n\n\t\n\t\t\n\t\n\t\n\t\tExamples\n\t\n\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\"svg_original\": The svg downloaded like this from the website\n\"svg_rect\": The svg reshaped to a rect and a slightly bigger border\n\"png_1024x1024\": The svg_rect images exported as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/piebro/factorio-blueprint-visualizations.","first_N":5,"first_N_keywords":["text-to-image","cc0-1.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/metaltiger775/test","creator_name":"metaltiger775","creator_url":"https://huggingface.co/metaltiger775","description":"\n\t\n\t\t\n\t\tHello!\n\t\n\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"minispider","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ravidborse/minispider","creator_name":"Ravikiran Borse","creator_url":"https://huggingface.co/ravidborse","description":"\n\t\n\t\t\n\t\tDataset Card for Spider\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ravidborse/minispider.","first_N":5,"first_N_keywords":["expert-generated","expert-generated","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"clothes_desc","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wbensvage/clothes_desc","creator_name":"Wolfgang Bensvage","creator_url":"https://huggingface.co/wbensvage","description":"\n\t\n\t\t\n\t\tDataset Card for H&M Clothes captions\n\t\n\n_Dataset used to train/finetune [Clothes text to image model]\nCaptions are generated by using the 'detail_desc' and 'colour_group_name' or 'perceived_colour_master_name' from kaggle/competitions/h-and-m-personalized-fashion-recommendations. Original images were also obtained from the url (https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data?select=images)\n\n\t\n\t\n\t\n\t\tFor each row the dataset contains image and text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wbensvage/clothes_desc.","first_N":5,"first_N_keywords":["text-to-image","human generated by using detail_desc and color","other","monolingual","www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-instruction","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"spider-context-validation-ranked-schema","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-context-validation-ranked-schema","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\n\t\n\t\t\n\t\tDataset Card for Spider Context Validation\n\t\n\n\n\t\n\t\t\n\t\tRanked Schema by ChatGPT\n\t\n\nThe database context used here is generated from ChatGPT after telling it to reorder the schema with the most relevant columns in the beginning of the db_info.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-validation-ranked-schema.","first_N":5,"first_N_keywords":["spider","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"openslr-slr69-ca-trimmed-denoised","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/openslr-slr69-ca-trimmed-denoised","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for openslr-slr69-ca-denoised\n\t\n\nThis is a post-processed version of the Catalan subset belonging to the Open Speech and Language Resources (OpenSLR) speech dataset. \nSpecifically the subset OpenSLR-69. \nThe original HFü§ó SLR-69 dataset is located here.\nSame license is maintained: Attribution-ShareAlike 4.0 International.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe processed the data of the Catalan OpenSLR with the following recipe:\n\nTrimming: Long‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/openslr-slr69-ca-trimmed-denoised.","first_N":5,"first_N_keywords":["text-to-speech","no-annotation","crowdsourced","monolingual","openslr"],"keywords_longer_than_N":true},
	{"name":"yato_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/yato_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of yato (Arknights)\n\t\n\nThis is the dataset of yato (Arknights), containing 90 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n90\nDownload\nRaw data with meta information.\nraw-stage3\n227\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/yato_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ranger_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/ranger_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of ranger (Azur Lane)\n\t\n\nThis is the dataset of ranger (Azur Lane), containing 45 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n45\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n120\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/ranger_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"durin_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/durin_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of durin (Arknights)\n\t\n\nThis is the dataset of durin (Arknights), containing 55 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n55\nDownload\nRaw data with meta information.\nraw-stage3\n133\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/durin_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"fang_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/fang_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of fang (Arknights)\n\t\n\nThis is the dataset of fang (Arknights), containing 61 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n61\nDownload\nRaw data with meta information.\nraw-stage3\n146\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/fang_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"MuLMS-Img","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Timbrt/MuLMS-Img","creator_name":"Tim Tarsi","creator_url":"https://huggingface.co/Timbrt","description":"\n\t\n\t\t\n\t\n\t\n\t\tMulti Layer Materials Science Image Corpus\n\t\n\nThis repository contains companion material for the following publication:\n\nTim Tarsi, Heike Adel, Jan Hendrik Metzen, Dan Zhang, Matteo Finco, Annemarie Friedrich. SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain. WACV 2024.\n\nPlease cite this paper if using the dataset, and direct any questions regarding the dataset\nto Tim Tarsi\n\n\t\n\t\t\n\t\n\t\n\t\tSummary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Timbrt/MuLMS-Img.","first_N":5,"first_N_keywords":["image-classification","text-to-image","object-detection","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"cardigan_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/cardigan_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of cardigan (Arknights)\n\t\n\nThis is the dataset of cardigan (Arknights), containing 85 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n85\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n207\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/cardigan_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"kroos_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/kroos_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of kroos (Arknights)\n\t\n\nThis is the dataset of kroos (Arknights), containing 196 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n196\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n497\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/kroos_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"adnachiel_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/adnachiel_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of adnachiel (Arknights)\n\t\n\nThis is the dataset of adnachiel (Arknights), containing 19 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n19\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n45\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/adnachiel_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"hibiscus_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/hibiscus_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of hibiscus (Arknights)\n\t\n\nThis is the dataset of hibiscus (Arknights), containing 99 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n99\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n247\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/hibiscus_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"nst-da-norm","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JackismyShephard/nst-da-norm","creator_name":"Christian Troelsen","creator_url":"https://huggingface.co/JackismyShephard","description":"\n\t\n\t\t\n\t\tDataset Card for NST-da Normalized\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): da\nLicense: cc0-1.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JackismyShephard/nst-da-norm.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","machine-generated","expert-generated","expert-generated"],"keywords_longer_than_N":true},
	{"name":"cryptocurrency-coins-hi-res","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SpectralDoor/cryptocurrency-coins-hi-res","creator_name":"SpectralDoor","creator_url":"https://huggingface.co/SpectralDoor","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nA small dataset of 42 high-resolution images of cryptocurrency coins with clipped *.txt descriptions. \nIt can be used to extend datasets or for tuning models.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"meteor_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/meteor_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of meteor (Arknights)\n\t\n\nThis is the dataset of meteor (Arknights), containing 161 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n161\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n409\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/meteor_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"4catac","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/4catac","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for 4catac\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n4catac: examples of phonetic transcription in 4  Catalan accents is a dataset of phonetic transcriptions in four Catalan accents: Balearic, Central, North-Western and Valencian. \nIt consists of 160 sentences transcribed using IPA, following the recommendations of the Institut d'Estudis Catalans.\nThese sentences are the same for the four accents but may have small morphological adaptations to make them more natural for the accent.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/4catac.","first_N":5,"first_N_keywords":["text-to-speech","expert-generated","expert-generated","monolingual","Catalan"],"keywords_longer_than_N":true},
	{"name":"universal_bulin_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/universal_bulin_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of universal_bulin (Azur Lane)\n\t\n\nThis is the dataset of universal_bulin (Azur Lane), containing 14 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n14\nDownloadRaw data with meta information.\n\n\nraw-stage3\n35\nDownload\n3-stage cropped raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/universal_bulin_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"vanilla_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/vanilla_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of vanilla (Arknights)\n\t\n\nThis is the dataset of vanilla (Arknights), containing 30 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n30\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n65\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/vanilla_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"cassin_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/cassin_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of cassin (Azur Lane)\n\t\n\nThis is the dataset of cassin (Azur Lane), containing 40 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n40\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n108\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/cassin_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"airi_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/airi_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of airi (Blue Archive)\n\t\n\nThis is the dataset of airi (Blue Archive), containing 79 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n79\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n212\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/airi_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"downes_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/downes_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of downes (Azur Lane)\n\t\n\nThis is the dataset of downes (Azur Lane), containing 15 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n15\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n41\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/downes_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"maury_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/maury_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of maury (Azur Lane)\n\t\n\nThis is the dataset of maury (Azur Lane), containing 18 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n18\nDownload\nRaw data with meta information.\nraw-stage3\n45\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/maury_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"charles_ausburne_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/charles_ausburne_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of charles_ausburne (Azur Lane)\n\t\n\nThis is the dataset of charles_ausburne (Azur Lane), containing 12 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n12\nDownloadRaw data with meta information.\n\n\nraw-stage3\n30\nDownload\n3-stage cropped‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/charles_ausburne_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"foote_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/foote_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of foote (Azur Lane)\n\t\n\nThis is the dataset of foote (Azur Lane), containing 26 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n26\nDownload\nRaw data with meta information.\nraw-stage3\n69\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/foote_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"plume_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/plume_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of plume (Arknights)\n\t\n\nThis is the dataset of plume (Arknights), containing 131 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n131\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n335\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/plume_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"sims_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/sims_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of sims (Azur Lane)\n\t\n\nThis is the dataset of sims (Azur Lane), containing 68 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n68\nDownload\nRaw data with meta information.\nraw-stage3\n181\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/sims_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"melantha_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/melantha_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of melantha (Arknights)\n\t\n\nThis is the dataset of melantha (Arknights), containing 166 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n166\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n384\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/melantha_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"beagle_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/beagle_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of beagle (Arknights)\n\t\n\nThis is the dataset of beagle (Arknights), containing 35 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n35\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n77\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/beagle_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"lava_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/lava_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of lava (Arknights)\n\t\n\nThis is the dataset of lava (Arknights), containing 63 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n63\nDownload\nRaw data with meta information.\nraw-stage3\n156\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/lava_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"ansel_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/ansel_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of ansel (Arknights)\n\t\n\nThis is the dataset of ansel (Arknights), containing 140 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n140\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n319\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/ansel_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"orchid_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/orchid_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of orchid (Arknights)\n\t\n\nThis is the dataset of orchid (Arknights), containing 32 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n32\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n76\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/orchid_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"haze_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/haze_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of haze (Arknights)\n\t\n\nThis is the dataset of haze (Arknights), containing 73 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n73\nDownload\nRaw data with meta information.\nraw-stage3\n163\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/haze_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"jessica_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/jessica_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of jessica (Arknights)\n\t\n\nThis is the dataset of jessica (Arknights), containing 307 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n307\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n746\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/jessica_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"shirayuki_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/shirayuki_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of shirayuki (Arknights)\n\t\n\nThis is the dataset of shirayuki (Arknights), containing 58 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n58\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n151\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/shirayuki_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"courier_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/courier_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of courier (Arknights)\n\t\n\nThis is the dataset of courier (Arknights), containing 34 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n34\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n91\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/courier_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"scavenger_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/scavenger_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of scavenger (Arknights)\n\t\n\nThis is the dataset of scavenger (Arknights), containing 30 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n30\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n80\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/scavenger_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"akane_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/akane_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of akane (Blue Archive)\n\t\n\nThis is the dataset of akane (Blue Archive), containing 498 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n498\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n1352\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/akane_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","n<1K","Image","Text"],"keywords_longer_than_N":true},
	{"name":"akari_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/akari_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of akari (Blue Archive)\n\t\n\nThis is the dataset of akari (Blue Archive), containing 57 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n57\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n160\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/akari_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"meme-imgflip-small-test-dataset","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SassyRong/meme-imgflip-small-test-dataset","creator_name":"SassyRong","creator_url":"https://huggingface.co/SassyRong","description":"SassyRong/meme-imgflip-small-test-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"vigna_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/vigna_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of vigna (Arknights)\n\t\n\nThis is the dataset of vigna (Arknights), containing 210 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n210\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n570\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/vigna_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"bubble_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/bubble_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of bubble (Arknights)\n\t\n\nThis is the dataset of bubble (Arknights), containing 15 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n15\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n35\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/bubble_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"jackie_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/jackie_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of jackie (Arknights)\n\t\n\nThis is the dataset of jackie (Arknights), containing 26 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n26\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n70\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/jackie_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"frost_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/frost_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of frost (Arknights)\n\t\n\nThis is the dataset of frost (Arknights), containing 74 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n74\nDownload\nRaw data with meta information.\nraw-stage3\n182\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/frost_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"pudding_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/pudding_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of pudding (Arknights)\n\t\n\nThis is the dataset of pudding (Arknights), containing 20 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n20\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n52\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/pudding_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"rockrock_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/rockrock_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of rockrock (Arknights)\n\t\n\nThis is the dataset of rockrock (Arknights), containing 48 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n48\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n132\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/rockrock_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"highmore_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/highmore_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of highmore (Arknights)\n\t\n\nThis is the dataset of highmore (Arknights), containing 48 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n48\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n133\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/highmore_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"lunacub_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/lunacub_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of lunacub (Arknights)\n\t\n\nThis is the dataset of lunacub (Arknights), containing 38 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n38\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n104\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/lunacub_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"vigil_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/vigil_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of vigil (Arknights)\n\t\n\nThis is the dataset of vigil (Arknights), containing 16 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n16\nDownload\nRaw data with meta information.\nraw-stage3\n41\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/vigil_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"firewhistle_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/firewhistle_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of firewhistle (Arknights)\n\t\n\nThis is the dataset of firewhistle (Arknights), containing 15 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n15\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n41\nDownload\n3-stage cropped raw data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/firewhistle_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"paprika_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/paprika_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of paprika (Arknights)\n\t\n\nThis is the dataset of paprika (Arknights), containing 13 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n13\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n33\nDownload\n3-stage cropped raw data with meta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/paprika_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"stainless_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/stainless_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of stainless (Arknights)\n\t\n\nThis is the dataset of stainless (Arknights), containing 17 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n17\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n39\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/stainless_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"qanipalaat_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/qanipalaat_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of qanipalaat (Arknights)\n\t\n\nThis is the dataset of qanipalaat (Arknights), containing 15 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n15\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n34\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/qanipalaat_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"chongyue_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/chongyue_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of chongyue (Arknights)\n\t\n\nThis is the dataset of chongyue (Arknights), containing 17 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization). \nThis is a WebUI contains crawlers and other thing: (LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n17\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n41\nDownload\n3-stage cropped raw data with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/chongyue_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Voice-KusanagiNene","keyword":"text-to-speech","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene","creator_name":"„ÅÜ„Åï„Åé","creator_url":"https://huggingface.co/MomoyamaSawa","description":"\n  \n\n ü•ï \n Â¶ÇÊûúÂÖîÂÖîÁöÑ‰ªìÂ∫ìÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÁöÑËØùÁÇπ‰∏™‚≠êÂñµ~ \n If Tutu's repository is helpful to you, please give it a ‚≠ê meow~ \n „ÇÇ„Åó„ÅÜ„Åï„Åé„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅåÂΩπ„Å´Á´ã„Å£„ÅüÂ†¥Âêà„ÅØ„ÄÅ‚≠ê„Çí„ÅΩ„Å°„Å£„Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„Å´„ÇÉ„Çì~  \n\n üçâ \n ‰ªª‰Ωï ‚ùìÈóÆÈ¢ò / üí≠ÊÄùËÄÉ /üí°ÊÉ≥Ê≥ï ÈÉΩÊ¨¢ËøéÊèêÂá∫ÔºÅ\n Any ‚ùìquestion / üí≠thought /üí°idea  is welcome! \n „Å©„Çì„Å™ ‚ùìË≥™Âïè / üí≠ËÄÉ„Åà /üí°„Ç¢„Ç§„Éá„Ç¢ „Åß„ÇÇÊ≠ìËøé„Åß„ÅôÔºÅ \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tÁÆÄ‰ªã\n\t\n\n\nËçâËñôÂØß„ÄÖ Âπ≤Â£∞Â∏¶Ê†áÁ≠æÊï∞ÊçÆÈõÜ\n\nÊú¨Êï∞ÊçÆÈõÜÂè™Êî∂ÈõÜ‰∫ÜÊ∏∏ÊàèÂÜÖÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂπ∂‰∏çÊòØÂÖ®ÈÉ®ÁöÑÂÆÅÂÆÅÂπ≤Â£∞ËØ≠Èü≥ÔºåÂÖ∂‰∏≠ nene_org.txt ÊòØÊ†áÁ≠æÊñá‰ª∂\npjsk ÂÖ®ÈÉ®ËßíËâ≤Âπ≤Â£∞Â∏¶Ê†áÁ≠æÊï∞ÊçÆÈõÜÁöÑËØùÂèØ‰ª•Âä†QQÁæ§Ôºö691795641ÔºåÁæ§ÂÖ¨ÂëäÈáåÊúâÁΩëÁõòÂú∞ÂùÄ\n\n\t\n\t\n\t\n\t\tÂèÇËÄÉ\n\t\n\n\nÂ£∞Ê∫êÂΩíÂ±ûÔºöËçâËñôÂØß„ÄÖ(CV:Machico)-„Äå„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Çª„Ç´„Ç§ „Ç´„É©„Éï„É´„Çπ„ÉÜ„Éº„Ç∏ÔºÅ feat. ÂàùÈü≥„Éü„ÇØ„Äç\n\t\n\t\t\n\t\tTODO\n\t\n\n\nÔºàÈïøÊúüÔºâË°•ÂÖ®ÂÆÅÂÆÅËØ≠Èü≥ÔºåËßÑËåÉÊï∞ÊçÆÈõÜÊ†ºÂºè„ÄÇ\n\n","first_N":5,"first_N_keywords":["other","text-to-speech","audio-to-audio","Japanese","gpl-3.0"],"keywords_longer_than_N":true},
	{"name":"VoxCelebSpoof","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MattyB95/VoxCelebSpoof","creator_name":"Matthew Boakes","creator_url":"https://huggingface.co/MattyB95","description":"\n\t\n\t\t\n\t\n\t\n\t\tVoxCelebSpoof\n\t\n\nVoxCelebSpoof is a dataset related to detecting spoofing attacks on automatic speaker verification systems. This dataset is part of a broader effort to improve the security of voice biometric systems against various types of spoofing attacks, such as replay attacks, voice synthesis, and voice conversion.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe VoxCelebSpoof dataset includes a range of audio samples from different types of synthesis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MattyB95/VoxCelebSpoof.","first_N":5,"first_N_keywords":["audio-classification","text-to-speech","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"sql-parsed","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"sixuxar_yijiri_mak7","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anzorq/sixuxar_yijiri_mak7","creator_name":"AQ","creator_url":"https://huggingface.co/anzorq","description":"\n\t\n\t\t\n\t\tDataset Info\n\t\n\nThis dataset consists of paired audio and text data sourced from the following book:\n\nTitle: –ö—ä—ç—Ä–º–æ–∫—ä—É—ç –ú. –©–∏—Ö—É—Ö—ç—Ä –∏–¥–∂—ã—Ä–∏ –º—ç–∫I. –Ø–ø—ç —Ç—Ö—ã–ª—ä.\nPublication: –ù–∞–ª—å—á–∏–∫: –≠–ª—å–±—Ä—É—Å, 1999\n\n\n\t\n\t\t\n\t\tAudio Specifications\n\t\n\n\nSample Rate: 16,000 Hz\nTotal Length: 10:36:40\nSource: adigabook.ru\n\n\n\t\n\t\t\n\t\tProcessing Information\n\t\n\nAudio-text pairs for this dataset were extracted and aligned using META AI's forced alignment algorithm.\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Kabardian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"vibravox","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Cnam-LMSSC/vibravox","creator_name":"Laboratoire de M√©canique des Structures et des Syst√®mes Coupl√©s","creator_url":"https://huggingface.co/Cnam-LMSSC","description":"\n\t\n\t\t\n\t\tDataset Card for VibraVox\n\t\n\n\n  \n\n\n\nüëÄ While waiting for the TooBigContentError issue to be resolved by the HuggingFace team, you can explore the dataset viewer of vibravox-test\nwhich has exactly the same architecture.\n\n\t\n\t\t\n\t\n\t\n\t\tDATASET SUMMARY\n\t\n\nThe VibraVox dataset is a general purpose audio dataset of french speech captured with body-conduction transducers.\nThis dataset can be used for various audio machine learning tasks :\n\nAutomatic Speech Recognition (ASR) (Speech-to-Text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cnam-LMSSC/vibravox.","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","audio-classification","text-to-speech","speaker-identification"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-sa","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/common-canvas/commoncatalog-cc-by-sa","creator_name":"CommonCanvas","creator_url":"https://huggingface.co/common-canvas","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for CommonCatalog CC-BY-SA\n\t\n\nThis dataset is a large collection of high-resolution Creative Common images (composed of different licenses, see paper Table 1 in the Appendix) collected in 2014 from users of Yahoo Flickr. \nThe dataset contains images of up to 4k resolution, making this one of the highest resolution captioned image datasets.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nWe provide captions synthetic captions to approximately 100‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/common-canvas/commoncatalog-cc-by-sa.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-sa-4.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"irish-traditional-tunes","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hdparmar/irish-traditional-tunes","creator_name":"Harshdeep Parmar","creator_url":"https://huggingface.co/hdparmar","description":"\n\t\n\t\t\n\t\tDataset Card for \"irish-traditional-tunes\"\n\t\n\nMore Information needed\n\n\t\n\t\t\n\t\tDataset Card for \"irish-tunes-spectrograms\"\n\t\n\n\n\t\n\t\t\n\t\t1. Dataset Description\n\t\n\n  Dataset is used for the following project\n\nHomepage: Trad-fusion\n\n\n\t\n\t\t\n\t\t1.1 Dataset Summary\n\t\n\nThis dataset contains 9604 Mel spectrograms that represent Traditional Irish Music. \nThis dataset is smaller compared to hdparmar/irish-tunes-spectrogram, to reduce the training time and increase the possibilty to train for longer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hdparmar/irish-traditional-tunes.","first_N":5,"first_N_keywords":["text-to-image","text-to-audio","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"irish-traditional-tunes","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hdparmar/irish-traditional-tunes","creator_name":"Harshdeep Parmar","creator_url":"https://huggingface.co/hdparmar","description":"\n\t\n\t\t\n\t\tDataset Card for \"irish-traditional-tunes\"\n\t\n\nMore Information needed\n\n\t\n\t\t\n\t\tDataset Card for \"irish-tunes-spectrograms\"\n\t\n\n\n\t\n\t\t\n\t\t1. Dataset Description\n\t\n\n  Dataset is used for the following project\n\nHomepage: Trad-fusion\n\n\n\t\n\t\t\n\t\t1.1 Dataset Summary\n\t\n\nThis dataset contains 9604 Mel spectrograms that represent Traditional Irish Music. \nThis dataset is smaller compared to hdparmar/irish-tunes-spectrogram, to reduce the training time and increase the possibilty to train for longer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hdparmar/irish-traditional-tunes.","first_N":5,"first_N_keywords":["text-to-image","text-to-audio","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"spider-syn","keyword":"text-to-sql","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aherntech/spider-syn","creator_name":"AhernTech s.r.o.","creator_url":"https://huggingface.co/aherntech","description":"\n\t\n\t\t\n\t\tDataset Card for Sypder-Syn\n\t\n\nSpyder-Syn is a human curated variant of the Spider Text-to-SQL database.\nThe database was created to test the robustness of text-to-SQL models for robustness of synonym substitution.\nThe source GIT repo for Sypder-Syn is located here: https://github.com/ygan/Spider-Syn\nDetails regarding the data perterbation methods used and objectives are described in ACL 2021: arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tPaper Abstract\n\t\n\n\nRecently, there has been significant progress in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aherntech/spider-syn.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"MAGBIG","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/felfri/MAGBIG","creator_name":"Felix Friedrich","creator_url":"https://huggingface.co/felfri","description":"\n\t\n\t\t\n\t\tMAGBIG benchmark\n\t\n\nThis is the MAGBIG benchmark proposed in https://arxiv.org/abs/2401.16092\nThis benchmark is intended for multilingual text-to-image models. With MAGBIG, you can generate images for a diverse set of prompts across ten different languages. These images can be evaluated for differences across languages. MAGBIG is designed to uncover and assess biases across languages such as gender, race, age, etc. This way, we can measure whether bias exists in a language, but also‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/felfri/MAGBIG.","first_N":5,"first_N_keywords":["text-to-image","English","German","Italian","French"],"keywords_longer_than_N":true},
	{"name":"common-voice-filtered","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/styletts2-community/common-voice-filtered","creator_name":"StyleTTS 2 Community","creator_url":"https://huggingface.co/styletts2-community","description":"\n\t\n\t\t\n\t\tCommon Voice Filtered\n\t\n\nA filtered subset of the Common Voice dataset. Currently, this dataset only includes a small subset of English speech.\nWe only include speech ranked above 3.75 (75%) on the MOS metric, as calculated by the UTMOS system. Approximately 7% of audio qualified for inclusion in this filtered dataset.\nThis data is not final. Processing the whole Common Voice dataset would require a significant amount of compute, this is just a small sample/MVP of the project.\nThe code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/styletts2-community/common-voice-filtered.","first_N":5,"first_N_keywords":["text-to-speech","cc-by-sa-4.0","< 1K","soundfolder","Audio"],"keywords_longer_than_N":true},
	{"name":"T2IScoreScore","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/saxon/T2IScoreScore","creator_name":"Michael Saxon","creator_url":"https://huggingface.co/saxon","description":"\n\t\n\t\t\n\t\tDataset Card for Text-to-Image ScoreScore (T2IScoreScore or TS2)\n\t\n\nThis dataset exists as part of the T2IScoreScore metaevaluation for assessing the faithfulness and consistency of text-to-image model prompt-image evaluation metrics.\nNecessary code for utilizing the resource is present at github.com/michaelsaxon/T2IScoreScore\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a test set of 165 \"target prompts\" which each have between 5 and 76 generated images of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/saxon/T2IScoreScore.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"T2IScoreScore","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/saxon/T2IScoreScore","creator_name":"Michael Saxon","creator_url":"https://huggingface.co/saxon","description":"\n\t\n\t\t\n\t\tDataset Card for Text-to-Image ScoreScore (T2IScoreScore or TS2)\n\t\n\nThis dataset exists as part of the T2IScoreScore metaevaluation for assessing the faithfulness and consistency of text-to-image model prompt-image evaluation metrics.\nNecessary code for utilizing the resource is present at github.com/michaelsaxon/T2IScoreScore\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis is a test set of 165 \"target prompts\" which each have between 5 and 76 generated images of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/saxon/T2IScoreScore.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"openslr63","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vrclc/openslr63","creator_name":"Virtual Resource Centre for Language Computing (Digital University Kerala)","creator_url":"https://huggingface.co/vrclc","description":"\n\t\n\t\t\n\t\tSLR63: Crowdsourced high-quality Malayalam multi-speaker speech data set\n\t\n\nThis data set contains transcribed high-quality audio of Malayalam sentences recorded by volunteers. The data set consists of wave files, and a TSV file (line_index.tsv). The file line_index.tsv contains a anonymized FileID and the transcription of audio in the file.\nThe data set has been manually quality checked, but there might still be errors.\nPlease report any issues in the following issue tracker on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vrclc/openslr63.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Malayalam","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"presto-athena-txt-2-sql","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cnatale/presto-athena-txt-2-sql","creator_name":"Chris Natale","creator_url":"https://huggingface.co/cnatale","description":"I created this dataset using sqlglot to auto-convert the Spider and Wikisql datasets to Presto syntax, along with running some regex's for additional cleanup.\nAn example use case is fine-tuning an existing model to respond with Presto/Athena text-to-sql, if it performs well at standard SQL syntax used by the major text to sql training datasets.\nExample of fine-tuning using this dataset (in this case for Mystral 7b Instruct):\nimport json\nimport pandas as pd\nfrom datasets import Dataset\n\ndef‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cnatale/presto-athena-txt-2-sql.","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"stable-diffusion-prompts-uncensored","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/stable-diffusion-prompts-uncensored","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\n\t\n\t\t\n\t\tDataset Card for \"stable-diffusion-prompts-uncensored\"\n\t\n\n\n\t\n\t\t\n\t\tNot SAFE for public - Definately Unfiltered\n\t\n\nThis dataset comes from prompts shared from images' metadata on Civitai. Not for the faint of heart. \nThanks to Civitai.com for all the models, building a playground, allowing fine tuning of models, and generally being a good influence on model building and generation.\nThe purpose of this dataset is to allow for analysis of prompts and feature analysis in prompts and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jtatman/stable-diffusion-prompts-uncensored.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"sd-ml-assignment","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/premai-io/sd-ml-assignment","creator_name":"Prem","creator_url":"https://huggingface.co/premai-io","description":"\n\t\n\t\t\n\t\tText to Image Dataset for Pixel Art style\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset contains 100 examples of Images representing different topics all with the same style.\n","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","arrow","Image"],"keywords_longer_than_N":true},
	{"name":"MCC-250","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AIML-TUDA/MCC-250","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","description":"\n\t\n\t\t\n\t\tMultimodal Concept Conjunction 250\n\t\n\nIn our paper MultiFusion: Fusing Pre-Trained Models for\nMulti-Lingual, Multi-Modal Image Generation we propose the MCC-250 benchmark to evaluate generative image composition capablities for multimodal inputs. \nMCC-250 is built on a subset of CC-500 which contains 500 text-only prompts of the pattern \"a red apple and a yellow banana\", textually\ndescribing two objects with respective attributes.\nWith MCC-250, we provide a set of reference images for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/MCC-250.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"Openclipart-Oldstyle","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuajewell/Openclipart-Oldstyle","creator_name":"Joshua Jewell","creator_url":"https://huggingface.co/joshuajewell","description":"Dataset Card for 16th Century(?) Black and White Style\n\nDataset used to train/finetune a black and white print style\nCaptions are generated by hand with the assistance of BLIP.\nImages were sourced from:\n  https://openclipart.org/artist/j4p4n\n  https://openclipart.org/artist/johnny_automatic\n  https://openclipart.org/artist/SnipsAndClips\nText file filenames correspond image file filenames as captions.\n","first_N":5,"first_N_keywords":["text-to-image","human generated","other","monolingual","https://openclipart.org/artist/j4p4n"],"keywords_longer_than_N":true},
	{"name":"32000-BlackSharpie","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/joshuajewell/32000-BlackSharpie","creator_name":"Joshua Jewell","creator_url":"https://huggingface.co/joshuajewell","description":"Dataset Card for a Black and White Sharpie Style\n\nDataset used to train/finetune a black and white sharpie style\nCaptions are generated by hand with the assistance of BLIP.\nImages were hand drawn.\nText file filenames correspond image file filenames as captions.\n","first_N":5,"first_N_keywords":["text-to-image","human generated","other","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"tts-rj-hi-karya","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/1rsh/tts-rj-hi-karya","creator_name":"Irsh Vijay","creator_url":"https://huggingface.co/1rsh","description":"\n\t\n\t\t\n\t\tRajasthani Hindi Speech Dataset\n\t\n\n\nThis dataset consists of audio recordings of participants reading out stories in Rajasthani Hindi, one sentence at a time. They had 98 participants from Soda, Rajasthan. Each participant read 30 stories. In total, we have 426872 recordings in this dataset. They had roughly 58 male participants and 40 female participants.\n\nPoint to Note:\nWhile random sampling suggests that most users have to their best effort tried to accurately read out the sentences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/1rsh/tts-rj-hi-karya.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Hindi","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"libritts","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mythicinfinity/libritts","creator_name":"Mythic Infinity","creator_url":"https://huggingface.co/mythicinfinity","description":"\n\t\n\t\t\n\t\tDataset Card for LibriTTS\n\t\n\n\n\nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, \nprepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is \ndesigned for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files \nfrom Project Gutenberg) of the LibriSpeech corpus.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is the LibriTTS dataset, adapted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mythicinfinity/libritts.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"speech-rj-hi","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/speech-rj-hi","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"\n\t\n\t\t\n\t\tRajasthani Hindi Speech Dataset\n\t\n\n\nThis dataset consists of audio recordings of participants reading out stories in Rajasthani Hindi, one sentence at a time. We had 98 participants from Soda, Rajasthan. Each participant read 30 stories. In total, we have 426873 recordings in this dataset. We had roughly 58 male participants and 40 female participants.\n\nPoint to Note:\nWhile random sampling suggests that most users have to their best effort tried to accurately read out the sentences, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/severo/speech-rj-hi.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Hindi","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"midjourney-images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ehristoforu/midjourney-images","creator_name":"Evgeniy Hristoforu","creator_url":"https://huggingface.co/ehristoforu","description":"\n\t\n\t\t\n\t\t‚õµ Midjourney Images Dataset\n\t\n\nThis is datase with images made by Midjourney V5/V6.\n\n\t\n\t\t\n\t\tDataset parameters\n\t\n\n\nCount of images: ~10.000\nZip file with dataset: True\nCaptions with images: False\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nLicense for this dataset: MIT\n\n\t\n\t\t\n\t\tUse in datasets\n\t\n\n\npip install -q datasets\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n  \"ehristoforu/midjourney-images\",\n  revision=\"main\"\n)\n\n\n\n\n\t\n\t\t\n\t\tEnjoy with this dataset!\n\t\n\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"vlsp2020_vinai_100h","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/vlsp2020_vinai_100h","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of VLSP 2020 - VinAI - ASR challenge dataset\n\t\n\nofficial announcement:\n\nti·∫øng vi·ªát: https://institute.vinbigdata.org/events/vinbigdata-chia-se-100-gio-du-lieu-tieng-noi-cho-cong-dong/\nin eglish: https://institute.vinbigdata.org/en/events/vinbigdata-shares-100-hour-data-for-the-community/\nVLSP 2020 workshop: https://vlsp.org.vn/vlsp2020\n\nofficial download: https://drive.google.com/file/d/1vUSxdORDxk-ePUt-bUVDahpoXiqKchMx/view?usp=sharing\ncontact: info@vinbigdata.org‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/vlsp2020_vinai_100h.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"fpt_fosd","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/fpt_fosd","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of FPT Open Speech Dataset (FOSD)\n\t\n\nreleased publicly in 2018 by FPT Corporation\n100h, 25.9k samples\nofficial link (dead): https://fpt.ai/fpt-open-speech-data/\nmirror: https://data.mendeley.com/datasets/k9sxg2twv4/4\nDOI: 10.17632/k9sxg2twv4.4\npre-process:\n\nremove non-sense strings: -N \\r\\n\nremove 4 files because missing transcription:\nSet001_V0.1_008210.mp3\nSet001_V0.1_010753.mp3\nSet001_V0.1_011477.mp3\nSet001_V0.1_011841.mp3\n\n\n\nneed to do: check misspelling\nusage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/fpt_fosd.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"infore1_25hours","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/infore1_25hours","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of InfoRe Technology public dataset ‚Ññ1\n\t\n\nofficial announcement: https://www.facebook.com/groups/j2team.community/permalink/1010834009248719/\n25h, 14.9k samples, InfoRe paid a contractor to read text\nofficial download: magnet:?xt=urn:btih:1cbe13fb14a390c852c016a924b4a5e879d85f41&dn=25hours.zip&tr=http%3A%2F%2Foffice.socials.vn%3A8725%2Fannounce\nmirror: https://files.huylenguyen.com/datasets/infore/25hours.zip\nunzip password: BroughtToYouByInfoRe\npre-process: see‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/infore1_25hours.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"infore2_audiobooks","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/infore2_audiobooks","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of InfoRe Technology public dataset ‚Ññ2\n\t\n\nofficial announcement: https://www.facebook.com/groups/j2team.community/permalink/1010834009248719/\n415h, 315k samples, vietnamese audiobooks of chinese w«îxi√° Ê≠¶‰ø† & xiƒÅnxi√° ‰ªô‰ø†\nb·ªô d·ªØ li·ªáu b√≥c ra t·ª´ YouTube ƒë·ªçc truy·ªán v√µ hi·ªáp & ti√™n hi·ªáp, √°p d·ª•ng kƒ© thu·∫≠t ƒë·ªëi chi·∫øu vƒÉn b·∫£n ƒë·ªÉ d√°n nh√£n t·ª± ƒë·ªông\nofficial download:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/infore2_audiobooks.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dalle-3-images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ehristoforu/dalle-3-images","creator_name":"Evgeniy Hristoforu","creator_url":"https://huggingface.co/ehristoforu","description":"\n\t\n\t\t\n\t\tüé® DALL‚Ä¢E 3 Images Dataset\n\t\n\nThis is datase with images made by Dalle3.\n\n\t\n\t\t\n\t\tDataset parameters\n\t\n\n\nCount of images: 3310\nZip file with dataset: True\nCaptions with images: False\n\n\n\t\n\t\t\n\t\tLicense\n\t\n\nLicense for this dataset: MIT\n\n\t\n\t\t\n\t\tUse in datasets\n\t\n\n\npip install -q datasets\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n  \"ehristoforu/dalle-3-images\",\n  revision=\"main\"\n)\n\n\n\n\n\t\n\t\t\n\t\tEnjoy with this dataset!\n\t\n\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"schaeffer_thesis_corrected","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dbschaeffer/schaeffer_thesis_corrected","creator_name":"SCHAEFFER Database","creator_url":"https://huggingface.co/dbschaeffer","description":"The SCHAEFFER dataset (Spectro-morphogical Corpus of Human-annotated Audio with Electroacoustic Features for Experimental Research), is a compilation of 788 raw audio data accompanied by human annotations and morphological acoustic features. \nThe audio files adhere to the concept of Sound Objects introduced by Pierre Scaheffer, a framework for the analysis and creation of sound that focuses on its typological and morphological characteristics.\nInside the dataset, the annotation are provided in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dbschaeffer/schaeffer_thesis_corrected.","first_N":5,"first_N_keywords":["text-to-audio","audio-classification","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"elevenlabs_dataset","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/skypro1111/elevenlabs_dataset","creator_name":"Serhii Kravchenko","creator_url":"https://huggingface.co/skypro1111","description":"\n\t\n\t\t\n\t\tSynthetic TTS Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset was created with the aim of exploring the concept of using synthetic datasets for training Text-to-Speech (TTS) models. It consists of 1,388 audio files with a total duration of 2 hours and 20 minutes and their corresponding textual transcripts. The dataset leverages the capabilities of advanced AI services, utilizing paid subscriptions to ChatGPT-4 for text generation and ElevenLabs.io for audio generation.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/skypro1111/elevenlabs_dataset.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","cc-by-4.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"spider-realistic","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aherntech/spider-realistic","creator_name":"AhernTech s.r.o.","creator_url":"https://huggingface.co/aherntech","description":"\n\t\n\t\t\n\t\tDataset Card for Spider-Releastic\n\t\n\nThis dataset variant contains only the Spider Realistic dataset used in \"Structure-Grounded Pretraining for Text-to-SQL\". The dataset is created based on the dev split of the Spider dataset (2020-06-07 version from https://yale-lily.github.io/spider). The authors of the dataset modified the original questions to remove the explicit mention of column names while keeping the SQL queries unchanged to better evaluate the model's capability in aligning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aherntech/spider-realistic.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"text2food-mmc4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tum-nlp/text2food-mmc4","creator_name":"Natural Language Processing @ TUM","creator_url":"https://huggingface.co/tum-nlp","description":"This dataset is filtered version of MMC4 Multimodal-C4 core fewer-faces dataset . It contains 144 474 pair of food image url and image caption.\nAll the code and model in the repository.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"AlbanianSpeech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Yakidev/AlbanianSpeech","creator_name":"Trust Oriakhi","creator_url":"https://huggingface.co/Yakidev","description":"Yakidev/AlbanianSpeech dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Albanian","mit","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"laion2B-multi-Vietnamese-subset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imthanhlv/laion2B-multi-Vietnamese-subset","creator_name":"Th√†nh L√™","creator_url":"https://huggingface.co/imthanhlv","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for LAION-2B-multi Vietnamese subset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nFilter the Vietnamese subset from Laion2B-multi\nTo get the subset of your language, check out this notebook\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Vietnamese","cc-by-4.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"LAION-EO","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mikonvergence/LAION-EO","creator_name":"Mikolaj Czerkawski","creator_url":"https://huggingface.co/mikonvergence","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for LAION-EO\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis dataset contains a subset of LAION-5B containing images that are likely to be satellite images. The procedure of acquiring and filtering the dataset has been described in https://arxiv.org/abs/2309.15535.\n\n\t\n\t\t\nVersion\nNumber of Samples\n\n\n\t\t\n0\n24,933\n\n\n1\n112,985\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nEach version of the dataset contains a .csv file with metadata with urls to images, which can be easily filtered. Note‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mikonvergence/LAION-EO.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","100K<n<1M","Image"],"keywords_longer_than_N":true},
	{"name":"logobookDB","keyword":"text-to-image","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mozci/logobookDB","creator_name":"mustafa ozci","creator_url":"https://huggingface.co/mozci","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\nThis dataset contains image caption pairs for logo designs screped from logobook.com. It is created for my research project to finetune text-image diffusion models with logo designs.\nLogobook.com has a very nice logo archive consisting of modernist and simplistic logo designs. Each design stored along with some keywords. I used these keywords to create a caption for the logo designs.\nSee example below:\n\nCaption:\nAdams Law, a prominent law firm in Ireland, features a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mozci/logobookDB.","first_N":5,"first_N_keywords":["text-to-image","English","afl-3.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Scenery_of_japan","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan","creator_name":"Japan Degital Material","creator_url":"https://huggingface.co/JapanDegitalMaterial","description":"\n\t\n\t\t\n\t\tScenery of japan.\n\t\n\nThis is a dataset to train text-to-image or other models without any copyright issue.\nAll materials used in this dataset are CC0 (Public domain /P.D.).\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JapanDegitalMaterial/Scenery_of_japan.","first_N":5,"first_N_keywords":["text-to-image","English","Japanese","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Texture_images","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JapanDegitalMaterial/Texture_images","creator_name":"Japan Degital Material","creator_url":"https://huggingface.co/JapanDegitalMaterial","description":"\n\t\n\t\t\n\t\tTextuer images\n\t\n\nThis is a dataset to train text-to-image or other models without any copyright issue.\nAll materials used in this dataset are CC0 (Public domain /P.D.).\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JapanDegitalMaterial/Texture_images.","first_N":5,"first_N_keywords":["text-to-image","English","Japanese","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Places_in_Japan","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan","creator_name":"Japan Degital Material","creator_url":"https://huggingface.co/JapanDegitalMaterial","description":"\n\t\n\t\t\n\t\tPlaces in japan.\n\t\n\nThis is a dataset to train text-to-image or other models without any copyright issue.\nAll materials used in this dataset are CC0 (Public domain /P.D.).\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JapanDegitalMaterial/Places_in_Japan.","first_N":5,"first_N_keywords":["text-to-image","English","Japanese","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pythia_img_beta","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GeneticApple/pythia_img_beta","creator_name":"Genetic Apple","creator_url":"https://huggingface.co/GeneticApple","description":"\n\t\n\t\t\n\t\tDataset Card for \"pythia_img_beta\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"nota","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/nota","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\n\t\n\t\t\n\t\tDataset Card for Nota\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis data was created by the public institution Nota, which is part of the Danish Ministry of Culture. Nota has a library audiobooks and audiomagazines for people with reading or sight disabilities. Nota also produces a number of audiobooks and audiomagazines themselves.  \nThe dataset consists of audio and associated transcriptions from Nota's audiomagazines \"Inspiration\" and \"Radio/TV\". All files related to one reading of one edition‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/nota.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Danish","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mls_eng_10k","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls_eng_10k","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng_10k.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls_eng_10k","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls_eng_10k","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls_eng_10k.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"train_video_and_instruction","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction","creator_name":"ShareGPTVideo","creator_url":"https://huggingface.co/ShareGPTVideo","description":"\n\t\n\t\t\n\t\n\t\n\t\tShareGPTVideo Training Data\n\t\n\nAll dataset and models can be found at ShareGPTVideo.\n\n\t\n\t\t\n\t\n\t\n\t\tContents:\n\t\n\n\nTrain 300k video frames: contains video frames used for SFT and DPO model, which is a subset of total 900k.\nActivityNet 50k + vidal 150k + webvid 100k.\n\nTrain 600k video frames: contains the rest 600k frames, the total 900k frames are used for pre-training stage. If you just do finetuning using our video QA, you can just download the 300k above.\n900k composition is 400k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","Video"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-chatml","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/recastai/sql-create-context-chatml","creator_name":"Re:cast AI","creator_url":"https://huggingface.co/recastai","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset has been created by Re:cast AI to extend the existing dataset b-mc2/sql-create-context into a chatml friendly format for use in SFT tasks with pretrained models.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nmessages = [\n     {'content': \"You are a powerful text-to-SQL AI assistant that helps users ... etc.\", 'role': 'system'},\n     {'content': '(Optional) Context information is below ... etc.', 'role': 'user'},\n     {'content': 'SELECT COUNT(*) FROM head WHERE age > 56'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/recastai/sql-create-context-chatml.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"vivechan-spritual-text-dataset-v2","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/om-ashish-soni/vivechan-spritual-text-dataset-v2","creator_name":"Om Ashishkumar Soni","creator_url":"https://huggingface.co/om-ashish-soni","description":"\n\t\n\t\t\n\t\tVivechan - Spiritual Text Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe Vivechan - Spiritual Text Dataset is an open and public collection of textual data extracted from significant spiritual texts, curated to support discussions, inquiries, doubts, and Q&A sessions within the realm of spirituality. This dataset provides valuable content from the following revered sources:\n\nShrimad Bhagwat Mahapurana\nShripad Shri Vallabha Charitramrutam\nShiv Mahapurana Sankshipt\nValmiki Ramayan‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/om-ashish-soni/vivechan-spritual-text-dataset-v2.","first_N":5,"first_N_keywords":["text-retrieval","text-to-speech","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"world-heightmaps-256px","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/novaia/world-heightmaps-256px","creator_name":"Novaia","creator_url":"https://huggingface.co/novaia","description":"\n\t\n\t\t\n\t\tWorld Heightmaps 256px\n\t\n\nThis is a dataset of 256x256 Earth heightmaps generated from SRTM 1 Arc-Second Global.\nEach heightmap is labelled according to its latitude and longitude. There are 573,995 samples. It is the same as \nWorld Heightmaps 360px but downsampled to 256x256.\n\n\t\n\t\t\n\t\n\t\n\t\tMethod\n\t\n\n\nConvert GeoTIFFs into PNGs with Rasterio.\n\nimport rasterio\nimport matplotlib.pyplot as plt\nimport os\n\ninput_directory = '...'\noutput_directory = '...'\nfile_list =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/novaia/world-heightmaps-256px.","first_N":5,"first_N_keywords":["image-classification","text-to-image","unconditional-image-generation","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"world-heightmaps-360px","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/novaia/world-heightmaps-360px","creator_name":"Novaia","creator_url":"https://huggingface.co/novaia","description":"\n\t\n\t\t\n\t\tWorld Heightmaps 360px\n\t\n\nThis is a dataset of 360x360 Earth heightmaps generated from SRTM 1 Arc-Second Global.\nEach heightmap is labelled according to its latitude and longitude. There are 573,995 samples.\n\n\t\n\t\t\n\t\tMethod\n\t\n\n\nConvert GeoTIFFs into PNGs with Python and Rasterio.\n\nimport rasterio\nimport matplotlib.pyplot as plt\nimport os\n\ninput_directory = '...'\noutput_directory = '...'\nfile_list = os.listdir(input_directory)\n\nfor i in range(len(file_list)):\n    image =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/novaia/world-heightmaps-360px.","first_N":5,"first_N_keywords":["unconditional-image-generation","image-classification","text-to-image","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"airbnb_embeddings","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MongoDB/airbnb_embeddings","creator_name":"MongoDB","creator_url":"https://huggingface.co/MongoDB","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset consists of AirBnB listings with property descriptions, reviews, and other metadata. \nIt also contains text embeddings of the property descriptions as well as image embeddings of the listing image. The text embeddings were created using OpenAI's text-embedding-3-small model and the image embeddings using OpenAI's clip-vit-base-patch32 model available on Hugging Face. \nThe text embeddings have 1536 dimensions, while the image embeddings have 512 dimensions.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MongoDB/airbnb_embeddings.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sparc","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aherntech/sparc","creator_name":"AhernTech s.r.o.","creator_url":"https://huggingface.co/aherntech","description":"\n\t\n\t\t\n\t\tDataset Card for SParC\n\t\n\nSParC is a context-dependant multi-turn version of the Spider task 1.0.\nThis dataset provides a chat-bot oriented test set for text-to-sql problems. Additional details may be obtained in the paper:\n\nhttps://arxiv.org/abs/1906.02285\n\n\n\t\n\t\t\n\t\n\t\n\t\tPaper Abstract\n\t\n\n\nWe present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aherntech/sparc.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"MediaSpeech","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/MediaSpeech","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tMediaSpeech\n\t\n\nMediaSpeech is a dataset of Arabic, French, Spanish, and Turkish media speech built with the purpose of testing Automated Speech Recognition (ASR) systems performance. The dataset contains 10 hours of speech for each language provided.\nThe dataset consists of short speech segments automatically extracted from media videos available on YouTube and manually transcribed, with some pre-processing and post-processing.\nBaseline models and WAV version of the dataset can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/MediaSpeech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Arabic","French","Spanish"],"keywords_longer_than_N":true},
	{"name":"VL-ICL","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ys-zong/VL-ICL","creator_name":"Yongshuo Zong","creator_url":"https://huggingface.co/ys-zong","description":"\n\t\n\t\t\n\t\tVL-ICL Bench\n\t\n\nVL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning\n[Webpage] [Paper] [Code]\n\n\t\n\t\t\n\t\tImage-to-Text Tasks\n\t\n\nIn all image-to-text tasks image is a list of image paths (typically one item - for interleaved cases there are two items).\n\n\t\n\t\t\n\t\tFast Open-Ended MiniImageNet\n\t\n\nFrozen introduces the task of fast concept binding for MiniImageNet. The benchmark has a fixed structure so only the given support examples can be used for a given‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ys-zong/VL-ICL.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","mit","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"Arabic-CivitAi-Images","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MohamedRashad/Arabic-CivitAi-Images","creator_name":"Mohamed Rashad","creator_url":"https://huggingface.co/MohamedRashad","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nTop +2k Images curated from CivitAI website, described using the great Qwen-VL-Max model and then translated using Command-R into the Arabic language\n\n\n    \n\n","first_N":5,"first_N_keywords":["text-to-image","Arabic","gpl-3.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"jalandhary_asr","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mirfan899/jalandhary_asr","creator_name":"Muhammad Irfan","creator_url":"https://huggingface.co/mirfan899","description":"Jalandhary dataset is created using whisper model for STT and TTS. Some audios are ommited due to issues while trimming them. If there are some isues \nin the dataset or audio not matching the text you can start a discussion or ping me to correcting it. \n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Urdu","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dreambooth-cell-images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mario-dg/dreambooth-cell-images","creator_name":"Mario da Graca","creator_url":"https://huggingface.co/mario-dg","description":"\n\t\n\t\t\n\t\tDataset Card for Dreambooth Brightfield Microscopy\n\t\n\nThis dataset was created as part of my masters research and thesis, where I am trying to generate realistic looking\nbrightfield microscopy images for dataset augmentation.\nWith the downstream goal of enhancing cell detection objects, increasing the dataset size of an object detection model\nis a necessary step.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nAs part of my research, I previously generated brightfield‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mario-dg/dreambooth-cell-images.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"libritts_r_tags_tagged_10k_generated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/libritts_r_tags_tagged_10k_generated","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotated LibriTTS-R\n\t\n\nThis dataset is an annotated version of LibriTTS-R [1]. LibriTTS-R [1] is a sound quality improved version of the LibriTTS corpus which is a multi-speaker English corpus of approximately 960 hours of read English speech at 24kHz sampling rate, published in 2019. \nIn the text_description column, it provides natural language annotations on the characteristics of speakers and utterances, that have been generated using the Data-Speech repository.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/libritts_r_tags_tagged_10k_generated.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Ergonomics_Chiar_Customer_Viewdata_E-commerse","keyword":"text-to-speech","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse","creator_name":"lia","creator_url":"https://huggingface.co/liaHa","description":"liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["feature-extraction","text-classification","zero-shot-classification","text-to-speech","English"],"keywords_longer_than_N":true},
	{"name":"DS-2","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Artples/DS-2","creator_name":"Artur Lauche","creator_url":"https://huggingface.co/Artples","description":"\n\t\n\t\t\n\t\tDataset Card for LAI-DS2\n\t\n\nThis Dataset is a dataset with pictures from Unsplash with great descriptions to the images.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset comprises a collection of Unsplash images accompanied by detailed descriptions.\n\nCurated by: Artur Lauche\nLicense: Apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\nAll images are from Unsplash and the descriptions are made by licensed AI.\n\n\t\n\t\t\n\t\tUses\n\t\n\nImage related task.\n\n\t\n\t\t\n\t\tOut-of-Scope Use\n\t\n\nThere is no NSFW‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Artples/DS-2.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"yandere2023","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nyanko7/yandere2023","creator_name":"Nyanko","creator_url":"https://huggingface.co/nyanko7","description":"\n\t\n\t\t\n\t\tYandere2023: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset\n\t\n\n\n\nYandere2023 is a comprehensive anime image dataset with over 1.2 million high-quality images sourced from various materials, including key frames, manga scans, artbooks, and more. While the average number of tags per image is relatively low, the dataset boasts a diverse collection of images with exceptional quality.\n\nShared by: Nyanko Devs\nLanguage(s): English, Japanese\nLicense: MIT\n\n\n\t\n\t\t\n\t\n\t\n\t\tUses‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyanko7/yandere2023.","first_N":5,"first_N_keywords":["image-classification","image-to-image","text-to-image","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"AISHELL-3","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AISHELL/AISHELL-3","creator_name":"aishelltech","creator_url":"https://huggingface.co/AISHELL","description":"AISHELL-3 is a large-scale and high-fidelity multi-speaker Mandarin speech corpus published by Beijing Shell Shell Technology Co.,Ltd. It can be used to train multi-speaker Text-to-Speech (TTS) systems.The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers and total 88035 utterances. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Accordingly, transcripts in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AISHELL/AISHELL-3.","first_N":5,"first_N_keywords":["text-to-speech","Chinese","apache-2.0","10K<n<100K","Audio"],"keywords_longer_than_N":true},
	{"name":"OmniVid","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vivym/OmniVid","creator_name":"Ming Yang","creator_url":"https://huggingface.co/vivym","description":"\n\t\n\t\t\n\t\tOmniVid\n\t\n\nYoutube Video: 24,037,110\n","first_N":5,"first_N_keywords":["text-to-video","apache-2.0","10M - 100M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"danbooru2025-metadata","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/danbooru2025-metadata","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":" \n   \n \n\nüé® Danbooru 2025 Metadata \n\n\n  Latest Post ID: 9,158,800\n  (as of Apr 16, 2025)\n\n\n\nüìÅ About the DatasetThis dataset provides structured metadata for user-submitted images on Danbooru, a large-scale imageboard focused on anime-style artwork.\nScraping began on January 2, 2025, and the data are stored in Parquet format for efficient programmatic access.Compared to earlier versions, this snapshot includes:\n\nMore consistent tag history tracking  \nBetter coverage of older or previously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/danbooru2025-metadata.","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["text-to-image","text-classification","image-classification","image-to-text","image-segmentation"],"keywords_longer_than_N":true},
	{"name":"EmbodiedEval","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EmbodiedEval/EmbodiedEval","creator_name":"EmbodiedEval","creator_url":"https://huggingface.co/EmbodiedEval","description":"This repository contains the dataset of the paper EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents.\nGithub repository: https://github.com/thunlp/EmbodiedEval\nProject Page: https://embodiedeval.github.io/\n","first_N":5,"first_N_keywords":["robotics","video-text-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DualMath-1.1M","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/URSA-MATH/DualMath-1.1M","creator_name":"URSA-MATH","creator_url":"https://huggingface.co/URSA-MATH","description":"\n\t\n\t\t\n\t\tDualMath-1.1M\n\t\n\nImage data can be downloaded from the following address:\n\nMAVIS: https://github.com/ZrrSkywalker/MAVIS, https://drive.google.com/drive/folders/1LGd2JCVHi1Y6IQ7l-5erZ4QRGC4L7Nol.\nMultimath: https://huggingface.co/datasets/pengshuai-rin/multimath-300k.\nGeo170k: https://huggingface.co/datasets/Luckyjhg/Geo170K.\nVarsityTutors: https://huggingface.co/datasets/Math-PUMA/Math-PUMA_Data_Stage2. \nMathV360K: https://huggingface.co/datasets/Zhiqiang007/MathV360K.\n\nThe image data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/URSA-MATH/DualMath-1.1M.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","Chinese","gpl-3.0"],"keywords_longer_than_N":true},
	{"name":"SIWIS_French_Speech_Synthesis_Database","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aviv-anthonnyolime/SIWIS_French_Speech_Synthesis_Database","creator_name":"Anthonny Olime","creator_url":"https://huggingface.co/Aviv-anthonnyolime","description":"\n\t\n\t\t\n\t\tSIWIS French Speech Synthesis Database\n\t\n\nThis README provides a concise description of the dataset, including its structure, file naming conventions, and known labeling issues. Additionally, suggestions for potential improvements are outlined in the TODO section.  \nThe dataset is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, permitting its use for any purpose.  \nFor more details about the database design and recording process, please refer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Aviv-anthonnyolime/SIWIS_French_Speech_Synthesis_Database.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","French","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"e621_2024-captions-1ktar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/6DammK9/e621_2024-captions-1ktar","creator_name":"Darren Laurie","creator_url":"https://huggingface.co/6DammK9","description":"\n\t\n\t\t\n\t\tE621 2024 captions only in 1k tar\n\t\n\n\nRaw captions jointed from lodestones/e621-captions\n\nIt doesn't align to any dataset yet.\n\nmeta_cap.json has been provided in compressed format if you want to train with kohyas triner. Currently I'm trying to merge this with my 2024 version.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tCore logic\n\t\n\n\nThe script building this 1ktar\n\nThere is not much choice, I don't have GPU to run for 1M captions with VLM so I just \"take it or leave it\".\n\n\nrearranged_tags = [row.regular_summary‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6DammK9/e621_2024-captions-1ktar.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","no-annotation","e621"],"keywords_longer_than_N":true},
	{"name":"opentts-lada","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/opentts-lada","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"\n\t\n\t\t\n\t\tOpen Text-to-Speech voices for üá∫üá¶ Ukrainian\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { opentts-uk (Revision 32abc9c) },\n    year         = 2025,\n    url          = { https://huggingface.co/datasets/Yehor/opentts-uk },\n    doi          = { 10.57967/hf/4551 }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/opentts-lada.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"opentts-oleksa","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/opentts-oleksa","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"\n\t\n\t\t\n\t\tOpen Text-to-Speech voices for üá∫üá¶ Ukrainian\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { opentts-uk (Revision 32abc9c) },\n    year         = 2025,\n    url          = { https://huggingface.co/datasets/Yehor/opentts-uk },\n    doi          = { 10.57967/hf/4551 }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/opentts-oleksa.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ViDoSeek","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/autumncc/ViDoSeek","creator_name":"QiuchenWang","creator_url":"https://huggingface.co/autumncc","description":"\n\t\n\t\t\n\t\tüöÄOverview\n\t\n\nThis is the Repo for ViDoSeek, a benchmark specifically designed for visually rich document retrieval-reason-answer, fully suited for evaluation of RAG within large document corpus. \n\nThe paper is available at https://arxiv.org/abs/2502.18017.\nViDoRAG Project: https://github.com/Alibaba-NLP/ViDoRAG\n\nViDoSeek sets itself apart with its heightened difficulty level, attributed to the multi-document context and the intricate nature of its content types, particularly the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/autumncc/ViDoSeek.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","visual-document-retrieval","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SCHAEFFER","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dbschaeffer/SCHAEFFER","creator_name":"SCHAEFFER Database","creator_url":"https://huggingface.co/dbschaeffer","description":"\n\t\n\t\t\n\t\tSCHAEFFER\n\t\n\n\n\t\n\t\t\n\t\tBy Maurizio Berta & Daniele Ghisi\n\t\n\nThe SCHAEFFER dataset (Spectro-morphogical Corpus of Human-annotated Audio with Electroacoustic Features for Experimental Research), is a compilation of 1000 raw audio files accompanied by human annotations and morphological acoustic features. The audio files adhere to the concept of Sound Objects introduced by Pierre Schaeffer, a framework for the analysis and creation of sound that focuses on its typological and morphological‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dbschaeffer/SCHAEFFER.","first_N":5,"first_N_keywords":["text-to-audio","audio-classification","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"nigerian_common_voice_dataset","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/benjaminogbonna/nigerian_common_voice_dataset","creator_name":"Benjamin Ogbonna","creator_url":"https://huggingface.co/benjaminogbonna","description":"\n\t\n\t\t\n\t\tDataset Card for Nigerian Common Voice Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Nigerian Common Voice Dataset is a comprehensive dataset consisting of 158 hours of audio recordings and corresponding transcription (sentence). \nThis dataset includes metadata like accent, locale that can help improve the accuracy of speech recognition engines. This dataset is specifically curated to address the gap in speech and language \ndatasets for African accents, making it a valuable resource for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/benjaminogbonna/nigerian_common_voice_dataset.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"wan_blinking","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_blinking","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"VidComposition_Benchmark","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark","creator_name":"JunJiaGuo","creator_url":"https://huggingface.co/JunJiaGuo","description":"\n\t\n\t\t\n\t\tVidComposition Benchmark\n\t\n\nüñ• Project Page | üöÄ Evaluation Space\nThe advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JunJiaGuo/VidComposition_Benchmark.","first_N":5,"first_N_keywords":["question-answering","multiple-choice","video-text-to-text","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"wan_blockify_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_blockify_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_blowing_bubble_with_gum","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_blowing_bubble_with_gum","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_blowing_out_candle","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_blowing_out_candle","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_bouncing_ball","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_bouncing_ball","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_burn_char_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_burn_char_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_buttoning_shirt","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_buttoning_shirt","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_checking_watch","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_checking_watch","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_clapping_hands","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_clapping_hands","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_closing_umbrella","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_closing_umbrella","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_crumble_disintegrate_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_crumble_disintegrate_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_curtseying","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_curtseying","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"OpenVid-10k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-10k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 10k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom torchcodec.decoders import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-10k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"OpenVid-10k-split","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/finetrainers/OpenVid-10k-split","creator_name":"Finetrainers","creator_url":"https://huggingface.co/finetrainers","description":"\n  \n\n\nCombination of part_id's from bigdata-pw/OpenVid-1M and video data from nkp37/OpenVid-1M.\nThis is a 10k video split of the original dataset for faster iteration during testing. The split was obtained by filtering on aesthetic and motion scores by iteratively increasing their values until there were at most 1000 videos. Only videos containing between 80 and 240 frames were considered.\nfrom datasets import load_dataset, disable_caching, DownloadMode\nfrom torchcodec.decoders import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/finetrainers/OpenVid-10k-split.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"text-to-speech-phonemized-sentences","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/text-to-speech-phonemized-sentences","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"Phonemized version of https://huggingface.co/datasets/speech-uk/text-to-speech-sentences with some additional fields.\n","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"wan_doing_peace_sign","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_peace_sign","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_doing_single_jumping_jack","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_single_jumping_jack","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_doing_single_squat","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_single_squat","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_doing_wave_arm","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_doing_wave_arm","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_drinking_water","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_drinking_water","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_eating","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_eating","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_emerge_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_emerge_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_finger_counting","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_finger_counting","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_folding_paper","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_folding_paper","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_freeze_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_freeze_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_glitch_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_glitch_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_glow_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_glow_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_inflate_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_inflate_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_jogging_in_place","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_jogging_in_place","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_laughing","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_laughing","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_licking_lips","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_licking_lips","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"hypnosis_dataset","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/hypnosis_dataset","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\n\t\n\t\t\n\t\tdataset card for \"hypnosis_dataset\"\n\t\n\n\n\t\n\t\t\n\t\thypnosis scripts based on Erickson progressions\n\t\n\nThis is a small dataset containing hypnosis scripts that were both obtained from legitimate (manual) sources, and also generated using the following closed and open models:\nlarge llm:\n\nopenai api\ncohere\npalm\nopen models:\nmistral-7b\ntrismegistus-mistral-7b\nzephyr-7b\nmistral-anima-phi-7b\nmistral-instruct\n\nThe data has been cleaned but not altered save for formatting. \nSome entries include a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jtatman/hypnosis_dataset.","first_N":5,"first_N_keywords":["text-generation","text-to-audio","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MultiCaRe_Dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"FLEURS-GA-EN","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/FLEURS-GA-EN","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis is the Irish-to-English portion of the FLEURS dataset.\nFleurs is the speech version of the FLoRes machine translation benchmark.\nThe Irish portion consists of 3991 utterances, which correspond to approximately 16 hours and 45 minutes (16:45:17) of audio data.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'audio', 'text_ga', 'text_en'],\n        num_rows: 3991\n    })\n})\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{fleurs2022arxiv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/FLEURS-GA-EN.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","Irish","English"],"keywords_longer_than_N":true},
	{"name":"arabic_xvector_embeddings","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/herwoww/arabic_xvector_embeddings","creator_name":"Hawau Olamide Toyin","creator_url":"https://huggingface.co/herwoww","description":"\n\t\n\t\t\n\t\tArabic Speaker Embeddings extracted from ASC and ClArTTS\n\t\n\nThere is one speaker embedding for each utterance in the validation set of both datasets. The speaker embeddings are 512-element X-vectors.\nArabic Speech Corpus has 100 files for a single male speaker and ClArTTS has 205 files for a single male speaker.\nThe X-vectors were extracted using this script, which uses the speechbrain/spkrec-xvect-voxceleb model.\nUsage:\nfrom datasets import load_dataset\n\nembeddings_dataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/herwoww/arabic_xvector_embeddings.","first_N":5,"first_N_keywords":["text-to-speech","audio-to-audio","Arabic","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"SpokenWords-GA-EN-MTed","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/SpokenWords-GA-EN-MTed","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nThis is the Irish portion of the Spoken Words dataset (available at MLCommons/ml_spoken_words),\nwith merged splits ‚Äútrain‚Äù, ‚Äúvalidation‚Äù, and ‚Äútest‚Äù, augmented with machine translation.\nThe Irish sentences are automatically translated into English using Google Translation API.\nThe dataset includes approximately 3 hours and 2 minutes of audio (03:02:02), spoken by multiple narrators.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nDataset({\n    features: ['keyword'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/SpokenWords-GA-EN-MTed.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","Irish","English"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-id","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a fork from sql-create-context \nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Indonesian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Rhulk_pt-br","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/satierf/Rhulk_pt-br","creator_name":"thiago freitas pimenta","creator_url":"https://huggingface.co/satierf","description":"satierf/Rhulk_pt-br dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-generation","Portuguese","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"assamese_speech_corpus","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/madhabpaul/assamese_speech_corpus","creator_name":"Madhab Paul","creator_url":"https://huggingface.co/madhabpaul","description":"madhabpaul/assamese_speech_corpus dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","Assamese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"assamese_speech_corpus","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/madhabpaul/assamese_speech_corpus","creator_name":"Madhab Paul","creator_url":"https://huggingface.co/madhabpaul","description":"madhabpaul/assamese_speech_corpus dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","Assamese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"StockImages-CC0","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KoalaAI/StockImages-CC0","creator_name":"Koala AI","creator_url":"https://huggingface.co/KoalaAI","description":"\n\t\n\t\t\n\t\tCC0 Stock Images Dataset\n\t\n\nThis dataset contains a collection of stock images that are covered by the Creative Commons Zero (CC0) License, meaning they are free for personal and commercial use with no attribution required. It is designed to support a variety of computer vision tasks such as image tagging, categorization, and machine learning model training.\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nWhile every effort has been made to ensure the reliability and correctness of the data presented, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KoalaAI/StockImages-CC0.","first_N":5,"first_N_keywords":["image-to-text","image-to-image","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"Living-Audio-Irish","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/Living-Audio-Irish","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nLiving Audio Irish speech corpus. This version is based on the Irish dataset on Kaggle.\nThe original dataset with audio in more languages is available on GitHub as part of the Idlak project.\nThe details of the Irish portion of the Living Audio dataset are as follows:\n\n\t\n\t\t\nSpeaker\nLanguage\nAccent\nGender\nTotal duration(mm:ss)\nSample rate (Hz)\n\n\n\t\t\nCLL\nIrish (ga)\nNon-native (ie)\nMan\n61:56\n48,000\n\n\n\t\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDataset({\n    features: ['sentence'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Living-Audio-Irish.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Irish","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"libritts_r","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/libritts_r","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\n\t\n\t\t\n\t\tDataset Card for LibriTTS-R\n\t\n\n\n\nLibriTTS-R [1] is a sound quality improved version of the LibriTTS corpus \n(http://www.openslr.org/60/) which is a multi-speaker English corpus of approximately \n585 hours of read English speech at 24kHz sampling rate, published in 2019.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is the LibriTTS-R dataset, adapted for the datasets library.\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tSplits\n\t\n\nThere are 7 splits (dots replace dashes from the original dataset, to comply with hf naming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/libritts_r.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"mls-eng-10k-tags_tagged_10k_generated","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of 10K hours of English MLS\n\t\n\nThis dataset consists in annotations of a 10K hours subset of English version of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/mls-eng-10k-tags_tagged_10k_generated.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"laion-coco-aesthetic","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guangyil/laion-coco-aesthetic","creator_name":"Guangyi Liu","creator_url":"https://huggingface.co/guangyil","description":"\n\t\n\t\t\n\t\tLAION COCO with aesthetic score and watermark score\n\t\n\nThis dataset contains 10% samples of the LAION-COCO dataset filtered by some text rules (remove url, special tokens, etc.), and image rules (image size > 384x384, aesthetic score>4.75 and watermark probability<0.5). There are total 8,563,753 data instances in this dataset. And the corresponding aesthetic score and watermark score are also included. \nNoted: watermark score in the table means the probability of the existence of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/guangyil/laion-coco-aesthetic.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"novelai3","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shareAI/novelai3","creator_name":"shareAI","creator_url":"https://huggingface.co/shareAI","description":"\n\t\n\t\t\n\t\n\t\n\t\tNovelai3 Images\n\t\n\nThe Novelai3 text-to-image distillation dataset contains over 30GB of anime-related (text, image) pairs, intended solely for educational and research purposes! It must not be used for any illicit activities.\n\n\t\n\t\t\n\t\n\t\n\t\tProduction Method\n\t\n\nThe dataset was created through automated browser operations, repeatedly clicking the \"generate image\" button and saving the resulting images. Over the course of a month, approximately 38GB of (image, text instruction) pairs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shareAI/novelai3.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","n<1K","Image"],"keywords_longer_than_N":true},
	{"name":"vais1000","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/vais1000","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of VAIS-1000\n\t\n\nofficial announcement: https://vais.vn/vi/tai-ve/hts_for_vietnamese (dead)\nmirror: https://github.com/undertheseanlp/text_to_speech/tree/run/data/vais1000/raw\nsmall only 1h40min audio - 1 speaker (female northern accent) - 1k samples\npre-process: none\nneed to do: check misspelling, restore foreign words phonetised to vietnamese\nusage with HuggingFace:\n# pip install -q \"datasets[audio]\"\nfrom datasets import load_dataset\nfrom torch.utils.data import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/vais1000.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"fake_voices","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unfake/fake_voices","creator_name":"Unfake","creator_url":"https://huggingface.co/unfake","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Fake Voices\n\t\n\nThis dataset contains deepfakes in Brazilian Portuguese created with XTTS model.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThe dataset was created using the XTTS model, which is a Text-to-Speech model pre-trained in several languages including Portuguese. \nIn order to generate the mentioned deepfakes, the model was fed with recordings from the CETUC Corpus, \nmade available by Fala Brasil Group. It contains speeches from 101 speakers, totaling 140 hours of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unfake/fake_voices.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Portuguese","mit","1B<n<10B"],"keywords_longer_than_N":true},
	{"name":"fake_voices","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unfake/fake_voices","creator_name":"Unfake","creator_url":"https://huggingface.co/unfake","description":"\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Fake Voices\n\t\n\nThis dataset contains deepfakes in Brazilian Portuguese created with XTTS model.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nThe dataset was created using the XTTS model, which is a Text-to-Speech model pre-trained in several languages including Portuguese. \nIn order to generate the mentioned deepfakes, the model was fed with recordings from the CETUC Corpus, \nmade available by Fala Brasil Group. It contains speeches from 101 speakers, totaling 140 hours of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unfake/fake_voices.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Portuguese","mit","1B<n<10B"],"keywords_longer_than_N":true},
	{"name":"LSVSC","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/LSVSC","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of LSVSC dataset (novel large-scale Vietnamese speech corpus)\n\t\n\nofficial announcement: https://www.mdpi.com/2079-9292/13/5/977\nofficial download: https://drive.google.com/drive/folders/1tiPKaIOC7bt6isv5qFqf61O_2jFK8ZOI\n100h, 57k samples\npre-process: see my code: https://github.com/phineas-pta/fine-tune-whisper-vi/blob/main/misc/clean-lsvsc.py\nneed to do: check misspelling, restore foreign words phonetised to vietnamese\nusage with HuggingFace:\n# pip install -q‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/LSVSC.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mj-v52-redux","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/bghira/mj-v52-redux","creator_name":"Bagheera","creator_url":"https://huggingface.co/bghira","description":"","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","unlicense","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Tatoeba-Speech-Irish","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/Tatoeba-Speech-Irish","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nSynthetic audio dataset, created using Azure text-to-speech service.\nThe bilingual text is a portion of the Tatoeba dataset, consisting of 1,983 text segments.\nThe dataset consists of two sets of audio data, one with a female voice (OrlaNeural) and the other with a male voice (ColmNeural).\nThe speech data comprises approximately 2 hours and 39 minutes (02:39:31) spread across 3,966 utterances.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDataset({\n    features: ['audio', 'text_ga'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Tatoeba-Speech-Irish.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","Irish","English"],"keywords_longer_than_N":true},
	{"name":"SPEC","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wjpoom/SPEC","creator_name":"Wujian Peng","creator_url":"https://huggingface.co/wjpoom","description":"\n\t\n\t\t\n\t\t[CVPR 2024] SPEC Benchmark: Evaluating VLMs in Fine-grained and Compositional Understanding\n\t\n\nintroduced in the CVPR 2024 paper Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding\nCode | ü§ó Paper | üìñ arXiv\nTo evaluate the understanding capability of visual-language models on fine-grained concepts, we propose a new benchmark, SPEC, \nwhich consists of six distinct subsets, distributed across the dimensions of Size, Position, Existence, and Count.\nEach‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wjpoom/SPEC.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Wikimedia-Speech-Irish","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/Wikimedia-Speech-Irish","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nSynthetic audio dataset, created using Azure text-to-speech service.\nThe bilingual text is a portion of the Wikimedia dataset, consisting of 7,545 text segments.\nThe dataset includes two sets of audio data, one with a female voice (OrlaNeural) and the other with a male voice (ColmNeural).\nThe speech data comprises approximately 34 hours and 23 minutes (34:23:12) spread across 15,090 utterances.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDataset({\n    features: ['audio', 'text_ga'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/Wikimedia-Speech-Irish.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","Irish","English"],"keywords_longer_than_N":true},
	{"name":"midjourney-detailed-prompts","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MohamedRashad/midjourney-detailed-prompts","creator_name":"Mohamed Rashad","creator_url":"https://huggingface.co/MohamedRashad","description":"\n\t\n\t\t\n\t\tMidjourney: Detailed Prompts\n\t\n\nThis dataset is my attempt in providing a high quality text-to-image dataset with detailed and several levels of prompting for images.\nHope it helps anyone in his research ^^\n\n\t\n\t\t\n\t\tThanks goes to ...\n\t\n\n\nmidjourney-images dataset\nQwen-VL-Max for descriping images in huge detail.\nCommand R for long & short prompt generation\n\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"StreetView360AtoZ","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/everettshen/StreetView360AtoZ","creator_name":"Everett Shen","creator_url":"https://huggingface.co/everettshen","description":"StreetView 360X is a dataset containing 6342 360 degree equirectangular street view images randomly sampled and downloaded from Google Street View. It is published as part of the paper \"StreetView360X: A Location-Conditioned Latent Diffusion Model for Generating Equirectangular 360 Degree Street Views\" (Princeton COS Senior Independent Work by Everett Shen). Images are labelled with their capture coordinates and panorama IDs. Scripts for extending the dataset (i.e. fetching additional images)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/everettshen/StreetView360AtoZ.","first_N":5,"first_N_keywords":["text-to-image","image-classification","image-to-text","image-feature-extraction","mit"],"keywords_longer_than_N":true},
	{"name":"ClArTTS","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ClArTTS","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe present a speech corpus for Classical Arabic Text-to-Speech (ClArTTS) to support the development of end-to-end TTS systems for Arabic. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The final ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 kHz.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nA typical data point comprises the name of the audio file, called‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ClArTTS.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Arabic","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ClArTTS","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ClArTTS","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe present a speech corpus for Classical Arabic Text-to-Speech (ClArTTS) to support the development of end-to-end TTS systems for Arabic. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The final ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 kHz.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nA typical data point comprises the name of the audio file, called‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ClArTTS.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Arabic","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"pony-speech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/synthbot/pony-speech","creator_name":"Synthbot Anon","creator_url":"https://huggingface.co/synthbot","description":"synthbot/pony-speech dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"pony-speech","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/synthbot/pony-speech","creator_name":"Synthbot Anon","creator_url":"https://huggingface.co/synthbot","description":"synthbot/pony-speech dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"pony-singing","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/synthbot/pony-singing","creator_name":"Synthbot Anon","creator_url":"https://huggingface.co/synthbot","description":"synthbot/pony-singing dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"pony-singing","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/synthbot/pony-singing","creator_name":"Synthbot Anon","creator_url":"https://huggingface.co/synthbot","description":"synthbot/pony-singing dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"limmits-2024","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iAkashPaul/limmits-2024","creator_name":"Akash","creator_url":"https://huggingface.co/iAkashPaul","description":"\n\t\n\t\t\n\t\n\t\n\t\tIndic TTS dataset\n\t\n\n7 languages from IISC's LIMMITS Challenge 2024\n\n\t\n\t\t\n\t\n\t\n\t\tRoadmap\n\t\n\nTo use this for training VALLE-X & VoiceBox based TTS models\n\n\t\n\t\t\n\t\n\t\n\t\tFetch the tars directly\n\t\n\nwget -O 'Bengali_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/Bengali_F.tar.gz'\nwget -O 'Chhattisgarhi_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/Chhattisgarhi_F.tar.gz'\nwget -O 'English_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/English_F.tar.gz'\nwget -O‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iAkashPaul/limmits-2024.","first_N":5,"first_N_keywords":["text-to-speech","English","Bengali","Hindi","Kannada"],"keywords_longer_than_N":true},
	{"name":"limmits-2024","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iAkashPaul/limmits-2024","creator_name":"Akash","creator_url":"https://huggingface.co/iAkashPaul","description":"\n\t\n\t\t\n\t\n\t\n\t\tIndic TTS dataset\n\t\n\n7 languages from IISC's LIMMITS Challenge 2024\n\n\t\n\t\t\n\t\n\t\n\t\tRoadmap\n\t\n\nTo use this for training VALLE-X & VoiceBox based TTS models\n\n\t\n\t\t\n\t\n\t\n\t\tFetch the tars directly\n\t\n\nwget -O 'Bengali_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/Bengali_F.tar.gz'\nwget -O 'Chhattisgarhi_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/Chhattisgarhi_F.tar.gz'\nwget -O 'English_F.tar.gz' 'https://ee.iisc.ac.in/limmitsdataset/download/English_F.tar.gz'\nwget -O‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iAkashPaul/limmits-2024.","first_N":5,"first_N_keywords":["text-to-speech","English","Bengali","Hindi","Kannada"],"keywords_longer_than_N":true},
	{"name":"danbooru2023_index","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepghs/danbooru2023_index","creator_name":"DeepGHS","creator_url":"https://huggingface.co/deepghs","description":"Tar index files for nyanko7/danbooru2023.\nYou can download images from both nyanko7/danbooru2023 and deepghs/danbooru_newest with cheesechaser.\nfrom cheesechaser.datapool import DanbooruNewestDataPool\n\npool = DanbooruNewestDataPool()\n\n# download danbooru original images from 7200000-7201000, to directory /data/danbooru_original\npool.batch_download_to_directory(\n    resource_ids=range(7200000, 7201000),\n    dst_dir='/data/danbooru_original',\n    max_workers=12,\n)\n\n","first_N":5,"first_N_keywords":["image-classification","image-to-image","text-to-image","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"new-spider-HM","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HusnaManakkot/new-spider-HM","creator_name":"HUSNA M","creator_url":"https://huggingface.co/HusnaManakkot","description":"\n\t\n\t\t\n\t\tDataset Card for Spider\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HusnaManakkot/new-spider-HM.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"surtr_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shiroup/surtr_arknights","creator_name":"ÊàèÊµ™","creator_url":"https://huggingface.co/shiroup","description":"\n\t\n\t\t\n\t\tDataset of surtr/„Çπ„É´„Éà/Âè≤Â∞îÁâπÂ∞î (Arknights)\n\t\n\nThis is the dataset of surtr/„Çπ„É´„Éà/Âè≤Â∞îÁâπÂ∞î (Arknights), containing 500 images and their tags.\nThe core tags of this character are horns, red_hair, long_hair, purple_eyes, bangs, breasts, very_long_hair, hair_between_eyes, medium_breasts, large_breasts, demon_horns, which are pruned in this dataset.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shiroup/surtr_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"DND-Monster-Diffusion","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TravisHudson/DND-Monster-Diffusion","creator_name":"Travis Hudson","creator_url":"https://huggingface.co/TravisHudson","description":"TravisHudson/DND-Monster-Diffusion dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"zh-taiwan","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ivanzhu109/zh-taiwan","creator_name":"IvanZhu","creator_url":"https://huggingface.co/ivanzhu109","description":"ivanzhu109/zh-taiwan dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Chinese","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Entity-Imagen","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Entity-Imagen","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"This dataset is a replication evaluation set from Re-Imagen (https://arxiv.org/pdf/2209.14491.pdf). The dataset is aimed for evaluating text-to-image generation on rare visual entities.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"hand_imgs","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Kiwinicki/hand_imgs","creator_name":"Dawid Koterwas","creator_url":"https://huggingface.co/Kiwinicki","description":"Images of hands downloaded from internet and captioned with BLIP2\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"updated-dataset","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HusnaManakkot/updated-dataset","creator_name":"HUSNA M","creator_url":"https://huggingface.co/HusnaManakkot","description":"\n\t\n\t\t\n\t\tDataset Card for Spider\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nThe leaderboard can be seen at https://yale-lily.github.io/spider\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HusnaManakkot/updated-dataset.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"fav_images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ehristoforu/fav_images","creator_name":"Evgeniy Hristoforu","creator_url":"https://huggingface.co/ehristoforu","description":"\n\t\n\t\t\n\t\tfav_images\n\t\n\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"haispider","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HusnaManakkot/haispider","creator_name":"HUSNA M","creator_url":"https://huggingface.co/HusnaManakkot","description":"\n\t\n\t\t\n\t\tDataset Card for Spider\n\t\n\nTable of Contents\nDataset Description\nDataset Summary\nSupported Tasks and Leaderboards\nLanguages\nDataset Structure\nData Instances\nData Fields\nData Splits\nDataset Creation\nCuration Rationale\nSource Data\nAnnotations\nPersonal and Sensitive Information\nConsiderations for Using the Data\nSocial Impact of Dataset\nDiscussion of Biases\nOther Known Limitations\nAdditional Information\nDataset Curators\nLicensing Information\nCitation Information\nContributions\nDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HusnaManakkot/haispider.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"omega-multimodal","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/omegalabsinc/omega-multimodal","creator_name":"OMEGA Labs, Inc.","creator_url":"https://huggingface.co/omegalabsinc","description":"\n\t\n\t\t\n\t\tOMEGA Labs Bittensor Subnet: Multimodal Dataset for AGI Research\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe OMEGA Labs Bittensor Subnet Dataset is a groundbreaking resource for accelerating Artificial General Intelligence (AGI) research and development. This dataset, powered by the Bittensor decentralized network, aims to be the world's largest multimodal dataset, capturing the vast landscape of human knowledge and creation.\nWith over 1 million hours of footage and 30 million+ 2-minute video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omegalabsinc/omega-multimodal.","first_N":5,"first_N_keywords":["video-text-to-text","video-classification","image-classification","image-to-text","image-to-video"],"keywords_longer_than_N":true},
	{"name":"MemeDatasetForStudy","keyword":"text-to-image","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/SassyRong/MemeDatasetForStudy","creator_name":"SassyRong","creator_url":"https://huggingface.co/SassyRong","description":"SassyRong/MemeDatasetForStudy dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","wtfpl","10K<n<100K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"cml-tts","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/cml-tts","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for CML-TTS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG).\nCML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The dataset includes recordings in Dutch, German, French, Italian, Polish‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/cml-tts.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Dutch","French","German"],"keywords_longer_than_N":true},
	{"name":"cml-tts","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/cml-tts","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for CML-TTS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG).\nCML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The dataset includes recordings in Dutch, German, French, Italian, Polish‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/cml-tts.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Dutch","French","German"],"keywords_longer_than_N":true},
	{"name":"hoshino_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/hoshino_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of hoshino (Blue Archive)\n\t\n\nThis is the dataset of hoshino (Blue Archive), containing 150 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n150\nDownload\nRaw data with meta information.\n\nraw-stage3\n420\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n477\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/hoshino_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","n<1K","Image","Text"],"keywords_longer_than_N":true},
	{"name":"momoi_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/momoi_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of momoi (Blue Archive)\n\t\n\nThis is the dataset of momoi (Blue Archive), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n560\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n668\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/momoi_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"kokona_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/kokona_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of kokona (Blue Archive)\n\t\n\nThis is the dataset of kokona (Blue Archive), containing 150 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n150\nDownload\nRaw data with meta information.\n\nraw-stage3\n416\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n505\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/kokona_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"iroha_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/iroha_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of iroha (Blue Archive)\n\t\n\nThis is the dataset of iroha (Blue Archive), containing 150 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n150\nDownload\nRaw data with meta information.\n\nraw-stage3\n416\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n482\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/iroha_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"midori_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/midori_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of midori (Blue Archive)\n\t\n\nThis is the dataset of midori (Blue Archive), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n556\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n676\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/midori_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"mari_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/mari_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of mari (Blue Archive)\n\t\n\nThis is the dataset of mari (Blue Archive), containing 150 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n150\nDownload\nRaw data with meta information.\n\nraw-stage3\n409\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n475\nDownload\n3-stage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/mari_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"unicorn_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/unicorn_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of unicorn (Azur Lane)\n\t\n\nThis is the dataset of unicorn (Azur Lane), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n522\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n597\nDownload\n3-stage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/unicorn_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"laffey_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/laffey_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of laffey (Azur Lane)\n\t\n\nThis is the dataset of laffey (Azur Lane), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n516\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n581\nDownload\n3-stage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/laffey_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"lego_sets_latest","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/merve/lego_sets_latest","creator_name":"merve","creator_url":"https://huggingface.co/merve","description":"\n\t\n\t\t\n\t\tA small datasets of Lego Sets with BLIP-2 Generated Captions\n\t\n\nThis can be used to fine-tune SDXL with data-efficient fine-tuning techniques like DreamBooth.\nExample image üëá \n\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"serina_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/serina_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of serina (Blue Archive)\n\t\n\nThis is the dataset of serina (Blue Archive), containing 194 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n194\nDownload\nRaw data with meta information.\n\nraw-stage3\n528\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n611\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/serina_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"le_malin_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/le_malin_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of le_malin (Azur Lane)\n\t\n\nThis is the dataset of le_malin (Azur Lane), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n530\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n598\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/le_malin_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"nagato_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/nagato_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of nagato (Azur Lane)\n\t\n\nThis is the dataset of nagato (Azur Lane), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n520\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n584\nDownload\n3-stage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/nagato_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"shimakaze_azurlane","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/shimakaze_azurlane","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of shimakaze (Azur Lane)\n\t\n\nThis is the dataset of shimakaze (Azur Lane), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n555\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n602\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/shimakaze_azurlane.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"yoshimi_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/yoshimi_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of yoshimi (Blue Archive)\n\t\n\nThis is the dataset of yoshimi (Blue Archive), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n564\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n660\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/yoshimi_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"tsurugi_bluearchive","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/tsurugi_bluearchive","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of tsurugi (Blue Archive)\n\t\n\nThis is the dataset of tsurugi (Blue Archive), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\nraw-stage3\n531\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n667\nDownload‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/tsurugi_bluearchive.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"english_dialects","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/english_dialects","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for \"english_dialects\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 31 hours of transcribed high-quality audio of English sentences recorded by 120 volunteers speaking with different accents of the British Isles. The dataset is intended for linguistic analysis as well as use for speech technologies. The speakers self-identified as native speakers of Southern England, Midlands, Northern England, Welsh, Scottish and Irish varieties of English.\nThe recording scripts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/english_dialects.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"english_dialects","keyword":"text-to-audio","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/english_dialects","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for \"english_dialects\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 31 hours of transcribed high-quality audio of English sentences recorded by 120 volunteers speaking with different accents of the British Isles. The dataset is intended for linguistic analysis as well as use for speech technologies. The speakers self-identified as native speakers of Southern England, Midlands, Northern England, Welsh, Scottish and Irish varieties of English.\nThe recording scripts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/english_dialects.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","English","cc-by-sa-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"google-chilean-spanish","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/google-chilean-spanish","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for Tamil Speech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 7 hours of transcribed high-quality audio of Chilean Spanish sentences recorded by 31 volunteers. The dataset is intended for speech technologies. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\ntext-to-speech, text-to-audio: The dataset can be used to train a model for Text-To-Speech (TTS).\nautomatic-speech-recognition‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/google-chilean-spanish.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Spanish","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"google-chilean-spanish","keyword":"text-to-audio","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ylacombe/google-chilean-spanish","creator_name":"Yoach Lacombe","creator_url":"https://huggingface.co/ylacombe","description":"\n\t\n\t\t\n\t\tDataset Card for Tamil Speech\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of 7 hours of transcribed high-quality audio of Chilean Spanish sentences recorded by 31 volunteers. The dataset is intended for speech technologies. \nThe data archives were restructured from the original ones from OpenSLR to make it easier to stream.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\ntext-to-speech, text-to-audio: The dataset can be used to train a model for Text-To-Speech (TTS).\nautomatic-speech-recognition‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ylacombe/google-chilean-spanish.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Spanish","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pykeio/librivox-tracks","creator_name":"pyke.io","creator_url":"https://huggingface.co/pykeio","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\n","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Achinese","Afrikaans","Ancient Greek (to 1453)"],"keywords_longer_than_N":true},
	{"name":"ch_en_arknights","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AppleHarem/ch_en_arknights","creator_name":"AppleHarem","creator_url":"https://huggingface.co/AppleHarem","description":"\n\t\n\t\t\n\t\tDataset of ch'en (Arknights)\n\t\n\nThis is the dataset of ch'en (Arknights), containing 200 images and their tags.\nImages are crawled from many sites (e.g. danbooru, pixiv, zerochan ...), the auto-crawling system is powered by DeepGHS Team(huggingface organization).(LittleAppleWebUI)\n\n\t\n\t\t\nName\nImages\nDownload\nDescription\n\n\n\t\t\nraw\n200\nDownload\nRaw data with meta information.\n\n\nraw-stage3\n490\nDownload\n3-stage cropped raw data with meta information.\n\n\nraw-stage3-eyes\n605\nDownload\n3-stage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AppleHarem/ch_en_arknights.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"TEdBench_plusplus","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIML-TUDA/TEdBench_plusplus","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","description":"\n\t\n\t\t\n\t\tTEdBench++\n\t\n\nThis dataset contains the TEdBench++ an image-to-image benchmark for text-based generative models. It contains original images (originals) and edited images (LEdits++) for benchmarking. tedbench++.csv contains the text-based edit instructions for the respective original image and parameters to reproduce the edited images with LEdits++.\nconsider citing our work\n@inproceedings{brack2024ledits,\n  year = { 2024 },\n  booktitle = { Proceedings of the IEEE/CVF Conference on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/TEdBench_plusplus.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Vibravox_dummy","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zinc75/Vibravox_dummy","creator_name":"√âric Bavu","creator_url":"https://huggingface.co/zinc75","description":"zinc75/Vibravox_dummy dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","audio-classification","text-to-speech","speaker-identification"],"keywords_longer_than_N":true},
	{"name":"M-BEIR","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/M-BEIR","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tUniIR: Training and Benchmarking Universal Multimodal Information Retrievers (ECCV 2024)\n\t\n\nüåê Homepage | ü§ó Model(UniIR Checkpoints) | ü§ó Paper | üìñ arXiv  | GitHub\nHow to download the M-BEIR Dataset\n\n\t\n\t\t\n\t\n\t\n\t\tüîîNews\n\t\n\n\nüî•[2023-12-21]: Our M-BEIR Benchmark is now available for use.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nM-BEIR, the Multimodal BEnchmark for Instructed Retrieval, is a comprehensive large-scale retrieval benchmark designed to train and evaluate unified multimodal retrieval‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/M-BEIR.","first_N":5,"first_N_keywords":["text-retrieval","text-to-image","image-to-text","visual-question-answering","English"],"keywords_longer_than_N":true},
	{"name":"Pexels-400k","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jovianzm/Pexels-400k","creator_name":"Jovian","creator_url":"https://huggingface.co/jovianzm","description":"\n\t\n\t\t\n\t\tPexels 400k\n\t\n\nDataset of 400,476 videos, their thumbnails, viewcounts, explicit classification, and caption.\nNote: The Pexels-320k dataset in the repo is this dataset, with videos <10s removed.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","English"],"keywords_longer_than_N":true},
	{"name":"Pexels-400k","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jovianzm/Pexels-400k","creator_name":"Jovian","creator_url":"https://huggingface.co/jovianzm","description":"\n\t\n\t\t\n\t\tPexels 400k\n\t\n\nDataset of 400,476 videos, their thumbnails, viewcounts, explicit classification, and caption.\nNote: The Pexels-320k dataset in the repo is this dataset, with videos <10s removed.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","English"],"keywords_longer_than_N":true},
	{"name":"DataComp-12M","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/DataComp-12M","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for DataComp-12M\n\t\n\n\n\nThis dataset contains a 12M subset of DataComp-1B-BestPool.\nWe distribute the image url-text samples and metadata under a standard Creative Common CC-BY-4.0 license. The individual images are under their own copyrights.\nImage-text models trained on DataComp-12M are significantly better than on CC-12M/YFCC-15M as well as DataComp-Small/Medium.\nDataComp-12M was introduced in MobileCLIP paper and along with the reinforced dataset DataCompDR-12M.\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/DataComp-12M.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","Image"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/OpenVid-1M","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\".\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512. Furthermore, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/OpenVid-1M","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\".\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512. Furthermore, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/OpenVid-1M","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\".\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512. Furthermore, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestone-horizon/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/OpenVid-1M","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n  \n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\".\nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nAll videos in the OpenVid-1M dataset have resolutions of at least 512√ó512. Furthermore, we‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestone-horizon/OpenVid-1M.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"MM_Math","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/THU-KEG/MM_Math","creator_name":"Knowledge Engineer Group @ Tsinghua University","creator_url":"https://huggingface.co/THU-KEG","description":"\n\t\n\t\t\n\t\tMM_Math Datasets\n\t\n\nWe introduce our multimodal mathematics dataset, MM-MATH,. \nThis dataset is collected from real middle school exams in China, and all the math problems are open-ended to evaluate the mathematical problem-solving abilities of current multimodal models. MM-MATH is annotated with fine-grained three-dimensional labels: difficulty, grade, and knowledge points. The difficulty level is determined based on the average scores of student exams, the grade labels are derived‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THU-KEG/MM_Math.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-aesthetics","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-anatomy","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-architecture","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/photo-typography","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\t\n\t\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photochat_plus","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/passing2961/photochat_plus","creator_name":"Young-Jun Lee","creator_url":"https://huggingface.co/passing2961","description":"\n\t\n\t\t\n\t\tDataset Card for PhotoChat++\n\t\n\n\nüö® Disclaimer: All models and datasets are intended for research purposes only.\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nPhotoChat++ is a publicly available multi-modal dialogue dataset, an extended version of PhotoChat. PhotoChat++ contains six intent labels, a triggering sentence, an image description, and salient information (e.g., ‚Äúwords‚Äù or ‚Äúphrases‚Äù) to invoke the image-sharing behavior. The purpose of this dataset is to thoroughly assess the image-sharing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/passing2961/photochat_plus.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","conversational","monolingual","PhotoChat"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-aesthetics","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-aesthetics","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-aesthetics","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Aesthetics Dataset\n\t\n\nPulled from Pexels in 2023.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-architecture","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-architecture","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-architecture","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\tPhoto Architecture Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of buildings and unique architecture. Some buildings may be copyrighted, though training is currently understood to fall under fair-use.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-anatomy","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-anatomy","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-anatomy","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\tPhoto Anatomy Dataset\n\t\n\nPulled from Pexels in 2023.\nImages contain a majority of images of \"people holding things\".\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\nCaptions were created with CogVLM.\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/photo-typography","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"photo-typography","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/photo-typography","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tPhoto Typography Dataset\n\t\n\nPulled from Pexels in 2023.\nA majority of these images contain text, captioned with CogVLM.\nImage filenames may be used as captions, or, the parquet table contains the same values.\nThis dataset contains the full images.\n","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","photographs","photos","image-data"],"keywords_longer_than_N":true},
	{"name":"Speech-MASSIVE_vie","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/Speech-MASSIVE_vie","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tVietnamse subset of the Speech-MASSIVE dataset\n\t\n\nextracted from:\n\nhttps://huggingface.co/datasets/FBK-MT/Speech-MASSIVE\nhttps://huggingface.co/datasets/FBK-MT/Speech-MASSIVE-test\n\nmy extraction code: https://github.com/phineas-pta/fine-tune-whisper-vi/blob/main/misc/load-speechmassive.py\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"BibleMMS_vie","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/BibleMMS_vie","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tVietnamse subset of the BibleMMS dataset\n\t\n\nextracted from: https://huggingface.co/datasets/Flux9665/BibleMMS\nmy extraction code: https://github.com/phineas-pta/fine-tune-whisper-vi/blob/main/misc/load-biblemms.py\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"NSFW-T2I","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zxbsmk/NSFW-T2I","creator_name":"Jun","creator_url":"https://huggingface.co/zxbsmk","description":"\n\t\n\t\t\n\t\n\t\n\t\tIntroduction (Version 1)\n\t\n\nAbout 38k image-text pairs(10k from LAION and 28k from nsfw_detect), and captions are generated by LLaVA-NeXT with prompt \"Describe the photo in detail (attributes of person)\".\nThe \"txt\" column shown in the dataset viewer is originated from LAION, not the captions yielded by LLaVA-NeXT.\n\n\t\n\t\t\n\t\n\t\n\t\tCaption Codes\n\t\n\npretrained = \"lmms-lab/llama3-llava-next-8b\"\nmodel_name = \"llava_llama3\"\ndevice = \"cuda:2\"\ndevice_map = \"auto\"\ntokenizer, model‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zxbsmk/NSFW-T2I.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"arabic_quranic_asr","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sadique5/arabic_quranic_asr","creator_name":"Sadique Abdullah","creator_url":"https://huggingface.co/Sadique5","description":"\n\t\n\t\t\n\t\tDataset details\n\t\n\nThis dataset contains quran recitations of every ayats or verses. Also contains 10k unique words from quran.\n\n\t\n\t\t\n\t\tDataset Purpose\n\t\n\nThis dataset can be used to train ASR models that can be used for teaching beginners to recite quran. It can also be used for training TTS models that produces quran recitations in a way so that beginners can easily learn.\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Arabic","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"AnyWord-3M","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stzhao/AnyWord-3M","creator_name":"steve z","creator_url":"https://huggingface.co/stzhao","description":"Dataset from AnyText: Multilingual Visual Text Generation And Editing.\nDataset description from Anytext Team:\nCurrently, there is a relative scarcity of public datasets for text generation tasks, especially those involving non-Latin script languages. To address this, we introduce a large-scale multilingual dataset called AnyWord-3M. The images in this dataset are sourced from Noah-Wukong, LAION-400M, and OCR recognition datasets such as ArT, COCO-Text, RCTW, LSVT, MLT, MTWI, ReCTS, etc. These‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stzhao/AnyWord-3M.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"violine_dataset","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zahidpichen/violine_dataset","creator_name":"ninjagamer","creator_url":"https://huggingface.co/zahidpichen","description":"zahidpichen/violine_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-audio","mit","< 1K","soundfolder","Audio"],"keywords_longer_than_N":true},
	{"name":"HumanRefiner","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/HumanRefiner","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tHumanRefiner\n\t\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nWelcome to the official code repository for the paper \"HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance.\"\nIn this project, we introduce AbHuman, the first large-scale benchmark focused on anatomical anomalies. The benchmark consists of 56K synthesized human images, each annotated with 147K human anomalies in 18 different categories. Based on this, we developed HumanRefiner, a novel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/HumanRefiner.","first_N":5,"first_N_keywords":["text-to-image","English","mit","Image","Text"],"keywords_longer_than_N":true},
	{"name":"persian_tts_stt","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SmartGitiCorp/persian_tts_stt","creator_name":"Smart Giti Corporation","creator_url":"https://huggingface.co/SmartGitiCorp","description":"This dataset contains more than 10k records and 15 hours of clear vocal voice aligning with text in csv file.\n","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Persian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"persian_tts_stt","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SmartGitiCorp/persian_tts_stt","creator_name":"Smart Giti Corporation","creator_url":"https://huggingface.co/SmartGitiCorp","description":"This dataset contains more than 10k records and 15 hours of clear vocal voice aligning with text in csv file.\n","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Persian","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"midjourney-niji-1m-llavanext","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/midjourney-niji-1m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for midjourney-niji-1m-llavanext\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nThis is a dataset of 2,079,886 synthetic captions for 1,039,943 images from midjourney-v6-520k-raw and nijijourney-v6-520k-raw. The captions were produced using https://huggingface.co/lmms-lab/llama3-llava-next-8b inferenced in float16 after tags were generated with wd-swinv2-tagger-v3, followed by cleanup and shortening with Meta-Llama-3-8B.\nAll images with metadata are available as MozJPEG encoded‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/midjourney-niji-1m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"VietMed_unlabeled","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/VietMed_unlabeled","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of VietMed (Vietnamese speech data in medical domain) unlabeled set\n\t\n\nofficial announcement: https://arxiv.org/abs/2404.05659\nofficial download: https://huggingface.co/datasets/leduckhai/VietMed\nthis repo contains the unlabeled set: 966h - 230k samples\ni also gather the metadata: see info.csv\nmy extraction code: https://github.com/phineas-pta/fine-tune-whisper-vi/blob/main/misc/vietmed-unlabeled.py\nneed to do: check misspelling, restore foreign words phonetised to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/VietMed_unlabeled.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VietMed_labeled","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doof-ferb/VietMed_labeled","creator_name":"Phan Tu·∫•n Anh","creator_url":"https://huggingface.co/doof-ferb","description":"\n\t\n\t\t\n\t\tunofficial mirror of VietMed (Vietnamese speech data in medical domain) labeled set\n\t\n\nofficial announcement: https://arxiv.org/abs/2404.05659\nofficial download: https://huggingface.co/datasets/leduckhai/VietMed\nthis repo contains the labeled set: 9.2k samples\ni also gather the metadata: see info.csv\nmy extraction code: https://github.com/phineas-pta/fine-tune-whisper-vi/blob/main/misc/vietmed-labeled.py\nneed to do: check misspelling, restore foreign words phonetised to vietnamese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doof-ferb/VietMed_labeled.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Vietnamese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"COSTG_v1","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QinLei086/COSTG_v1","creator_name":"Qin Lei","creator_url":"https://huggingface.co/QinLei086","description":"\n\t\n\t\t\n\t\tCOSTG_v1\n\t\n\nThis dataset has been introduced in the ECCV 2024 paper titled Enriching Information and Preserving Semantic Consistency in Expanding Curvilinear Object Segmentation Datasets.\nIt encompasses three data types (directories), namely angiography (angiography coronary artery disease), crack, and retina (retinal vessels), which collectively contain six public datasets as described in the paper.\nAdditionally, the unprocessed_json directory includes raw, unprocessed textual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QinLei086/COSTG_v1.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"dreambench_plus","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yuangpeng/dreambench_plus","creator_name":"Yuang Peng","creator_url":"https://huggingface.co/yuangpeng","description":"\nThe image above shows the visualization of data distribution. (a) Images comparison between DreamBench and DreamBench++ using t-SNE. (b) Image and prompt distribution of DreamBench++.\nDreamBench++ contains three categories: live subject (animals and humans), object, and style, with a total of 150 images. Among them, 120 images are photorealistic and 30 are non-photorealistic. Each image has 9 corresponding prompts, each with varying levels of difficulty, including 4 prompts for photorealistic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuangpeng/dreambench_plus.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","Image","Text"],"keywords_longer_than_N":true},
	{"name":"danbooru2023-florence2-caption","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption","creator_name":"Shih-Ying Yeh","creator_url":"https://huggingface.co/KBlueLeaf","description":"\n\t\n\t\t\n\t\tDanbooru2023 - Florence2 Caption dataset\n\t\n\nThis dataset contains captions of danbooru2023 images generated by microsoft/Florence-2-large \nI use original one with  task token\n\n\t\n\t\t\n\t\tFormat\n\t\n\nparquet:\n\nkey: the danbooru id of the image\nparsed: parsed florence 2 output of the image\n\n\n\t\n\t\t\n\t\tStat\n\t\n\n\n\t\n\t\t\n\t\tMORE_DETAILED_CAPTION\n\t\n\n\nEntries: 7,438,449\nOutput Tokens (Min/Max/Mean/Median):\nFlan T5 Tokenizer: 19/736/120/114\nDFN CLIP Tokenizer: 19/826/108.7/103\nQwen2 Tokenizer:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KBlueLeaf/danbooru2023-florence2-caption.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"svg-stack-labeled","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/svg-stack-labeled","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tSvg Stack - Labeled\n\t\n\nThis dataset consists of the central storage for all datasets related to the SVG Stack dataset. I found it to be lovely, detailed, and of decent to extremely good quality upon observing many different icons and logos during the labeling process.\nThis is the central dataset, and is currently UNDER CONSTRUCTION.  Use with caution, and be aware that the format HAS NOT been frozen. I will make a post announcing when I freeze this dataset, as that will also be the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-labeled.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pixel_sorting","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/1aurent/pixel_sorting","creator_name":"LaureŒ∑t Fainsin","creator_url":"https://huggingface.co/1aurent","description":"\n\t\n\t\t\n\t\tPixel Sorting\n\t\n\n\nThis dataset contains urls to images with a \"pixel sorting\" style.\n\n\t\n\t\t\n\t\tColumns Details\n\t\n\n\nurl: the url to the image\nhash: the blake2 hash of the image\ncaption: a CogVLM caption\n\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"neoneye/simon-arc-rle-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"e621-2024-webp-4Mpixel","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NebulaeWis/e621-2024-webp-4Mpixel","creator_name":"Nebulae","creator_url":"https://huggingface.co/NebulaeWis","description":"Dataset Description:\nThis is a processed version of the https://huggingface.co/datasets/boxingscorpionbagel/e621-2024 dataset, primarily prepared for personal use in future projects.\nTherefore, for licensing and other legal information, please refer to the original project.\nYou can directly download tar file,or use https://deepghs.github.io/hfutils/main/api_doc/index/fetch.html#hf-tar-file-download to download anything .webp file you want.\nThe following modifications have been made to the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NebulaeWis/e621-2024-webp-4Mpixel.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"Libriheavy-HQ","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mythicinfinity/Libriheavy-HQ","creator_name":"Mythic Infinity","creator_url":"https://huggingface.co/mythicinfinity","description":"\n\t\n\t\t\n\t\tDataset Card for Libriheavy-HQ\n\t\n\n\n\nLibriheavy: a 50,000 hours ASR corpus with punctuation casing \nand context. Libriheavy is a labeled version of Libri-Light.\nLibriheavy-HQ replaces the default Libri-Light audio files with the highest quality available versions from librivox \nwithout re-encoding them. \nIn most cases, this consists an upgrade of the source audio from a 64kbps .mp3 to a 128kbps .mp3.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the Libriheavy-HQ dataset, adapted for the datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mythicinfinity/Libriheavy-HQ.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Libriheavy-HQ","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mythicinfinity/Libriheavy-HQ","creator_name":"Mythic Infinity","creator_url":"https://huggingface.co/mythicinfinity","description":"\n\t\n\t\t\n\t\tDataset Card for Libriheavy-HQ\n\t\n\n\n\nLibriheavy: a 50,000 hours ASR corpus with punctuation casing \nand context. Libriheavy is a labeled version of Libri-Light.\nLibriheavy-HQ replaces the default Libri-Light audio files with the highest quality available versions from librivox \nwithout re-encoding them. \nIn most cases, this consists an upgrade of the source audio from a 64kbps .mp3 to a 128kbps .mp3.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the Libriheavy-HQ dataset, adapted for the datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mythicinfinity/Libriheavy-HQ.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MiraData","keyword":"text-to-image","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/MiraData","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"\n\t\n\t\t\n\t\tMiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions\n\t\n\n\nXuan Ju1*, Yiming Gao1*, Zhaoyang Zhang1*#, Ziyang Yuan1,  Xintao Wang1,  Ailing Zeng, Yu Xiong, Qiang Xu,  Ying Shan1 \n1ARC Lab, Tencent PCG  2The Chinese University of Hong Kong  *Equal Contribution  #Project Lead\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nVideo datasets play a crucial role in video generation such as Sora.\nHowever, existing text-video datasets often fall short when it comes to handling long video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/MiraData.","first_N":5,"first_N_keywords":["image-to-video","text-to-image","text-to-video","video-classification","English"],"keywords_longer_than_N":true},
	{"name":"MiraData","keyword":"text-to-video","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/MiraData","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"\n\t\n\t\t\n\t\tMiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions\n\t\n\n\nXuan Ju1*, Yiming Gao1*, Zhaoyang Zhang1*#, Ziyang Yuan1,  Xintao Wang1,  Ailing Zeng, Yu Xiong, Qiang Xu,  Ying Shan1 \n1ARC Lab, Tencent PCG  2The Chinese University of Hong Kong  *Equal Contribution  #Project Lead\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nVideo datasets play a crucial role in video generation such as Sora.\nHowever, existing text-video datasets often fall short when it comes to handling long video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/MiraData.","first_N":5,"first_N_keywords":["image-to-video","text-to-image","text-to-video","video-classification","English"],"keywords_longer_than_N":true},
	{"name":"svg-stack-tmp-alpha-chunk","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tSvg Stack Labeled - Temporary Split Alpha ( Chunk )\n\t\n\nThis dataset is a chunk of SVG Stack Labeled, and was uploaded solely because I lacked reliable high-volume cloud storage at the time, and was going to make the dataset available on HuggingFace in any case.\nHowever, while I will be deleting the now defunct and unused chunks, this one received a few users, and I truly appreciate your usage of my dataets. Thus, this dataset will remain, even as the others perish.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/svg-stack-tmp-alpha-chunk.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"icons","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ActuallyTaylor/icons","creator_name":"Taylor Lineman","creator_url":"https://huggingface.co/ActuallyTaylor","description":"A set of macOS Icons from https://macosicons.com/. Labeled using GPT-4o.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-rle-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-rle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type.\nThe LLM learned some of the types fine. However histograms are causing problems.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. Since this is what my LLM is struggling with.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mabama-v6","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gitgato/mabama-v6","creator_name":"Git Porter","creator_url":"https://huggingface.co/gitgato","description":"\n\t\n\t\t\n\t\tDataset mabama-v6\n\t\n\nDataset de audio para entrenar modelos de s√≠ntesis de voz (TTS) en espa√±ol.\n\n\t\n\t\t\n\t\tDescripci√≥n\n\t\n\n\nContenido: Audios en espa√±ol con sus transcripciones textuales.\nHablante: mabama (voz √∫nica).\nDuraci√≥n total: X horas (ajusta este valor).\nTasa de muestreo: 22.05 kHz (o la que uses).\n\n\n\t\n\t\t\n\t\tEstructura\n\t\n\n","first_N":5,"first_N_keywords":["text-to-speech","Spanish","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"housey-home","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/housey-home","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tHousey Home v1 ( DEFUNCT )\n\t\n\nThe data has a flaw, it occurred during the initial synthesis. The erroneous fields have been removed, and the data is currently being selectively re-synthesized.\nAs of 07-15-2024, this dataset is now defunct. It will no longer receive stability, content, or null row fixes. However, as all images are sourced from generative AI models, open source ones at that, I have decided to make this MIT, and will perform tasks upon request if needed. Just ask.\n-<3\n","first_N":5,"first_N_keywords":["unconditional-image-generation","text-to-image","image-to-text","English","mit"],"keywords_longer_than_N":true},
	{"name":"e621-2024_index","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NebulaeWis/e621-2024_index","creator_name":"Nebulae","creator_url":"https://huggingface.co/NebulaeWis","description":"Index files of  https://huggingface.co/datasets/boxingscorpionbagel/e621-2024. \n","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","mit","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"housey-home-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MrOvkill/housey-home-v2","creator_name":"Samuel L Meyers","creator_url":"https://huggingface.co/MrOvkill","description":"\n\t\n\t\t\n\t\tHousey Home v2 - Like v1 never happened\n\t\n\nI was in the process of producing a fully synthetic dataset for ungrounded image generation using an unconventional combination of layers. As such, I needed a dataset of highly similar objects with 'themes'. In order to produce log(x, y) combinations of options in the final model. This is that dataset.\nThe initial ( 07/15/2024 ) release includes ~2k unique houses, each processed using a VQA, ybelkada/blip-vqa-base, to be precise.\nRelease 2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MrOvkill/housey-home-v2.","first_N":5,"first_N_keywords":["text-to-image","unconditional-image-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-recap","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nPhi-3 Vision„ÅßDense Captioning„Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\n‰∏ª„Ç≠„Éº„ÅØphotoid„Åß„Åô„ÅÆ„Åß„ÄÅCommonCatalog CC-BY„Å®ÁµêÂêà„Åô„Çã„Å™„Çä„Åó„Å¶‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ streaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-recap.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"filtered-coyo-700M-beta","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\n\t\n\t\t\n\t\tDataset Card for filterred-coyo-700M-beta\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe texts in the COYO-700M dataset consist of English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nEach instance in COYO-700M represents single image-text pair information with meta-attributes:\n{\n  'id': 841814333321,\n  'url': 'https://blog.dogsof.com/wp-content/uploads/2021/03/Image-from-iOS-5-e1614711641382.jpg',\n  'text': 'A Pomsky dog‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/filtered-coyo-700M-beta.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","zero-shot-classification","image-captioning","no-annotation"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Seed","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the seed dataset we used to conduct Google Search.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"kuon-audio","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lissette/kuon-audio","creator_name":"kuon","creator_url":"https://huggingface.co/lissette","description":"‰πÖËøúËØ≠Èü≥\nÊù•Ê∫ê‰∫éTVÂä®ÁîªÊèêÂèñÔºåËøõËøáuvrÂàÜÁ¶ªËÉåÊôØÈü≥Ôºå‰∫∫Â∑•Á≠õÈÄâË¥®ÈáèËæÉÂ•ΩÁöÑ„ÄÇ\n610‰∏™Áü≠ËØ≠Èü≥Ôºå1‰∏™Ê≠åÊõ≤\n","first_N":5,"first_N_keywords":["audio-to-audio","text-to-audio","Japanese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"crypto","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Abdulmoiz02/crypto","creator_name":"Abdul moiz","creator_url":"https://huggingface.co/Abdulmoiz02","description":"Abdulmoiz02/crypto dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"ChemistryImages","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LucasEllenberger/ChemistryImages","creator_name":"Lucas Ellenberger","creator_url":"https://huggingface.co/LucasEllenberger","description":"LucasEllenberger/ChemistryImages dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"danbooru2023-webp-4Mpixel_index","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepghs/danbooru2023-webp-4Mpixel_index","creator_name":"DeepGHS","creator_url":"https://huggingface.co/deepghs","description":"Index files of KBlueLeaf/danbooru2023-webp-4Mpixel.\nYou can download images from KBlueLeaf/danbooru2023-webp-4Mpixel with cheesechaser.\nfrom cheesechaser.datapool import DanbooruWebpDataPool\n\npool = DanbooruWebpDataPool()\n\n# download danbooru images with webp format, to directory /data/danbooru_webp\npool.batch_download_to_directory(\n    resource_ids=range(6000000, 6001000),\n    dst_dir='/data/danbooru_webp',\n    max_workers=12,\n)\n\n","first_N":5,"first_N_keywords":["image-classification","image-to-image","text-to-image","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"sd3-images","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leafspark/sd3-images","creator_name":"leafspark","creator_url":"https://huggingface.co/leafspark","description":"\n\t\n\t\t\n\t\tStable Diffusion 3 Images\n\t\n\nA dataset of 1:1 images generated by Stable Diffusion 3, through glif.app.\nFind the prompts in prompts.json, they correspond to the image based on number, for example the first element in the JSON array is x, then the image you're looking for is 0.jpg, and so on.\nPrompts sourced from MohamedRashad/midjourney-detailed-prompts.\n\n\t\n\t\t\n\t\n\t\n\t\tData\n\t\n\nYou can find enhanced images by Gigapixel AI in the enhanced folder; these are the same 1024x1024 quality, but‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/leafspark/sd3-images.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","n<1K","Image"],"keywords_longer_than_N":true},
	{"name":"A-Bench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/q-future/A-Bench","creator_name":"Q-Future","creator_url":"https://huggingface.co/q-future","description":"Project Page\n\n\n\t\n\t\t\n\t\tGlance at A-Bench Performance\n\t\n\nFor open-source models, LLaVA-NeXT (Qwen-110B) takes the first place. For closed-source models, GEMINI 1.5 PRO takes the first place.\n\n\n\t\n\t\t\n\t\n\t\n\t\tEvaluate your model on A-Bench\n\t\n\nFirst download the dataset and meta information from Huggingface.\nThe imgs.zip contains all the AI-generated images and Abench.json contains all the meta information including the img_path, questions, answers, and categories. The item of Abench.json is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/q-future/A-Bench.","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"kikongo-bible-asr","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Svngoku/kikongo-bible-asr","creator_name":"NIONGOLO Chrys F√©-Marty","creator_url":"https://huggingface.co/Svngoku","description":"\n\t\n\t\t\n\t\tKikongo Bible ASR\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Svngoku/kikongo-bible-asr.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Kongo","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"UltraEdit_500k","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BleachNick/UltraEdit_500k","creator_name":"Hans Zhao","creator_url":"https://huggingface.co/BleachNick","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BleachNick/UltraEdit_500k.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n [TPAMI 2025] MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\t\n\t\t\n\t\tüê≥ ChronoMagic Dataset\n\t\n\nChronoMagic with 2265 metamorphic time-lapse videos, each accompanied by a detailed caption. We released the subset of ChronoMagic used to train MagicTime. The dataset can be downloaded at HuggingFace Dataset, or you can download it with the following command. Some samples can be found‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"GLOBE","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MushanW/GLOBE","creator_name":"MushanW","creator_url":"https://huggingface.co/MushanW","description":"\n\t\n\t\t\n\t\tUpdated\n\t\n\n\n\t\n\t\t\n\t\tYou can use the V3 version, which includes more data, detailed speech quality annotations, and the original Common Voice IDs.\n\t\n\n\n\t\n\t\t\n\t\tAlternatively, you can use the V2 version to avoid the abnormal voice volume issue in this version.\n\t\n\n\n\t\n\t\t\n\t\tGlobe\n\t\n\nThe full paper can be accessed here: arXiv\nAn online demo can be accessed here: Github\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nThis paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushanW/GLOBE.","first_N":5,"first_N_keywords":["text-to-audio","automatic-speech-recognition","audio-to-audio","audio-classification","mozilla-foundation/common_voice_14_0"],"keywords_longer_than_N":true},
	{"name":"UltraEdit_Region_Based_100k","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BleachNick/UltraEdit_Region_Based_100k","creator_name":"Hans Zhao","creator_url":"https://huggingface.co/BleachNick","description":"\n\t\n\t\t\n\t\tBibtex citation\n\t\n\n@misc{zhao2024ultraeditinstructionbasedfinegrainedimage,\n      title={UltraEdit: Instruction-based Fine-Grained Image Editing at Scale}, \n      author={Haozhe Zhao and Xiaojian Ma and Liang Chen and Shuzheng Si and Rujie Wu and Kaikai An and Peiyu Yu and Minjia Zhang and Qing Li and Baobao Chang},\n      year={2024},\n      eprint={2407.05282},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2407.05282}, \n}\n\n","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"UltraEdit","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BleachNick/UltraEdit","creator_name":"Hans Zhao","creator_url":"https://huggingface.co/BleachNick","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BleachNick/UltraEdit.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","arxiv:2407.05282","doi:10.57967/hf/2481"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ja","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Ja\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nLLaVA-JP„ÇíÊîπËâØ„Åó„Åü„É¢„Éá„É´„Å´„Çà„ÇãÁ∞°Êòì„Å™Êó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥1„Å§\nLLaVA-JP„ÇíÊîπËâØ„Åó„Åü„É¢„Éá„É´„Å´„Çà„Çã„Åß„Åç„Çã„Å†„ÅëË©≥Á¥∞„Å™Êó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥3„Å§ (‰∫àÂÆö)\n\n\n\t\n\t\t\n\t\tSample Code\n\t\n\ndf2=pandas.read_csv(\"cc-by-ja.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nfor i,data in enumerate(tqdm(dataset)):\n    data[\"jpg\"].save(f\"/mnt/my_raid/pixart_jp/InternImgs/{i:09}.jpg\")\n\n    data_info.append({\n        \"height\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ja.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","Japanese","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"BibleMMS","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Flux9665/BibleMMS","creator_name":"Florian Lux","creator_url":"https://huggingface.co/Flux9665","description":"The Dataset associated with the Paper \"Meta Learning Text-to-Speech Synthesis in over 7000 Languages\" by Florian Lux, Sarina Meyer, Lyonel Behringer, Frank Zalkow, Phat Do, Matt Coler, Emanu√´l A. P. Habets and Ngoc Thang Vu (Interspeech 2024).\nWe generate 2000 spoken utterances per language using the subsets of the eBible dataset [1] that are under free licenses as the text input to the MMS TTS models [2]. \nThe languages associated with the following ISO-639-3 codes are represented in this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Flux9665/BibleMMS.","first_N":5,"first_N_keywords":["text-to-speech","mit","100K - 1M","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"living-room-passes","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Nfiniteai/living-room-passes","creator_name":"Nfinite","creator_url":"https://huggingface.co/Nfiniteai","description":"\n\t\n\t\t\n\t\tnfinite-living-room-passes\n\t\n\nVersion of the release: 1.0.0-alphaRelease date: 2024/06/17\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe nfinite-living-room-passes dataset is a dataset of images from 3D models for objects usually found in the living room space. 500 products are available, across 10500 images.  \nEach product has been rendered photo-realistically from a 3D model and is also available as a series of images depicting its normal map, its depth map, and some other information.Those 3D‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nfiniteai/living-room-passes.","first_N":5,"first_N_keywords":["depth-estimation","image-classification","image-segmentation","text-to-image","image-to-text"],"keywords_longer_than_N":true},
	{"name":"mabama-v","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aztro/mabama-v","creator_name":"Jose Omar Vieyra","creator_url":"https://huggingface.co/aztro","description":"aztro/mabama-v dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Spanish","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/common-canvas/commoncatalog-cc-by","creator_name":"CommonCanvas","creator_url":"https://huggingface.co/common-canvas","description":"\n\t\n\t\t\n\t\tDataset Card for CommonCatalog CC-BY\n\t\n\nThis dataset is a large collection of high-resolution Creative Common images (composed of different licenses, see paper Table 1 in the Appendix) collected in 2014 from users of Yahoo Flickr. \nThe dataset contains images of up to 4k resolution, making this one of the highest resolution captioned image datasets.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWe provide captions synthetic captions to approximately 100 million high‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/common-canvas/commoncatalog-cc-by.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10M - 100M","parquet"],"keywords_longer_than_N":true},
	{"name":"DeepFashion-MultiModal-Parts2Whole","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huanngzh/DeepFashion-MultiModal-Parts2Whole","creator_name":"zehuan-huang","creator_url":"https://huggingface.co/huanngzh","description":"\n\t\n\t\t\n\t\tDeepFashion MultiModal Parts2Whole\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis human image dataset comprising about 41,500 reference-target pairs. Each pair in this dataset includes multiple reference images, which encompass human pose images (e.g., OpenPose, Human Parsing, DensePose), various aspects of human appearance (e.g., hair, face, clothes, shoes) with their short textual labels, and a target image featuring the same individual (ID) in the same outfit‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huanngzh/DeepFashion-MultiModal-Parts2Whole.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"flickr8k-pt-br","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ Flickr8K Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nFlickr8K Portuguese Translation, a multimodal dataset for Portuguese image captioning with 8,000 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr8k-pt-br.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"flickr30k-pt-br","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ Flickr30K Translated for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nFlickr30K Portuguese Translated, a multimodal dataset for Portuguese image captioning with 31,014 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\nThe dataset is one of the results of work available at:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/flickr30k-pt-br.","first_N":5,"first_N_keywords":["text-generation","image-to-text","text-to-image","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ewe_bible_v1","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/ewe_bible_v1","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tEwe bible for Text-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Ewe","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ewe_bible_v1","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/ewe_bible_v1","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tEwe bible for Text-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Ewe","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"twi_bible_v1","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/twi_bible_v1","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tTwi Text-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","text-to-speech","text-to-audio","Twi"],"keywords_longer_than_N":true},
	{"name":"twi_bible_v1","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/twi_bible_v1","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tTwi Text-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","text-to-speech","text-to-audio","Twi"],"keywords_longer_than_N":true},
	{"name":"conceptual-captions-cc12m-llavanext","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/conceptual-captions-cc12m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for conceptual-captions-cc12m-llavanext\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of 21,930,344 synthetic captions for 10,965,172 images from conceptual_12m. In the interest of reproducibility, an archive found here on Huggingface was used (cc12m-wds). The captions were produced using llama3-llava-next-8b inferenced in float16, followed by cleanup and shortening with Meta-Llama-3-8B.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThe captions are in English.\n\n\t\n\t\t\n\t\n\t\n\t\tData Instances\n\t\n\nAn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/conceptual-captions-cc12m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"imagenetpp-laion-t2i","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nyu-dice-lab/imagenetpp-laion-t2i","creator_name":"NYU DICE Lab","creator_url":"https://huggingface.co/nyu-dice-lab","description":"Dataset Card for ImageNet++'s LAION Text-to-Image Split\n","first_N":5,"first_N_keywords":["mit","100K - 1M","webdataset","Image","Text"],"keywords_longer_than_N":true},
	{"name":"ewe_bible_v2_tts","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/ewe_bible_v2_tts","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tText-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","translation","Ewe"],"keywords_longer_than_N":true},
	{"name":"ewe_bible_v2_tts","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/ewe_bible_v2_tts","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tText-to-Speech\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","translation","Ewe"],"keywords_longer_than_N":true},
	{"name":"twi_bible_v2_tts","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/twi_bible_v2_tts","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tText-to-Speech Dataset\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Twi","Akan"],"keywords_longer_than_N":true},
	{"name":"twi_bible_v2_tts","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/worldboss/twi_bible_v2_tts","creator_name":"Theophilus Siameh","creator_url":"https://huggingface.co/worldboss","description":"\n\t\n\t\t\n\t\tText-to-Speech Dataset\n\t\n\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Twi","Akan"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-thai","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from sql-create-context.\n@misc{b-mc2_2023_sql-create-context,\n  title   = {sql-create-context Dataset},\n  author  = {b-mc2}, \n  year    = {2023},\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\n  note    = {This dataset was created by modifying data from the following sources: \\cite{zhongSeq2SQL2017, yu2018spider}.},\n}\n\n","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Thai","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"coco-stuff-geodiffusion","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KaiChen1998/coco-stuff-geodiffusion","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","description":"\n\t\n\t\t\n\t\n\t\n\t\tCOCO-Stuff-GeoDiffusion Dataset Card\n\t\n\nCOCO-Stuff-GeoDiffusion is the official dataset annotation file used to train GeoDiffusion on the COCO-Stuff dataset. We follow the official implementations of LAMA, while saving the merged results of COCO-instance and COCO-Stuff datasets within a single file of standard COCO format. Check detailed usage in our Github repo.\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","arxiv:2306.04607","üá∫üá∏ Region: US","layout-to-image"],"keywords_longer_than_N":false},
	{"name":"knives_and_time","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/calm-and-collected/knives_and_time","creator_name":"Niels van der Burg","creator_url":"https://huggingface.co/calm-and-collected","description":"\n\t\n\t\t\n\t\tKnives and Time, a broken dataset\n\t\n\nThis dataset is created with custom collection and semi-automated annotation for damaged images in public domain or Common Creatives 0 (CC0).\n\n\t\n\t\t\n\t\tCollection method\n\t\n\nData was manually collected for painting, images and photography with either:\n\nAbrasion.\nWater damage.\nOver composure.\nScratches.\nTorn.\nBurned.\nCut.\nPierced\n\n\n\t\n\t\t\n\t\tAnnotation method\n\t\n\nData was annotated with BLIP for an initial sweep with basic annotation. Followed by manual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/calm-and-collected/knives_and_time.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"coco-captions-pt-br","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ COCO Captions Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nCOCO Captions Portuguese Translation, a multimodal dataset for Portuguese image captioning with 123,287 images, each accompanied by five descriptive captions that have been\ngenerated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/coco-captions-pt-br.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v68","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v68","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v68.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"EUbookshop-Speech-Irish","keyword":"text-to-speech","license":"European Union Public License 1.1","license_url":"https://choosealicense.com/licenses/eupl-1.1/","language":"en","dataset_url":"https://huggingface.co/datasets/ymoslem/EUbookshop-Speech-Irish","creator_name":"Yasmin Moslem","creator_url":"https://huggingface.co/ymoslem","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nSynthetic audio dataset, created using Azure text-to-speech service.\nThe bilingual text is a portion of the EUbookshop dataset, consisting of 33,634 text segments.\nThe dataset includes two sets of audio data, one with a female voice (OrlaNeural) and the other with a male voice (ColmNeural).\nThe speech data comprises approximately 159 hours and 45 minutes (159:45:05) spread across 67,268 utterances.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nDataset({\n    features: ['audio'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ymoslem/EUbookshop-Speech-Irish.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","Irish","English"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-1B","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/Recap-DataComp-1B","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Recap-DataComp-1B\n\t\n\n\n\nRecap-DataComp-1B is a large-scale image-text dataset that has been recaptioned using an advanced LLaVA-1.5-LLaMA3-8B model to enhance the alignment and detail of textual descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\nOur paper aims to bridge this community effort, leveraging the powerful and open-sourced LLaMA-3, a GPT-4 level LLM.\nOur recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/Recap-DataComp-1B.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"libritts-r-filtered-speaker-descriptions","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/libritts-r-filtered-speaker-descriptions","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Annotated LibriTTS-R\n\t\n\nThis dataset is an annotated version of a filtered LibriTTS-R [1]. \nLibriTTS-R [1] is a sound quality improved version of the LibriTTS corpus which is a multi-speaker English corpus of approximately 960 hours of read English speech at 24kHz sampling rate, published in 2019. \nIn the text_description column, it provides natural language annotations on the characteristics of speakers and utterances, that have been generated using the Data-Speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/libritts-r-filtered-speaker-descriptions.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"libritts_r_filtered","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/parler-tts/libritts_r_filtered","creator_name":"Parler TTS","creator_url":"https://huggingface.co/parler-tts","description":"\n\t\n\t\t\n\t\tDataset Card for Filtered LibriTTS-R\n\t\n\nThis is a filtered version of LibriTTS-R. It has been filtered based on two sources:\n\nLibriTTS-R paper [1], which lists samples for which speech restoration have failed\nLibriTTS-P [2] list of excluded speakers for which multiple speakers have been detected.\n\nLibriTTS-R [1] is a sound quality improved version of the LibriTTS corpus which is a multi-speaker English corpus of approximately \n585 hours of read English speech at 24kHz sampling rate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/parler-tts/libritts_r_filtered.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"TexSD","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilopes/TexSD","creator_name":"Lopes","creator_url":"https://huggingface.co/ilopes","description":"We generate a high-resolution tileable texture dataset by prompting a pre-trained Stable Diffusion model. The dataset consists of 130 classes resulting in more than 10,000 unique textures. More details can be found in our paper.\nThe generated samples have been curated manually to select only the most relevant textures.\n\n","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"COCO_Person","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hamdy20002/COCO_Person","creator_name":"Abdelrahman Hamdy","creator_url":"https://huggingface.co/Hamdy20002","description":"This Dataset is a subsets of COCO 2017 -train- images using \"Crowd\" & \"person\" Labels With the First Caption of Each one\n\nCOCO Summary:\nThe COCO dataset is a comprehensive collection designed for object detection, segmentation, and captioning tasks.\nIt comprises over 200,000 images, encompassing a diverse array of everyday scenes and objects.\nEach image features multiple objects and scenes across 80 distinct object categories, all of which are annotated with descriptive image captions.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"civitai","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bigdata-pw/civitai","creator_name":"BIG data","creator_url":"https://huggingface.co/bigdata-pw","description":"\n\t\n\t\t\n\t\tCivitai Images\n\t\n\nImages+metadata from Civitai\nStats:\n\n~4.1M\n\nFormats:\n\nWebDataset\n10k per shard, ~2GB\njpg + json\n__key__ is Civitai image id\n\n\n\n\n\t\n\t\t\n\t\tNotes\n\t\n\n\n~464k images with no meta field are excluded, this is ~10% of images collected\nFiles for some entries are actually videos, these will be released separately\nCivitai extract metadata on upload, the exact fields in meta will depend on the UI used, some are common e.g. prompt, others are UI specific\nIncludes reaction data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigdata-pw/civitai.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","apache-2.0","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"dataset-mmb-v1","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gitgato/dataset-mmb-v1","creator_name":"Git Porter","creator_url":"https://huggingface.co/gitgato","description":"\n\t\n\t\t\n\t\tmabama-v6-audio Dataset\n\t\n\nEste dataset, mabama-v6-audio, est√° dise√±ado para tareas de text-to-speech (TTS) y contiene grabaciones de audio junto con sus correspondientes transcripciones en espa√±ol. Est√° dividido en tres partes: entrenamiento, prueba y validaci√≥n, permitiendo un desarrollo y evaluaci√≥n efectivos de modelos TTS.\n\n\t\n\t\t\n\t\tEstructura del Dataset\n\t\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nfile_name: Nombre del archivo de audio.\ntext: Transcripci√≥n del audio.\nspeaker_id: Identificador del‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gitgato/dataset-mmb-v1.","first_N":5,"first_N_keywords":["text-to-speech","Spanish","mit","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"mabama-v1-audio","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ovieyra21/mabama-v1-audio","creator_name":"Oma Vieyra","creator_url":"https://huggingface.co/ovieyra21","description":"ovieyra21/mabama-v1-audio dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Spanish","mit","10M<n<100M","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"pixelprose","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tomg-group-umd/pixelprose","creator_name":"Tom Goldstein's Lab at University of Maryland, College Park","creator_url":"https://huggingface.co/tomg-group-umd","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[ arXiv paper ]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n\n\t\n\t\t\n\t\n\t\n\t\t1. Details\n\t\n\nTotal number of image-caption pairs: 16,896,214 (16.9M)\n\n6,538,898 (6.5M) pairs in the split of CommonPool\n9,066,455 (9.1M) pairs in the split of CC12M\n1,290,861 (1.3M) pairs in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tomg-group-umd/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestones/pixelprose","creator_name":"rock","creator_url":"https://huggingface.co/lodestones","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayuka Jayawardhana‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestones/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"pixelprose","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lodestone-horizon/pixelprose","creator_name":"Horizon","creator_url":"https://huggingface.co/lodestone-horizon","description":"\n\t\n\t\t\n\t\n\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\n[[ arXiv paper ]]\nPixelProse is a comprehensive dataset of over 16M (million) synthetically generated captions, \nleveraging cutting-edge vision-language models (Gemini 1.0 Pro Vision) for detailed and accurate descriptions.\n@article{pixelprose24,\n  title   = {{From Pixels to Prose: A Large Dataset of Dense Image Captions}},\n  author  = {Vasu Singla and Kaiyu Yue and Sukriti Paul and Reza Shirkavand and Mayuka‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lodestone-horizon/pixelprose.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"multimodalpragmatic","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tongliuphysics/multimodalpragmatic","creator_name":"Tong Liu","creator_url":"https://huggingface.co/tongliuphysics","description":"\n\t\n\t\t\n\t\tMultimodal Pragmatic Jailbreak on Text-to-image Models\n\t\n\nProject page | Paper | Code\nThe Multimodal Pragmatic Unsafe Prompts (MPUP) is a dataset designed to assess the multimodal pragmatic safety in Text-to-Image (T2I) models. \nIt comprises two key sections: image_prompt, and text_prompt. \n\n\t\n\t\t\n\t\n\t\n\t\tDataset Usage\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDownloading the Data\n\t\n\nTo download the dataset, install Huggingface Datasets and then use the following command:\nfrom datasets import load_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tongliuphysics/multimodalpragmatic.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-pt","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emdemor/sql-create-context-pt","creator_name":"Eduardo Morais","creator_url":"https://huggingface.co/emdemor","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nEste dataset  √© uma vers√£o traduzida para o portugu√™s do dataset b-mc2/sql-create-context,\nque foi constru√≠do a partir dos datasets WikiSQL e Spider. Ele cont√©m exemplos de perguntas\nem portugu√™s, instru√ß√µes SQL CREATE TABLE e consultas SQL que respondem √†s perguntas\nutilizando a instru√ß√£o CREATE TABLE como contexto.\nO principal objetivo deste dataset √© ajudar modelos de linguagem natural  em portugu√™s a gerar consultas\nSQL precisas e contextualizadas, prevenindo a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emdemor/sql-create-context-pt.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Portuguese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"anime-bg","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/deepghs/anime-bg","creator_name":"DeepGHS","creator_url":"https://huggingface.co/deepghs","description":"Anime background and wallpaper images based on skytnt/anime-segmentation.\nArchived with indexed tar files, you can easily download any of these images with hfutils or dghs-imgutils library.\nFor example:\nfrom imgutils.resource import get_bg_image_file, random_image\n\n# get and download this background image file\n# return value should be the local path of given file\nget_bg_image_file('000001.jpg')\n\n# random select one background image from deepghs/anime-bg\nrandom_image()\n\nSee Documentation of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepghs/anime-bg.","first_N":5,"first_N_keywords":["text-to-image","cc0-1.0","1K - 10K","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"ru-filtered-web-captions","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DiTy/ru-filtered-web-captions","creator_name":"Dmitry Tishencko","creator_url":"https://huggingface.co/DiTy","description":"\n\t\n\t\t\n\t\tDiTy/ru-filtered-web-captions\n\t\n\n\n\t\n\t\t\n\t\tDataset summary\n\t\n\nThis is a translated Russian¬†part of the filtered web captions.\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n{\n    'caption': 'gladiator standing in a smoke with torch and sword',\n    'url': 'https://thumb9.shutterstock.com/display_pic_with_logo/78238/155376242/stock-photo-gladiator-standing-in-a-smoke-with-torch-and-sword-155376242.jpg',\n    'translated_caption': '–≥–ª–∞–¥–∏–∞—Ç–æ—Ä, —Å—Ç–æ—è—â–∏–π –≤ –¥—ã–º—É —Å —Ñ–∞–∫–µ–ª–æ–º –∏ –º–µ—á–æ–º'\n}\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ncaption:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DiTy/ru-filtered-web-captions.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","feature-extraction","image-feature-extraction","Russian"],"keywords_longer_than_N":true},
	{"name":"chuvash_voice","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexantonov/chuvash_voice","creator_name":"Alexander Antonov","creator_url":"https://huggingface.co/alexantonov","description":"\n\t\n\t\t\n\t\tHow to use\n\t\n\nWe recommend using our dataset in conjunction with the Common Voice Corpus. We have attempted to maintain a consistent structure.\nfrom datasets import load_dataset, DatasetDict, concatenate_datasets, Audio\n\ncomm_voice = DatasetDict()\ncomm_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"cv\", split=\"train+validation\", use_auth_token=True)\ncomm_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"cv\", split=\"test\", use_auth_token=True)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexantonov/chuvash_voice.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Chuvash","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"vintage-artworks-60k-captioned","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SilentAntagonist/vintage-artworks-60k-captioned","creator_name":"John Smith","creator_url":"https://huggingface.co/SilentAntagonist","description":"This is a dataset consisting of 60k vintage artworks from the 20th century, consisting of vintage pulp, sci-fi and pinup artworks from that era.\nThe dataset has short and long captions for each image, as well as resolution information. The large captions (large_caption column) were made with florence-2-large-ft, and then shortened with llama 3 8b (see short_caption column).\n","first_N":5,"first_N_keywords":["feature-extraction","image-classification","image-feature-extraction","text-to-image","image-to-text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"coral-tts","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CoRal-project/coral-tts","creator_name":"CoRal","creator_url":"https://huggingface.co/CoRal-project","description":"\n\t\n\t\t\n\t\tDataset Card for CoRal TTS\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset consists of two professional Danish speakers, female and male, recording roughly 17 hours of Danish speech each.\nThe dataset is part of the CoRal project which is funded by the Danish Innovation Fund.\nThe text data was selected by the Alexandra Institute (Github repo for the dataset creation) and consists of sentences from sundhed.dk, borger.dk, names of bus stops and stations, manually filtered Reddit comments, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CoRal-project/coral-tts.","first_N":5,"first_N_keywords":["text-to-speech","Danish","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v18","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v19","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v20","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v21","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v22","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"RSTeller_legacy","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","description":"\n\t\n\t\t\n\t\t‚õî Usage Warning\n\t\n\nThis is the legacy version of the RSTeller dataset and is not the latest version referenced in our paper. We are keeping it available here to provide the community with easy access to additional data.\nFor the details and the usage of the dataset, please refer to our github page.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find the dataset and our paper useful, please consider citing our paper:\n@article{ge2025rsteller,\n  title={RSTeller: Scaling up visual language modeling in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_legacy.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v23","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v24","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v25","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v25.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-histogram-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe counters are in the range 1-20.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe counters are in the range 1-50.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nThe counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nThe counters are in the range 1-200.\nHistogram.remove_other_colors() added.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nI forgot to update the range of the counters when doing comparisons.\nNow the counters are in the range 1-100.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nThe counters are in the range 1-200.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nThe counters are in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-histogram-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"librivox-tracks","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xacer/librivox-tracks","creator_name":"xacer","creator_url":"https://huggingface.co/xacer","description":"A dataset of all audio files uploaded to LibriVox before 26th September 2023.\nForked from https://huggingface.co/datasets/pykeio/librivox-tracks\nChanges:\n\nUsed archive.org metadata API to annotate rows with \"duration\" column\n\n","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","feature-extraction","Achinese","Afrikaans"],"keywords_longer_than_N":true},
	{"name":"layoutbench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/j-min/layoutbench","creator_name":"Jaemin Cho","creator_url":"https://huggingface.co/j-min","description":"\n\t\n\t\t\n\t\tLayoutBench\n\t\n\nRelease of LayoutBench dataset from Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation (CVPR 2024 Workshop)\nSee also LayoutBench-COCO for zero-shot evaluation on OOD layouts with real objects.\n[Project Page]\n[Paper]\nAuthors: \nJaemin Cho,\nLinjie Li,\nZhengyuan Yang,\nZhe Gan,\nLijuan Wang,\nMohit Bansal\n\n\t\n\t\t\n\t\tSummary\n\t\n\nLayoutBench is a diagnostic benchmark that examines layout-guided image generation models on arbitrary, unseen layouts.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/j-min/layoutbench.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"layoutbench-coco","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/j-min/layoutbench-coco","creator_name":"Jaemin Cho","creator_url":"https://huggingface.co/j-min","description":"\n\t\n\t\t\n\t\n\t\n\t\tLayoutBench-COCO\n\t\n\nRelease of LayoutBench-COCO dataset from Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation (CVPR 2024 Workshop)\nSee also LayoutBench for fine-grained evaluation on OOD layouts with CLEVR objects.\n[Project Page]\n[Paper]\nAuthors: \nJaemin Cho,\nLinjie Li,\nZhengyuan Yang,\nZhe Gan,\nLijuan Wang,\nMohit Bansal\n\n\t\n\t\t\n\t\n\t\n\t\tExamples layout inputs in 4 skills\n\t\n\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSummary\n\t\n\nLayoutBench-COCO is a diagnostic benchmark that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/j-min/layoutbench-coco.","first_N":5,"first_N_keywords":["text-to-image","English","mit","arxiv:2304.06671","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"vintage-photography-450k-high-quality-captions","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SilentAntagonist/vintage-photography-450k-high-quality-captions","creator_name":"John Smith","creator_url":"https://huggingface.co/SilentAntagonist","description":"This is a 450k image datastet focused on photography from the 20th century, and their analog aspect. Many of the images are in high resolution. This dataset currently has 20k images captioned with InternVL2 26B, and is a work in progress (I plan to caption the entire dataset and also have short captions for all of the images, compute is an issue for now).\n","first_N":5,"first_N_keywords":["image-classification","image-feature-extraction","text-to-image","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"data-juicer-t2v-optimal-data-pool","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tData-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development\n\t\n\n\n\t\n\t\t\n\t\tProject description\n\t\n\nThe emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality. \nHowever, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"data-juicer-t2v-optimal-data-pool","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\n\t\n\t\t\n\t\tData-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development\n\t\n\n\n\t\n\t\t\n\t\tProject description\n\t\n\nThe emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality. \nHowever, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/data-juicer-t2v-optimal-data-pool.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-pair-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nImage-size 1-10.\nCompare histograms between 2 images.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nImage-size 1-20.\nHistogram.remove_other_colors() exclude colors between two histograms.\nThese bigger images are causing problems for the model to learn.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nSmaller image sizes: width 1-20. height 1-5.\nThis is training much better.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSmaller image sizes: width 1-5. height 1-20.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nSlightly bigger image sizes: width 1-10. height 1-20.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-pair-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-task-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-task-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM validation loss went down and then continue to rise afterwards,\nso I guess the complexity of the dataset was too high.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are histograms. \nSmaller images. Here the image sizes are between 1 and 5 pixels.\nLet's see if the LLM does better on this one.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on pair comparisons finding color‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-task-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vibravox_enhanced_by_EBEN","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Cnam-LMSSC/vibravox_enhanced_by_EBEN","creator_name":"Laboratoire de M√©canique des Structures et des Syst√®mes Coupl√©s","creator_url":"https://huggingface.co/Cnam-LMSSC","description":"\n\t\n\t\t\n\t\tDataset Card\n\t\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis dataset features a speech-enhanced version of the test split from the speech_clean subset of the Vibravox Dataset.\nIt is not intended for training.\n\n\t\n\t\t\n\t\tEnhancement procedure\n\t\n\nThe Bandwidth extension task has been individually achieved for each sensor using configurable EBEN (arXiv link) models available at https://huggingface.co/Cnam-LMSSC/vibravox_EBEN_models.\n\n\t\n\t\t\n\t\tRessources\n\t\n\nResults for speech-to-phoneme and speaker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Cnam-LMSSC/vibravox_enhanced_by_EBEN.","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","audio-classification","text-to-speech","speaker-identification"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic-Pro","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Pro","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\t\n\t\t\n\t\tChronoMagic Dataset\n\t\n\nThis dataset contains time-lapse video-text pairs curated for metamorphic video generation. It was presented in the paper ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation.\nProject page: https://pku-yuangroup.github.io/ChronoMagic-Bench\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\ncat ChronoMagic-Pro.zip.part-* > ChronoMagic-Pro.zip \nunzip ChronoMagic-Pro.zip\n\n\n\n\n [NeurIPS D&B 2024 Spotlight] ChronoMagic-Bench: A Benchmark for Metamorphic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Pro.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic-Pro","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Pro","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\t\n\t\t\n\t\tChronoMagic Dataset\n\t\n\nThis dataset contains time-lapse video-text pairs curated for metamorphic video generation. It was presented in the paper ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation.\nProject page: https://pku-yuangroup.github.io/ChronoMagic-Bench\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\ncat ChronoMagic-Pro.zip.part-* > ChronoMagic-Pro.zip \nunzip ChronoMagic-Pro.zip\n\n\n\n\n [NeurIPS D&B 2024 Spotlight] ChronoMagic-Bench: A Benchmark for Metamorphic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Pro.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"ESLTTS","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MushanW/ESLTTS","creator_name":"MushanW","creator_url":"https://huggingface.co/MushanW","description":"\n\t\n\t\t\n\t\tESLTTS\n\t\n\nThe full paper can be accessed here: arXiv, IEEE Xplore.\n\n\t\n\t\t\n\t\tDataset Access\n\t\n\nYou can access this dataset through Huggingface or Google Driver or IEEE Dataport.\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nWith the progress made in speaker-adaptive TTS approaches, advanced approaches have shown a remarkable capacity to reproduce the speaker‚Äôs voice in the commonly used TTS datasets. However, mimicking voices characterized by substantial accents, such as non-native English speakers, is still‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MushanW/ESLTTS.","first_N":5,"first_N_keywords":["text-to-audio","automatic-speech-recognition","audio-to-audio","audio-classification","English"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic-ProH","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic-ProH","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\t\n\t\t\n\t\tChronoMagic Dataset\n\t\n\nThis dataset contains time-lapse video-text pairs curated for metamorphic video generation. It was presented in the paper ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation.\nProject page: https://pku-yuangroup.github.io/ChronoMagic-Bench\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\ncat ChronoMagic-ProH_part_* > ChronoMagic-ProH.zip \nunzip ChronoMagic-ProH.zip\n\n\n\n\n [NeurIPS D&B 2024 Spotlight] ChronoMagic-Bench: A Benchmark for Metamorphic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic-ProH.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic-ProH","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic-ProH","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\t\n\t\t\n\t\tChronoMagic Dataset\n\t\n\nThis dataset contains time-lapse video-text pairs curated for metamorphic video generation. It was presented in the paper ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation.\nProject page: https://pku-yuangroup.github.io/ChronoMagic-Bench\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\ncat ChronoMagic-ProH_part_* > ChronoMagic-ProH.zip \nunzip ChronoMagic-ProH.zip\n\n\n\n\n [NeurIPS D&B 2024 Spotlight] ChronoMagic-Bench: A Benchmark for Metamorphic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic-ProH.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"augmented-recap-datacomp-3m","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m","creator_name":"Ontocord.AI","creator_url":"https://huggingface.co/ontocord","description":"This is an experimental augmentation of about 3 million synthetic captions from Recap-Datacomp-1B. This dataset includes about 2 million multilingual captions. \nIt attempts to balance for gender stereotypes, added occupations, race, union membership, and religion to a subsample. We have also performed hair color and eye color balancing. It also includes some permutations of sentence orders, and modificaitons of the number of items (\"Two\" is changed to \"Three\", \"Four\", etc.)\nWe have also run‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ontocord/augmented-recap-datacomp-3m.","first_N":5,"first_N_keywords":["zero-shot-classification","text-retrieval","image-to-text","text-to-image","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"coyo-hd-11m-llavanext","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/coyo-hd-11m-llavanext","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for coyo-hd-11m-llavanext\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of 22,794,288 synthetic captions for 11,397,144 images from coyo-700m. The \"hd\" in the title refers to two aspects: high density and high definition. While large alt-text image pair datasets have many images, only a very small proportion of these images are in higher resolutions and have substantial concept density. For example, many of these datasets consist of more than 50% thumbnail sized or very‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/coyo-hd-11m-llavanext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"commoncanvas-cc-by-recap-2","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Recaptioning 2\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nFlorence-2-large-ft„ÅßDense Captioning (More detailed caption) „Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\nstreaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3.csv\")\n\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\",split=\"train\",streaming=True)\n\ndata_info=[]\nfor‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncanvas-cc-by-recap-2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"mabama-v6-audio","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ovieyra21/mabama-v6-audio","creator_name":"Oma Vieyra","creator_url":"https://huggingface.co/ovieyra21","description":"\n\t\n\t\t\n\t\tmabama-v6-audio Dataset\n\t\n\nEste dataset, mabama-v6-audio, est√° dise√±ado para tareas de text-to-speech (TTS) y contiene grabaciones de audio junto con sus correspondientes transcripciones en espa√±ol. Est√° dividido en tres partes: entrenamiento, prueba y validaci√≥n, permitiendo un desarrollo y evaluaci√≥n efectivos de modelos TTS.\n\n\t\n\t\t\n\t\tEstructura del Dataset\n\t\n\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nfile_name: Nombre del archivo de audio.\ntext: Transcripci√≥n del audio.\nspeaker_id: Identificador del‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ovieyra21/mabama-v6-audio.","first_N":5,"first_N_keywords":["text-to-speech","Spanish","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sbucaptions","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eaglewatch/sbucaptions","creator_name":"Yongwoo Jeong","creator_url":"https://huggingface.co/eaglewatch","description":"eaglewatch/sbucaptions dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic-Bench","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Bench","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n [NeurIPS D&B 2024 Spotlight] ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation \n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüí° Description\n\t\n\n\nRepository: Code, Page, Data\nPaper: https://huggingface.co/papers/2406.18522\nPoint of Contact: Shenghai Yuan\n\n\n\t\n\t\t\n\t\t‚úèÔ∏è Citation\n\t\n\nIf you find our paper and code useful in your research, please consider giving a star and citation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/ChronoMagic-Bench.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"SingleFloorPlans","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShazShoaib/SingleFloorPlans","creator_name":"Shaz Shoaib","creator_url":"https://huggingface.co/ShazShoaib","description":"ShazShoaib/SingleFloorPlans dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"SingleFloorDatasetDemo","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShazShoaib/SingleFloorDatasetDemo","creator_name":"Shaz Shoaib","creator_url":"https://huggingface.co/ShazShoaib","description":"ShazShoaib/SingleFloorDatasetDemo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Omnixxx","keyword":"text-to-image","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Abgrande615/Omnixxx","creator_name":"Adam Grande","creator_url":"https://huggingface.co/Abgrande615","description":"Abgrande615/Omnixxx dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","text-to-video","afl-3.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Omnixxx","keyword":"text-to-video","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Abgrande615/Omnixxx","creator_name":"Adam Grande","creator_url":"https://huggingface.co/Abgrande615","description":"Abgrande615/Omnixxx dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","text-to-video","afl-3.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"Plot2Code","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/Plot2Code","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"\n\t\n\t\t\n\t\tPlot2Code Benchmark\n\t\n\nPlot2Code benchmark is now open-sourced at huggingface (ARC Lab) and GitHub. More information can be found in our paper. \n\n\t\n\t\t\n\t\tWhy we need Plot2Code?\n\t\n\n\nüßê While MLLMs have demonstrated potential in visual contexts, their capabilities in visual coding tasks have not been thoroughly evaluated. Plot2Code offers a platform for comprehensive assessment of these models.\n\nü§ó To enable individuals to ascertain the proficiency of AI assistants in generating code that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TencentARC/Plot2Code.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"moondream2-coyo-5M-captions","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/isidentical/moondream2-coyo-5M-captions","creator_name":"Batuhan","creator_url":"https://huggingface.co/isidentical","description":"\n\t\n\t\t\n\t\tMoondream2 COYO-700M 5M subset captions\n\t\n\nA 5-million image, text pair subset of COYO-700M dataset, captioned with Moondream2 (rev=2024-05-08).  Captioning question is Write a long caption for this image given the alt text: {alt_text}.\n\n\t\n\t\t\n\t\tSampling conditions\n\t\n\nRandomly sampled from 5 million images from COYO-700M images that fit to the following filters:\nfilters = [\n    (\"width\", \">=\", 256),\n    (\"height\", \">=\", 256),\n    (\"aesthetic_score_laion_v2\", \">=\", 5.2)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/isidentical/moondream2-coyo-5M-captions.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","visual-question-answering","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-cellular-automaton-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nThe images are run length encoded (RLE).\nThe image sizes are between 4 and 10 pixels.\nCellular automaton types:\n\ngameoflife\nhighlife\nserviettes\ncave\nmaze\n\nNumber of CA steps, range 1-2.\nThe LLM was not happy about this dataset.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDisabled nowrap. It's only CAs that wraps around.\nThe image sizes are between 4 and 11 pixels.\nNumber of CA steps = 1.\n\n\t\n\t\t\n\t\tVersion 4‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-cellular-automaton-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"table-vqa","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmarkea/table-vqa","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","description":"\n\t\n\t\t\n\t\tDataset description\n\t\n\nThe table-vqa Dataset integrates images of tables from the dataset AFTdb (Arxiv Figure Table Database) curated by cmarkea. \nThis dataset consists of pairs of table images and corresponding LaTeX source code, with each image linked to an average of ten questions and answers. Half of the Q&A pairs are in English and the other half in French. These questions and answers were generated using Gemini 1.5 Pro and Claude 3.5 sonnet, making the dataset well-suited for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/table-vqa.","first_N":5,"first_N_keywords":["text-generation","text-to-image","image-to-text","table-question-answering","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v32","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v32.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v33","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v33.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v4-rev3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\nTEST rev3. I'm making yet another‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v4-rev3.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wong-kar-wai-BLIP-captions","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rayaanoidpr/wong-kar-wai-BLIP-captions","creator_name":"Rayaan Ghosh","creator_url":"https://huggingface.co/rayaanoidpr","description":"Dataset Card for Wong Kar Wai movie frame BLIP captions\n\nDataset built for finetuning Stable Diffusion for Fatima Fellowship application.\nThe original images were obtained from film-grab.com and captioned with the pre-trained BLIP model by SalesForce.\nFor each row the dataset contains image and text keys. image is a varying size PIL jpeg, and text is the accompanying text caption. Only a train split is provided.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Saudilang-Code-Switch-Corpus","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SDAIANCAI/Saudilang-Code-Switch-Corpus","creator_name":"SDAIA NCAI","creator_url":"https://huggingface.co/SDAIANCAI","description":"\n\t\n\t\t\n\t\tSCC - Saudilang Code-Switch Corpus\n\t\n\nThe National Center for Artificial Intelligence at the Saudi Data and Artificial Intelligence Authority (SDAIA), published the \"SCC\" dataset, which stands for \"Saudilang Code-Switch Corpus‚Äù.\nThis dataset contains a transcription of general conversations taken from a YouTube podcast \"Thmanyah\" that has been transcribed by the National Center for Artificial Intelligence in SDAIA. The data features three episodes covering different domains: investment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SDAIANCAI/Saudilang-Code-Switch-Corpus.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Arabic","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-shape-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-shape-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nDetect shape2x2 and shape3x3_center.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDetect shape2x2 and shape3x3_center and shape3x3_opposite.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nFocus on counting the unique number of colors. corners and diamond4.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nSame weight to all transformations.\nThe image sizes are between 1 and 30 pixels.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-shape-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"doodles-dataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/riversnow/doodles-dataset","creator_name":"Aditya Shankar","creator_url":"https://huggingface.co/riversnow","description":"A complete doodles dataset!\n","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"myanmar-speech-dataset-openslr-80","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chuuhtetnaing/myanmar-speech-dataset-openslr-80","creator_name":"Chuu Htet Naing","creator_url":"https://huggingface.co/chuuhtetnaing","description":"Please visit to the GitHub repository for other Myanmar Langauge datasets.\n\n\t\n\t\t\n\t\tMyanmar Speech Dataset (OpenSLR-80)\n\t\n\nThis dataset consists exclusively of Myanmar speech recordings, extracted from the larger multilingual OpenSLR dataset. \nFor the complete multilingual dataset and additional information, please visit the original dataset repository \nof OpenSLR HuggingFace page.\n\n\t\n\t\t\n\t\tOriginal Source\n\t\n\nOpenSLR is a site devoted to hosting speech and language resources, such as training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chuuhtetnaing/myanmar-speech-dataset-openslr-80.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Burmese","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v34","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v34.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v35","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v35.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v36","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v36.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v37","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v37.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v38","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v38.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v39","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v39.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v40","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v40.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v41","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v41.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v42","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v42.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v43","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v43.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OnePiece_Characters","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/khanitachi/OnePiece_Characters","creator_name":"Mohd Hozaifa Khan","creator_url":"https://huggingface.co/khanitachi","description":"\n\t\n\t\t\n\t\tDataset Card for One Piece Image Dataset\n\t\n\n\n\t\n\t\t\n\t\tMetadata\n\t\n\n\nIdentifier: OnePiece_Characters\nContributions: Mohd Hozaifa Khan\nLicense: apache-2.0\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains images of famous One Piece characters scraped from the web using simple image search. It includes approximately 15 images per character for a total of 20 characters. \n\n\t\n\t\t\n\t\tIntended Use\n\t\n\nThis dataset is intended for educational and research purposes, including:\n\nImage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/khanitachi/OnePiece_Characters.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"megalith-10m-florence2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aipicasso/megalith-10m-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","description":"\n\t\n\t\t\n\t\tMegalith-10M with Florence-2 Caption\n\t\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis reposity is the supplymentary of Megalith-10M.\nMegalith-10M is an CC-0 like image dataset. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/megalith-10m-florence2\")\n\n\n\t\n\t\n\t\n\t\tHow to get images\n\t\n\ngit lfs install\ngit clone https://huggingface.co/datasets/drawthingsai/megalith-10m‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/megalith-10m-florence2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"soa-full-florence2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aipicasso/soa-full-florence2","creator_name":"aipicasso","creator_url":"https://huggingface.co/aipicasso","description":"\n\t\n\t\t\n\t\tSmithsonian Open Access Dataset with Florence-2 Caption\n\t\n\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis dataset is made of soa-full.\nsoa-full is an CC-0 image dataset from Smithsonian Open Access. However, the dataset does not contain the image caption.\nTherefore, we caption the images by Florence 2.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"aipicasso/soa-full-florence2\")\n\n\n\t\n\t\n\t\n\t\tIntended Use\n\t\n\n\nResearch Vision & Language\nDevelop text-to-image model or image-to-text model.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aipicasso/soa-full-florence2.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v44","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v44.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v45","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v45.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v46","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v46.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v47","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v47.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v48","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v48.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v49","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v49.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v50","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v50.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v51","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v51.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v52","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v52.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v53","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v53.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v54","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v54.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v55","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v55.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v56","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v56.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v57","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v57.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v58","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v58.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v59","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v59","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v59.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v60","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v60","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v60.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v61","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v61","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v61.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v62","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v62","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v62.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"artbench-pd-256x256","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tDataset Card for ArtBench Public Domain 256x256\n\t\n\n\nÊó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ\nThis repository is the subset of ArtBench.\nArtBench is the dataset for historical arts such as Art Nouveau and Ukiyo-e.\nI picked up public domain images from ArtBench. Then, I create new dataset.\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nYou can use huggingface datasets to download the dataset.\nYou can also download the tar file.\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"alfredplpl/artbench-pd-256x256\")\n\n\n\t\n\t\t\n\t\tIntended Use‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/artbench-pd-256x256.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v63","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v63","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v63.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v64","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v64","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v64.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v65","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v65","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v65.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v66","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v66","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v66.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-image-v67","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-image-v67","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nHave dataset items that are somewhat evenly of each type. The LLM learned some of the types fine. However rotated images are causing problems.\nThe image sizes are between 1 and 10 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nHere the majority of dataset items are rotated images. Since this is what my LLM is struggling with.\nSmaller images. Here the image sizes are between 1 and 5 pixels.\nThis helped a lot on the validation loss.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nMain focus is now on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-image-v67.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v26","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v26.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v27","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v27.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v28","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v28","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v28.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v29","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v29","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v29.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v30","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v30","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v30.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v31","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v31","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v31.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v32","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v32","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v32.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v33","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v33","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v33.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v34","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v34","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v34.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v35","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v35","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v35.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v36","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v36","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v36.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v37","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v37","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v37.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-symmetry-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nFrom an input image, create a symmetric output image. image size 1-10.\n\nhstack(a b)\nhstack(a b c)\nvstack(a b)\nvstack(a b c)\n2x2(a b c d)\n\nThe abcd can be: orig, flipx, flipy, 180.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v38","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v38","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v38.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v39","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v39","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v39.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v40","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v40","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v40.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v41","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v41","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v41.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v42","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v42","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v42.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v43","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v43","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v43.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-scale-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nScale up/down an image by the x and y axis.\nmax_scale=3.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nRecognize what kind of scale transformation is happening.\nmax_scale=3.\nimage_size: 1-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nmax_scale=5.\nimage_size: 1-30.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ndifferent seed\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nmax_scale=7.\nimage_size: 1-30.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"CHUBS","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chen-yingfa/CHUBS","creator_name":"Yingfa Chen","creator_url":"https://huggingface.co/chen-yingfa","description":"\n\t\n\t\t\n\t\tCHUBS: A Large-Scale Dataset of Chu Bamboo Slip Script\n\t\n\n\n  Code | Paper (upcoming)\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is a large-scale dataset of Chu bamboo slip (CBS, Chinese: Ê•öÁÆÄ, chujian) script, an ancient Chinese script used during the Spring and Autumn period over 2,000 years ago. This dataset consists of two parts: \n\nThe main dataset where each example is an image and the corresponding text label. This part is contained in the glyphs.zip ZIP file.\nA character detection dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chen-yingfa/CHUBS.","first_N":5,"first_N_keywords":["image-classification","text-to-image","token-classification","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v44","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v44","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v44.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v18","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v19","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v45","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v45","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v45.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v46","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v46","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v46.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v20","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v21","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v22","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PIXELPROSE_HU","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU","creator_name":"Obscure Entropy","creator_url":"https://huggingface.co/Obscure-Entropy","description":"\n\t\n\t\t\n\t\tFrom Pixels to Prose: A Large Dataset of Dense Image Captions\n\t\n\nThis dataset is an extension of an existing image captioning dataset, enhanced for PixelProse and augmented with Hungarian translations. It provides a valuable resource for researchers and developers working on image captioning, especially those interested in PixelProse and cross-lingual applications. üåê\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Statistics\n\t\n\nWe report below the number of successfully fetched images and the number of failed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Obscure-Entropy/PIXELPROSE_HU.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Hungarian","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v47","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v47","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v47.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v48","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v48","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v48.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v49","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v49","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v49.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v23","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v50","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v50","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v50.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v51","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v51","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v51.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Mana-TTS","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MahtaFetrat/Mana-TTS","creator_name":"Mahta Fetrat","creator_url":"https://huggingface.co/MahtaFetrat","description":"\n\t\n\t\t\n\t\tManaTTS-Persian-Speech-Dataset\n\t\n\nManaTTS is the largest publicly available single-speaker Persian corpus, comprising over 114 hours of high-quality audio (sampled at 44.1 kHz). Released under the permissive CC-0 license, this dataset is freely usable for both educational and commercial purposes.  \nCollected from Nasl-e-Mana magazine, the dataset covers a diverse range of topics, making it ideal for training robust text-to-speech (TTS) models. The release includes a fully transparent‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MahtaFetrat/Mana-TTS.","first_N":5,"first_N_keywords":["Persian","cc0-1.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v52","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v52","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v52.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v53","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v53","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v53.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-erosion-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the erosion mask for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-20.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v54","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v54","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v54.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-dilation-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-dilation-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nCompute the dilation mask sum for each colored area, for all PixelConnectivity items.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v55","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v55","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v55.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"midjourney-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CortexLM/midjourney-v6","creator_name":"Cortex Foundation","creator_url":"https://huggingface.co/CortexLM","description":"\n\n\n\t\n\t\t\n\t\tMidJourney v6 Dataset by Bittensor Network (NetUID 19)\n\t\n\nDescription : This dataset was generated by Subnetwork 19 (Bittensor), utilizing the capabilities of MidJourney v6.\nDisclaimer: Image Attribution and Copyright Notice\nThe images included in this dataset have been sourced from an API. While every effort has been made to ensure compliance with copyright and intellectual property rights, Cortex Foundation cannot guarantee the absence of any copyright or intellectual property‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CortexLM/midjourney-v6.","first_N":5,"first_N_keywords":["text-to-image","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"lexica_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vera365/lexica_dataset","creator_name":"Xinyue Shen","creator_url":"https://huggingface.co/vera365","description":"\n\t\n\t\t\n\t\tLexicaDataset\n\t\n\nLexicaDataset is a large-scale text-to-image prompt dataset shared in [USENIX'24] Prompt Stealing Attacks Against Text-to-Image Generation Models.\nIt contains 61,467 prompt-image pairs collected from Lexica.\nAll prompts are curated by real users and images are generated by Stable Diffusion.\nData collection details can be found in the paper.\n\n\t\n\t\t\n\t\n\t\n\t\tData Splits\n\t\n\nWe randomly sample 80% of a dataset as the training dataset and the rest 20% as the testing dataset.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vera365/lexica_dataset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Products-10k-BLIP-captions","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/VikramSingh178/Products-10k-BLIP-captions","creator_name":"Vikramjeet  Singh","creator_url":"https://huggingface.co/VikramSingh178","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Products-10k BLIP CAPTIONS dataset consists of 10000 images of various products along with their automatically generated captions. The captions are generated using the BLIP (Bootstrapping Language-Image Pre-training) model. This dataset aims to aid in tasks related to image captioning, visual recognition, and product classification.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nDataset Name: Products-10k\nGenerated Captions Model: Salesforce/blip-image-captioning-large\nNumber‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VikramSingh178/Products-10k-BLIP-captions.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","text-to-image","English","mit"],"keywords_longer_than_N":true},
	{"name":"glaswegian_audio","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/divakaivan/glaswegian_audio","creator_name":"Ivan Ivanov","creator_url":"https://huggingface.co/divakaivan","description":"\n\t\n\t\t\n\t\tWARNING! Some derogatory slang is included in the dataset\n\t\n\nLatest total length: 120 minutes\nSource:\nScottish phrases 1-10, privately recorded audio, Limmy, and 1 episode of Glasga Da\nMetadata:\n\n\t\n\t\t\nColumn Name\nData Type\nInformation\n\n\n\t\t\nindex\nint\nUnique identifier for each row\n\n\nfile_name\nstring\nPath to the audio file\n\n\ntranscription\nstring\nText transcription of the audio\n\n\nlength_seconds\nfloat\nLength of the audio file in seconds\n\n\nsampling_rate\nint\nSampling rate of the audio file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/divakaivan/glaswegian_audio.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"nuimages-geodiffusion","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KaiChen1998/nuimages-geodiffusion","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","description":"\n\t\n\t\t\n\t\n\t\n\t\tnuImages-GeoDiffusion Dataset Card\n\t\n\nnuImages-GeoDiffusion is the official dataset annotation file used to train GeoDiffusion on the nuImages dataset. We follow the implementations of mmdetection3d, while saving the annotation results in standard COCO format. Check detailed usage in our Github repo.\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","arxiv:2306.04607","üá∫üá∏ Region: US","layout-to-image"],"keywords_longer_than_N":false},
	{"name":"midjourney-prompts-highquality","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gaodrew/midjourney-prompts-highquality","creator_name":"Andrew Gao","creator_url":"https://huggingface.co/gaodrew","description":"\n\nThank you to the Akash Network for sponsoring this project and providing A100s/H100s for compute!\n\n\t\n\t\t\n\t\tAbout\n\t\n\nA filtered version of the vivym/midjourney-prompts dataset\n\n\t\n\t\t\n\t\tFiltering criteria\n\t\n\n\ntop 10% in length (assuming that longer prompts = more effort and higher quality)\nused on an image to be upscaled (assuming that users are more likely to upscale an image that is aesthetically pleasing)\nused on midjourney version 5.0+\ndeduplicated\n\n\n\t\n\t\t\n\t\tRun yourself\n\t\n\nfilter.py script‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gaodrew/midjourney-prompts-highquality.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","10K - 100K","csv","Image"],"keywords_longer_than_N":true},
	{"name":"openai-voices","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leafspark/openai-voices","creator_name":"leafspark","creator_url":"https://huggingface.co/leafspark","description":"\n\t\n\t\t\n\t\tOpenAI Voices\n\t\n\nA collection of TTS samples collected from the OpenAI API and app.\nCurrently the following voices are available:\n\nSky\nJuniper\n\nThese are not labeled, however they are clean lossless audio files, and may contain noise from the model.\nPlease refer to sky/statement.wav for the highest quality voice sample!\n","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"commoncatalog-cc-by-ext","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext","creator_name":"Yasunori Ozaki","creator_url":"https://huggingface.co/alfredplpl","description":"\n\t\n\t\t\n\t\tCommonCatalog CC-BY Extention\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØCommonCatalog CC-BY„ÇíÊã°Âºµ„Åó„Å¶„ÄÅËøΩÂä†„ÅÆÊÉÖÂ†±„ÇíÂÖ•„Çå„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÅåËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\nPhi-3 Vision„ÅßDense Captioning„Åó„ÅüËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\nËã±Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥„ÇíPhi-3 Medium„ÅßÊó•Êú¨Ë™ûÂåñ„Åó„ÅüÊó•Êú¨Ë™û„Ç≠„É£„Éó„Ç∑„Éß„É≥\n\n‰∏ª„Ç≠„Éº„ÅØphotoid„Åß„Åô„ÅÆ„Åß„ÄÅCommonCatalog CC-BY„Å®ÁµêÂêà„Åô„Çã„Å™„Çä„Åó„Å¶‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\nstreaming=True„ÅßË™≠„ÅøËæº„ÇÄ„Å®Âêå„ÅòÈ†Ü„Å´Ë™≠„ÅøËæº„Åæ„Çå„Åæ„Åô„ÅÆ„Åß„Åù„Çå„ÇíÂà©Áî®„Åô„Çã„ÅÆ„Åå‰∏ÄÁï™Ê•Ω„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tLicense\n\t\n\nÁîªÂÉè„ÅåCC BY„Å™„Åü„ÇÅ„ÄÅ„Çè„Åã„Çä„ÇÑ„Åô„ÅèCC BY„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åü„Åå„Å£„Å¶„ÄÅÂïÜÁî®Âà©Áî®ÂèØËÉΩ„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\tSample Code\n\t\n\nimport pandas\nfrom datasets import load_dataset\n\ndf=pandas.read_csv(\"commoncatalog-cc-by-phi3-ja.csv\")\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alfredplpl/commoncatalog-cc-by-ext.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"M3CoT","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LightChen2333/M3CoT","creator_name":"Qiguang Chen","creator_url":"https://huggingface.co/LightChen2333","description":"\n ü¶Ñ M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought\n\n\n      \n      \n    \n    \n       \n      \n       \n       \n      \n      \n       \n      \n    \n      \n\n\n\n      \n    [ArXiv] | [ü§óHuggingFace] | [Website]\n    \n    \n\n\nüåü Any contributions via PRs, issues, emails or other methods are greatly appreciated.\n\n\t\n\t\t\n\t\tüî•News\n\t\n\n\nüéñÔ∏è Our work is accepted by ACL2024.\n\nüî• We have release benchmark on [ü§óHuggingFace].\n\nüî• The paper is also available on [ArXiv].\n\nüîÆ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LightChen2333/M3CoT.","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","reinforcement-learning","English","mit"],"keywords_longer_than_N":true},
	{"name":"Mixed_VQA_GenQA_EvalQA_1.5M","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","description":"This repository contains the data for the paper LOVA3: Learning to Visual Question Answering, Asking and Assessment.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tüéì Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuan Zhao and Pan Zhou and Difei Gao and Zechen Bai and Mike Zheng Shou},\n    booktitle={The Thirty-eighth Annual Conference on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/Mixed_VQA_GenQA_EvalQA_1.5M.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-combine-v56","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v56","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v56.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v57","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v57","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v57.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v58","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v58","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v58.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"tts-100-v1","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mastermani305/tts-100-v1","creator_name":"Manikandan","creator_url":"https://huggingface.co/mastermani305","description":"mastermani305/tts-100-v1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-audio","Tamil","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Microcosmos","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pixel-Dust/Microcosmos","creator_name":"Pixel Dust","creator_url":"https://huggingface.co/Pixel-Dust","description":"\n\t\n\t\t\n\t\tMicrocosmos Dataset\n\t\n\nThis dataset consists of a carefully curated collection of Creative Commons (CC0) images or similar, combined with both synthetic and human-generated captions. It was assembled to facilitate the training of diffusion models with a focus on efficiency and ethical data practices. The dataset was compiled over several months, highlighting the dedication to responsible data collection and management.\nDataset Details\nDataset Description\nMicrocosmos is designed to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pixel-Dust/Microcosmos.","first_N":5,"first_N_keywords":["text-to-image","English","cc0-1.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"simon-arc-combine-v69","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v69","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v69.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v70","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v70","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v70.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v71","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v71","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v71.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v72","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v72","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v72.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v73","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v73","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v73.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v74","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v74","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v74.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v75","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v75","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v75.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v76","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v76","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v76.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wit","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mteb/wit","creator_name":"Massive Text Embedding Benchmark","creator_url":"https://huggingface.co/mteb","description":"\n  WITT2IRetrieval\n  An MTEB dataset\n  Massive Text Embedding Benchmark\n\n\nRetrieve images based on multilingual descriptions.\n\n\t\n\t\t\n\n\n\n\n\t\t\nTask category\nt2i\n\n\nDomains\nEncyclopaedic, Written\n\n\nReference\nhttps://proceedings.mlr.press/v162/bugliarello22a/bugliarello22a.pdf\n\n\n\t\n\n\n\t\n\t\t\n\t\tHow to evaluate on this task\n\t\n\nYou can evaluate an embedding model on this dataset using the following code:\nimport mteb\n\ntask = mteb.get_tasks([\"WITT2IRetrieval\"])\nevaluator = mteb.MTEB(task)\n\nmodel =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mteb/wit.","first_N":5,"first_N_keywords":["visual-document-retrieval","image-to-text","text-to-image","derived","multilingual"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v77","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v77","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v77.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"garfield","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/terminusresearch/garfield","creator_name":"Terminus Research Group","creator_url":"https://huggingface.co/terminusresearch","description":"\n\n\t\n\t\t\n\t\tGarfield dataset\n\t\n\nCaptioned with InternVL2 40B over multiple 3090s.\nSome problems exist with these captions, but they're mostly accurate enough.\nA few strips were selected from the 1970s, early 1980s, but most are from the 1990s.\nThese are the highest resolution versions of these that were available at the time and are archived here for research purposes and preservation.\n\n\t\n\t\t\n\t\tFlux embeds for SimpleTuner\n\t\n\nTo save time for training Flux models on these samples, the 16ch Flux VAE‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/terminusresearch/garfield.","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\n\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 6\n\t\n\nOnly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v78","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v78","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v78.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v79","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v79","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v79.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v80","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v80","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v80.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v81","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v81","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v81.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-16.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v82","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v82","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v82.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v83","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v83","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v83.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v84","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v84","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v84.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v85","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v85","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v85.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"pexels-568k-internvl2","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/pexels-568k-internvl2","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for pexels-568k-internvl2\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 567,573 synthetic captions for the images found in ptx0/photo-concept-bucket. The captions were produced using OpenGVLab/InternVL2-40B-AWQ. The dataset was grounded for captioning using the tags originally listed.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text is in English, but occasionally text in images in other languages is transcribed.\n\n\t\n\t\t\n\t\tIntended Usage\n\t\n\nTraining text-to-image models and other machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/pexels-568k-internvl2.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v86","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v86","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v86.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v87","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v87","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v87.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM. The others are disabled.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v88","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v88","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v88.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v89","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v89","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v89.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v90","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v90","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v90.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v91","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v91","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v91.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v92","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v92","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v92.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v93","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v93","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v93.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"dreamlip_long_captions","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qidouxiong619/dreamlip_long_captions","creator_name":"Yifei Zhang","creator_url":"https://huggingface.co/qidouxiong619","description":"\n\t\n\t\t\n\t\tDataset Card for DreamLIP-30M\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDreamLIP-Long-Captions is a dataset consisting of ~30M image annotations, i.e. detailed long captions. In contrast with the curated style of other synthetic image caption annotations, DreamLIP-30M utilizes pre-trained Multi-modality Large Language Model to obtain detailed descriptions with an average length of 247. More precisely, the detailed descriptions are generated by asking the ShareGPT4V/InstructBLIP/LLava1.5 the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qidouxiong619/dreamlip_long_captions.","first_N":5,"first_N_keywords":["text-to-image","zero-shot-classification","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v94","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v94","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v94.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v95","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v95","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v95.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v96","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v96","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v96.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v97","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v97","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v97.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v98","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v98","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v98.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v99","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v99","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v99.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked area.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v100","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v100","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v100.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v101","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v101","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v101.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v102","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v102","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v102.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wolof_tts","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galsenai/wolof_tts","creator_name":"GalsenAI Lab","creator_url":"https://huggingface.co/galsenai","description":"\n\t\n\t\t\n\t\tWolof TTS\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a Wolof Text To Speech (TTS) dataset collected by Baamtu Datamation as part of the AI4D African language program. \nThe original dataset is hosted on Zenodo and it contains recordings from two (02) natif Wolof speakers (a male and female voice). Each speaker recored more than 20,000 sentences.\n\n\t\n\t\t\n\t\n\t\n\t\tSpeaking time:\n\t\n\n-- Male: 22h 28mn 41s\n-- Female: 18h 47mn 19s\n\nThe text dataset comes from news websites, Wikipedia and self‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/galsenai/wolof_tts.","first_N":5,"first_N_keywords":["text-to-speech","Wolof","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v103","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v103","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v103.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v104","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v104","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v104.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Sofie","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Sofie","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Samael1976","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Samael1976","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-Hampsty","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-Hampsty","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v105","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v105","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v105.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v106","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v106","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v106.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Legacy-Mage-JohnSlegers","keyword":"text-to-image","license":"The Unlicense","license_url":"https://choosealicense.com/licenses/unlicense/","language":"en","dataset_url":"https://huggingface.co/datasets/johnslegers/Legacy-Mage-JohnSlegers","creator_name":"John Slegers","creator_url":"https://huggingface.co/johnslegers","description":"\n\t\n\t\t\n\t\n\t\n\t\tDiffusionDBXL\n\t\n\nTODO\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-captioning","no-annotation","found"],"keywords_longer_than_N":true},
	{"name":"blip3-grounding-50m","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-grounding-50m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-GROUNDING-50M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-GROUNDING-50M dataset is designed to enhance the ability of Vision-Language Models (VLMs) to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions (e.g., \"the object to the left of the dog\"). Traditional datasets often lack the necessary granularity for such tasks, making it challenging for models to accurately localize and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-grounding-50m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v107","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v107","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v107.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v108","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v108","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v108.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"YoutubeThumbnails","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SarimOne/YoutubeThumbnails","creator_name":"Stream","creator_url":"https://huggingface.co/SarimOne","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nThis dataset is constructed for finetuning of SDXL model for thumbnails.\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use\n\t\n\n\n\n[More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SarimOne/YoutubeThumbnails.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v109","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v109","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v109.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"flickr-megalith-10m-internvl2-multi-caption","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for flickr-megalith-10m-internvl2-multi-caption\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is approximately 57.3 million synthetic captions for the images found in madebyollin/megalith-10m. \nIt includes the following captions:\n\nInternVL2 8B long captions (by CaptionEmporium)\nInternVL2 8B short captions (by CaptionEmporium)\nFlorence2 long captions (by aipicasso)\nFlorence2 short captions (by CaptionEmporium)\nShareCaptioner long captions (by drawthingsai)\nShareCaptioner short‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/flickr-megalith-10m-internvl2-multi-caption.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v110","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v110","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v110.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v111","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v111","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v111.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v112","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v112","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v112.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v113","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v113","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v113.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v114","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v114","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v114.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"vietnam-normalize-24k","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thanhkt/vietnam-normalize-24k","creator_name":"Tran Khanh Thanh","creator_url":"https://huggingface.co/thanhkt","description":"thanhkt/vietnam-normalize-24k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","text-to-speech","summarization","sentence-similarity","Vietnamese"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"kenyan_national_parks","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gikebe/kenyan_national_parks","creator_name":"Elizabeth Gikebe","creator_url":"https://huggingface.co/gikebe","description":"gikebe/kenyan_national_parks dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","text-classification","summarization","text-to-speech","question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v115","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v115","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v115.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v116","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v116","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v116.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v117","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v117","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v117.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v118","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v118","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v118.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v119","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v119","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v119.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v120","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v120","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v120.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v121","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v121","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v121.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v122","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v122","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v122.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v123","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v123","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v123.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"GPTInformal-Persian","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MahtaFetrat/GPTInformal-Persian","creator_name":"Mahta Fetrat","creator_url":"https://huggingface.co/MahtaFetrat","description":"\n\t\n\t\t\n\t\tGPTInformal Persian\n\t\n\n\nGPTInformal Persian is a Persian dataset of 6+ hours of audio and text pairs designed for speech synthesis and other speech-related tasks. The dataset has been collected, processed, and annotated as a part of the Mana-TTS project. For details on data processing pipeline and statistics, please refer to the paper in the Citation secition.\n\n\t\n\t\t\n\t\n\t\n\t\tData Source\n\t\n\nThe text for this dataset was generated using GPT4o, with prompts covering a wide range of subjects‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MahtaFetrat/GPTInformal-Persian.","first_N":5,"first_N_keywords":["cc0-1.0","1K - 10K","parquet","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recap-DataComp-100K","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nRecap-DataComp-100K is a subset of UCSC-VLAA/Recap-DataComp-1B. \nThis dataset aims to ease the development of vision-language models by providing a readily-available small collection of image-text pairs.\nUse this dataset for sanity checks, developing POCs, or other quick multimodal dev. For serious model training please refer to the original repo linked above.  \n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nAlways cite the original authors . I've copied their citation info here for your‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/Recap-DataComp-100K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v124","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v124","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v124.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v125","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v125","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v125.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v126","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v126","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v126.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v127","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v127","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v127.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v128","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v128","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v128.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v129","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v129","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v129.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v130","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v130","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v130.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v131","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v131","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v131.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v132","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v132","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v132.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v133","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v133","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v133.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v134","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v134","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v134.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Facecaption-15M-Embeddings","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFacecaption-15M-Embeddings\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\nWe chose about 5M image-text pairs with the highest resolution from Facecaption-15M, extracted the embeddings of the [CLS]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/Facecaption-15M-Embeddings.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v135","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v135","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v135.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AI2D-Caption","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/abhayzala/AI2D-Caption","creator_name":"Abhay Zala","creator_url":"https://huggingface.co/abhayzala","description":"\n\t\n\t\t\n\t\n\t\n\t\tDiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning\n\t\n\nOfficial implementation of DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs to generate more accurate open-domain, open-platform diagrams.\n  \nAbhay Zala,\nHan Lin,\nJaemin Cho,\nMohit Bansal\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tAI2D-Caption Dataset\n\t\n\nThis dataset is primarily based off the AI2D Dataset (see here).\nSee Section 4.1 of our paper for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhayzala/AI2D-Caption.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K<n<10K","arxiv:2310.12128"],"keywords_longer_than_N":true},
	{"name":"rule34lol-webm","keyword":"text-to-video","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/rule34lol-webm","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Rule34.lol WebM\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about WebM files from Rule34.lol, a booru-style imageboard. The dataset includes metadata for 22,733 WebM files, including URLs, tags, and file information. The actual WebM files are stored in zip archives, with each archive containing 500 WebM files.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset metadata is primarily in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/rule34lol-webm.","first_N":5,"first_N_keywords":["video-classification","text-to-video","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Video-Detailed-Caption","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/Video-Detailed-Caption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\n\n\t\n\t\t\n\t\tVideo Detailed Caption Benchmark\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tBenchmark Collection and Processing\n\t\n\nWe building VDC upon Panda-70M, Ego4D, Mixkit, Pixabay, and Pexels. Structured detailed captions construction pipeline. We develop a structured detailed captions construction pipeline to generate extra detailed descriptions from various‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/Video-Detailed-Caption.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v136","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v136","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v136.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v137","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v137","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v137.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v138","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v138","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v138.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v139","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v139","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v139.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v140","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v140","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v140.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v141","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v141","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v141.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v142","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v142","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v142.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v143","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v143","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v143.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v18","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v19","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v144","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v144","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v144.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"blip3-ocr-200m","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Salesforce/blip3-ocr-200m","creator_name":"Salesforce","creator_url":"https://huggingface.co/Salesforce","description":"\n\t\n\t\t\n\t\tBLIP3-OCR-200M Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe BLIP3-OCR-200M dataset is designed to address the limitations of current Vision-Language Models (VLMs) in processing and interpreting text-rich images, such as documents and charts. Traditional image-text datasets often struggle to capture nuanced textual information, which is crucial for tasks requiring complex text comprehension and reasoning. \n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nOCR Integration: The dataset incorporates Optical Character‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/blip3-ocr-200m.","first_N":5,"first_N_keywords":["English","apache-2.0","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage isze: 3-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v145","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v145","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v145.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v146","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v146","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v146.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v147","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v147","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v147.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 3-12.\noutput pattern image size: 1-4.\npixel count: 1-5.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v148","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v148","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v148.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v149","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v149","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v149.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v20","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v20.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v150","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v150","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v150.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"styles","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rezashkv/styles","creator_name":"Reza Shirkavand","creator_url":"https://huggingface.co/rezashkv","description":"\n\t\n\t\t\n\t\tStyled Image Dataset Generated with FLUX.1-dev and LoRAs from the community\n\t\n\nAccess the generation scripts here.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 60,000 text-image-pairs. The images are generated by adding trained LoRA weights to the diffusion transformer model black-forest-labs/FLUX.1-dev. The images were created using 6 different style models, with each style having its own set of 10,000 images. Each style includes 10,000 captions sampled from the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rezashkv/styles.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v151","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v151","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v151.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v21","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v21.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"OseDialogs","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MakarMD/OseDialogs","creator_name":"Mariya","creator_url":"https://huggingface.co/MakarMD","description":"MakarMD/OseDialogs dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["translation","text-to-audio","Ossetian","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"INDspeech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/dzakybd/INDspeech","creator_name":"Dzaky Zakiyal Fawwaz","creator_url":"https://huggingface.co/dzakybd","description":"dzakybd/INDspeech dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Indonesian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"REIRCOCO","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/haoxiangzhao/REIRCOCO","creator_name":"Xiangzhao Hao","creator_url":"https://huggingface.co/haoxiangzhao","description":"\n\t\n\t\t\n\t\tReferring Expression Instance Retrieval and A Strong End-to-End Baseline (ACMMM 2025)\n\t\n\nüåê Homepage | ü§ó Model(CLARE Checkpoints)(coming soon) | üìñ arXiv  | GitHub\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nREIRCOCO is a large-scale benchmark specifically designed for Referring Expression Instance Retrtieval(REIR). It features uniquely aligned referring expressions for over 215,000 object instances in 30,000+ images, totaling 613,000 fine-grained descriptions. The dataset is constructed through a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/haoxiangzhao/REIRCOCO.","first_N":5,"first_N_keywords":["text-to-image","text-retrieval","English","mit","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"SeaS","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/HUST-SLOW/SeaS","creator_name":"HUST-SLOW","creator_url":"https://huggingface.co/HUST-SLOW","description":"This repository contains the generated image-mask pairs and split VisA Dataset of the paper SeaS: Few-shot Industrial Anomaly Image Generation with Separation and Sharing Fine-tuning.\nCode: https://github.com/HUST-SLOW/SeaS\n","first_N":5,"first_N_keywords":["text-to-image","mit","Image","arxiv:2410.14987","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"OseProverb_Voice","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MakarMD/OseProverb_Voice","creator_name":"Mariya","creator_url":"https://huggingface.co/MakarMD","description":"MakarMD/OseProverb_Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["translation","text-to-audio","Ossetian","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"bird-sql","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sudnya/bird-sql","creator_name":"Diamos","creator_url":"https://huggingface.co/Sudnya","description":"\n\t\n\t\t\n\t\tBIRD-SQL Dataset\n\t\n\nBIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQL Evaluation) is a comprehensive text-to-SQL dataset featuring realistic databases and complex queries across multiple domains.\nThis dataset maintains the exact original BIRD format and field names.\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\nTrain: ~9,400 examples\nValidation: ~1,500 examples  \nTotal: ~10,900 examples\nDatabases: 80+ realistic databases\n\n\n\t\n\t\t\n\t\tQuick Start\n\t\n\nfrom datasets import load_dataset\n\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sudnya/bird-sql.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"synthetic_transcript_pt","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yuriyvnv/synthetic_transcript_pt","creator_name":"Yuriy Perezhohin","creator_url":"https://huggingface.co/yuriyvnv","description":"\n\t\n\t\t\n\t\tPortuguese Speech Dataset with Multiple Training Configurations\n\t\n\nA comprehensive Portuguese speech dataset offering three distinct training configurations for speech recognition research, each designed for different experimental scenarios and training paradigms.\n\n\t\n\t\t\n\t\tüéØ Dataset Configurations Overview\n\t\n\nThis dataset provides three carefully curated subsets to enable comprehensive speech recognition research:\n\n\t\n\t\t\nConfiguration\nTraining Data\nValidation\nTest\nTotal Samples\nUse Case‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuriyvnv/synthetic_transcript_pt.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","audio-classification","Portuguese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1‚Äì4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict object‚Äìsurface‚Äìscene compatibility mappings.\n\nCurated by: Bodhisatta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"Prompt2SceneBench","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench","creator_name":"Bodhisatta Maiti","creator_url":"https://huggingface.co/bodhisattamaiti","description":"\n\t\n\t\t\n\t\tDataset Card for Prompt2SceneBench\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nPrompt2SceneBench is a structured prompt dataset with 12,606 text descriptions designed for evaluating text-to-image models in realistic indoor environments. \nEach prompt describes the spatial arrangement of 1‚Äì4 common household objects on compatible surfaces and in contextually appropriate scenes, sampled using strict object‚Äìsurface‚Äìscene compatibility mappings.\n\nCurated by: Bodhisatta‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bodhisattamaiti/Prompt2SceneBench.","first_N":5,"first_N_keywords":["text-to-image","question-answering","zero-shot-classification","image-to-text","English"],"keywords_longer_than_N":true},
	{"name":"video-SALMONN_2_testset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsinghua-ee/video-SALMONN_2_testset","creator_name":"Electronic Engineering @Tsinghua University","creator_url":"https://huggingface.co/tsinghua-ee","description":"\n\t\n\t\t\n\t\tvideo-SALMONN 2 Benchmark\n\t\n\nGithub Link\nPaper Link\n\nGenerate the caption corresponding to the video and the audio with video_salmonn2_test.json\nOrganize your results in the format like the following example:\n\n[\n    {\n        \"id\": [\"0.mp4\"], \n        \"pred\": \"Generated Caption\"\n    }\n]\n\n\nReplace res_file in eval.py with your result file.\nRun python3 eval.py\n\n","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v156","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v156","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v156.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v152","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v152","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v152.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"synthchat","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nnethercott/synthchat","creator_name":"Nate Nethercott","creator_url":"https://huggingface.co/nnethercott","description":"\n\n\t\n\t\t\n\t\tRendered synthetic chats from llama3.1\n\t\n\nThis dataset contains 2.2k screenshots of multi-turn conversations generated by Llama-3.1-70B-Instruct. Each conversation consists of 3-4 short exchanges between a User and an AI Assistant about a certain topic.\nThe original dataset comprising of pure text exchanges can be found here: HuggingFaceTB/everyday-conversations-llama3.1-2k\n\n\t\n\t\t\n\t\n\t\n\t\tMotivation\n\t\n\nThis dataset aims to improve the OCR performance of vision-language models in terms of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nnethercott/synthchat.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"watercolor-flux-samples","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DarkMoonDragon/watercolor-flux-samples","creator_name":"DarkmnDragon","creator_url":"https://huggingface.co/DarkMoonDragon","description":"Generated by https://huggingface.co/SebastianBodza/Flux_Aquarell_Watercolor_v2\nFlux-dev generated samples\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v153","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v153","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v153.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v157","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v157","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v157.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v4.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v18","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 8-14.\nrectangle size: 3-5.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 8-16.\nrectangle size: 3-6.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 8-16.\nrectangle size: 3-7.\nnumber of rects: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v158","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v158","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v158.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v159","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v159","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v159.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"cool_images_urban_and_nature","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AlexandrosChariton/cool_images_urban_and_nature","creator_name":"Alexandros Chariton","creator_url":"https://huggingface.co/AlexandrosChariton","description":"This is an amazing dataset. From pythess with love.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v161","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v161","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v161.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v18","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v18.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rectangle-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rectangle-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform a few rectangles.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-12.\nrectangle size: 3-4.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v160","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v160","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v160.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"speech-to-text","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeVy4/speech-to-text","creator_name":"Le Thao Vy","creator_url":"https://huggingface.co/LeVy4","description":"LeVy4/speech-to-text dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Vietnamese","cc0-1.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"laion-pop-llama3.2-11b","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/laion-pop-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for laion-pop-llama3.2-11b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 1,580,595 new synthetic captions for the images found in laion/laion-pop. The dataset was restricted to SFW-only images by filtering out every image with a nsfw_prediction greater than or equal to 0.995. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using meta-llama/Llama-3.1-8B-Instruct The dataset was grounded for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/laion-pop-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"t2i-compbench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NinaKarine/t2i-compbench","creator_name":"Nina","creator_url":"https://huggingface.co/NinaKarine","description":"Hub version of the T2I-CompBench dataset. All credits and licensing belong to the creators of the dataset. \nThis version was obtained as described below. \nFirst, the \".txt\" files were obtained from this directory.\n\nCode\n\nimport requests\nimport os\n\n# Set the necessary parameters\nowner = \"Karine-Huang\"\nrepo = \"T2I-CompBench\"\nbranch = \"main\"\ndirectory = \"examples/dataset\"\nlocal_directory = \".\"\n\n# GitHub API URL to get contents of the directoryurl =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NinaKarine/t2i-compbench.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"EvalQABench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hhenryz/EvalQABench","creator_name":"Henry Hengyuan Zhao","creator_url":"https://huggingface.co/hhenryz","description":"This repository contains the data for LOVA3: Learning to Visual Question Answering, Asking and Assessment. \nLOVA3 is a framework designed to equip MLLMs with the capabilities to answer, ask, and assess questions in the context of images.\nCode: https://github.com/showlab/LOVA3\n\n\t\n\t\t\n\t\tüéì Citation\n\t\n\nIf you find LOVA3 useful, please cite using this BibTeX:\n@inproceedings{\n    zhao2024lova,\n    title={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\n    author={Hengyuan‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hhenryz/EvalQABench.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2405.14974","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"znanio-videos","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/znanio-videos","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Znanio.ru Educational Videos\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 6,653 educational videos from the znanio.ru platform, a resource for teachers, educators, students, and parents providing diverse educational content. Znanio.ru has been a pioneer in educational technologies and distance learning in the Russian-speaking internet since 2009.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in Russian, with potential multilingual content:\n\nRussian (ru): The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/znanio-videos.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"MixEval-X","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MixEval/MixEval-X","creator_name":"MixEval","creator_url":"https://huggingface.co/MixEval","description":"\n\n\nüöÄ Project Page | üìú arXiv | üë®‚Äçüíª Github | üèÜ Leaderboard | üìù blog | ü§ó HF Paper | ùïè Twitter\n\n\n\n\n\n\n\nMixEval-X encompasses eight input-output modality combinations and can be further extended. Its data points reflect real-world task distributions. The last grid presents the scores of frontier organizations‚Äô flagship models on MixEval-X, normalized to a 0-100 scale, with MMG tasks using win rates instead of Elo. Section C of the paper presents example data samples and model responses.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MixEval/MixEval-X.","first_N":5,"first_N_keywords":["image-to-text","video-text-to-text","audio-classification","text-generation","text-to-audio"],"keywords_longer_than_N":true},
	{"name":"ukiyo-e-face-blip2-captions","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/py-img-gen/ukiyo-e-face-blip2-captions","creator_name":"Image Generation with Python","creator_url":"https://huggingface.co/py-img-gen","description":"\n\t\n\t\t\n\t\tDataset Card for ukiyo-e-face-blip2-captions\n\t\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nukiyo-e-face-blip2-captions is a dataset that adds captions to Ukiyo-e face dataset using BLIP2 model.\n\n\n\n\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\n\n\t\n\t\n\t\n\t\tLanguages\n\t\n\nThe language data in ukiyo-e-face-blip2-captions is in English.\n\n\n\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nimport datasets as ds\n\ndataset = ds.load_dataset(\"py-img-gen/ukiyo-e-face-blip2-captions\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/py-img-gen/ukiyo-e-face-blip2-captions.","first_N":5,"first_N_keywords":["text-to-image","machine-generated","found","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"VideoChat2","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shenxq/VideoChat2","creator_name":"Xiaoqian Shen","creator_url":"https://huggingface.co/shenxq","description":"Video training data of LongVU downloaded from\nhttps://huggingface.co/datasets/OpenGVLab/VideoChat2-IT\n\n\t\n\t\t\n\t\tVideo\n\t\n\nPlease download the original videos from the provided links:\n\nBDD100K: bdd.zip\nShareGPTVideo: https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction/tree/main/train_300k\nCLEVRER: clevrer_qa.zip\nDiDeMo: didemo.zip\nEgoQA: https://huggingface.co/datasets/ynhe/videochat2_data/resolve/main/egoqa_split_videos.zipKinetics-710: k400.zip\nMovieChat: moviechat.zip‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shenxq/VideoChat2.","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"KOFFVQA_Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maum-ai/KOFFVQA_Data","creator_name":"maum-ai","creator_url":"https://huggingface.co/maum-ai","description":"\n\t\n\t\t\n\t\tAbout this data\n\t\n\nKOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language\nKOFFVQA is a general-purpose VLM benchmark in the Korean language. For more information, refer to our leaderboard page and the official evaluation code.\nThis contains the data for the benchmark consisting of images, their corresponding questions, and response grading criteria.  The benchmark focuses on free-form visual question answering, evaluating the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maum-ai/KOFFVQA_Data.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","Korean","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"temporal-vqa","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fazliimam/temporal-vqa","creator_name":"Fazli Imam","creator_url":"https://huggingface.co/fazliimam","description":"\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Temporal-VQA dataset is a challenging benchmark designed to evaluate the temporal reasoning capabilities of Multimodal Large Language Models (MLLMs) in tasks requiring visual temporal understanding. It emphasizes real-world temporal dynamics through two core evaluation tasks:- \n\nTemporal Order Understanding: This task presents MLLMs with temporally consecutive frames from video sequences. The models must analyze and determine the correct sequence of events‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fazliimam/temporal-vqa.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Gemini-2.0-Flash-Fenrir-Voice","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fireblade2534/Gemini-2.0-Flash-Fenrir-Voice","creator_name":"fireblade2534","creator_url":"https://huggingface.co/fireblade2534","description":"fireblade2534/Gemini-2.0-Flash-Fenrir-Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"bird-critic-1.0-postgresql","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/birdsql/bird-critic-1.0-postgresql","creator_name":"The BIRD Team","creator_url":"https://huggingface.co/birdsql","description":"\n\t\n\t\t\n\t\tUpdate 2025-06-08\n\t\n\nWe release the full version of BIRD-Critic-PG, a dataset containing 530 high-quality user issues focused on real-world PostgreSQL database applications. The schema file is include in the code repository https://github.com/bird-bench/BIRD-CRITIC-1/blob/main/baseline/data/post_schema.jsonl\n\n\t\n\t\t\n\t\tBIRD-CRITIC-1.0-PG\n\t\n\nBIRD-Critic is the first SQL debugging benchmark designed to answer a critical question:\nCan large language models (LLMs) fix user issues in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birdsql/bird-critic-1.0-postgresql.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"nocaps-pt-br","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/laicsiifes/nocaps-pt-br","creator_name":"Laborat√≥rio de Intelig√™ncia Computacional e Sistemas de informa√ß√£o","creator_url":"https://huggingface.co/laicsiifes","description":"\n\t\n\t\t\n\t\tüéâ nocaps Dataset Translation for Portuguese Image Captioning\n\t\n\n\n\t\n\t\t\n\t\tüíæ Dataset Summary\n\t\n\nnocaps Portuguese Translation, a multimodal dataset for Portuguese image captioning benchmark, each image accompanied by ten descriptive captions\nthat have been generated by human annotators for every individual image. The original English captions were rendered into Portuguese\nthrough the utilization of the Google Translator API.\n\n\t\n\t\t\n\t\tüßë‚Äçüíª Hot to Get Started with the Dataset\n\t\n\nfrom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laicsiifes/nocaps-pt-br.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","text-generation","Portuguese","mit"],"keywords_longer_than_N":true},
	{"name":"furry-e621-safe-llama3.2-11b","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/furry-e621-safe-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tfurry-e621-safe-llama3.2-11b: A new anthropomorphic art dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 2,987,631 synthetic captions for 995,877 images found in e921, which is just e621 filtered to the \"safe\" tag. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using meta-llama/Llama-3.1-8B-Instruct The dataset was grounded for captioning using the ground truth tags on every post categorized and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/furry-e621-safe-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"negopt_full","keyword":"text-to-image","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/mikeogezi/negopt_full","creator_name":"Michael Ogezi","creator_url":"https://huggingface.co/mikeogezi","description":"This is the dataset constructed in and used to fine-tune the models proposed in our paper Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation. The data is gotten from Playground.\nIf you find this dataset useful, please cite us here:\n@article{ogezi2024optimizing,\n  title={Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation},\n  author={Ogezi, Michael and Shi, Ning},\n  journal={arXiv preprint arXiv:2403.07605}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mikeogezi/negopt_full.","first_N":5,"first_N_keywords":["text-to-image","text-generation","English","bsd-3-clause","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Recap-Long-Coyo","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwu-ww/Recap-Long-Coyo","creator_name":"Wei Wu","creator_url":"https://huggingface.co/weiwu-ww","description":"\n\t\n\t\t\n\t\tDataset Card for Recap-Long-Coyo\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset consists of long captions of ~24M images from Coyo-700M dataset. The long captions are generated by pre-trained Multi-modality Large Language Models (ShareGPT4V/InstructBLIP/LLava1.5) with the text prompt \"Describe the image in detail\".\n\n\t\n\t\t\n\t\tLicensing Information\n\t\n\nWe distribute the image url with long captions under a standard Creative Common CC-BY-4.0 license. The individual images are under their own‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weiwu-ww/Recap-Long-Coyo.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10M - 100M","csv"],"keywords_longer_than_N":true},
	{"name":"mscoco","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/romrawinjp/mscoco","creator_name":"Romrawin Chumpu","creator_url":"https://huggingface.co/romrawinjp","description":"\n\t\n\t\t\n\t\tCommon Objects in Context (COCO) Dataset\n\t\n\nThis dataset is English captions of COCO dataset. \nThe splits in this dataset is set according to Andrej Karpathy's split from dataset_coco.json file. The collection was created specifically for simplicity of use in training and evaluation pipeline by non-commercial and research purposes. The COCO images dataset is licensed under a Creative Commons Attribution 4.0 License.\n\n\t\n\t\t\n\t\n\t\n\t\tReference\n\t\n\n@misc{lin2015microsoftcococommonobjects‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/romrawinjp/mscoco.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"EGY2K","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rahafvii/EGY2K","creator_name":"xx","creator_url":"https://huggingface.co/rahafvii","description":"rahafvii/EGY2K dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"EGY2K","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rahafvii/EGY2K","creator_name":"xx","creator_url":"https://huggingface.co/rahafvii","description":"rahafvii/EGY2K dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","Arabic","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-HQ-311K","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-HQ-311K\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing Agreement\nHumanCaption-HQ-311K: Approximately 311,000 human-related images and their corresponding natural language descriptions.\nCompared to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-HQ-311K.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"kokoro-82M-voices","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ecyht2/kokoro-82M-voices","creator_name":"Tan Hong Kai","creator_url":"https://huggingface.co/ecyht2","description":"\n\t\n\t\t\n\t\tKokoro-82M Voices\n\t\n\nThis dataset contains all the voices available in hexgrad/Kokoro-82M.\nThis dataset provides the voices in 3 different formats.\n\nIndividual voices embeddings in different JSON file\nSingle JSON which contains all the voices in a JSON object.\nParquet format for usage via datasets\nThe voices name is the same as the .pth file names shown below.\n\nvoices = [\n    \"af\",\n    \"af_bella\",\n    \"af_nicole\",\n    \"af_sarah\",\n    \"af_sky\",\n    \"am_adam\",\n    \"am_michael\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ecyht2/kokoro-82M-voices.","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"bird-critic-1.0-open","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/birdsql/bird-critic-1.0-open","creator_name":"The BIRD Team","creator_url":"https://huggingface.co/birdsql","description":"\n\t\n\t\t\n\t\tUpdate 2025-05-22\n\t\n\nThe previous issue regarding mismatched MySQL instances has been resolved. The updated version of BIRD-CRITIC-Open is now available. Thank you for your patience and understanding.\n\n\t\n\t\t\n\t\tUpdate 2025-04-25\n\t\n\nWe‚Äôve identified a mismatch issue in some uploaded MySQL instances. Our team is actively working to resolve this, and we‚Äôll release the updated version promptly. Please refrain from using MySQL until the fix is deployed. Apologies for any inconvenience caused.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birdsql/bird-critic-1.0-open.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"bird-critic-1.0-flash-exp","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/birdsql/bird-critic-1.0-flash-exp","creator_name":"The BIRD Team","creator_url":"https://huggingface.co/birdsql","description":"\n\t\n\t\t\n\t\tBIRD-CRITIC-1.0-Flash\n\t\n\nBIRD-Critic is the first SQL debugging benchmark designed to answer a critical question:\nCan large language models (LLMs) fix user issues in real-world database applications? Each task in BIRD-CRITIC has been verified by human experts on the following dimensions:\n\nReproduction of errors on BIRD env to prevent data leakage.\nCarefully curate test case functions for each task specifically. \nSoft EX: This metric can evaluate SELECT-ONLY tasks.\nSoft EX + Parsing:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birdsql/bird-critic-1.0-flash-exp.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Dataset","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\n\t\n\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\n\n\t\n\t\t\n\t\n\t\n\t\tInst-IT Dataset Overview\n\t\n\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset.","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","video-text-to-text","image-text-to-text","LVVIS"],"keywords_longer_than_N":true},
	{"name":"MangaZero","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jianzongwu/MangaZero","creator_name":"Jianzong Wu","creator_url":"https://huggingface.co/jianzongwu","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nMangaZero dataset from paper DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation\nPlease see GitHub repo to get the usage\nProject page: https://jianzongwu.github.io/projects/diffsensei\n\n\t\n\t\t\n\t\n\t\n\t\tThe \"type\" key in character annotation\n\t\n\nType = 0: Characters with clean faces\nType = 1: Characters with vague faces\nType = 2: Exaggerated, artistic characters with appearances that don't conform to typical standards. \n","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"houses","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vvmatorin/houses","creator_name":"Vladislav Matorin","creator_url":"https://huggingface.co/vvmatorin","description":"\n\t\n\t\t\n\t\tHouses\n\t\n\nThis synthetic dataset contains images of simple houses and their corresponding drawing instructions. It is designed for training models to predict missing elements in visual sequences to facilitate systems such as Copilot. \n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nImages: 256x256 images representing house drawings.\nCommands: structured commands in JSON format describing the components of the house (e.g., body, roof, windows).\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset can be used to train machine learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vvmatorin/houses.","first_N":5,"first_N_keywords":["text-to-image","text-generation","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"dalle3-llama3.2-11b","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for dalle3-llama3.2-11b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 3,577,716 new synthetic captions for the 1,192,572 images found in ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions. The dataset was filtered for duplicates and then re-encoded with JPEGXL lossless or lossy depending on the source. The long captions were produced using meta-llama/Llama-3.2-11B-Vision-Instruct. Medium and short captions were produced from these captions using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CUPUM/mid-space","creator_name":"CUPUM","creator_url":"https://huggingface.co/CUPUM","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CUPUM/mid-space.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sib-fleurs","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/sib-fleurs","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","description":"\n\t\n\t\t\n\t\tSIB-Fleurs\n\t\n\nSIB-Fleurs is a dataset suitable to evaluate Multilingual Spoken Language Understanding. For each utterance in Fleurs, the task is to determine the topic the utterance belongs to. \nThe topics are:\n\nScience/Technology\nTravel\nPolitics\nSports\nHealth\nEntertainment\nGeography\n\nPreliminary evaluations can be found at the bottom of the README. The preliminary results in full detail are available in ./results.csv*.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset creation\n\t\n\nThis dataset processes and merges‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/sib-fleurs.","first_N":5,"first_N_keywords":["audio-classification","automatic-speech-recognition","audio-text-to-text","text-to-speech","question-answering"],"keywords_longer_than_N":true},
	{"name":"nuscenes2d-time-weather-geodiffusion","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KaiChen1998/nuscenes2d-time-weather-geodiffusion","creator_name":"Kai Chen","creator_url":"https://huggingface.co/KaiChen1998","description":"\n\t\n\t\t\n\t\n\t\n\t\tnuScenes-time-weather-GeoDiffusion Dataset Card\n\t\n\nnuScenes-time-weather-GeoDiffusion is the official dataset annotation file used to train GeoDiffusion on the nuScenes dataset with time of day (i.e., daytime/night) and weather (i.e., sunny/rain).\nSince the nuImages dataset does not equip with those meta tags, we opt for the nuScenes dataset and generate the 2D bounding box annotations via inference with a Mask R-CNN pre-trained on the nuImages dataset, which is then saved in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KaiChen1998/nuscenes2d-time-weather-geodiffusion.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","arxiv:2306.04607","üá∫üá∏ Region: US","layout-to-image"],"keywords_longer_than_N":false},
	{"name":"HRVideoBench","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/HRVideoBench","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tHRVideoBench\n\t\n\nThis repo contains the test data for HRVideoBench, which is released under the paper \"VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation\". VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\nüåê Homepage | üìñ arXiv | üíª GitHub | ü§ó VISTA-400K | ü§ó Models | ü§ó HRVideoBench‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/HRVideoBench.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","< 1K","Image"],"keywords_longer_than_N":true},
	{"name":"ImageDataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bk1832004/ImageDataset","creator_name":"Bharath Krishna","creator_url":"https://huggingface.co/bk1832004","description":"bk1832004/ImageDataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"jl-speech","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JacobLinCool/jl-speech","creator_name":"Jacob Lin","creator_url":"https://huggingface.co/JacobLinCool","description":"\n\t\n\t\t\n\t\tJL Speech Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nJL Speech is a male voice version of the LJ Speech dataset. It contains 13,100 short audio clips of a single speaker reading passages from books. The total audio duration is approximately 24 hours, with audio quality improved to 48 kHz sampling rate.\nThis dataset is licensed under the CC-BY-4.0 License.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tCreation\n\t\n\nThe JL Speech dataset was created by converting the original LJ Speech dataset to a male‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JacobLinCool/jl-speech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"jl-speech","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JacobLinCool/jl-speech","creator_name":"Jacob Lin","creator_url":"https://huggingface.co/JacobLinCool","description":"\n\t\n\t\t\n\t\tJL Speech Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nJL Speech is a male voice version of the LJ Speech dataset. It contains 13,100 short audio clips of a single speaker reading passages from books. The total audio duration is approximately 24 hours, with audio quality improved to 48 kHz sampling rate.\nThis dataset is licensed under the CC-BY-4.0 License.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tCreation\n\t\n\nThe JL Speech dataset was created by converting the original LJ Speech dataset to a male‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JacobLinCool/jl-speech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"LongVA-TPO-10k","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ruili0/LongVA-TPO-10k","creator_name":"Rui Li","creator_url":"https://huggingface.co/ruili0","description":"\n \n \n\n\n\t\n\t\n\t\n\t\t10kTemporal Preference Optimization Dataset for LongVA\n\t\n\nLongVA-TPO-10k, introduced by paper Temporal Preference Optimization for Long-form Video Understanding\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MSTS","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/felfri/MSTS","creator_name":"Felix Friedrich","creator_url":"https://huggingface.co/felfri","description":"\n\t\n\t\t\n\t\tDisclaimer\n\t\n\nThe MSTS dataset contains content that may be offensive or upsetting in nature. Topics include, but are not limited to, discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matter. \nPlease only engage with the data in accordance with your own personal risk tolerance. The data are intended for research purposes, especially research that can make models less harmful.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/felfri/MSTS.","first_N":5,"first_N_keywords":["image-text-to-text","English","Arabic","French","German"],"keywords_longer_than_N":true},
	{"name":"Tridis","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/magistermilitum/Tridis","creator_name":"Sergio Torres","creator_url":"https://huggingface.co/magistermilitum","description":"This is the first version of the dataset derived from the corpora used for TRIDIS (Tria Digita Scribunt). \nTRIDIS encompasses a series of Handwriting Text Recognition (HTR) models trained using semi-diplomatic transcriptions of medieval and early modern manuscripts.\nThe semi-diplomatic transcription approach involves resolving abbreviations found in the original manuscripts and normalizing Punctuation and Allographs.\nThe dataset contains approximately 4,000 pages of manuscripts and is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/magistermilitum/Tridis.","first_N":5,"first_N_keywords":["image-to-text","French","Spanish","Latin","German"],"keywords_longer_than_N":true},
	{"name":"InstanceVid","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AnonMegumi/InstanceVid","creator_name":"tiehan fan","creator_url":"https://huggingface.co/AnonMegumi","description":"\n\t\n\t\t\n\t\tInstanceVid\n\t\n\nPaper: [https://arxiv.org/abs/2412.09283)\nCode: https://github.com/NJU-PCALab/InstanceCap\n\n\t\n\t\t\n\t\tUsage\n\t\n\nInstanceVid is a subset of the OpenVid - 1 m, you need to provide the file to this warehouse index from OpenVid-1M to obtain the corresponding video files.\nIn train, we published three files, The original InstanceCap(Instancecap.jsonl), follow the content of paper on compression of Dense form (InstanceCap_Dense.csv/jsonl). Select a file as required. Besides Caption‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AnonMegumi/InstanceVid.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","10K<n<100K","arxiv:2412.09283"],"keywords_longer_than_N":true},
	{"name":"InstanceVid","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AnonMegumi/InstanceVid","creator_name":"tiehan fan","creator_url":"https://huggingface.co/AnonMegumi","description":"\n\t\n\t\t\n\t\tInstanceVid\n\t\n\nPaper: [https://arxiv.org/abs/2412.09283)\nCode: https://github.com/NJU-PCALab/InstanceCap\n\n\t\n\t\t\n\t\tUsage\n\t\n\nInstanceVid is a subset of the OpenVid - 1 m, you need to provide the file to this warehouse index from OpenVid-1M to obtain the corresponding video files.\nIn train, we published three files, The original InstanceCap(Instancecap.jsonl), follow the content of paper on compression of Dense form (InstanceCap_Dense.csv/jsonl). Select a file as required. Besides Caption‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AnonMegumi/InstanceVid.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","10K<n<100K","arxiv:2412.09283"],"keywords_longer_than_N":true},
	{"name":"highresolution-laioncoco-aesthetic-MEG","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaoxiaxu/highresolution-laioncoco-aesthetic-MEG","creator_name":"Xiaoxia Xu","creator_url":"https://huggingface.co/xiaoxiaxu","description":"This dataset is filtered from laioncoco-aesthetic, which is used for academic research on mobile edge generation (MEG).\nIt includes high-resolution 1024-by-1024 text-to-image samples generated by a distilled SDXL with 4-12 denoising steps. \n\nThe dataset mainly involves the following fields:\n\ncaption: The text prompt of the image.\nimage: The target image corresponding to the prompt.\ndiffusion: The generative results of the distilled SDXL.\nlatents: The latent features of the distilled SDXL.\n\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","arrow","Tabular"],"keywords_longer_than_N":true},
	{"name":"cml-tts-filtered","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/cml-tts-filtered","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\n\t\n\t\t\n\t\tDataset Card for Filtred and CML-TTS\n\t\n\nThis dataset is a filtred version of a CML-TTS [1]. \nCML-TTS [1] CML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG). CML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The dataset includes recordings in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/cml-tts-filtered.","first_N":5,"first_N_keywords":["text-to-speech","French","German","Dutch","Polish"],"keywords_longer_than_N":true},
	{"name":"e621-2024_index","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepghs/e621-2024_index","creator_name":"DeepGHS","creator_url":"https://huggingface.co/deepghs","description":"Tar index files for boxingscorpionbagel/e621-2024.\nYou can download images from both boxingscorpionbagel/e621-2024 and deepghs/e621_newest with cheesechaser.\nfrom cheesechaser.datapool import E621NewestDataPool\n\npool = E621NewestDataPool()\n\n# download e621 #2010000-2010300, to directory /data/e621\npool.batch_download_to_directory(\n    resource_ids=range(2010000, 2010300),\n    dst_dir='/data/e621',\n    max_workers=12,\n)\n\n","first_N":5,"first_N_keywords":["image-classification","image-to-image","text-to-image","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"aesthetic_anime_curated_8.5k","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sdtana/aesthetic_anime_curated_8.5k","creator_name":"sdtana","creator_url":"https://huggingface.co/sdtana","description":"This ~8.5k dataset is a curated selection of posts with more than 2,000 likes on pixiv between 2024/07/01 and 2024-09/25.\nCollection date: 2024-09-29\nTags collected: 'Â•≥„ÅÆÂ≠ê', 'usersÂÖ•„Çä', 'Â•≥„ÅÆÂ≠ê', '„Åµ„Å®„ÇÇ„ÇÇ', 'Â§™„ÇÇ„ÇÇ', 'È≠ÖÊÉë„ÅÆÂ§™„ÇÇ„ÇÇ', 'È≠ÖÊÉë„ÅÆ„Åµ„Å®„ÇÇ„ÇÇ', '„Å∏„Åù', '„Åä„Å∏„Åù', '„Åä„Å£„Å±„ÅÑ', 'È≠ÖÊÉë„ÅÆË∞∑Èñì', 'Ê•µ‰∏ä„ÅÆ‰π≥', 'Â∑®‰π≥', 'ÁùÄË°£Â∑®‰π≥', 'Â¥©Â£ä:„Çπ„Çø„Éº„É¨„Ç§„É´', '„Éñ„É´„Éº„Ç¢„Éº„Ç´„Ç§„Éñ', '„Éê„Éº„ÉÅ„É£„É´YouTuber', 'ÂéüÁ•û\nTags excluded: '3DCG','Á≠ãËÇâÂ®ò','Áî∑„ÅÆÂ®ò','Â•≥Ë£ÖÁî∑Â≠ê','ËÖêÂêë„Åë','BL','Koikatsu','Koikatsu!','„Ç≥„Ç§„Ç´„ÉÑ','„Ç≥„Ç§„Ç´„ÉÑ!','„Åµ„Åü„Å™„Çä','„É°„ÇπÁî∑Â≠ê', '„ÇÄ„Å°„ÇÄ„Å°','„É†„ÉÅ„É†„ÉÅ', '„ÅΩ„Å£„Å°„ÇÉ„Çä', 'ÁÜüÂ•≥','AI','AIÁîüÊàê','AI-generated','AI„Ç§„É©„Çπ„Éà'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sdtana/aesthetic_anime_curated_8.5k.","first_N":5,"first_N_keywords":["image-classification","text-to-image","apache-2.0","1K<n<10K","Image"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-trainset","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/AuroraCap-trainset","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\n\n\t\n\t\t\n\t\tAuroraCap Trainset\n\t\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nWe use over 20 million high-quality image/video-text pairs to train AuroraCap in three stages. \nPretraining stage. We first align visual features with the word embedding space of LLMs. To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector.\nVision stage. We‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-trainset.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mass-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the mass of objects with a specific size.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-6.\nfind mass: 1-2.\nconnectivity: ALL8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-15.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nfind mass: 1-3.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 1-8.\nfind mass: 1-4.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nCompare mass of adjacent rows/columns. image size: 4-7. color count: 10.\nThis was something‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mass-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v14","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v14.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v16","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v16.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"waifuset-wiki","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Eugeoter/waifuset-wiki","creator_name":"Euge","creator_url":"https://huggingface.co/Eugeoter","description":"Useful wikis for Waifuset, keeping up with the times.\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","üá∫üá∏ Region: US","art"],"keywords_longer_than_N":false},
	{"name":"simon-arc-solve-mask-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-translate-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets translated by plus/minus 1 pixel in up/down/left/right directions.\nThe image sizes are between 1 and 4 pixels.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly translate plus/minus 1 up/down are enabled.\nimage width: 1-4, image height: 3-4.\nMy hypothesis is that it's easy with RLE data to translate up/down.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nOnly translate plus/minus 1 left/right are enabled.\nimage width: 3-4, image height: 1-4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAll transformations have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-translate-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Pexels_Gemini_capitoned","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned","creator_name":"Pixel Dust","creator_url":"https://huggingface.co/Pixel-Dust","description":"This dataset features a collection of high-quality images sourced from Pexels and captioned using the Gemini-1.5-Flash API. This dataset is designed to provide accurate, detailed descriptions of various visual content, suitable for text-to-image tasks, training AI models, and more.\nGemini promt:\n\"Describe this image, for a text-to-image train to be accurate, max 74 tokens. (the common theme between these images is '{theme}'), prefer the use of ',' dont use '.' and there is no need to have a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Pixel-Dust/Pexels_Gemini_capitoned.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v1","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v1","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\nnumber of rects: 2-4.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PVIT-3M","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sterzhang/PVIT-3M","creator_name":"Jianshu Zhang","creator_url":"https://huggingface.co/Sterzhang","description":"\n\t\n\t\t\n\t\tPVIT-3M\n\t\n\nThe paper titled \"Personalized Visual Instruction Tuning\" introduces a novel dataset called PVIT-3M. This dataset is specifically designed for tuning MLLMs in the context of personalized visual instruction tasks. The dataset consists of 3 million image-text pairs that aim to improve MLLMs' abilities to generate responses based on personalized visual inputs, making them more tailored and adaptable to individual user needs and preferences.\nHere‚Äôs the PVIT-3M statistics:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sterzhang/PVIT-3M.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"FilteredLAIONCOCOAesthetic","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xiaoxiaxu/FilteredLAIONCOCOAesthetic","creator_name":"Xiaoxia Xu","creator_url":"https://huggingface.co/xiaoxiaxu","description":"A filtered LAIONCOCO-aesthetic dataset for 1024*1024 text-guided image generation\n","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"jeli-asr","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RobotsMali/jeli-asr","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","description":"The **Jeli-ASR Audio Dataset** is a multilingual dataset converted into the optimized Arrow format, \nensuring fast access and compatibility with modern data workflows. It contains audio samples in Bambara \nwith semi-expert transcriptions and French translations. Each subset of the dataset is organized by \nconfiguration (`jeli-asr-rmai`, `bam-asr-oza`, and `jeli-asr`) and further split into training and testing sets. \nThe dataset is designed for tasks like automatic speech recognition (ASR), text-to-speech synthesis (TTS), \nand translation. Data was recorded in Mali with griots, then transcribed and translated into French.\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","audio-language-identification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"mid-space","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mila-ai4h/mid-space","creator_name":"Mila AI4H","creator_url":"https://huggingface.co/mila-ai4h","description":"\n\t\n\t\t\n\t\tMID-Space: Aligning Diverse Communities‚Äô Needs to Inclusive Public Spaces\n\t\n\n\n\t\n\t\t\n\t\tA new version of the dataset will be released soon, incorporating user identity markers and expanded annotations.\n\t\n\n\n\t\n\t\t\n\t\tLIVS PAPER \n\t\n\n\nClick below to see more:\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe MID-Space dataset is designed to align AI-generated visualizations of urban public spaces with the preferences of diverse and marginalized communities in Montreal. It includes textual prompts, Stable Diffusion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mila-ai4h/mid-space.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","image-to-text","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"AnyEdit","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bin1117/AnyEdit","creator_name":"QifanYu","creator_url":"https://huggingface.co/Bin1117","description":"\n\n\n\nCelebrate! AnyEdit resolved the data alignment with the re-uploading process (but the view filter is not working:(, though it has 25 edit types). You can view the validation split for a quick look. You can also refer to anyedit-split dataset to view and download specific data for each editing type.\n\n\t\t\n\t\tDataset Card for AnyEdit-Dataset\n\t\n\nInstruction-based image editing aims to modify specific image elements with natural language instructions. However, current models in this domain often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bin1117/AnyEdit.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v154","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v154","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v154.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-edge-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right edge of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-5.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-10.\nFocus on identifying diagonal edges.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-10.\nEnabled all edge_names: top_left, top, top_right, left, right, bottom_left, bottom‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-edge-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-flip-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-flip-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the transformations are: flip x/y/a/b, with random padding.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-to-image-2M","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jackyhate/text-to-image-2M","creator_name":"kzou","creator_url":"https://huggingface.co/jackyhate","description":"\n\t\n\t\t\n\t\ttext-to-image-2M: A High-Quality, Diverse Text-to-Image Training Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\ntext-to-image-2M is a curated text-image pair dataset designed for fine-tuning text-to-image models. The dataset consists of approximately 2 million samples, carefully selected and enhanced to meet the high demands of text-to-image model training. The motivation behind creating this dataset stems from the observation that datasets with over 1 million samples tend to produce better‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jackyhate/text-to-image-2M.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","image-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-halfplane-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-halfplane-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the halfplane: halfplane_with_two_pixels, halfplane_with_one_pixel_DIRECTION.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 5-8.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-12.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"linto-dataset-audio-ar-tn","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\n\t\n\t\t\n\t\tLinTO DataSet Audio for Arabic Tunisian A collection of Tunisian dialect audio and its annotations for STT task\n\t\n\nThis is the first packaged version of the datasets used to train the Linto Tunisian dialect with code-switching STT\n(linagora/linto-asr-ar-tn).\n\nDataset Summary\nDataset composition\nSources\nData Table\nData sources\nContent Types\nLanguages and Dialects\n\n\nExample use (python)\nLicense\nCitations\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe LinTO DataSet Audio for Arabic Tunisian is a diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"linto-dataset-audio-ar-tn","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\n\t\n\t\t\n\t\tLinTO DataSet Audio for Arabic Tunisian A collection of Tunisian dialect audio and its annotations for STT task\n\t\n\nThis is the first packaged version of the datasets used to train the Linto Tunisian dialect with code-switching STT\n(linagora/linto-asr-ar-tn).\n\nDataset Summary\nDataset composition\nSources\nData Table\nData sources\nContent Types\nLanguages and Dialects\n\n\nExample use (python)\nLicense\nCitations\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe LinTO DataSet Audio for Arabic Tunisian is a diverse‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"linto-dataset-audio-ar-tn-augmented","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn-augmented","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\n\t\n\t\t\n\t\tLinTO DataSet Audio for Arabic Tunisian Augmented A collection of Tunisian dialect audio and its annotations for STT task\n\t\n\nThis is the augmented datasets used to train the Linto Tunisian dialect with code-switching STT linagora/linto-asr-ar-tn.\n\nDataset Summary\nDataset composition\nSources\nContent Types\nLanguages and Dialects\n\n\nExample use (python)\nLicense\nCitations\n\n\n\t\t\n\t\tDataset Summary\n\t\n\nThe LinTO DataSet Audio for Arabic Tunisian Augmented is a dataset that builds on LinTO‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn-augmented.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"linto-dataset-audio-ar-tn-augmented","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn-augmented","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\n\t\n\t\t\n\t\tLinTO DataSet Audio for Arabic Tunisian Augmented A collection of Tunisian dialect audio and its annotations for STT task\n\t\n\nThis is the augmented datasets used to train the Linto Tunisian dialect with code-switching STT linagora/linto-asr-ar-tn.\n\nDataset Summary\nDataset composition\nSources\nContent Types\nLanguages and Dialects\n\n\nExample use (python)\nLicense\nCitations\n\n\n\t\t\n\t\tDataset Summary\n\t\n\nThe LinTO DataSet Audio for Arabic Tunisian Augmented is a dataset that builds on LinTO‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/linto-dataset-audio-ar-tn-augmented.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","text-to-audio","Arabic","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-fractal-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform fractal input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nScale up the input/output images. Scale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nScale up the input/output images. Scale factor: 1-3.\nRandomly invert the pattern_image.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nRandom add padding around the input image, that the model has to crop.\nmax_pad_count = 5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nBigger images\nmax_image_size = 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-fractal-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-half-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-half-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify where the top/bottom/left/right half of the object is located.\nexample count: 4-5.\ntest count: 1-2.\nimage size: 4-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-mask-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to repair the masked areas/rectangles.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 4-7.\nnoise: 0.1, 0.2.\nThere are these transformations: identify_the_masked_area, repair_the_masked_area\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 4-10.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 4-13.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nStill having all the other transformations enabled.\nAdded generate_task_repair_rectangle_and_crop.\ninput image size: 4-8.\nmask size: 2-3.\n\n\t\n\t\t\n\t\tVersion 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-mask-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v155","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v155","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v155.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v22","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v22.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"ArtVee_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Said2k/ArtVee_dataset","creator_name":"Anon","creator_url":"https://huggingface.co/Said2k","description":"Said2k/ArtVee_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1K - 10K","text"],"keywords_longer_than_N":true},
	{"name":"Disney-VideoGeneration-Dataset","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset","creator_name":"Wild-Heart","creator_url":"https://huggingface.co/Wild-Heart","description":"\n\t\n\t\t\n\t\tSteamboat Willie - Video Generation Dataset\n\t\n\n‰∏≠ÊñáÈòÖËØª\nThis dataset contains 69 videos clipped from Disney's Steamboat Willie.\n\nThe length of each video is 6 seconds.\nThe frame rate of the videos is 30 frames per second.\nThe video resolution is 640 x 380.\nAll videos are black and white, not in color.\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Format\n\t\n\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.csv\n‚îú‚îÄ‚îÄ prompt.txt\n‚îú‚îÄ‚îÄ videos\n‚îî‚îÄ‚îÄ videos.txt\n\nThe prompt.txt file contains descriptions for each video, each description containing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Wild-Heart/Disney-VideoGeneration-Dataset.","first_N":5,"first_N_keywords":["text-to-video","image-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"filatov_24000","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/patriotyk/filatov_24000","creator_name":"Serhiy Stetskovych ","creator_url":"https://huggingface.co/patriotyk","description":"patriotyk/filatov_24000 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"inbrowser-proctor-dataset","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lord-reso/inbrowser-proctor-dataset","creator_name":"Aayush Man Shrestha","creator_url":"https://huggingface.co/lord-reso","description":"\n\t\n\t\t\n\t\tDataset Card for Inbrowser Proctor Dataset\n\t\n\n\n\t\n\t\t\n\t\tProject Description\n\t\n\nInbrowser Proctoring is an online browser proctoring application designed to supervise exams and prevent cheating in real-time. Utilizing a combination of video, audio, and screen recording technologies, along with advanced AI algorithms, the system closely monitors test-takers to identify suspicious behaviors and activities. By analyzing audio and visual data, it can detect anomalies that may indicate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lord-reso/inbrowser-proctor-dataset.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"X2I-text-to-image","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yzwang/X2I-text-to-image","creator_name":"Yueze Wang","creator_url":"https://huggingface.co/yzwang","description":"\n\t\n\t\t\n\t\n\t\n\t\tX2I Dataset\n\t\n\n\nProject Page: https://vectorspacelab.github.io/OmniGen/\nGithub: https://github.com/VectorSpaceLab/OmniGen\nPaper: https://arxiv.org/abs/2409.11340\nModel: https://huggingface.co/Shitao/OmniGen-v1\n\nTo achieve robust multi-task processing capabilities, it is essential to train the OmniGen on large-scale and diverse datasets. However, in the field of unified image generation, a readily available dataset has yet to emerge. For this reason, we have curated a large-scale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzwang/X2I-text-to-image.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1M<n<10M","arxiv:2409.11340"],"keywords_longer_than_N":true},
	{"name":"X2I-mm-instruction","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yzwang/X2I-mm-instruction","creator_name":"Yueze Wang","creator_url":"https://huggingface.co/yzwang","description":"\n\t\n\t\t\n\t\tX2I Dataset\n\t\n\n\nProject Page: https://vectorspacelab.github.io/OmniGen/\nGithub: https://github.com/VectorSpaceLab/OmniGen\nPaper: https://arxiv.org/abs/2409.11340\nModel: https://huggingface.co/Shitao/OmniGen-v1\n\nTo achieve robust multi-task processing capabilities, it is essential to train the OmniGen on large-scale and diverse datasets. However, in the field of unified image generation, a readily available dataset has yet to emerge. For this reason, we have curated a large-scale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzwang/X2I-mm-instruction.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"X2I-subject-driven","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yzwang/X2I-subject-driven","creator_name":"Yueze Wang","creator_url":"https://huggingface.co/yzwang","description":"\n\t\n\t\t\n\t\n\t\n\t\tX2I Dataset\n\t\n\n\nProject Page: https://vectorspacelab.github.io/OmniGen/\nGithub: https://github.com/VectorSpaceLab/OmniGen\nPaper: https://arxiv.org/abs/2409.11340\nModel: https://huggingface.co/Shitao/OmniGen-v1\n\nTo achieve robust multi-task processing capabilities, it is essential to train the OmniGen on large-scale and diverse datasets. However, in the field of unified image generation, a readily available dataset has yet to emerge. For this reason, we have curated a large-scale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzwang/X2I-subject-driven.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"X2I-computer-vision","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yzwang/X2I-computer-vision","creator_name":"Yueze Wang","creator_url":"https://huggingface.co/yzwang","description":"\n\t\n\t\t\n\t\n\t\n\t\tX2I Dataset\n\t\n\n\nProject Page: https://vectorspacelab.github.io/OmniGen/\nGithub: https://github.com/VectorSpaceLab/OmniGen\nPaper: https://arxiv.org/abs/2409.11340\nModel: https://huggingface.co/Shitao/OmniGen-v1\n\nTo achieve robust multi-task processing capabilities, it is essential to train the OmniGen on large-scale and diverse datasets. However, in the field of unified image generation, a readily available dataset has yet to emerge. For this reason, we have curated a large-scale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzwang/X2I-computer-vision.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"diffusion_stage_design_japanese_anime_style","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mintz1104/diffusion_stage_design_japanese_anime_style","creator_name":"Huang","creator_url":"https://huggingface.co/mintz1104","description":"mintz1104/diffusion_stage_design_japanese_anime_style dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","Chinese","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"simon-arc-mass-v24","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-mass-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nMeasure the mass of objects for pixel connectivity 4 and pixel connectivity 8.\nimage size: 1-10.\nmax_mass: 4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nmax_mass: 5.\nThis converged too slowly. I was too optimistic. I will have to proceed slower.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-12.\nmax_mass: 5.\nToo big spikes in the training loss. I will have to lower the max_mass, and gradually increase it.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-15.\nmax_mass: 2.\nThe validation loss for this is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-mass-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanCaption-10M","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tHumanCaption-10M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing the Agreement\nHumanCaption-10M: a large, diverse, high-quality dataset of human-related images with natural language descriptions (image to text).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/HumanCaption-10M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-outline-v2","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-outline-v2","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform do edge detection of the input images.\nexample count: 3-5.\ntest count: 1-2.\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v23","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v23.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v24","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v24.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"text-to-image-prompts","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/codeShare/text-to-image-prompts","creator_name":"codeShare","creator_url":"https://huggingface.co/codeShare","description":"If you have questions about this dataset , feel free to ask them on the fusion-discord : https://discord.gg/8TVHPf6Edn\nThis collection contains sets from the fusion-t2i-ai-generator on perchance.\nThis datset is used in this notebook: https://huggingface.co/datasets/codeShare/text-to-image-prompts/tree/main/Google%20Colab%20Notebooks\nTo see the full sets, please use the url \"https://perchance.org/\" + url\n, where the urls are listed below:\n_generator\n  gen_e621\n    fusion-t2i-e621-tags-1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/codeShare/text-to-image-prompts.","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","mit","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v25","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v25.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"rel_dataset","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/m522t/rel_dataset","creator_name":"Mehrshad Taji","creator_url":"https://huggingface.co/m522t","description":"m522t/rel_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Persian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-augment-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-augment-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nAugmentation of the ARC-AGI tasks.\nexample count: 1-3.\ntest count: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nOnly skew up/down/left/right\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nInstead of making tasks out of input/output images.\nI'm now focusing on preserving the original puzzle, with some transformations applied.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"AuroraCap-recaption","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wchai/AuroraCap-recaption","creator_name":"Wenhao Chai","creator_url":"https://huggingface.co/wchai","description":"\n\t\n\t\t\n\t\tAuroraCap-recaption\n\t\n\n\n\n\n\t\n\t\t\n\t\tResources\n\t\n\n\nWebsite\narXiv: Paper\nGitHub: Code\nHuggingface: AuroraCap Model\nHuggingface: VDC Benchmark\nHuggingface: Trainset\n\n\n\t\n\t\t\n\t\n\t\n\t\tFeatures\n\t\n\nVideo recaption data by AuroraCap. Continue updating...\nFor some video source, we could upload the raw videos but for the others we could only provide the url since the well-known reason.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\n@article{chai2024auroracap,\n  title={AuroraCap: Efficient, Performant Video Detailed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wchai/AuroraCap-recaption.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mls-annotated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/mls-annotated","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\n\t\n\t\t\n\t\tDataset Card for Annotations of non English MLS\n\t\n\nThis dataset consists in annotations of a the Non English subset of the Multilingual LibriSpeech (MLS) dataset. \nMLS dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of \n8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/mls-annotated.","first_N":5,"first_N_keywords":["text-to-speech","French","German","Dutch","Portuguese"],"keywords_longer_than_N":true},
	{"name":"Diffuse_Map_Surfaces","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alastandy/Diffuse_Map_Surfaces","creator_name":"Andrew Smith","creator_url":"https://huggingface.co/alastandy","description":"\n\t\n\t\t\n\t\tDataset Card for Diffuse Map Surface\n\t\n\nDetailed surface textures without shadows or hotspots. (Diffuse Maps)\nAn LORA for Flux.1-Dev using this dataset is aviable at https://civitai.com/models/939491\n","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc0-1.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mosel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FBK-MT/mosel","creator_name":"FBK-MT","creator_url":"https://huggingface.co/FBK-MT","description":"\n\n\n\t\n\t\t\n\t\tDataset Description, Collection, and Source\n\t\n\nThe MOSEL corpus is a multilingual dataset collection including up to 950K hours of open-source speech recordings covering the 24 official languages of the European Union. We collect data by surveying labeled and unlabeled speech corpora under open-source compliant licenses.\nIn particular, MOSEL includes the automatic transcripts of 441k hours of unlabeled speech from VoxPopuli and LibriLight. The data is transcribed using Whisper large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FBK-MT/mosel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","machine-generated","found","multilingual"],"keywords_longer_than_N":true},
	{"name":"Gemini-2.0-Flash-Kore-Voice","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fireblade2534/Gemini-2.0-Flash-Kore-Voice","creator_name":"fireblade2534","creator_url":"https://huggingface.co/fireblade2534","description":"fireblade2534/Gemini-2.0-Flash-Kore-Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"dataset_for_STT_TTSmodels","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Beehzod/dataset_for_STT_TTSmodels","creator_name":"Hoshimov","creator_url":"https://huggingface.co/Beehzod","description":"Beehzod/dataset_for_STT_TTSmodels dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Uzbek","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"contrabandistas_outfit","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jtorregrosa/contrabandistas_outfit","creator_name":"Jorge Torregrosa Lloret","creator_url":"https://huggingface.co/jtorregrosa","description":"\n\t\n\t\t\n\t\tContrabandistas Outfit Dataset\n\t\n\nThis dataset provides a collection of labeled images featuring individuals wearing the traditional Contrabandista outfit, typically worn by participants in the Comparsa Contrabandistas in San Vicente del Raspeig. These images are annotated with labels detailing the pose, orientation, background, and outfit specifics, offering a detailed visual reference for this culturally significant attire.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\n\nTotal Images: 27\nFile‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jtorregrosa/contrabandistas_outfit.","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"diffusion-models-hf-hub","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeFactOfficial/diffusion-models-hf-hub","creator_name":"Sam Rahimi","creator_url":"https://huggingface.co/DeFactOfficial","description":"DeFactOfficial/diffusion-models-hf-hub dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","image-to-image","text-generation","mit","Image"],"keywords_longer_than_N":true},
	{"name":"watercolour2k","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jtorregrosa/watercolour2k","creator_name":"Jorge Torregrosa Lloret","creator_url":"https://huggingface.co/jtorregrosa","description":"jtorregrosa/watercolour2k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","mit","10K<n<100K","Image","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"cc12m-cleaned","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/opendiffusionai/cc12m-cleaned","creator_name":"Open Diffusion AI","creator_url":"https://huggingface.co/opendiffusionai","description":"\n\t\n\t\t\n\t\tCC12m-cleaned\n\t\n\nThis dataset builds on two others: The Conceptual Captions 12million dataset, which lead to the LLaVa captioned subset done by\nCaptionEmporium\n(The latter is the same set, but swaps out the (Conceptual Captions 12million) often-useless alt-text captioning for decent ones_\nI have then used the llava captions as a base, and used the detailed descrptions to filter out\nimages with things like watermarks, artist signatures, etc.\nI have also manually thrown out all‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/opendiffusionai/cc12m-cleaned.","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"thai_handwriting_dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/iapp/thai_handwriting_dataset","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","description":"\n\t\n\t\t\n\t\tThai Handwriting Dataset\n\t\n\nThis dataset combines two major Thai handwriting datasets:\n\nBEST 2019 Thai Handwriting Recognition dataset (train-0000.parquet)\nThai Handwritten Free Dataset by Wang (train-0001.parquet onwards)\n\n\n\t\n\t\t\n\t\tMaintainer\n\t\n\nkobkrit@iapp.co.th\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tBEST 2019 Dataset\n\t\n\nContains handwritten Thai text images along with their ground truth transcriptions. The images have been processed and standardized for machine learning tasks.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/thai_handwriting_dataset.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","Thai","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Theresa","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Theresa","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Theresa dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","text-generation","Chinese","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"danbooru-json","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Disty0/danbooru-json","creator_name":"Disty","creator_url":"https://huggingface.co/Disty0","description":"Saved the JSON responses from Danbooru.Scraped directly from Danbooru.  \nStart id: 1End id: 9500000File count: 9445886Folder names: int(id / 10000)File names: id.jsonAccount: None  \n IDs 0 to 8180000: September 2024 IDs 8180000 to 9000000: March 2025 IDs 9000000 to 9500000: June 2025  \n","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"Open-Qwen2VL-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data","creator_name":"Weizhi Wang","creator_url":"https://huggingface.co/weizhiwang","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis repository contains the data for Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources.\nProject page: https://victorwz.github.io/Open-Qwen2VL\nCode: https://github.com/Victorwz/Open-Qwen2VL\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\n\nccs_ebdataset: CC3M-CC12M-SBU filtered by CLIP, we directly download the webdataset based on the released of curated subset of BLIP-1\ndatacomp_medium_dfn_webdataset: DataComp-Medium-128M filtered by DFN, we just‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weizhiwang/Open-Qwen2VL-Data.","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"youtube-cc-by-music","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WaveGenAI/youtube-cc-by-music","creator_name":"WaveAI","creator_url":"https://huggingface.co/WaveGenAI","description":"üì∫ YouTube-CC-BY-Music üì∫\nYouTube-CC-BY-Music is a comprehensive collection of metadata for 316,000 music tracks shared on YouTube.\nIf you want the version of this dataset including prompt, see https://huggingface.co/datasets/WaveGenAI/youtube-cc-by-music_annoted.\n\n\t\n\t\t\n\t\tContent\n\t\n\nThe dataset includes descriptions, tags, and other metadata associated with 316,000 music videos uploaded to YouTube under the CC-BY license. These videos come from a diverse range of artists and genres, providing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WaveGenAI/youtube-cc-by-music.","first_N":5,"first_N_keywords":["audio-to-audio","audio-classification","text-to-audio","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"e621-2024-webp-4Mpixel_index","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepghs/e621-2024-webp-4Mpixel_index","creator_name":"DeepGHS","creator_url":"https://huggingface.co/deepghs","description":"Index files of NebulaeWis/e621-2024-webp-4Mpixel.\nYou can download images from NebulaeWis/e621-2024-webp-4Mpixel with cheesechaser.\nfrom cheesechaser.datapool import E621NewestWebpDataPool\n\npool = E621NewestWebpDataPool()\n\n# download e621 #2010000-2010300, to directory /data/e621\npool.batch_download_to_directory(\n    resource_ids=range(2010000, 2010300),\n    dst_dir='/data/e621',\n    max_workers=12,\n)\n\n","first_N":5,"first_N_keywords":["image-classification","image-to-image","text-to-image","English","Japanese"],"keywords_longer_than_N":true},
	{"name":"cml-tts-filtered-annotated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PHBJT/cml-tts-filtered-annotated","creator_name":"Paul Henri Biojout","creator_url":"https://huggingface.co/PHBJT","description":"\n\t\n\t\t\n\t\tDataset Card for Filtred and annotated CML TTS\n\t\n\nThis dataset is an annotated and filtred version of a CML-TTS [1]. \nCML-TTS [1] CML-TTS is a recursive acronym for CML-Multi-Lingual-TTS, a Text-to-Speech (TTS) dataset developed at the Center of Excellence in Artificial Intelligence (CEIA) of the Federal University of Goias (UFG). CML-TTS is a dataset comprising audiobooks sourced from the public domain books of Project Gutenberg, read by volunteers from the LibriVox project. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PHBJT/cml-tts-filtered-annotated.","first_N":5,"first_N_keywords":["text-to-speech","French","German","Italian","Spanish"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v5","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v5.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-boundingbox-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify the boundingbox.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-8.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-15.\nfilled bounding box.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-15.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-20.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 3-30.\nfilled+hollow bounding box.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 3-30.\nAdded more variants: filled_inner‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-boundingbox-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v26","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v26.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-erosion-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-erosion-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to erode images by removing the outermost pixels from the colored areas.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-span-v12","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform images that have intersection patterns between row/column spans.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 3-8.\nspan count: 3-5.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\nspan count: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 3-12.\nspan count: 3-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size: 4-8.\nspan count: 4-5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nFocus only on generate_task_with_template_lines.\nimage size:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-span-v12.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-reverse-v3","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-reverse-v3","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to reverse chunks of pixels in a specified direction.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 4-7.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v4","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v4","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"JA_audio_JA_text_180k_samples","keyword":"text-to-speech","license":"Artistic License 2.0","license_url":"https://choosealicense.com/licenses/artistic-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sin2pi/JA_audio_JA_text_180k_samples","creator_name":"Danielle","creator_url":"https://huggingface.co/Sin2pi","description":"","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","text-to-speech","text-to-audio","Japanese"],"keywords_longer_than_N":true},
	{"name":"JA_audio_JA_text_180k_samples","keyword":"text-to-audio","license":"Artistic License 2.0","license_url":"https://choosealicense.com/licenses/artistic-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sin2pi/JA_audio_JA_text_180k_samples","creator_name":"Danielle","creator_url":"https://huggingface.co/Sin2pi","description":"","first_N":5,"first_N_keywords":["automatic-speech-recognition","translation","text-to-speech","text-to-audio","Japanese"],"keywords_longer_than_N":true},
	{"name":"TV-44kHz-Full","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full","creator_name":"Thorsten M√ºller","creator_url":"https://huggingface.co/Thorsten-Voice","description":"\n\t\n\t\t\n\t\tThe \"Thorsten-Voice\" dataset\n\t\n\nThis truly open source (CC0 license) german (üá©üá™) voice dataset contains about 40 hours of transcribed voice recordings by Thorsten M√ºller, \na single male, native speaker in over 38.000 wave files.\n\nMono\nSamplerate: 44.100Hz\nTrimmed silence at begin/end\nDenoised\nNormalized to -24dB\n\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\n\"Please keep in mind, I am not a professional speaker, just an open source speech technology enthusiast who donates his voice. I contribute my personal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","German","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"TV-44kHz-Full","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full","creator_name":"Thorsten M√ºller","creator_url":"https://huggingface.co/Thorsten-Voice","description":"\n\t\n\t\t\n\t\tThe \"Thorsten-Voice\" dataset\n\t\n\nThis truly open source (CC0 license) german (üá©üá™) voice dataset contains about 40 hours of transcribed voice recordings by Thorsten M√ºller, \na single male, native speaker in over 38.000 wave files.\n\nMono\nSamplerate: 44.100Hz\nTrimmed silence at begin/end\nDenoised\nNormalized to -24dB\n\n\n\t\n\t\t\n\t\tDisclaimer\n\t\n\n\"Please keep in mind, I am not a professional speaker, just an open source speech technology enthusiast who donates his voice. I contribute my personal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Thorsten-Voice/TV-44kHz-Full.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","German","cc0-1.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"text_to_speech_dataset","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Kishor798/text_to_speech_dataset","creator_name":"Kishor thagunna","creator_url":"https://huggingface.co/Kishor798","description":"Kishor798/text_to_speech_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sft_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Share4oReasoning/sft_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nSFT instruction: Contains GPT-4o distilled chain-of-thought reasoning data covering wide range of tasks. Together with corresponding short-answer prediction data.\n\nImage: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODO‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/sft_data.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"HumanEdit","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BryanW/HumanEdit","creator_name":"Jinbin Bai","creator_url":"https://huggingface.co/BryanW","description":"\n\t\n\t\t\n\t\tDataset Card for HumanEdit\n\t\n\nPaper (CVPR 2025 AI for Content Creation (AI4CC) Workshop)\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\nfrom datasets import load_dataset\nfrom PIL import Image\n\n# Load the dataset\nds = load_dataset(\"BryanW/HumanEdit\")\n\n# Print the total number of samples and show the first sample\nprint(f\"Total number of samples: {len(ds['train'])}\")\nprint(\"First sample in the dataset:\", ds['train'][0])\n\n# Retrieve the first sample's data\ndata_dict = ds['train'][0]\n\n# Save the input image (INPUT_IMG)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BryanW/HumanEdit.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TIE_shorts","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/raianand/TIE_shorts","creator_name":"Anand Rai","creator_url":"https://huggingface.co/raianand","description":"\n\t\n\t\t\n\t\tDataset Card for TIE_Shorts\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTIE_shorts is a derived version of the Technical Indian English (TIE) dataset, a large-scale speech dataset (~ 8K hours) originally consisting of approximately 750 GB of content \nsourced from the NPTEL platform. The original TIE dataset contains around 9.8K technical lectures in English delivered by instructors from various regions across India, \nwith each lecture averaging about 50 minutes. These lectures cover a wide range of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raianand/TIE_shorts.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Vulpisfoglia","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Vulpisfoglia","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Vulpisfoglia dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["audio-to-audio","text-to-speech","Chinese","Japanese","Italian"],"keywords_longer_than_N":true},
	{"name":"LLaVA-Video-small-swift","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malterei/LLaVA-Video-small-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-small-swift\n\t\n\nSmall subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sample_synthetic_text_to_sql","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chakshu2/sample_synthetic_text_to_sql","creator_name":"vodala chakshu","creator_url":"https://huggingface.co/chakshu2","description":"\n\t\n\t\t\n\t\n\t\n\t\tSample Synthetic Text to SQL Dataset\n\t\n\nThe dataset presents a substantial collection of expertly crafted Text-to-SQL samples, generated using open source LLM's and \nshared under an open-source license. Highlights of the dataset include:\n-- 1563 examples, divided into a training set of 1200 samples and a test set of 363 samples.-- Approximately less than 1 million tokens in total, with nearly 0.5 million representing code-specific tokens.-- Coverage spans a diverse range of 3‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chakshu2/sample_synthetic_text_to_sql.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"sample_synthetic_text_to_sql","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chakshu2/sample_synthetic_text_to_sql","creator_name":"vodala chakshu","creator_url":"https://huggingface.co/chakshu2","description":"\n\t\n\t\t\n\t\n\t\n\t\tSample Synthetic Text to SQL Dataset\n\t\n\nThe dataset presents a substantial collection of expertly crafted Text-to-SQL samples, generated using open source LLM's and \nshared under an open-source license. Highlights of the dataset include:\n-- 1563 examples, divided into a training set of 1200 samples and a test set of 363 samples.-- Approximately less than 1 million tokens in total, with nearly 0.5 million representing code-specific tokens.-- Coverage spans a diverse range of 3‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chakshu2/sample_synthetic_text_to_sql.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"Lappland-the-Decadenza","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Lappland-the-Decadenza","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Lappland-the-Decadenza dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["audio-to-audio","text-to-speech","Chinese","Japanese","Italian"],"keywords_longer_than_N":true},
	{"name":"jacob-common-voice-19-zh-TW-curated","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JacobLinCool/jacob-common-voice-19-zh-TW-curated","creator_name":"Jacob Lin","creator_url":"https://huggingface.co/JacobLinCool","description":"JacobLinCool/jacob-common-voice-19-zh-TW-curated dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Chinese","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"hailuo-ai-voices","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-voices","creator_name":"Christian Peterson","creator_url":"https://huggingface.co/unlimitedbytes","description":"\n\t\n\t\t\n\t\tHailuo AI Voices Dataset üé§\n\t\n\n\n\nA curated collection of high-quality voice recordings with corresponding transcriptions and phoneme analysis. This dataset is designed for speech recognition, text-to-speech, and voice analysis tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tüìä Dataset Overview\n\t\n\nThe dataset provides a comprehensive collection of voice samples with the following features:\n\n\t\n\t\t\nFeature\nDescription\n\n\n\t\t\nAudio Files\nHigh-quality WAV format recordings\n\n\nTranscription\nAccurate transcriptions of each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-voices.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","English","mit"],"keywords_longer_than_N":true},
	{"name":"hailuo-ai-voices","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-voices","creator_name":"Christian Peterson","creator_url":"https://huggingface.co/unlimitedbytes","description":"\n\t\n\t\t\n\t\tHailuo AI Voices Dataset üé§\n\t\n\n\n\nA curated collection of high-quality voice recordings with corresponding transcriptions and phoneme analysis. This dataset is designed for speech recognition, text-to-speech, and voice analysis tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tüìä Dataset Overview\n\t\n\nThe dataset provides a comprehensive collection of voice samples with the following features:\n\n\t\n\t\t\nFeature\nDescription\n\n\n\t\t\nAudio Files\nHigh-quality WAV format recordings\n\n\nTranscription\nAccurate transcriptions of each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-voices.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","English","mit"],"keywords_longer_than_N":true},
	{"name":"danbooru2024-latents-sdxl-1ktar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/6DammK9/danbooru2024-latents-sdxl-1ktar","creator_name":"Darren Laurie","creator_url":"https://huggingface.co/6DammK9","description":"\n\t\n\t\t\n\t\tDanbooru 2024 SDXL VAE latents in 1k tar\n\t\n\n\nDedicated dataset to align deepghs/danbooru2024-webp-4Mpixel. \"4MP-Focus\" for average raw image resolution. \nLatents are ARB with maximum size of 1024x1024 as the recommended setting in kohyas. Major reason is to make sure I can finetune with RTX 3090. VRAM usage will raise drastically after 1024.\nGenerated from prepare_buckets_latents_v2.py, modified from prepare_buckets_latents.py.\nUsed for kohya-ss/sd-scripts. In theory it may replace‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6DammK9/danbooru2024-latents-sdxl-1ktar.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","no-annotation","danbooru"],"keywords_longer_than_N":true},
	{"name":"PhysBench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/USC-GVL/PhysBench","creator_name":"USC Geomtry, Vision, and Learning Lab","creator_url":"https://huggingface.co/USC-GVL","description":"\n  PhysBench \n\n\n    üåê Homepage | ü§ó Dataset | üìë Paper | üíª Code | üî∫ EvalAI\n\n\nThis repo contains evaluation code for the paper \"PhysBench: Benchmarking and Enhancing VLMs for Physical World Understanding\"\nIf you like our project, please give us a star ‚≠ê on GitHub for latest update.\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nUnderstanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/USC-GVL/PhysBench.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2501.16411","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"prl_vintage_ero_comic_style","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paralaif/prl_vintage_ero_comic_style","creator_name":"paralaif","creator_url":"https://huggingface.co/paralaif","description":"\n\t\n\t\t\n\t\tprl_vintage_ero_comic_style Dataset\n\t\n\nImage cut from old erotic Mexican comics.\n","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-style-likert-scoring","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on their visual appeal, without seeing the prompts used to generate them. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CVPR-2021-Accepted-Papers","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepNLP/CVPR-2021-Accepted-Papers","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","description":"\n\t\n\t\t\n\t\tCVPR 2021 Accepted Paper Meta Info Dataset\n\t\n\nThis dataset is collect from the CVPR 2021 Open Access website (https://openaccess.thecvf.com/CVPR2021) as well as the arxiv website DeepNLP paper arxiv (http://www.deepnlp.org/content/paper/cvpr2021). For researchers who are interested in doing analysis of CVPR 2021 accepted papers and potential trends, you can use the already cleaned up json files. Each row contains the meta information of a paper in the CVPR 2021 conference. To explore‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/CVPR-2021-Accepted-Papers.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"emojis","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/emojis","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Emojis.com\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 3,264,372 AI-generated emoji images from Emojis.com. Each entry represents an emoji with associated metadata including prompt text and image URLs.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English (en).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following fields:\n\nslug: Unique identifier for the emoji (string)\nid: Internal ID (string) \nnoBackgroundUrl:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/emojis.","first_N":5,"first_N_keywords":["text-to-image","image-classification","multi-class-image-classification","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"ChronoMagic","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/buraska/ChronoMagic","creator_name":"vadim zolotarenko","creator_url":"https://huggingface.co/buraska","description":"\n\n\n MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\t\n\t\t\n\t\tüê≥ ChronoMagic Dataset\n\t\n\nChronoMagic with 2265 metamorphic time-lapse videos, each accompanied by a detailed caption. We released the subset of ChronoMagic used to train MagicTime. The dataset can be downloaded at HuggingFace Dataset, or you can download it with the following command. Some samples can be found on our Project‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/buraska/ChronoMagic.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"tts-test2","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deniskaanalpay/tts-test2","creator_name":"denis kaan alpay","creator_url":"https://huggingface.co/deniskaanalpay","description":"deniskaanalpay/tts-test2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Turkish","mit","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"RSTeller_metadata","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata","creator_name":"Slytherin Ge","creator_url":"https://huggingface.co/SlytherinGe","description":"\n\t\n\t\t\n\t\tMetadata for RSTeller\n\t\n\nThis dataset contains the necessary metadata for the dataset SlytherinGe/RSTeller.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThe metadata table provides detailed information for the RSTeller dataset, with the following columns:\n\npatch_id: The primary key of the table, corresponding to the \"__key__\" or the \"patch_id\" field in the JSON of the RSTeller dataset.\n\npatch_lat and patch_lon: The latitude and longitude coordinates of the patch center in WGS84‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SlytherinGe/RSTeller_metadata.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","visual-question-answering","zero-shot-classification","summarization"],"keywords_longer_than_N":true},
	{"name":"Theresa-Recording","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Theresa-Recording","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Theresa-Recording dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Japanese","mit","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"anta_women_tts","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galsenai/anta_women_tts","creator_name":"GalsenAI Lab","creator_url":"https://huggingface.co/galsenai","description":"\n\t\n\t\t\n\t\tAnta Women TTS\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a cleaned version of the Wolof TTS dataset by GalsenAI. \nWe extracted the female voice, denoised it and enhanced it with the Resemble Enhance library. \nWe also cleaned up the annotations by removing special characters, emojis, Arabic and Russian characters. \nWe've corrected a few annotation errors, but there are potentially many more to come. \nSome lines and audios judged not qualitative enough have been removed from the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/galsenai/anta_women_tts.","first_N":5,"first_N_keywords":["text-to-speech","Wolof","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"pops_20k","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AmitIsraeli/pops_20k","creator_name":"Amit Israeli","creator_url":"https://huggingface.co/AmitIsraeli","description":"AmitIsraeli/pops_20k dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"IndicTTS_Bengali","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SPRINGLab/IndicTTS_Bengali","creator_name":"SPRINGLab","creator_url":"https://huggingface.co/SPRINGLab","description":"\n\t\n\t\t\n\t\tBengali Indic TTS Dataset\n\t\n\nThis dataset is derived from the Indic TTS Database project, specifically using the Bengali monolingual recordings from both male and female speakers. The dataset contains high-quality speech recordings with corresponding text transcriptions, making it suitable for text-to-speech (TTS) research and development.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguage: Bengali\nTotal Duration: ~15.06 hours (Male: 10.05 hours, Female: 5.01 hours)\nAudio Format: WAV\nSampling Rate:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SPRINGLab/IndicTTS_Bengali.","first_N":5,"first_N_keywords":["text-to-speech","Bengali","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"wan_opening_book","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_opening_book","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_origami_fold_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_origami_fold_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"latam-spanish-speech-orpheus-tts-24khz","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GianDiego/latam-spanish-speech-orpheus-tts-24khz","creator_name":"Gian Diego Javes Lecca","creator_url":"https://huggingface.co/GianDiego","description":"\n\t\n\t\t\n\t\tLATAM Spanish High-Quality Speech Dataset (24kHz - Orpheus TTS Ready)\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains approximately 24 hours of high-quality speech audio in Latin American Spanish, specifically prepared for Text-to-Speech (TTS) applications like OrpheusTTS, which require a 24kHz sampling rate.\nThe audio files are derived from the Crowdsourced high-quality speech datasets made by Google and were obtained via OpenSLR. The original recordings were high-quality‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GianDiego/latam-spanish-speech-orpheus-tts-24khz.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","monolingual","original:openslr","Spanish"],"keywords_longer_than_N":true},
	{"name":"wan_paint_over_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_paint_over_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_petrify_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_petrify_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_picking_up_object","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_picking_up_object","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_pixelate_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_pixelate_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_playing_piano","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_playing_piano","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"talromur3_without_emotions","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/atlithor/talromur3_without_emotions","creator_name":"Atli","creator_url":"https://huggingface.co/atlithor","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis corpus is an emotion-less version of Talromur3_with_prompts. talromur_3_without_emotions is a prompt-labelled corpus that can be used for fine-tuning models, such as ParlerTTS.The corpus consists of approximately 15,000 utterances, spoken by 7 named speakers in 6 different emotions (see more info here).\nThe dataset is an expanded version of Talromur-3: an Icelandic emotional speech corpus.We have added natural-language descriptions of utterance-level pitch, speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/atlithor/talromur3_without_emotions.","first_N":5,"first_N_keywords":["text-to-speech","Icelandic","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"rgb48-image-encodings","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/korrykatti/rgb48-image-encodings","creator_name":"KorryKatti","creator_url":"https://huggingface.co/korrykatti","description":"Uses a dataset of image ( namely : jxie/flickr8k ) and resized them all to 48x48 after which saved each of their pixel value as rgb values and stored them along with caption\n","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wan_puffing_cheeks","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_puffing_cheeks","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_putting_down_object","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_putting_down_object","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_putting_on_glasses","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_putting_on_glasses","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_putting_on_hat","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_putting_on_hat","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_raising_eyebrows","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_raising_eyebrows","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"MediBeng","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng","creator_name":"Promila Ghosh","creator_url":"https://huggingface.co/pr0mila-gh0sh","description":"\n\n  \n\n\n\n  \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MediBeng\n\t\n\nThis dataset includes synthetic code-switched conversations in Bengali and English. It is designed to help train models for tasks like speech recognition (ASR), text-to-speech (TTS), and machine translation, focusing on bilingual code-switching in healthcare settings. The dataset is free to use.For a detailed guide on how this dataset was created, follow the steps outlined in the GitHub repository: ParquetToHuggingFace.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-audio","text-to-speech","translation"],"keywords_longer_than_N":true},
	{"name":"MediBeng","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng","creator_name":"Promila Ghosh","creator_url":"https://huggingface.co/pr0mila-gh0sh","description":"\n\n  \n\n\n\n  \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MediBeng\n\t\n\nThis dataset includes synthetic code-switched conversations in Bengali and English. It is designed to help train models for tasks like speech recognition (ASR), text-to-speech (TTS), and machine translation, focusing on bilingual code-switching in healthcare settings. The dataset is free to use.For a detailed guide on how this dataset was created, follow the steps outlined in the GitHub repository: ParquetToHuggingFace.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-audio","text-to-speech","translation"],"keywords_longer_than_N":true},
	{"name":"MediBeng","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng","creator_name":"Promila Ghosh","creator_url":"https://huggingface.co/pr0mila-gh0sh","description":"\n\n  \n\n\n\n  \n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for MediBeng\n\t\n\nThis dataset includes synthetic code-switched conversations in Bengali and English. It is designed to help train models for tasks like speech recognition (ASR), text-to-speech (TTS), and machine translation, focusing on bilingual code-switching in healthcare settings. The dataset is free to use.For a detailed guide on how this dataset was created, follow the steps outlined in the GitHub repository: ParquetToHuggingFace.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pr0mila-gh0sh/MediBeng.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-audio","text-to-speech","translation"],"keywords_longer_than_N":true},
	{"name":"wan_scrolling_on_phone","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_scrolling_on_phone","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_shaking_head","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_shaking_head","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_shrugging","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_shrugging","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_sketchify_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_sketchify_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"GUI-Lasagne-L1","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SenseLLM/GUI-Lasagne-L1","creator_name":"SenseLLM","creator_url":"https://huggingface.co/SenseLLM","description":"This repository contains the GUI-Lasagne dataset used to train SpiritSight Agent, as described in the paper SpiritSight Agent: Advanced GUI Agent with One Look.\nProject Page: https://hzhiyuan.github.io/SpiritSight-Agent\n","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2503.03196","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Qilin","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUIR/Qilin","creator_name":"THUIR","creator_url":"https://huggingface.co/THUIR","description":"\n\t\n\t\t\n\t\tQilin\n\t\n\nQilin is a large-scale multimodal dataset designed for advancing research in search, recommendation, and Retrieval-Augmented Generation (RAG) systems. This repository contains the official implementation of the dataset paper, baseline models, and evaluation tools.  This dataset was presented in Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions.\nGithub: https://github.com/RED-Search/Qilin\nThe image data can be found at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUIR/Qilin.","first_N":5,"first_N_keywords":["question-answering","text-classification","sentence-similarity","text-retrieval","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"qirimtatar-tts","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yehor/qirimtatar-tts","creator_name":"Smoliakov","creator_url":"https://huggingface.co/Yehor","description":" \n\n\n\t\n\t\t\n\t\tOpen Source Crimean Tatar Text-to-Speech datasets\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tVoices\n\t\n\n\n\t\n\t\t\n\t\tMale\n\t\n\n\n\t\n\t\t\n\t\tAbibullah\n\t\n\n\nQuality: high\nDuration: 2h + 50m\nAudio formats: OPUS\nFrequency: 48000 Hz\n\n\n\t\n\t\t\n\t\tArslan\n\t\n\n\nQuality: high\nDuration: 40m + 40m\nAudio formats: OPUS\nFrequency: 48000 Hz\n\n\n\t\n\t\t\n\t\tFemale\n\t\n\n\n\t\n\t\t\n\t\tSevil\n\t\n\n\nQuality:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Yehor/qirimtatar-tts.","first_N":5,"first_N_keywords":["text-to-speech","Crimean Tatar","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"DeepFashion-MultiModal-Parts2Whole","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LIAGM/DeepFashion-MultiModal-Parts2Whole","creator_name":"Yu-Ju Tsai","creator_url":"https://huggingface.co/LIAGM","description":"\n\t\n\t\t\n\t\tDeepFashion MultiModal Parts2Whole\n\t\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis human image dataset comprising about 41,500 reference-target pairs. Each pair in this dataset includes multiple reference images, which encompass human pose images (e.g., OpenPose, Human Parsing, DensePose), various aspects of human appearance (e.g., hair, face, clothes, shoes) with their short textual labels, and a target image featuring the same individual (ID) in the same outfit‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LIAGM/DeepFashion-MultiModal-Parts2Whole.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"M2RAG","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/whalezzz/M2RAG","creator_name":"Tianshuo Zhou","creator_url":"https://huggingface.co/whalezzz","description":"\n\t\n\t\t\n\t\tData statices of M2RAG\n\t\n\nClick the links below to view our paper and Github project.\n\nIf you find this work useful, please cite our paper  and give us a shining star üåü in Github \n@misc{liu2025benchmarkingretrievalaugmentedgenerationmultimodal,\n      title={Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts}, \n      author={Zhenghao Liu and Xingsheng Zhu and Tianshuo Zhou and Xinyi Zhang and Xiaoyuan Yi and Yukun Yan and Yu Gu and Ge Yu and Maosong Sun}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/whalezzz/M2RAG.","first_N":5,"first_N_keywords":["text-to-image","visual-question-answering","English","mit","Image"],"keywords_longer_than_N":true},
	{"name":"ABC-Pretraining-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tABC Pretraining Data\n\t\n\n\nThis the the pretraining data for ABC. This dataset is derived from Google's Conceptual Captions dataset.\nThe each item in the dataset contain a URL where the corresponding image can be downloaded and mined negatives for each item. Full dataaset is ~300 GB of images. For a detailed description of how we mined the negatives please check out our ppaer ;).Update I have added the images to this repository, for an example of how to use and download this dataset see‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"ABC-VG-Instruct","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tVG Instruct\n\t\n\nThis is the instruction finetuning dataset for ABC: Achieving better control of multimodal embeddings using VLMs.\nEach element in this dataset contains 4 instruction-captions pairs for images in the visual genome dataset, corresponding to different bounding boxes in the image.\nWe use this dataset to train an embedding model that can use instruction to embeds specific aspects of a scene.\n\nCombined with our pretraining step, this results in a model that can create high‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"tts-crh-abibullah","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/tts-crh-abibullah","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":" \n\n\n\t\n\t\t\n\t\tOpen Source Crimean Tatar Text-to-Speech datasets\n\t\n\nThis is subset of Abibullah voice with train/test splits.\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nQuality: high\nDuration: 2h50m\nFrequency: 48 kHz\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { qirimtatar-tts (Revision c2ceec6)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/tts-crh-abibullah.","first_N":5,"first_N_keywords":["text-to-speech","Crimean Tatar","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"tts-crh-sevil","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/tts-crh-sevil","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":" \n\n\n\t\n\t\t\n\t\tOpen Source Crimean Tatar Text-to-Speech datasets\n\t\n\nThis is subset of Sevil voice with train/test splits.\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nQuality: high\nDuration: 2h29m\nFrequency: 48 kHz\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { qirimtatar-tts (Revision c2ceec6) }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/tts-crh-sevil.","first_N":5,"first_N_keywords":["text-to-speech","Crimean Tatar","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"tts-crh-arslan","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/tts-crh-arslan","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":" \n\n\n\t\n\t\t\n\t\tOpen Source Crimean Tatar Text-to-Speech datasets\n\t\n\nThis is subset of Arslan voice with train/test splits.\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tStatistics\n\t\n\n\nQuality: high\nDuration: 1h20m\nFrequency: 48 kHz\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { qirimtatar-tts (Revision c2ceec6) }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/tts-crh-arslan.","first_N":5,"first_N_keywords":["text-to-speech","Crimean Tatar","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"opentts-tetiana","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/opentts-tetiana","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"\n\t\n\t\t\n\t\tOpen Text-to-Speech voices for üá∫üá¶ Ukrainian\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { opentts-uk (Revision 32abc9c) },\n    year         = 2025,\n    url          = { https://huggingface.co/datasets/Yehor/opentts-uk },\n    doi          = { 10.57967/hf/4551 }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/opentts-tetiana.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"opentts-mykyta","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/speech-uk/opentts-mykyta","creator_name":"Speech-UK initiative","creator_url":"https://huggingface.co/speech-uk","description":"\n\t\n\t\t\n\t\tOpen Text-to-Speech voices for üá∫üá¶ Ukrainian\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tCite this work\n\t\n\n@misc {smoliakov_2025,\n    author       = { {Smoliakov} },\n    title        = { opentts-uk (Revision 32abc9c) },\n    year         = 2025,\n    url          = { https://huggingface.co/datasets/Yehor/opentts-uk },\n    doi          = { 10.57967/hf/4551 }‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/speech-uk/opentts-mykyta.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"StoryFrames","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/StoryFrames","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\n\t\n\t\t\n\t\tThe StoryFrames Dataset\n\t\n\nStoryFrames is a human-annotated dataset created to enhance a model's capability of understanding and reasoning over sequences of images.\nIt is specifically designed for tasks like generating a description for the next scene in a story based on previous visual and textual information.\nThe dataset repurposes the StoryBench dataset, a video dataset originally designed to predict future frames of a video.\nStoryFrames subsamples frames from those videos and pairs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/StoryFrames.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","text-to-image","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"HAIC","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KuaishouHAIC/HAIC","creator_name":"KuaishouHAIC","creator_url":"https://huggingface.co/KuaishouHAIC","description":"\n\t\n\t\t\n\t\tHAIC: Human Action and Interaction Comprehension Dataset\n\t\n\nFrom the paper: \"HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models\"\nRead the Paper\n\n\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nHAICBench is a comprehensive video dataset featuring manually annotated, fine-grained human captions that features:\n\nMultiple Human Subjects: Captions detail interactions and activities involving more than one person, capturing the complexity of human‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KuaishouHAIC/HAIC.","first_N":5,"first_N_keywords":["video-text-to-text","English","Chinese","mit","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"VL-Health","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lintw/VL-Health","creator_name":"Lin Tianwei","creator_url":"https://huggingface.co/lintw","description":"\n\t\n\t\t\n\t\tVL-Health Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe VL-Health dataset is designed for multi-stage training of unified LVLMs in the medical domain. It consists of two key phases:\n\nAlignment ‚Äì Focused on training image captioning capabilities and learning representations of input visual information.\n\nInstruct Fine-Tuning ‚Äì Designed for enhancing the model's ability to handle various vision-language tasks, including both visual comprehension and visual generation tasks.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lintw/VL-Health.","first_N":5,"first_N_keywords":["question-answering","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"reverse-instruct-1.3m","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m","creator_name":"Patrick (Tsung-Han) Wu","creator_url":"https://huggingface.co/tsunghanwu","description":"\n\t\n\t\t\n\t\tREVERSE Visual Instruct 1.3M\n\t\n\n\n  \n\n\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDataset Type:REVERSE Visual Instruct 1.3M is a GPT-generated instruction-following dataset designed for training hallucination-aware vision-language models (VLMs). It builds on the LLaVA Instruct 665K dataset and includes structured annotations to indicate model confidence. We introduce three special tokens:  \n\n<SPAN>: marks the beginning of a key phrase  \n</CN>: denotes a confident (grounded) phrase  \n</UN>: denotes an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tsunghanwu/reverse-instruct-1.3m.","first_N":5,"first_N_keywords":["visual-question-answering","question-answering","image-text-to-text","mit","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"FeruzaSpeech_to_fine_tuning","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nickoo004/FeruzaSpeech_to_fine_tuning","creator_name":"Nicholas","creator_url":"https://huggingface.co/nickoo004","description":"\n\t\n\t\t\n\t\tFeruzaSpeech_to_fine_tuning\n\t\n\nA speech corpus of ‚è±Ô∏è¬†~59.1 total hours of Uzbek audio paired with Latin‚Äëscript transcripts, intended for fine‚Äëtuning ASR / speech‚Äëto‚Äëtext models.\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains recordings of native Uzbek speakers reading a mix of classical literature excerpts and school‚Äëlevel writing prompts:\n\n001:‚ÄØCholiqushi (a novel by Rashod‚ÄØNuri‚ÄØGuntekin, trans. by Mirzakalon‚ÄØIsmoiliy; first pub. Sept‚ÄØ1900).  \n002:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nickoo004/FeruzaSpeech_to_fine_tuning.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-audio","Uzbek","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"EmoVoice-DB","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yhaha/EmoVoice-DB","creator_name":"yangguanrou","creator_url":"https://huggingface.co/yhaha","description":"\n\t\n\t\t\n\t\tDataset Card for EmoVoice-DB\n\t\n\n\n\t\n\t\t\n\t\tOverview of EmoVoice-DB\n\t\n\nEmoVoice-DB is an English emotional speech dataset featuring fine-grained emotion labels expressed through natural language descriptions. This dataset contains over 20,000 emotionally expressive speech samples, each annotated with detailed and precise emotional descriptions, totaling approximately 40 hours of audio. EmoVoice-DB is built using synthetic data generated by the powerful‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yhaha/EmoVoice-DB.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K<n<100K","Audio"],"keywords_longer_than_N":true},
	{"name":"MR-Video","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ziqipang/MR-Video","creator_name":"Ziqi Pang","creator_url":"https://huggingface.co/ziqipang","description":"This repository contains the data presented in MR. Video: \"MapReduce\" is the Principle for Long Video Understanding.\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","100K - 1M","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"SMMILE-plusplus","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/smmile/SMMILE-plusplus","creator_name":"smmile","creator_url":"https://huggingface.co/smmile","description":"\n\t\n\t\t\n\t\tSMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning\n\t\n\nPaper | Project page | Code\n\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\nMultimodal in-context learning (ICL) remains underexplored despite the profound potential it could have in complex application domains such as medicine. Clinicians routinely face a long tail of tasks which they need to learn to solve from few examples, such as considering few relevant previous cases or few differential diagnoses. While MLLMs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/smmile/SMMILE-plusplus.","first_N":5,"first_N_keywords":["image-text-to-text","English","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"t2i-finegrain","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain","creator_name":"Kevin David Hayes","creator_url":"https://huggingface.co/KevinDavidHayes","description":"\n\t\n\t\t\n\t\tt2i-finegrain Dataset\n\t\n\nThis dataset evaluates text-to-image (T2I) diffusion models using a benchmark of prompts designed to elicit specific failure modes. Human labels allow for T2I benchmarking evaluations.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\n3,750+ generated images\n750+ prompts\n\n\n11 failure mode categories\n27 specific failure modes\n\n\n5 diffusion models evaluated:\nSD3-XL\nSD3-M\nSD3.5-Large\nSD3.5-Medium\nFlux\n\n\n\n\n\t\n\t\t\n\t\tFolder Structure\n\t\n\nfinegrain_dataset/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.csv \n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"t2i-finegrain","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain","creator_name":"Kevin David Hayes","creator_url":"https://huggingface.co/KevinDavidHayes","description":"\n\t\n\t\t\n\t\tt2i-finegrain Dataset\n\t\n\nThis dataset evaluates text-to-image (T2I) diffusion models using a benchmark of prompts designed to elicit specific failure modes. Human labels allow for T2I benchmarking evaluations.\n\n\t\n\t\t\n\t\tContents\n\t\n\n\n3,750+ generated images\n750+ prompts\n\n\n11 failure mode categories\n27 specific failure modes\n\n\n5 diffusion models evaluated:\nSD3-XL\nSD3-M\nSD3.5-Large\nSD3.5-Medium\nFlux\n\n\n\n\n\t\n\t\t\n\t\tFolder Structure\n\t\n\nfinegrain_dataset/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.csv \n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"T2I-ConBench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/T2I-ConBench/T2I-ConBench","creator_name":"T2I-ConBench","creator_url":"https://huggingface.co/T2I-ConBench","description":"T2I-ConBench/T2I-ConBench dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"RealEdit","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peter-sushko/RealEdit","creator_name":"Peter Sushko","creator_url":"https://huggingface.co/peter-sushko","description":"\n\t\n\t\t\n\t\tRealEdit Dataset\n\t\n\nRealEdit is a large-scale, authentic dataset of image edits collected from Reddit's r/PhotoshopRequest and r/estoration. It is divided into two splits: train and test.\n\nNote: This dataset contains image URLs that may become inactive over time. If you are a researcher and would like access to archived images, please fill out this Google Form.\n\nWe recommend using gallery-dl to download the images, though you are free to use any method.\n\n\n\t\t\n\t\tTest Split Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/peter-sushko/RealEdit.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-Eval","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\t‚ú® Summary\n\t\n\nOpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, \nto accurately align human preferences with S2V benchmarks, we propose three automatic metrics: NexusScore, NaturalScore, GmeScore\nto separately quantify‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"OpenS2V-Eval","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval","creator_name":"YSH","creator_url":"https://huggingface.co/BestWishYsh","description":"\n\n\n OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\n\n If you like our project, please give us a star ‚≠ê on GitHub for the latest update.  \n\n\n\n\t\n\t\t\n\t\t‚ú® Summary\n\t\n\nOpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, \nto accurately align human preferences with S2V benchmarks, we propose three automatic metrics: NexusScore, NaturalScore, GmeScore\nto separately quantify‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BestWishYsh/OpenS2V-Eval.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"MolLangBench","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ChemFM/MolLangBench","creator_name":"ChemFM","creator_url":"https://huggingface.co/ChemFM","description":"\n\t\n\t\t\n\t\tMolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation\n\t\n\n\nThe MolLangBench paper is available on arXiv:2505.15054.\nThe code for using and evaluating the MolLangBench datasets is provided in this GitHub repository.\n\n\n\n\n\nMolLangBench is a comprehensive benchmark designed to evaluate the fundamental capabilities of AI models in language-prompted molecular structure recognition, editing, and generation.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ChemFM/MolLangBench.","first_N":5,"first_N_keywords":["question-answering","text-to-image","image-to-text","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"4H_Test","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jianqunZ/4H_Test","creator_name":"Jianqun Zhou","creator_url":"https://huggingface.co/jianqunZ","description":"jianqunZ/4H_Test dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"wan_skipping_rope","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_skipping_rope","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"synthetic_text_to_sql_en_es","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TafcoMetawireless/synthetic_text_to_sql_en_es","creator_name":"Tafco Metawireless","creator_url":"https://huggingface.co/TafcoMetawireless","description":"\n\t\n\t\t\n\t\tDataset basado en la versi√≥n de GretelAI - SyntheticSQL\n\t\n\n\n\t\n\t\t\n\t\tsynthetic_text_to_sql_en_es\n\t\n\nSe trata de una expansi√≥n mediante la traducci√≥n al espa√±ol de la columna 'sql_prompt'.\nSe ha a√±adido una columna extra 'sql_prompt_es' que contiene el prompt original de ingl√©s traducido al espa√±ol.\nPara obtener estas traducciones, se utiliz√≥ few-shot prompting + CoT mediante el modelo Qwen/Qwen2.5-32B-Instruct-AWQ\n\n\t\n\t\t\n\t\tActualizaci√≥n 6/27/25\n\t\n\nEn la versi√≥n pasada se encontraron‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TafcoMetawireless/synthetic_text_to_sql_en_es.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","text-generation","English","Spanish"],"keywords_longer_than_N":true},
	{"name":"TurkicTTS-Chuvash","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gaydmi/TurkicTTS-Chuvash","creator_name":"Dmitry Gaynullin","creator_url":"https://huggingface.co/gaydmi","description":"\n\t\n\t\t\n\t\tTurkic_TTS-Chuvash\n\t\n\n[Original repository] \n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTurkic_TTS-Chuvash is a speech dataset sourced from the Turkic_TTS GitHub repository. The dataset comprises recordings of text extracted from news articles on chuvash.org and list of digits, all read by a single female speaker at a rapid tempo. The dataset is intended for text-to-speech (TTS) research and development in the Chuvash language. The license and citation information presented in this dataset card has‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gaydmi/TurkicTTS-Chuvash.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Chuvash","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"wan_smiling_gradually","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_smiling_gradually","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_snapping_fingers","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_snapping_fingers","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_spinning_pen","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_spinning_pen","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_standing_up","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_standing_up","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_stretch_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_stretch_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"robotwin-action-prediction-dataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bryandts/robotwin-action-prediction-dataset","creator_name":"Bryan Delton Tawarikh Sibarani","creator_url":"https://huggingface.co/bryandts","description":"\n\t\n\t\t\n\t\tRobotic Action Prediction Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains triplets of (current observation, action instruction, future observation) for training models to predict future frames of robotic actions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ncurrent_frame: Input image (RGB) of the current observation\ninstruction: Textual description of the action to perform\nfuture_frame: Target image (RGB) showing the expected outcome 50 frames later‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bryandts/robotwin-action-prediction-dataset.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Jedi","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xlangai/Jedi","creator_name":"XLang NLP Lab","creator_url":"https://huggingface.co/xlangai","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nNOTE: Before you use this dataset, make sure you understand the logic of absolute coordinates and image processor for Qwen2.5-VL. \nThis dataset is set with the image processor max tokens to be 2700, a.k.a max_pixels=2700x14x14x2x2 , the coordinates were resized to be smaller and you have to resize the image as well within max_pixels=2700x14x14x2x2 via image processor to make them align.\nMake sure you also follow it in your training procedure, otherwise the performance will not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xlangai/Jedi.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2502.13923","arxiv:2505.13227","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"wan_stretching_arms","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_stretching_arms","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_submerge_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_submerge_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_taking_off_glasses","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_taking_off_glasses","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_taking_photo","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_taking_photo","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_throwing_object","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_throwing_object","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_thumbs_down_gesture","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_thumbs_down_gesture","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_thumbs_up_gesture","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_thumbs_up_gesture","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_turning_page","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_turning_page","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_unwrapping_gift","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_unwrapping_gift","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"ruslan_sova_ai","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/intexcp/ruslan_sova_ai","creator_name":"Ivan Shivalov","creator_url":"https://huggingface.co/intexcp","description":"This is a saved ruslan dataset from SOVA AI\n","first_N":5,"first_N_keywords":["text-to-speech","Russian","mit","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"VideoUFO","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WenhaoWang/VideoUFO","creator_name":"Wenhao Wang","creator_url":"https://huggingface.co/WenhaoWang","description":"\n\t\n\t\t\n\t\tNews\n\t\n\n‚ú® Ranked Top 1 in the Hugging Face Dataset Trending List for text-to-video generation on March 7, 2025.\nüíó Financially supported by OpenAI through the Researcher Access Program.\nüåü Downloaded 10,000+ times on Hugging Face after one month of release.\nüî• Featured in Hugging Face Daily Papers on March 4, 2025.\nüëç Drawn interest from leading companies such as Alibaba Group and Baidu Inc.\nüòä Recommended by a famous writer on machine learning: see blog.\n\n\t\n\t\t\n\t\n\t\n\t\tSummary\n\t\n\nThis is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WenhaoWang/VideoUFO.","first_N":5,"first_N_keywords":["text-to-video","text-to-image","image-to-video","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"VideoUFO","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WenhaoWang/VideoUFO","creator_name":"Wenhao Wang","creator_url":"https://huggingface.co/WenhaoWang","description":"\n\t\n\t\t\n\t\tNews\n\t\n\n‚ú® Ranked Top 1 in the Hugging Face Dataset Trending List for text-to-video generation on March 7, 2025.\nüíó Financially supported by OpenAI through the Researcher Access Program.\nüåü Downloaded 10,000+ times on Hugging Face after one month of release.\nüî• Featured in Hugging Face Daily Papers on March 4, 2025.\nüëç Drawn interest from leading companies such as Alibaba Group and Baidu Inc.\nüòä Recommended by a famous writer on machine learning: see blog.\n\n\t\n\t\t\n\t\n\t\n\t\tSummary\n\t\n\nThis is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WenhaoWang/VideoUFO.","first_N":5,"first_N_keywords":["text-to-video","text-to-image","image-to-video","image-to-image","English"],"keywords_longer_than_N":true},
	{"name":"Step-Video-T2V-Eval","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stepfun-ai/Step-Video-T2V-Eval","creator_name":"StepFun","creator_url":"https://huggingface.co/stepfun-ai","description":"This dataset contains the data of the paper Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model.\nCode: https://github.com/stepfun-ai/Step-Video-T2V\nProject page: https://yuewen.cn/videos\n","first_N":5,"first_N_keywords":["text-to-video","mit","< 1K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"danbooru2024-captions-1ktar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/6DammK9/danbooru2024-captions-1ktar","creator_name":"Darren Laurie","creator_url":"https://huggingface.co/6DammK9","description":"\n\t\n\t\t\n\t\tDanbooru 2024 captions only in 1k tar\n\t\n\n\nRaw captions jointed by 7.62M unpublished extended dataset from KBlueLeaf/danbooru2023-metadata-database and 0.48M generated dataset via Minthy/ToriiGate-v0.4-7B in exl2-8bpw mode. There are 8.13M in total.\n\npython convert_meta_to_tar.py\nReading source JSON\nKeys count: 8136011\nmax id: 8360499\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6DammK9/danbooru2024-captions-1ktar.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","no-annotation","danbooru"],"keywords_longer_than_N":true},
	{"name":"HumanEval-V-Benchmark","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark","creator_name":"HumanEval-V","creator_url":"https://huggingface.co/HumanEval-V","description":"\n\t\n\t\t\n\t\tHumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks\n\t\n\n\n    üìÑ Paper  ‚Ä¢\n    üè† Home Page ‚Ä¢\n    üíª GitHub Repository  ‚Ä¢\n    üèÜ Leaderboard ‚Ä¢\n    ü§ó Dataset Viewer \n\n\nHumanEval-V is a novel benchmark designed to evaluate the diagram understanding and reasoning capabilities of Large Multimodal Models (LMMs) in programming contexts. Unlike existing benchmarks, HumanEval-V focuses on coding tasks that require sophisticated visual reasoning over‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Audio-FLAN-Dataset","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HKUSTAudio/Audio-FLAN-Dataset","creator_name":"HKUST Audio","creator_url":"https://huggingface.co/HKUSTAudio","description":"\n\t\n\t\t\n\t\tAudio-FLAN Dataset (Paper)\n\t\n\n(the FULL audio files and jsonl files are still updating)\nAn Instruction-Tuning Dataset for Unified Audio Understanding and Generation Across Speech, Music, and Sound. \n\n\t\n\t\t\n\t\t1. Dataset Structure\n\t\n\nThe Audio-FLAN-Dataset has the following directory structure:\nAudio-FLAN-Dataset/\n‚îú‚îÄ‚îÄ audio_files/\n‚îÇ   ‚îú‚îÄ‚îÄ audio/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 177_TAU_Urban_Acoustic_Scenes_2022/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 179_Audioset_for_Audio_Inpainting/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îú‚îÄ‚îÄ music/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HKUSTAudio/Audio-FLAN-Dataset.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"Audio-FLAN-Dataset","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HKUSTAudio/Audio-FLAN-Dataset","creator_name":"HKUST Audio","creator_url":"https://huggingface.co/HKUSTAudio","description":"\n\t\n\t\t\n\t\tAudio-FLAN Dataset (Paper)\n\t\n\n(the FULL audio files and jsonl files are still updating)\nAn Instruction-Tuning Dataset for Unified Audio Understanding and Generation Across Speech, Music, and Sound. \n\n\t\n\t\t\n\t\t1. Dataset Structure\n\t\n\nThe Audio-FLAN-Dataset has the following directory structure:\nAudio-FLAN-Dataset/\n‚îú‚îÄ‚îÄ audio_files/\n‚îÇ   ‚îú‚îÄ‚îÄ audio/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 177_TAU_Urban_Acoustic_Scenes_2022/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 179_Audioset_for_Audio_Inpainting/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îÇ   ‚îú‚îÄ‚îÄ music/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HKUSTAudio/Audio-FLAN-Dataset.","first_N":5,"first_N_keywords":["text-to-speech","text-to-audio","automatic-speech-recognition","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"wiki-en-in-neerja-speech","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/wiki-en-in-neerja-speech","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"This dataset contains 10K audio samples generated using Microsoft Edge Text-to-Speech via EdgeTTS. \n\nTotal samples: 10K\nAudio format: MP3\nSample rate: 24kHz\nTotal duration: 95735.86 seconds (26.59 hours)\nAverage duration: 9.57 seconds\nLanguages included: English\nVoices used: en-IN-NeerjaExpressiveNeural\n\nOverall this is low quality and should only be used for training toy tts models.\nIn my case this was for finetuning a low quality Piper TTS model.\nInput sentences were randomly sampled from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shb777/wiki-en-in-neerja-speech.","first_N":5,"first_N_keywords":["text-to-audio","automatic-speech-recognition","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SpeechBrown","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-lab/SpeechBrown","creator_name":"LLM-Lab-Org  @QCRI-ALT","creator_url":"https://huggingface.co/llm-lab","description":"  \nModels | Springer Link | arXiv Link | Proposed Dataset  | ACM Digital Library | Website\n\n\t\t\n\t\tDataset Summary\n\t\n\nSpeech Brown is a comprehensive, synthetic, and diverse paired speech-text dataset in 15 categories, covering a wide range of topics from fiction to religion. This dataset consists of over 55,000 sentence-level samples.  \nTo train the CLASP model, we created this dataset based on the Brown Corpus. The synthetic speech was generated using the NVIDIA Tacotron 2 text-to-speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-lab/SpeechBrown.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K<n<100K","Audio"],"keywords_longer_than_N":true},
	{"name":"VisualWebInstruct-Recall","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThis is the dataset recalled from Google Search from the seed images.\n\n\t\n\t\t\n\t\tLinks\n\t\n\nGithub|\nPaper|\nWebsite\n\n\t\n\t\t\n\t\tCitation\n\t\n\n@article{visualwebinstruct,\n    title={VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},\n    author = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},\n    journal={arXiv preprint arXiv:2503.10582},\n    year={2025}\n}\n\n","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Gemini-2.0-Flash-Puck-Voice","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fireblade2534/Gemini-2.0-Flash-Puck-Voice","creator_name":"fireblade2534","creator_url":"https://huggingface.co/fireblade2534","description":"fireblade2534/Gemini-2.0-Flash-Puck-Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"Gemini-2.0-Flash-Aoede-Voice","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fireblade2534/Gemini-2.0-Flash-Aoede-Voice","creator_name":"fireblade2534","creator_url":"https://huggingface.co/fireblade2534","description":"fireblade2534/Gemini-2.0-Flash-Aoede-Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"LJSpeech-1.1-48kHz","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alibabasglab/LJSpeech-1.1-48kHz","creator_name":"Alibaba_Speech_Lab_SG","creator_url":"https://huggingface.co/alibabasglab","description":"\n\t\n\t\t\n\t\tLJSpeech-1.1 High-Resolution Dataset (48,000 Hz)\n\t\n\nThis dataset was created using the method described in HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution and is part of ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment (Github).\nThe LJSpeech-1.1 dataset, widely recognized for its utility in text-to-speech (TTS) and other speech processing tasks, has now been enhanced through‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alibabasglab/LJSpeech-1.1-48kHz.","first_N":5,"first_N_keywords":["audio-to-audio","text-to-speech","apache-2.0","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"ecom-prod-demo","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ariwala99/ecom-prod-demo","creator_name":"Pratham Ariwala","creator_url":"https://huggingface.co/ariwala99","description":"ariwala99/ecom-prod-demo dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-pika2.2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Pika 2.2 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~756k human responses from ~29k human annotators were collected to evaluate Pika 2.2 video generation model on our benchmark. This dataset was collected in ~1 day total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-pika2.2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Pika 2.2 Human Preference\n\t\n\n\n\n\n\n\n\n\nIn this dataset, ~756k human responses from ~29k human annotators were collected to evaluate Pika 2.2 video generation model on our benchmark. This dataset was collected in ~1 day total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would like to see more in the future, please consider‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-pika2.2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"audio_recordings","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/da2ce7/audio_recordings","creator_name":"Cam","creator_url":"https://huggingface.co/da2ce7","description":"\n\t\n\t\t\n\t\tAudio Recordings\n\t\n\nThese are some personal and open source audio recordings. Feel free to use.\n","first_N":5,"first_N_keywords":["audio-to-audio","audio-text-to-text","any-to-any","text-to-speech","English"],"keywords_longer_than_N":true},
	{"name":"aigciqa-20k","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/strawhat/aigciqa-20k","creator_name":"ËçâÂ∏Ω‰∏çÊòØÁå´","creator_url":"https://huggingface.co/strawhat","description":"Dataset from paper: `[CVPR2024] Aigiqa-20k: A large database for ai-generated image quality assessment\nCode: https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image\n@inproceedings{li2024aigiqa,\n  title={Aigiqa-20k: A large database for ai-generated image quality assessment},\n  author={Li, Chunyi and Kou, Tengchuan and Gao, Yixuan and Cao, Yuqin and Sun, Wei and Zhang, Zicheng and Zhou, Yingjie and Zhang, Zhichao and Zhang, Weixia and Wu, Haoning and others},\n  booktitle={Proceedings of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/strawhat/aigciqa-20k.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"EgoLife","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lmms-lab/EgoLife","creator_name":"LMMs-Lab","creator_url":"https://huggingface.co/lmms-lab","description":"Data cleaning, stay tuned! Please refer to https://egolife-ai.github.io/ first for general info.\nCheckout the paper EgoLife (https://arxiv.org/abs/2503.03803) for more information.\nCode: https://github.com/egolife-ai/EgoLife\n","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","mit","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"danbooru2023","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zenless-lab/danbooru2023","creator_name":"Zenless Lab","creator_url":"https://huggingface.co/zenless-lab","description":"\n\t\n\t\t\n\t\t[Mirror]Danbooru2023: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset\n\t\n\nDanbooru2023 is an extension of Danbooru2021, featuring over 6.8 million anime-style images, totaling more than 8.3 TB. \nEach image is accompanied by community-contributed tags that provide detailed descriptions of its content, including characters, \nartists, copyright information, concepts, and attire. \nThis makes it a crucial resource for stylized computer vision tasks and transfer learning.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zenless-lab/danbooru2023.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-to-image","image-classification","English"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M-mapping","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phil329/OpenVid-1M-mapping","creator_name":"binglei li","creator_url":"https://huggingface.co/phil329","description":"\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the extent dataset proposed in the paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\". \nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nNew Feature: Video-ZIP mapping files now available for efficient video lookup (see Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phil329/OpenVid-1M-mapping.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"OpenVid-1M-mapping","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phil329/OpenVid-1M-mapping","creator_name":"binglei li","creator_url":"https://huggingface.co/phil329","description":"\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the extent dataset proposed in the paper \"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation\". \nOpenVid-1M is a high-quality text-to-video dataset designed for research institutions to enhance video quality, featuring high aesthetics, clarity, and resolution. It can be used for direct training or as a quality tuning complement to other video datasets.\nNew Feature: Video-ZIP mapping files now available for efficient video lookup (see Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phil329/OpenVid-1M-mapping.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"3d_layout_reasoning","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning","creator_name":"Zhenyu Pan","creator_url":"https://huggingface.co/zhenyupan","description":"Dataset for MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse\nGithub: https://github.com/PzySeere/MetaSpatial\n","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","< 1K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"V1Q","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\nds = load_dataset(\"b3x0m/Chinese-H-Novels\")\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\n\t\n\t\t\n\t\tHub Model configuration. https://huggingface.co/models\n\t\n\nhub = {\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\n    'HF_TASK':'any-to-any'\n}\n\n\t\n\t\t\n\t\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q.","first_N":5,"first_N_keywords":["text-classification","token-classification","table-question-answering","question-answering","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"VideoUFO","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VideoUFO/VideoUFO","creator_name":"VideoUFO","creator_url":"https://huggingface.co/VideoUFO","description":"\n\t\n\t\t\n\t\tNews\n\t\n\n‚ú® Ranked Top 1 in the Hugging Face Dataset Trending List for text-to-video generation on March 7, 2025.\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation\nVideoUFO is the first dataset curated in alignment with real-world users‚Äô focused topics for text-to-video generation. Specifically, the dataset comprises over 1.09 million video clips spanning 1,291 topics. Here, we select the top 20 most‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VideoUFO/VideoUFO.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"sticers-for-diffusion-course","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gstranger/sticers-for-diffusion-course","creator_name":"George Krupenchenkov","creator_url":"https://huggingface.co/gstranger","description":"gstranger/sticers-for-diffusion-course dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"TVC-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Allen8/TVC-Data","creator_name":"Allen Sun","creator_url":"https://huggingface.co/Allen8","description":"\n\t\n\t\t\n\t\tDataset Card for TVC-Data\n\t\n\nThis repository contains the data presented in Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning.\nProject page: https://sun-hailong.github.io/projects/TVC\nCode: https://github.com/sun-hailong/TVC\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\nA mixture of 345K multimodal long-chain reasoning data. \nFor more statistics of the dataset, please refer to our paper (coming soon)\n\n\t\n\t\t\n\t\n\t\n\t\tSource Data\n\t\n\nLLaVA-OneVision:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Allen8/TVC-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","apache-2.0","arxiv:2503.13360"],"keywords_longer_than_N":true},
	{"name":"augmented_codealpaca-20k-using-together-ai-deepseek-v1","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1","creator_name":"Yiqiao Yin","creator_url":"https://huggingface.co/eagle0504","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis dataset, named CodeAlpaca-20k, consists of examples that blend coding instructions with outputs and reasoning. Each entry includes structured fields like output, instruction, input, and cot (Chain of Thought). It is particularly designed to train and evaluate AI models that generate code and explanations based on simple programming tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tData Collection and Preparation\n\t\n\nData entries are augmented using the augment_answer function that makes API‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eagle0504/augmented_codealpaca-20k-using-together-ai-deepseek-v1.","first_N":5,"first_N_keywords":["reinforcement-learning","text-generation","text-to-speech","English","mit"],"keywords_longer_than_N":true},
	{"name":"t2i_tiny_nasa","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kaangml/t2i_tiny_nasa","creator_name":"Kaan GML","creator_url":"https://huggingface.co/kaangml","description":"\n\t\n\t\t\n\t\tDataset Card for t2i_tiny_nasa\n\t\n\n\n\t\n\t\t\n\t\tNASA Image Dataset\n\t\n\nThis dataset is created using images obtained from NASA's official image library. The dataset contains a collection of images along with their corresponding textual descriptions (prompts). This dataset can be used for various applications, including image-to-text tasks, text-to-image generation, and other AI-based image analysis studies.\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\n\nSource: NASA Image Library\nContent: Images and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaangml/t2i_tiny_nasa.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"globalrg-grounding-task","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UBC-VL/globalrg-grounding-task","creator_name":"UBCVL","creator_url":"https://huggingface.co/UBC-VL","description":"UBC-VL/globalrg-grounding-task dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["image-to-text","text-to-image","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"Mini-YoChameleon-Data","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data","creator_name":"Thao Nguyen","creator_url":"https://huggingface.co/thaoshibe","description":"\n\t\n\t\t\n\t\tMini Yo'Chameleon Data\n\t\n\nThis is a mini-training-data for Yo'Chameleon, with example of personalized subject called <bo> (From Yo'LLaVA).\nWhat you will find:\n\n10/10 positive images for training/ testing\n1000 HARD negative images for training (retrieved from LAION-5B based on similarity with subject)\n1000 random images for training\n\nThe folder structure:\nmini-yochameleon-data\n |_ random_negative_example\n |   |_ [1000 random images example for training recognition abilities]\n |_ test\n |‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thaoshibe/Mini-YoChameleon-Data.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"myanmar-speech-dataset-google-fleurs","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chuuhtetnaing/myanmar-speech-dataset-google-fleurs","creator_name":"Chuu Htet Naing","creator_url":"https://huggingface.co/chuuhtetnaing","description":"Please visit to the GitHub repository for other Myanmar Langauge datasets.\n\n\t\n\t\t\n\t\tMyanmar Speech Dataset (Google Fleurs)\n\t\n\nThis dataset consists exclusively of Myanmar speech recordings, extracted from the larger multilingual Google Fleurs dataset.\nFor the complete multilingual dataset and additional information, please visit the original dataset repository \nof Google Fleurs HuggingFace page.\n\n\t\n\t\t\n\t\tOriginal Source\n\t\n\nFleurs is the speech version of the FLoRes machine translation benchmark.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chuuhtetnaing/myanmar-speech-dataset-google-fleurs.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Burmese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"MS-Bench","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doge1516/MS-Bench","creator_name":"Sherry Doge","creator_url":"https://huggingface.co/doge1516","description":"\n\t\n\t\t\n\t\tDataset Card for MS-Bench\n\t\n\n\n\nThis dataset card is about the multi-subject personalization benchmark used in MS-Diffusion.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nRepository: https://github.com/MS-Diffusion/MS-Diffusion\nPaper[ICLR 2025]: https://arxiv.org/pdf/2406.07209\nModel: https://huggingface.co/doge1516/MS-Diffusion\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\n\n\nThis benchmark contains 7 categories, 40 subjects, and 13 combinations. Details of the data structure are defined in the paper and msbench.py.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/doge1516/MS-Bench.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Video-R1-data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Video-R1/Video-R1-data","creator_name":"Video-R1","creator_url":"https://huggingface.co/Video-R1","description":"This repository contains the data presented in Video-R1: Reinforcing Video Reasoning in MLLMs.\nCode: https://github.com/tulerfeng/Video-R1\nVideo data folder: CLEVRER, LLaVA-Video-178K, NeXT-QA, PerceptionTest, STAR\nImage data folder: Chart, General, Knowledge, Math, OCR, Spatial\nVideo-R1-COT-165k.json is for SFT cold start, and Video-R1-260k.json is for RL training.\nData Format in Video-R1-COT-165k:\n  {\n      \"problem_id\": 2,\n      \"problem\": \"What appears on the screen in Russian during the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Video-R1/Video-R1-data.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Image"],"keywords_longer_than_N":true},
	{"name":"sdf_dataset_en","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/minghanw/sdf_dataset_en","creator_name":"Minghan Wang","creator_url":"https://huggingface.co/minghanw","description":"\n\t\n\t\t\n\t\tSpeechDialogueFactory Dataset\n\t\n\n\n\t\n\t\t\n\t\tBackground\n\t\n\nThis dataset is part of the SpeechDialogueFactory project, a comprehensive framework for generating high-quality speech dialogues at scale. Speech dialogue datasets are essential for developing and evaluating Speech-LLMs, but existing datasets face limitations including high collection costs, privacy concerns, and lack of conversational authenticity. This dataset addresses these challenges by providing synthetically generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/minghanw/sdf_dataset_en.","first_N":5,"first_N_keywords":["text-generation","text-to-speech","audio-to-audio","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SEED-Bench-R1","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TencentARC/SEED-Bench-R1","creator_name":"ARC Lab, Tencent PCG","creator_url":"https://huggingface.co/TencentARC","description":"This repository contains the datasets presented in Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1.\n","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","arxiv:2503.24376","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"Audio-Children-Stories-Collection-Large","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection-Large","creator_name":"Feynman Innovations","creator_url":"https://huggingface.co/ajibawa-2023","description":"Audio Chidren Stories Collection Large\nThis dataset has 5600++ audio files in .mp3 format. This has been created using my existing dataset Children-Stories-Collection.\nI have used first 5600++ stories from Children-Stories-1-Final.json file for creating this audio dataset.\nYou can use this for training and research purpose.\nThank you for your love & support.\n","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Google DeepMind Veo2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to date‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-veo2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Google DeepMind Veo2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to date‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"KVG","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MaxyLee/KVG","creator_name":"Xinyu Ma","creator_url":"https://huggingface.co/MaxyLee","description":"\n\t\n\t\t\n\t\tDeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding\n\t\n\nXinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun\n\n\n \n \n \nThis is the official repository of KVG training data for DeepPerception.\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K<n<100K","Image"],"keywords_longer_than_N":true},
	{"name":"test_ham10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tarunroy/test_ham10","creator_name":"Tarun Roy","creator_url":"https://huggingface.co/tarunroy","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tarunroy/test_ham10.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Corinth_dataset","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OlameMend/Corinth_dataset","creator_name":"leo","creator_url":"https://huggingface.co/OlameMend","description":"OlameMend/Corinth_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","cc-by-4.0","< 1K","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"LSDBench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TainU/LSDBench","creator_name":"QU Tianyuan","creator_url":"https://huggingface.co/TainU","description":"\n\t\n\t\t\n\t\tDataset Card for LSDBench: Long-video Sampling Dilemma Benchmark\n\t\n\nA benchmark that focuses on the sampling dilemma in long-video tasks. Through well-designed tasks, it evaluates the sampling efficiency of long-video VLMs.\nArxiv Paper: üìñ Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?\nGithub : https://github.com/dvlab-research/LSDBench\n(Left) In Q1, identifying a camera wearer's visited locations requires analyzing the entire video. However, key frames‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TainU/LSDBench.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"reflect-dit-train-images","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KonstantinosKK/reflect-dit-train-images","creator_name":"Konstantinos Kallidromitis","creator_url":"https://huggingface.co/KonstantinosKK","description":"\n\t\n\t\t\n\t\tReflect-DiT Training Images\n\t\n\nThis is the official dataset repository for the training images used in Reflect-DiT, a Reflective Diffusion Transformer for image generation.\nüîó Paper: Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection\n\n\t\n\t\t\n\t\tContents\n\t\n\nThe dataset is stored in multiple .tar archives located in the data/ directory:\ndata/\n‚îú‚îÄ‚îÄ gen_eval_sana_part_0.tar\n‚îú‚îÄ‚îÄ gen_eval_sana_part_1.tar\n‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ gen_eval_sana_part_9.tar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KonstantinosKK/reflect-dit-train-images.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","üá∫üá∏ Region: US","code"],"keywords_longer_than_N":true},
	{"name":"reflect-dit-train-images","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KonstantinosKK/reflect-dit-train-images","creator_name":"Konstantinos Kallidromitis","creator_url":"https://huggingface.co/KonstantinosKK","description":"\n\t\n\t\t\n\t\tReflect-DiT Training Images\n\t\n\nThis is the official dataset repository for the training images used in Reflect-DiT, a Reflective Diffusion Transformer for image generation.\nüîó Paper: Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection\n\n\t\n\t\t\n\t\tContents\n\t\n\nThe dataset is stored in multiple .tar archives located in the data/ directory:\ndata/\n‚îú‚îÄ‚îÄ gen_eval_sana_part_0.tar\n‚îú‚îÄ‚îÄ gen_eval_sana_part_1.tar\n‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ gen_eval_sana_part_9.tar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KonstantinosKK/reflect-dit-train-images.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","üá∫üá∏ Region: US","code"],"keywords_longer_than_N":true},
	{"name":"infotainment_dataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Abhinav9605/infotainment_dataset","creator_name":"Abhinav Lather","creator_url":"https://huggingface.co/Abhinav9605","description":"Abhinav9605/infotainment_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"unidisc_hq","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aswerdlow/unidisc_hq","creator_name":"Alexander Swerdlow","creator_url":"https://huggingface.co/aswerdlow","description":"This repository contains the dataset used in the paper Unified Multimodal Discrete Diffusion.\nCode: https://github.com/AlexSwerdlow/unidisc\nAdditionally, we release a synthetic dataset available here and the corresponding generation scripts as well as the raw data.\n","first_N":5,"first_N_keywords":["image-text-to-text","mit","10M - 100M","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"TinyLLaVA-Video-v1-training-data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data","creator_name":"Zhang Xingjian","creator_url":"https://huggingface.co/Zhang199","description":"TinyLLaVA-Video\n\nThis dataset combines data from multiple sources for pre-training and fine-tuning.\nPretrain Data: Four subsets of LLaVA-Video-178K (0_30_s_academic_v0_1, 30_60_s_academic_v0_1, 0_30_s_youtube_v0_1, 30_60_s_youtube_v0_1), supplemented with filtered Video-LLaVA data (https://huggingface.co/datasets/LanguageBind/Video-LLaVA) and data from Valley (https://github.com/RupertLuo/Valley). The video data can be downloaded from the linked datasets, and cleaned annotations are provided‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zhang199/TinyLLaVA-Video-v1-training-data.","first_N":5,"first_N_keywords":["video-text-to-text","apache-2.0","arxiv:2501.15513","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"GenDS","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sudarshan2002/GenDS","creator_name":"Sudarshan Rajagopalan","creator_url":"https://huggingface.co/Sudarshan2002","description":"\n\t\n\t\t\n\t\t[CVPR-2025] GenDeg: Diffusion-based Degradation Synthesis for Generalizable All-In-One Image Restoration\n\t\n\n\n\t\n\t\t\n\t\tDataset Card for GenDS dataset\n\t\n\n\nThe GenDS dataset is a large dataset to boost the generalization of image restoration models. It is a combination of existing image restoration datasets and \ndiffusion-generated degraded samples from GenDeg. \n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThe dataset is fairly large at ~360GB. We recommend having at least 800GB of free space. To download the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sudarshan2002/GenDS.","first_N":5,"first_N_keywords":["text-to-image","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"syntetic_necoarc_rus","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Kostya165/syntetic_necoarc_rus","creator_name":"pleroma_cascade","creator_url":"https://huggingface.co/Kostya165","description":"\n\t\n\t\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞—É–¥–∏–æ–∑–∞–ø–∏—Å–∏ —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ RVC (Retrieval-based Voice Conversion) NecoArc. –î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n–Ø –Ω–µ –æ—Å–∏–ª–∏–ª —Ñ–∞–π–Ω—Ç—é–Ω TTS –º–æ–¥–µ–ª–∏ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–π , —Ç–∞–∫ —á—Ç–æ –µ—Å–ª–∏ —Å–º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –±—É–¥—É –±–ª–∞–≥–æ–¥–∞—Ä–µ–Ω –∑–∞ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å\n\n–Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π\n–õ–∏—Ü–µ–Ω–∑–∏—è: MIT\n\n\n\t\n\t\t\n\t\n\t\n\t\t–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\t\n\n–î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è:\n\ntext‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kostya165/syntetic_necoarc_rus.","first_N":5,"first_N_keywords":["text-to-speech","Russian","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TDC_training_data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hoar012/TDC_training_data","creator_name":"Haoran Hao","creator_url":"https://huggingface.co/Hoar012","description":"\n\t\n\t\t\n\t\tTraining data used in TDC.\n\t\n\nPaper Link:\nhttps://arxiv.org/pdf/2504.10443\nProject Page:\nhttps://hoar012.github.io/TDC-Project\nCode:\nhttps://github.com/Hoar012/TDC-Video\n","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","apache-2.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"emilia-yodas","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TTS-AGI/emilia-yodas","creator_name":"TTS AGI","creator_url":"https://huggingface.co/TTS-AGI","description":"A mirror of the Emilia-YODAS dataset. Only includes the YODAS subset from the original dataset.\nhttps://huggingface.co/datasets/amphion/Emilia-Dataset\n","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","English","German","French"],"keywords_longer_than_N":true},
	{"name":"siglip-recursion-ffhq-thumbnails","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/hibana2077/siglip-recursion-ffhq-thumbnails","creator_name":"XUAN-HAO LI","creator_url":"https://huggingface.co/hibana2077","description":"\n\t\n\t\t\n\t\tSIGLIP-Recursion-FFHQ-ThumbNail\n\t\n\n\n\n\t\n\t\t\n\t\tLabels\n\t\n\n‚Ä¢\tsmileÔºösmiling, no smile\n‚Ä¢\tpitchÔºödownward, neutral, upward\n‚Ä¢\trollÔºötilted, neutral\n‚Ä¢\tyawÔºöturned left, frontal, turned right\n‚Ä¢\tgenderÔºömale, female\n‚Ä¢\tageÔºöadult, baby, child, teenager, senior\n‚Ä¢\tfacialHairÔºömoustache, beard, sideburns, none\n‚Ä¢\tglassesÔºöNoGlasses, SwimmingGoggles, Sunglasses, ReadingGlasses\n‚Ä¢\temotionÔºöhappiness, sadness, surprise, contempt, disgust, neutral, anger, fear\n‚Ä¢\tblurÔºöhigh, medium, low\n‚Ä¢\texposureÔºögoodExposure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hibana2077/siglip-recursion-ffhq-thumbnails.","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","text","Text"],"keywords_longer_than_N":true},
	{"name":"SyntheticFurGroundtruth","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BXYMartin/SyntheticFurGroundtruth","creator_name":"Martin Bai","creator_url":"https://huggingface.co/BXYMartin","description":"\n\t\n\t\t\n\t\tProcessed Synthetic Fur Dataset\n\t\n\nThis dataset is part of the dataset located here https://github.com/google-research-datasets/synthetic-fur, with modifications to keep only the ground truth samples and add correponding description ending in *.txt.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nContains high-quality synthetic rendered fur in static and moving scenes, with different lightning conditions.\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","apache-2.0","10K - 100K","webdataset"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Alibaba Wan2.1 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Alibaba Wan2.1 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"NumPro_FT","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Liang0223/NumPro_FT","creator_name":"Yongliang","creator_url":"https://huggingface.co/Liang0223","description":"This repository contains the dataset described in the paper Number it: Temporal Grounding Videos like Flipping Manga.\nCode: https://github.com/yongliang-wu/NumPro\n","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","webdataset","Text"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v194","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v194","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v194.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v212","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v212","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v212.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"suno","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/suno","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Suno.ai Music Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 659,788 songs generated by artificial intelligence on the suno.com platform, a service that generates music using artificial intelligence. The songs were discovered by search queries with words from the dwyl/english-words word list.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is multilingual with English as the primary language:\n\nEnglish (en): Primary language for metadata and most lyrics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/suno.","first_N":5,"first_N_keywords":["audio-classification","text-to-audio","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Lappland","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/None1145/Lappland","creator_name":"None","creator_url":"https://huggingface.co/None1145","description":"None1145/Lappland dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["audio-to-audio","text-to-speech","Chinese","Japanese","English"],"keywords_longer_than_N":true},
	{"name":"Sparrow-Synthetic","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic","creator_name":"Shukang Yin","creator_url":"https://huggingface.co/xjtupanda","description":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation\n\n        üíª GitHub¬†¬† | ¬†¬† üìë Paper ¬†¬†  \n\n\n\n\n\t\n\t\t\n\t\tDataset Card for Sparrow-Synthetic\n\t\n\n\n\nThis is a synthetic \"video\" instruction dataset derived from language data.\nIt is designed to enrich the instruction diversity of video training corpus.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThis dataset is curated from longQA text datasets, LongAlpaca-12k and LongQLoRA-39K.\nEach sample can be abstracted as a (long-context, instruction, answer)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xjtupanda/Sparrow-Synthetic.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K<n<100K","arxiv:2411.19951"],"keywords_longer_than_N":true},
	{"name":"mike-no-hito","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/v2ray/mike-no-hito","creator_name":"LagPixelLOL","creator_url":"https://huggingface.co/v2ray","description":"\n\t\n\t\t\n\t\tMike No Hito\n\t\n\nThis dataset contains nearly all the images from artist Mike No Hito, huge thanks to him for the cute catgirls :3.\nCleaned, deduped, and tagged using 9001/copyparty, LagPixelLOL/mitgw, and SmilingWolf/wd-eva02-large-tagger-v3.\n>:3 me when on my way to steal everything and throw them into gradient descent >:P\n","first_N":5,"first_N_keywords":["text-to-image","mit","< 1K","webdataset","Image"],"keywords_longer_than_N":true},
	{"name":"visdrone-text-to-image","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Westcott/visdrone-text-to-image","creator_name":"Issac Westcott","creator_url":"https://huggingface.co/Westcott","description":"Westcott/visdrone-text-to-image dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v10","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v10.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-scale-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the images gets scaled up/down in both x and y direction.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nscale factor: 1-3.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-20.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-30.\nscale factor: 1-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a few noise to the images.\nimage size: 1-10.\nscale factor: 1-7.\nOnly scale down.\nNumber of noise pixels per pixel cell: 0-2.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nMore noisy images for down scaling.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-scale-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-color-v17","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the colors gets manipulated.\nCurrently it's two-color images, where the transformation is to swap colors.\nThe image sizes are between 1 and 5 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nNumber of test: 1-2. Previously it was always 1 test.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\ninput image size: 1-3.\nNumber of tests: 1.\nIdentify most popular color, and least popular color. The output size is always 1x1.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\ninput image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-color-v17.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-skew-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply skew/unkew in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 1-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-7.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nReplaced RLE compressed response with raw pixel response.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 1-9.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nSmaller images‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-skew-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-symmetry-v27","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to transform symmetric images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-3.\nsymmetry types: hstack2, hstack3, vstack2, vstack3, grid2x2.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded HSTACK4, VSTACK4.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded HSTACK5, VSTACK5.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded ImageSymmetrySquare, so images can be rotated by 90 degrees, and flipped over the diagonals.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nOnly exercising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-symmetry-v27.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VISTA-400K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VISTA-400K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tVISTA-400K\n\t\n\nThis repo contains all subsets for VISTA-400K. VISTA is a video spatiotemporal augmentation method that generates long-duration and high-resolution video instruction-following data to enhance the video understanding capabilities of video LMMs.\n\n\t\n\t\t\n\t\tThis repo is under construction. Please stay tuned.\n\t\n\nüåê Homepage | üìñ arXiv | üíª GitHub | ü§ó VISTA-400K | ü§ó Models | ü§ó HRVideoBench\n\n\t\n\t\n\t\n\t\tVideo Instruction Data Synthesis Pipeline\n\t\n\n\nVISTA leverages insights from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/VISTA-400K.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","mit","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"realistic-vision-dslr-tr3s","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/trazer1/realistic-vision-dslr-tr3s","creator_name":"t smith","creator_url":"https://huggingface.co/trazer1","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [T,Smith]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [curl -X GET‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trazer1/realistic-vision-dslr-tr3s.","first_N":5,"first_N_keywords":["image-feature-extraction","text-to-image","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"try_1","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/itzayush21/try_1","creator_name":"Ayush Kumar Singh","creator_url":"https://huggingface.co/itzayush21","description":"itzayush21/try_1 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"sample-id","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/grandhigh/sample-id","creator_name":"Reyhan Al","creator_url":"https://huggingface.co/grandhigh","description":"grandhigh/sample-id dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Indonesian","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ai-gospel-music-dictionaries","keyword":"text-to-audio","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmathhug/ai-gospel-music-dictionaries","creator_name":"Cruz Macias","creator_url":"https://huggingface.co/cmathhug","description":"\n\t\n\t\t\n\t\tAI Gospel Music Dictionaries\n\t\n\nA comprehensive collection of JSON dictionaries designed to support AI-powered gospel music and lyrics generation, with a focus on biblical and theological content.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository contains a curated set of dictionaries that can be used for:\n\nMusic generation tasks\nLyrics generation with biblical themes\nInstrument and genre specifications\nBiblical reference materials\n\n\n\t\n\t\t\n\t\tDictionaries\n\t\n\n\n\t\n\t\t\n\t\tBiblical Content‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmathhug/ai-gospel-music-dictionaries.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","cc0-1.0","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"ai-gospel-music-dictionaries","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmathhug/ai-gospel-music-dictionaries","creator_name":"Cruz Macias","creator_url":"https://huggingface.co/cmathhug","description":"\n\t\n\t\t\n\t\tAI Gospel Music Dictionaries\n\t\n\nA comprehensive collection of JSON dictionaries designed to support AI-powered gospel music and lyrics generation, with a focus on biblical and theological content.\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis repository contains a curated set of dictionaries that can be used for:\n\nMusic generation tasks\nLyrics generation with biblical themes\nInstrument and genre specifications\nBiblical reference materials\n\n\n\t\n\t\t\n\t\tDictionaries\n\t\n\n\n\t\n\t\t\n\t\tBiblical Content‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cmathhug/ai-gospel-music-dictionaries.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","cc0-1.0","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Prompt Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-physics-likert-scoring","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Physics Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on if gravity and colisions make sense, without seeing the prompts used to generate them.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"laion-coco-13m-molmo-d-7b","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CaptionEmporium/laion-coco-13m-molmo-d-7b","creator_name":"Caption Emporium","creator_url":"https://huggingface.co/CaptionEmporium","description":"\n\t\n\t\t\n\t\tDataset Card for laion-coco-13m-molmo-d-7b\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is 41,409,699 new synthetic captions for the 13,803,233 images found in laion/laion-coco. It includes the original captions from that repository as well as new captions. The dataset was filtered to images >= 512px on the short edge.\nThe long captions were produced using allenai/Molmo-7B-D-0924. Medium and short captions were produced from these captions using allenai/Llama-3.1-Tulu-3-8B-DPO. The dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaptionEmporium/laion-coco-13m-molmo-d-7b.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","other","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Runway Alpha Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Runway Alpha Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-aligned-words","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Word for Word Alignment Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"sora-video-generation-time-flow","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Time flow Annotation Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \"How‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow.","first_N":5,"first_N_keywords":["video-classification","text-to-video","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"dino","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YaArtemNosenko/dino","creator_name":"Nosenko","creator_url":"https://huggingface.co/YaArtemNosenko","description":"YaArtemNosenko/dino dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","Russian","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MJ-BENCH-VIDEO","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO","creator_name":"MJ-Bench-Team","creator_url":"https://huggingface.co/MJ-Bench","description":"This repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\".\nPaper: https://arxiv.org/abs/2502.01719\nCode: https://github.com/aiming-lab/MJ-Video\nProject Page: https://aiming-lab.github.io/MJ-VIDEO.github.io/\nThis repository contains the implementation of the paper \"MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation\". We create a fine-grained video preference dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MJ-Bench/MJ-BENCH-VIDEO.","first_N":5,"first_N_keywords":["video-text-to-text","mit","10K - 100K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"multimodal_meme_classification_singapore","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aliencaocao/multimodal_meme_classification_singapore","creator_name":"Billy Cao","creator_url":"https://huggingface.co/aliencaocao","description":"\n\t\n\t\t\n\t\tDataset Card for Offensive Memes in Singapore Context\n\t\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset is a collection of memes from various existing datasets, online forums, and freshly scrapped contents. It contains both global-context memes and Singapore-context memes, in different splits. It has textual description and a label stating if it is offensive under Singapore society's standards.\n\nCurated by: Cao Yuxuan, Wu Jiayang, Alistair Cheong, Theodore Lee‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aliencaocao/multimodal_meme_classification_singapore.","first_N":5,"first_N_keywords":["text-generation","visual-question-answering","image-text-to-text","found","expert-generated"],"keywords_longer_than_N":true},
	{"name":"t2isafety_evaluation","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenSafetyLab/t2isafety_evaluation","creator_name":"OpenSafetyLab","creator_url":"https://huggingface.co/OpenSafetyLab","description":"OpenSafetyLab/t2isafety_evaluation dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"fastcup-highlights","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/fastcup-highlights","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Fastcup.net Highlights\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 85,488 video clips from the gaming platform Fastcup.net, with 78,143 clips from Counter-Strike 2 and 7,345 clips from Counter-Strike: Global Offensive. The clips showcase gameplay highlights and include detailed metadata such as player statistics, weapon information, and engagement metrics. The total size of raw video content is approximately 34 TB.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/fastcup-highlights.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"TextAtlasEval","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CSU-JPG/TextAtlasEval","creator_name":"Jinpeng Group","creator_url":"https://huggingface.co/CSU-JPG","description":"This dataset is a evaluation set for TextAtlas, described in the paper TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation.\n\n\t\n\t\t\n\t\tEvaluation\n\t\n\nOur evaluation scripts are now available on github !\n\n\t\n\t\t\n\t\tDataset subsets\n\t\n\nSubsets in this dataset are styledtextsynth, textsceneshq and textvisionblend. The dataset features are as follows: \n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nimage (img): The GT image.\nannotation (string): The input prompt used to generate the text.\nimage_path‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CSU-JPG/TextAtlasEval.","first_N":5,"first_N_keywords":["text-to-image","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"kinh-phap-hoa-ke-trom-huong","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hr16/kinh-phap-hoa-ke-trom-huong","creator_name":"Abel Greyrat","creator_url":"https://huggingface.co/hr16","description":"Normalized using https://github.com/oysterlanguage/emiliapipex\n@article{emilia,\n      title={Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation},\n      author={He, Haorui and Shang, Zengqiang and Wang, Chaoren and Li, Xuyuan and Gu, Yicheng and Hua, Hua and Liu, Liwei and Yang, Chen and Li, Jiaqi and Shi, Peiyang and Wang, Yuancheng and Chen, Kai and Zhang, Pengyuan and Wu, Zhizheng},\n      journal={arXiv},\n      volume={abs/2407.05361}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hr16/kinh-phap-hoa-ke-trom-huong.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","automatic-speech-recognition","Vietnamese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"kinh-phap-hoa-ke-trom-huong","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hr16/kinh-phap-hoa-ke-trom-huong","creator_name":"Abel Greyrat","creator_url":"https://huggingface.co/hr16","description":"Normalized using https://github.com/oysterlanguage/emiliapipex\n@article{emilia,\n      title={Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation},\n      author={He, Haorui and Shang, Zengqiang and Wang, Chaoren and Li, Xuyuan and Gu, Yicheng and Hua, Hua and Liu, Liwei and Yang, Chen and Li, Jiaqi and Shi, Peiyang and Wang, Yuancheng and Chen, Kai and Zhang, Pengyuan and Wu, Zhizheng},\n      journal={arXiv},\n      volume={abs/2407.05361}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hr16/kinh-phap-hoa-ke-trom-huong.","first_N":5,"first_N_keywords":["text-to-audio","text-to-speech","automatic-speech-recognition","Vietnamese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"opentts-uk-aesthetics","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Yehor/opentts-uk-aesthetics","creator_name":"Smoliakov","creator_url":"https://huggingface.co/Yehor","description":"\n\t\n\t\t\n\t\tAesthetics of Open Text-to-Speech for üá∫üá¶ Ukrainian dataset\n\t\n\n\n\t\n\t\t\n\t\tCommunity\n\t\n\n\nDiscord: https://bit.ly/discord-uds\nSpeech Recognition: https://t.me/speech_recognition_uk\nSpeech Synthesis: https://t.me/speech_synthesis_uk\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThis dataset contains metrics for https://huggingface.co/datasets/Yehor/opentts-uk dataset retrieved by https://github.com/facebookresearch/audiobox-aesthetics \n\n\t\n\t\t\n\t\tHow metrics calculated?\n\t\n\nYou can find a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Yehor/opentts-uk-aesthetics.","first_N":5,"first_N_keywords":["text-to-speech","Ukrainian","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"piper_italiano","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kirys79/piper_italiano","creator_name":"Federico Improta","creator_url":"https://huggingface.co/kirys79","description":"\n\t\n\t\t\n\t\tPiper Italiano\n\t\n\nSto cercando di creare un nuovo checkpoint per PiperTTS in italiano.\nLa fonte per il traine √® il Multilingual LibriSpeech (MLS) rilasciato sotto licenza Creative Commons\nQui metter√≤ i dataset estratti dal suddetto blocco dati\nIl dataset √® nel formato che gradisce PiperTTS come indicato a questo link\n\nAurora √® lo speaker 6807\nLeonardo √® lo speaker 1595 - Probabile voce di Riccardo (modello originale di piper) ma ad una maggiore qualit√†\n\n","first_N":5,"first_N_keywords":["text-to-speech","Italian","cc-by-4.0","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"medmax_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mint-medmax/medmax_data","creator_name":"mint-medmax","creator_url":"https://huggingface.co/mint-medmax","description":"\n\t\n\t\t\n\t\tMedMax Dataset\n\t\n\n\n\t\n\t\t\n\t\tMixed-Modal Instruction Tuning for Training Biomedical Assistants\n\t\n\nAuthors: Hritik Bansal, Daniel Israel‚Ä†, Siyan Zhao‚Ä†, Shufan Li, Tung Nguyen, Aditya GroverInstitution: University of California, Los Angeles‚Ä† Equal Contribution\nPaper | Code | Project Page\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nLarge Language Models (LLMs) and Large Multimodal Models (LMMs) have demonstrated remarkable capabilities in multimodal information integration, opening transformative possibilities‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mint-medmax/medmax_data.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"X2I-in-context-learning","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yzwang/X2I-in-context-learning","creator_name":"Yueze Wang","creator_url":"https://huggingface.co/yzwang","description":"\n\t\n\t\t\n\t\tX2I Dataset\n\t\n\n\nProject Page: https://vectorspacelab.github.io/OmniGen/\nGithub: https://github.com/VectorSpaceLab/OmniGen\nPaper: https://arxiv.org/abs/2409.11340\nModel: https://huggingface.co/Shitao/OmniGen-v1\n\nTo achieve robust multi-task processing capabilities, it is essential to train the OmniGen on large-scale and diverse datasets. However, in the field of unified image generation, a readily available dataset has yet to emerge. For this reason, we have curated a large-scale‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yzwang/X2I-in-context-learning.","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","100K<n<1M"],"keywords_longer_than_N":true},
	{"name":"synthetic-emotions","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/aadityaubhat/synthetic-emotions","creator_name":"Aaditya Bhat","creator_url":"https://huggingface.co/aadityaubhat","description":"\n\t\n\t\t\n\t\tSynthetic Emotions Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nSynthetic Emotions is a video dataset of AI-generated human emotions created using OpenAI Sora. It features short (5-sec, 480p, 9:16) videos depicting diverse individuals expressing emotions like happiness, sadness, anger, fear, surprise, and more.\nThis dataset is ideal for emotion recognition, facial expression analysis, affective computing, and AI-human interaction research.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\nTotal Videos: 100\nVideo Format:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aadityaubhat/synthetic-emotions.","first_N":5,"first_N_keywords":["video-classification","text-to-video","mit","< 1K","Text"],"keywords_longer_than_N":true},
	{"name":"MANGO","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4bharat/MANGO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","description":"\n\t\n\t\t\n\t\tMANGO: A Corpus of Human Ratings for Speech\n\t\n\nMANGO (MUSHRA Assessment corpus using Native listeners and Guidelines to understand human Opinions at scale) is the first large-scale dataset designed for evaluating Text-to-Speech (TTS) systems in Indian languages. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\n255,150 human ratings of TTS-generated outputs and ground-truth human speech.\nCovers two major Indian languages: Hindi & Tamil, and English.\nBased on the MUSHRA (Multiple Stimuli with Hidden Reference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/MANGO.","first_N":5,"first_N_keywords":["text-to-speech","crowd-sourced","Hindi","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"MANGO","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai4bharat/MANGO","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","description":"\n\t\n\t\t\n\t\tMANGO: A Corpus of Human Ratings for Speech\n\t\n\nMANGO (MUSHRA Assessment corpus using Native listeners and Guidelines to understand human Opinions at scale) is the first large-scale dataset designed for evaluating Text-to-Speech (TTS) systems in Indian languages. \n\n\t\n\t\t\n\t\tKey Features:\n\t\n\n\n255,150 human ratings of TTS-generated outputs and ground-truth human speech.\nCovers two major Indian languages: Hindi & Tamil, and English.\nBased on the MUSHRA (Multiple Stimuli with Hidden Reference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/MANGO.","first_N":5,"first_N_keywords":["text-to-speech","crowd-sourced","Hindi","Tamil","English"],"keywords_longer_than_N":true},
	{"name":"sTinyStories","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/slprl/sTinyStories","creator_name":"SLP-RL HUJI","creator_url":"https://huggingface.co/slprl","description":"\n\t\n\t\t\n\t\tsTinyStories\n\t\n\nA spoken version of TinyStories Synthesized with LJ voice using FastSpeech2.\nThe dataset was synthesized to boost the training of Speech Language Models as detailed in the paper \"Slamming: Training a Speech Language Model on One GPU in a Day\".\nIt was first suggested by Cuervo et. al 2024.\nWe refer you to the SlamKit codebase to see how you can train a SpeechLM with this dataset.\n\n\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importload_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/slprl/sTinyStories.","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","text-to-speech","English","mit"],"keywords_longer_than_N":true},
	{"name":"EgoNormia","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/open-social-world/EgoNormia","creator_name":"Open Social World","creator_url":"https://huggingface.co/open-social-world","description":"\n                EgoNormia: Benchmarking Physical-Social Norm Understanding      \n\n    MohammadHossein Rezaei*,¬†\n    Yicheng Fu*,¬†\n    Phil Cuvin*,¬†\n    Caleb Ziems,¬†\n    Yanzhe Zhang,¬†\n    Hao Zhu,¬†\n    Diyi Yang,¬†\n\n\n\n    üåéWebsite |\n    ü§ó Dataset |\n    üìÑ arXiv |\n    üìÑ HF Paper\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tEgoNormia\n\t\n\nEgoNormia is a challenging QA benchmark that tests VLMs' ability to reason over norms in context.\nThe datset consists of 1,853 physically grounded egocentric \ninteraction clips from Ego4D‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-social-world/EgoNormia.","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","question-answering","English","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"cc12m-4mp-realistic","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/opendiffusionai/cc12m-4mp-realistic","creator_name":"Open Diffusion AI","creator_url":"https://huggingface.co/opendiffusionai","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis is a hand-selected subset of our larger attempts to filter the well known CC12M dataset.\nThis one focuses on large (4 megapixels) images that are real world, high quality images, and the captioning\nspecifically matches either \"A man\" or \"A woman\".\nNote that I did not have the diskspace/time to go through the ENTIRE set. It was perhaps only from the first 2 million of our\nCC12M-cleaned subset.\nIf an effort were made to go through the entire 4mp image set, there might be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/opendiffusionai/cc12m-4mp-realistic.","first_N":5,"first_N_keywords":["text-to-image","cc-by-sa-4.0","10K - 100K","json","Image"],"keywords_longer_than_N":true},
	{"name":"danbooru2023-captions-1ktar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/6DammK9/danbooru2023-captions-1ktar","creator_name":"Darren Laurie","creator_url":"https://huggingface.co/6DammK9","description":"\n\t\n\t\t\n\t\tDanbooru 2023 captions only in 1k tar\n\t\n\n\nRaw captions jointed by unpublished extended dataset from KBlueLeaf/danbooru2023-metadata-database\n\nIt aligns to nyanko7/danbooru2023. There are around 200k missing for the 2024 version, I'll try to use Minthy/ToriiGate-v0.4-7B to fill in the rest.\n\nmeta_cap.json has been provided in compressed format if you want to train with kohyas triner. Currently I'm trying to merge this with my 2024 version.\n\n\n\n\t\n\t\t\n\t\tCore logic\n\t\n\n\nThe script building‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6DammK9/danbooru2023-captions-1ktar.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","no-annotation","danbooru"],"keywords_longer_than_N":true},
	{"name":"dpo_data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Share4oReasoning/dpo_data","creator_name":"Share4oReasoning","creator_url":"https://huggingface.co/Share4oReasoning","description":"\n\t\n\t\t\n\t\tShareGPT4oReasoning Training Data DPO\n\t\n\nAll dataset and models can be found at Share4oReasoning.\n\n\t\n\t\t\n\t\tContents:\n\t\n\n\nDPO_preview: Contains model generated CoT judged my outcome reward.\n\nImage use same in sft repo: contains the zipped image data (see below for details) used for SFT above.\n\n[Inference and Instruction for DPO](To be added): uploading now\nTraining pipeline refer to LLaVA-Reasoner-DPO training TODO separate readme for setup and train.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSet up:\n\t\n\ngit clone‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Share4oReasoning/dpo_data.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"myanmar-written-corpus","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/freococo/myanmar-written-corpus","creator_name":"Wynn","creator_url":"https://huggingface.co/freococo","description":"\n\t\n\t\t\n\t\tMyanmar Written Corpus\n\t\n\nThe Myanmar Written Corpus is a comprehensive collection of high-quality, but not fully CLEAN, written Myanmar text, designed to address the lack of large-scale, openly accessible resources for Myanmar Natural Language Processing (NLP). It is tailored to support various tasks such as text-to-speech (TTS), automatic speech recognition (ASR), translation, text generation, and more.\nThis dataset serves as a critical resource for researchers and developers aiming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/myanmar-written-corpus.","first_N":5,"first_N_keywords":["text-classification","text-generation","text-to-speech","Burmese","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"OVO-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JoeLeelyf/OVO-Bench","creator_name":"Yifei Li","creator_url":"https://huggingface.co/JoeLeelyf","description":"\n  OVO-Bench: How Far is Your Video-LLMs from Real-World Online VideO Understanding?\n\n\n\n  üî•üî•OVO-Bench is accepted by CVPR 2025!üî•üî•\n\n\n\n   \n    \n  \n\n\n\nImportant Note: Current codebase is modified compared to our initial arXiv paper. We strongly recommend that any use of OVO-Bench should be based on current edition.\n\n\n\t\n\t\t\n\t\n\t\n\t\tIntroduction\n\t\n\n\n\t\n\t\n\t\n\t\tüåü Three distinct problem-solving modes\n\t\n\n\nBackward Tracing: trace back to past events to answer the question.Real-Time Visual Perception:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JoeLeelyf/OVO-Bench.","first_N":5,"first_N_keywords":["video-text-to-text","English","cc-by-sa-4.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"HomoRich-G2P-Persian","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian","creator_name":"Mahta Fetrat","creator_url":"https://huggingface.co/MahtaFetrat","description":"\n\t\n\t\t\n\t\tHomoRich: A Persian Homograph Dataset for G2P Conversion\n\t\n\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nHomoRich is the first large-scale, sentence-level Persian homograph dataset designed for grapheme-to-phoneme (G2P) conversion tasks. It addresses the scarcity of balanced, contextually annotated homograph data for low-resource languages. The dataset was created using a semi-automated pipeline combining human expertise and LLM-generated samples, as described in the paper:\"Fast, Not Fancy: Rethinking G2P‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MahtaFetrat/HomoRich-G2P-Persian.","first_N":5,"first_N_keywords":["translation","text-to-speech","Persian","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"libritts-r-mimi","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jkeisling/libritts-r-mimi","creator_name":"Jacob Keisling","creator_url":"https://huggingface.co/jkeisling","description":"\n\t\n\t\t\n\t\tLibriTTS-R Mimi encoding\n\t\n\nThis dataset converts all audio in the dev.clean, test.clean, train.100 and train.360 splits of the LibriTTS-R dataset from waveforms to tokens in Kyutai's Mimi neural codec.\nThese tokens are intended as targets for DualAR audio models, but also allow you to simply download all audio in ~50-100x less space, if you're comfortable decoding later on with rustymimi or Transformers.\nThis does NOT contain the original audio, please use the regular LibriTTS-R for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jkeisling/libritts-r-mimi.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description.\nTraditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tKey‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM.","first_N":5,"first_N_keywords":["English","apache-2.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"M4-IT","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ColorfulAI/M4-IT","creator_name":"Yuxuan Wang","creator_url":"https://huggingface.co/ColorfulAI","description":"\n\t\n\t\t\n\t\tM4-IT\n\t\n\nThis dataset, M4-IT, is a synthetic instruction finetuning dataset used in the development of the M4 framework, designed to enhance real-time interactive reasoning in multi-modal language models.\nThe M4 framework is evaluated on OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts.\n\n\t\n\t\t\n\t\n\t\n\t\tData Description\n\t\n\nBuilding on the LLaVA-NeXT-Data, we crafted a small video-free synthetic instruction finetuning dataset, M4-IT, with the assistance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ColorfulAI/M4-IT.","first_N":5,"first_N_keywords":["video-text-to-text","mit","arxiv:2503.22952","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"MiSide-Japanese","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AkitoP/MiSide-Japanese","creator_name":"L","creator_url":"https://huggingface.co/AkitoP","description":"AkitoP/MiSide-Japanese dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Japanese","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"e621_2024-latents-sdxl-1ktar","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/6DammK9/e621_2024-latents-sdxl-1ktar","creator_name":"Darren Laurie","creator_url":"https://huggingface.co/6DammK9","description":"\n\t\n\t\t\n\t\tE621 2024 SDXL VAE latents in 1k tar\n\t\n\n\nDedicated dataset to align both NebulaeWis/e621-2024-webp-4Mpixel and deepghs/e621_newest-webp-4Mpixel. \"4MP-Focus\" for average raw image resolution. \nLatents are ARB with maximum size of 1024x1024 as the recommended setting in kohyas. Major reason is to make sure I can finetune with RTX 3090. VRAM usage will raise drastically after 1024.\nGenerated from prepare_buckets_latents_v2.py, modified from prepare_buckets_latents.py.\nUsed for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6DammK9/e621_2024-latents-sdxl-1ktar.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","no-annotation","danbooru"],"keywords_longer_than_N":true},
	{"name":"odoo-sql-query-dataset","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VPCSinfo/odoo-sql-query-dataset","creator_name":"Vinay Rana","creator_url":"https://huggingface.co/VPCSinfo","description":"\n\t\n\t\t\n\t\tOdoo SQL Query Dataset\n\t\n\nThis dataset contains natural language to SQL query pairs specifically for Odoo 17.0 Community Edition. It's designed to help train and fine-tune language models for generating accurate SQL queries for Odoo databases.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe dataset consists of 6815 carefully curated examples of natural language questions paired with their corresponding SQL queries for Odoo databases. Each example includes detailed instructions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VPCSinfo/odoo-sql-query-dataset.","first_N":5,"first_N_keywords":["text-generation","language-modeling","text-simplification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"XTD-10","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Haon-Chen/XTD-10","creator_name":"Haonan Chen","creator_url":"https://huggingface.co/Haon-Chen","description":"\n\t\n\t\t\n\t\tXTD Multimodal Multilingual Data With Instruction\n\t\n\nThis dataset contains datasets (with English instruction) used for evaluating the multilingual capability of a multimodal embedding model, including seven languages:\n\nit, es, ru, zh, pl, tr, ko\n\n\n\t\n\t\t\n\t\tDataset Usage\n\t\n\n\nThe instruction on the query side is: \"Retrieve an image of this caption.\"\nThe instruction on the document side is: \"Represent the given image.\"\nEach example contains a query and a set of targets. The first one in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Haon-Chen/XTD-10.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"TextAtlas5M","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CSU-JPG/TextAtlas5M","creator_name":"Jinpeng Group","creator_url":"https://huggingface.co/CSU-JPG","description":"\n\t\n\t\t\n\t\tTextAtlas5M\n\t\n\nThis dataset is a training set for TextAtlas.\nPaper: https://huggingface.co/papers/2502.07870\n(All the data in this repo is being uploaded, will meet you soon. :>)\n\n\t\n\t\t\n\t\tDataset subsets\n\t\n\nSubsets in this dataset are CleanTextSynth, PPT2Details, PPT2Structured,LongWordsSubset-A,LongWordsSubset-M,Cover Book,Paper2Text,TextVisionBlend,StyledTextSynth and TextScenesHQ. The dataset features are as follows: \n\n\t\n\t\t\n\t\tDataset Features\n\t\n\n\nimage (img): The GT image.\nannotation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CSU-JPG/TextAtlas5M.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"VideoChat-Flash-Training-Data","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data","creator_name":"OpenGVLab","creator_url":"https://huggingface.co/OpenGVLab","description":"\n\t\n\t\t\n\t\tü¶ú VideoChat-Flash-Training-Data\n\t\n\nThis repos contains all annotaions and most videos for training VideoChat-Flash.\n\n\t\n\t\t\n\t\tüìï How to use the LongVid data?\n\t\n\nFor video_dir like longvid_subset/coin_grounding_10k_zip, you need to concat this dir to a zip file as follows:\ncat ego4dhcap_eventunderstanding_2k_zip/* > ego4dhcap_eventunderstanding_2k.zip\n\n\n\t\n\t\t\n\t\t‚úèÔ∏è Citation\n\t\n\n\n@article{li2024videochatflash,\n  title={VideoChat-Flash: Hierarchical Compression for Long-Context Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenGVLab/VideoChat-Flash-Training-Data.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","10K - 100K","Datasets"],"keywords_longer_than_N":true},
	{"name":"waikato_aerial_2017_sd_ft","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dushj98/waikato_aerial_2017_sd_ft","creator_name":"Dinushi Jayasinghe","creator_url":"https://huggingface.co/dushj98","description":"This is a re-upload of a random sample of 260 images + curresponding blip captions obtained from the original waikato_aerial_imagery_2017 classification dataset residing at https://datasets.cms.waikato.ac.nz/taiao/waikato_aerial_imagery_2017/ under the same license. You can find additional dataset information using the provided URL.  \nThe BLIP model used for captioning: Salesforce/blip-image-captioning-large  \nThe images belong to 13 unique categories and each caption contains a unique token‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dushj98/waikato_aerial_2017_sd_ft.","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"hailuo-ai-jokes","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-jokes","creator_name":"Christian Peterson","creator_url":"https://huggingface.co/unlimitedbytes","description":"\n\t\n\t\t\n\t\tHailuo AI Jokes Dataset üé§\n\t\n\n\n\nA curated collection of high-quality voice recordings with corresponding transcriptions and phoneme analysis. This dataset is designed for speech recognition, text-to-speech, and voice analysis tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tüéôÔ∏è Dataset Content\n\t\n\nThe dataset contains a diverse set of synthetic voice recordings generated by Hailuo AI Audio. The texts are sourced from a variety of public domain jokes and humorous anecdotes. Each audio sample is accompanied by the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-jokes.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","English","mit"],"keywords_longer_than_N":true},
	{"name":"hailuo-ai-jokes","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-jokes","creator_name":"Christian Peterson","creator_url":"https://huggingface.co/unlimitedbytes","description":"\n\t\n\t\t\n\t\tHailuo AI Jokes Dataset üé§\n\t\n\n\n\nA curated collection of high-quality voice recordings with corresponding transcriptions and phoneme analysis. This dataset is designed for speech recognition, text-to-speech, and voice analysis tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tüéôÔ∏è Dataset Content\n\t\n\nThe dataset contains a diverse set of synthetic voice recordings generated by Hailuo AI Audio. The texts are sourced from a variety of public domain jokes and humorous anecdotes. Each audio sample is accompanied by the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/unlimitedbytes/hailuo-ai-jokes.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","English","mit"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v182","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v182","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v182.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v183","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v183","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v183.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v184","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v184","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v184.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v185","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v185","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v185.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v187","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v187","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v187.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\n\t\n\t\t\n\t\tOpen Image Preferences\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\n      \n          \n              \n              Image 1\n          \n          \n              \n              Image 2\n          \n      \n  \n\n\n\n  \n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\n          \n              \n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v190","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v190","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v190.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v195","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v195","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v195.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v196","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v196","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v196.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v202","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v202","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v202.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v203","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v203","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v203.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"melodymaster-v1","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/opentunes-ai/melodymaster-v1","creator_name":"Opentunes AI","creator_url":"https://huggingface.co/opentunes-ai","description":"\n\t\n\t\t\n\t\tMelodyMaster V1 Training Dataset\n\t\n\nThis dataset contains music-text pairs for fine-tuning MelodyMaster V1, an AI music generation model. Based on MusicCaps dataset.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tData Format\n\t\n\n\nFile Type: Parquet\nContains music-text pairs for fine-tuning\n\n\n\t\n\t\t\n\t\tData Structure\n\t\n\nyoutube_id,start_s,end_s,caption\n\n\nyoutube_id: Identifier for the audio source\nstart_s: Start time in seconds\nend_s: End time in seconds\ncaption: Text description of the music‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/opentunes-ai/melodymaster-v1.","first_N":5,"first_N_keywords":["text-to-audio","audio-classification","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v205","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v205","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v205.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"OregonCoastin4K","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Overlaiai/OregonCoastin4K","creator_name":"Overlai.ai","creator_url":"https://huggingface.co/Overlaiai","description":"\n\n\t\n\t\t\n\t\tOREGON COAST IN 4K\n\t\n\n\n\n\"Oregon Coast in 4K\" is a fine tuning text-to-video dataset consisting of dynamic videos captured in 8K resolution on the DJI Inspire 3 and RED Weapon Helium.\n\n\t\n\t\t\n\t\tKey Features\n\t\n\n\nüé• Oversampled: Every clip is captured in stunning 8K resolution, delivering rich detail ideal for fine tuning scenic landscapes and ocean dynamics.\nüîÑ Parallax: Shot using DJI Inspire 3 featuring parallax effects that provide AI models with enhanced context on depth and movement‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Overlaiai/OregonCoastin4K.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","Tabular"],"keywords_longer_than_N":true},
	{"name":"noob-wiki","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Laxhar/noob-wiki","creator_name":"Laxhar Dream Lab","creator_url":"https://huggingface.co/Laxhar","description":"\n\t\n\t\t\n\t\n\t\n\t\tNoob SDXL Wiki\n\t\n\nThis is the WIKI database for Noob SDXL Models.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","üá∫üá∏ Region: US","wiki"],"keywords_longer_than_N":false},
	{"name":"LLaVA-Video-large-swift","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/malterei/LLaVA-Video-large-swift","creator_name":"Malte","creator_url":"https://huggingface.co/malterei","description":"\n\t\n\t\t\n\t\tDataset Card LLaVA-Video-medium-swift\n\t\n\nA subset of LLaVA-Video-178K for educational purposes to learn how to fine-tune video models.\n","first_N":5,"first_N_keywords":["visual-question-answering","video-text-to-text","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-bool-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply boolean operations between 2 images that are stacked.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-5.\noperations: same, and, or, xor.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\noperations: and, or, xor. Eliminated the same, since it's the same as xor.\nDifferent palette for input_a and input_b.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 2-7.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-bool-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-ray-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the lonely pixels emit rays in multiple directions.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 5-10.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 5-20.\nnumber of lonely pixels: 1-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 5-15.\nnumber of lonely pixels: 1-4.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-ray-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-gravity-v15","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to apply gravity in the directions up/down/left/right.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 3-10.\nnumber of pixels to apply gravity to: 2-5.\nExercises image_gravity_move().\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nExercises image_gravity_draw().\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nExercises image_gravity_move() and image_gravity_draw().\nIncreased max_number_of_positions from 5 to 8.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 3-30.\nmax number of positions: 5.\n\n\t\n\t\t\n\t\tVersion‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-gravity-v15.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-rotate-v13","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the image gets rotated cw/ccw/180 and transposed.\nThe image sizes are between 1 and 4 pixels.\nPredict the number of rows in the output image.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 1-5.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nimage size: 1-5.\nAdded flipx and flipy transformations.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 1-5.\nnumber of tests: 1-2. Previously there were always just 1 test.\nAdded flipa and flipb transformations, that flips over the diagonal.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-rotate-v13.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-grid-v7","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to extract content from a grid.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\ncell size: 1-5.\ngrid line size: 1.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-5.\ncell size: 1-6.\ngrid line size: 1-2.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded generate_task_mutate_content_inside_grid, that does flipx, flipy, rotate 180, while preserving the grid.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nExtended generate_task_extract_content_from_grid so it does mutations of the output: flip x/y/a/b‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-grid-v7.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-probecolor-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to probe-colors in different directions.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nexample image size: 3-8.\ntest image size: 1-12. Out of distribution data.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nexample image size: 3-9.\ntest image size: 1-14. Out of distribution data.\nThis was too hard for the model to make sense of.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nOnly enabled: TOP, BOTTOM (since these are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-probecolor-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-zindex-v11","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to do z-index transformations of input/output images.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 6-10.\nnoise: 0.1, 0.2.\nmask_of_primary_rectangle\nRandom noisy background with two colors.\nDraw a rectangle on top of the background.\nThe job is to identify the rectangle.\n\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nmask_of_obscured_rectangle added.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nbigger images. image size: 6-12.\nmore noise: noise: 0.1, 0.2, 0.3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nbigger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-zindex-v11.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-template-v19","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to insert objects into templates.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 8-10.\ntemplate size: 2-4.\nnumber of rects: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nSmaller images.\nexample count: 2-3.\ntest count: 1-2.\nimage size: 6-8.\ntemplate size: 2-2.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nAdded transformation: without_insertion_image\nimage size: 6-8.\ntemplate size: 2-3.\nnumber of rects: 2-3.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded transformations:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-template-v19.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-compress-v8","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to compress images with duplicate rows and columns.\nexample count: 2-4.\ntest count: 1-2.\nimage size: 2-4.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 2-6.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nimage size: 2-8.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nimage size: 2-10.\n\n\t\n\t\t\n\t\tVersion 6\n\t\n\nimage size: 2-12.\n\n\t\n\t\t\n\t\tVersion 7\n\t\n\nAdded fields: arc_task, test_index, earlier_output.\n\n\t\n\t\t\n\t\tVersion 8\n\t\n\nReplaced RLE compressed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-compress-v8.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-count-v9","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to count N number of pixels in the input, and in the output repeat a pattern N times.\nexample count: 3-4.\ntest count: 1-2.\ninput image size: 3-8.\noutput pattern image size: 1-3.\npixel count: 1-3.\nI had a serious mistake in number_of_positions where I didn't deal with clashing xy coordinates, causing the pixel count to not match with the pattern count in the output.\n\n\t\n\t\t\n\t\n\t\n\t\tVersion 2\n\t\n\ninput image size: 3-10.\npixel count: 1-4.\nI had a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-count-v9.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-solve-cross-v6","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nARC-AGI Tasks where the job is to identify how 2 lines are intersecting, what line is the top-most, bottom-most.\nexample count: 3-4.\ntest count: 1-2.\nimage size: 3-6.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nimage size: 3-10.\n\n\t\n\t\t\n\t\tVerison 3\n\t\n\nimage size: 3-15.\nAdded new task type:\nIdentify from an intersection point, what are the lines that goes through the intersection point.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nEarlier predictions added to some of the rows.\n\n\t\n\t\t\n\t\tVersion 5\n\t\n\nAdded fields: arc_task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-solve-cross-v6.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v164","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v164","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v164.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v166","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v166","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v166.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v169","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v169","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v169.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v170","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v170","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v170.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v171","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v171","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v171.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v173","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v173","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v173.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v175","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v175","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v175.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v178","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v178","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v178.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v179","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v179","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v179.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"IndicTTS_Tamil","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SPRINGLab/IndicTTS_Tamil","creator_name":"SPRINGLab","creator_url":"https://huggingface.co/SPRINGLab","description":"\n\t\n\t\t\n\t\tTamil Indic TTS Dataset\n\t\n\nThis dataset is derived from the Indic TTS Database project, specifically using the Tamil monolingual recordings from both male and female speakers. The dataset contains high-quality speech recordings with corresponding text transcriptions, making it suitable for text-to-speech (TTS) research and development.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nLanguage: Tamil\nTotal Duration: ~20.33 hours (Male: 10.3 hours, Female: 10.03 hours)\nAudio Format: WAV\nSampling Rate:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SPRINGLab/IndicTTS_Tamil.","first_N":5,"first_N_keywords":["text-to-speech","Tamil","cc-by-4.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v186","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v186","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v186.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v188","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v188","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v188.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v189","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v189","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v189.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"emova-sft-speech-231k","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-sft-speech-231k","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-SFT-Speech-231K\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-SFT-Speech-231K is a comprehensive dataset curated for omni-modal instruction tuning and emotional spoken dialogue. This dataset is created by converting existing text and visual instruction datasets via Text-to-Speech (TTS) tools. EMOVA-SFT-Speech-231K is part of EMOVA-Datasets collection and is used in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-sft-speech-231k.","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","text-to-speech","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"belebele-fleurs","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WueNLP/belebele-fleurs","creator_name":"W√ºNLP","creator_url":"https://huggingface.co/WueNLP","description":"\n\t\n\t\t\n\t\tBelebele-Fleurs\n\t\n\nBelebele-Fleurs is a dataset suitable to evaluate two core tasks:\n\nMultilingual Spoken Language Understanding (Listening Comprehension): For each spoken paragraph, the task is to answer a multiple-choice question. The question and four answer choices are provided in text form.\nMultilingual Long-Form Automatic Speech Recognition (ASR) with Diverse Speakers: By concatenating sentence-level utterances, long-form audio clips (ranging from 30 seconds to 1 minute 30‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WueNLP/belebele-fleurs.","first_N":5,"first_N_keywords":["audio-classification","automatic-speech-recognition","audio-text-to-text","text-to-speech","question-answering"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v191","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v191","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v191.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v192","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v192","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v192.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v193","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v193","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v193.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v197","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v197","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v197.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v198","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v198","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v198.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v199","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v199","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v199.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v200","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v200","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v200.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v201","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v201","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v201.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v204","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v204","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v204.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v206","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v206","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v206.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v207","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v207","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v207.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v208","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v208","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v208.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"lego_minifigure_captions","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions","creator_name":"Alessandro","creator_url":"https://huggingface.co/armaggheddon97","description":"\n\t\n\t\t\n\t\tLEGO Minifigure Captions\n\t\n\nThe LEGO Minifigure Captions dataset contains 12966 images of LEGO minifigures with captions. The dataset contains the following columns:\n\nimage: The jpeg image of the minifigure in the format {\"bytes\": bytes, \"path\": str} so that can be interpreted as PIL.Image objects in the huggingface datasets library.\nshort_caption: The short caption describing the minifigure in the image.\ncaption: The caption describing the minifigure which is generated using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/armaggheddon97/lego_minifigure_captions.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v209","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v209","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v209.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v210","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v210","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v210.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"simon-arc-combine-v211","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neoneye/simon-arc-combine-v211","creator_name":"Simon Strandgaard","creator_url":"https://huggingface.co/neoneye","description":"\n\t\n\t\t\n\t\tVersion 1\n\t\n\nA combination of multiple datasets.\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 2\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 3\n\t\n\nDatasets: dataset_solve_color.jsonl, dataset_solve_rotate.jsonl, dataset_solve_translate.jsonl.\n\n\t\n\t\t\n\t\tVersion 4\n\t\n\nAdded a shared dataset name for all these datasets: SIMON-SOLVE-V1. There may be higher‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neoneye/simon-arc-combine-v211.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"chatjsonsql","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"acl-paper","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sleeping-ai/acl-paper","creator_name":"Sleeping AI","creator_url":"https://huggingface.co/sleeping-ai","description":"\n\t\n\t\t\n\t\tACL Entire\n\t\n\n\n  \n\n\nACL Entire is a comprehensive dataset containing all papers from both ACL and Non-ACL events listed on the ACL Anthology website. This dataset includes complete bibliographic information for all years.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\nEvents Covered: Papers from ACL and Non-ACL events.\nBibliography: Includes complete bibliographic details for every paper.\nYears Covered: Comprehensive data spanning all available years.\n\n\n\t\n\t\t\n\t\tSource\n\t\n\nAll data has been compiled from the ACL‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sleeping-ai/acl-paper.","first_N":5,"first_N_keywords":["text-classification","translation","summarization","text-to-speech","English"],"keywords_longer_than_N":true},
	{"name":"emova-alignment-7m","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-Alignment-7M\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-Alignment-7M is a comprehensive dataset curated for omni-modal pre-training, including vision-language and speech-language alignment. \nThis dataset is created using open-sourced image-text pre-training datasets, OCR datasets, and 2,000 hours of ASR and TTS data. \nThis dataset is part of the EMOVA-Datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-alignment-7m.","first_N":5,"first_N_keywords":["image-to-text","text-generation","audio-to-audio","automatic-speech-recognition","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"art-museums-pd-440k","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mitsua/art-museums-pd-440k","creator_name":"elanmitsua","creator_url":"https://huggingface.co/Mitsua","description":"\n\t\n\t\t\n\t\tArt Museums PD 440K\n\t\n\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a dataset to train text-to-image or any text and image multimodal models with minimized copyright/licensing concerns.\nAll images and texts in this dataset are orignally shared under CC0 or public domain, and no pretrained models or any AI models are used to build this dataset except for our ElanMT model to translate English captions to Japanese.\nElanMT model is trained solely on licensed corpus.\n\n\t\n\t\t\n\t\n\t\n\t\tData sources\n\t\n\nImages and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/art-museums-pd-440k.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","Japanese","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"safe-commons-pd-3m","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mitsua/safe-commons-pd-3m","creator_name":"elanmitsua","creator_url":"https://huggingface.co/Mitsua","description":"\n\t\n\t\t\n\t\tSafe Commons PD 3M\n\t\n\n\nThis is a balanced and safe-to-use public domain / CC0 images dataset.\nAll images and texts come from Wikimedia Commons and Wikidata with strict filtering.\nImages license is either Public Domain or CC0 (varies by image).\nTexts license is either CC0 or CC BY-SA (varies by caption source).\nNo synthetic data (AI generated images or captions) is in the dataset.\n\nTo build this dataset, we tried to avoid any knowledge leaks from existing pre-trained models at the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mitsua/safe-commons-pd-3m.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","Japanese","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"open-image-preferences-v1-more-results","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\n\n\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for image-preferences-results Original\n\t\n\n\n\n\n\n  \n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-10K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512√ó512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-10K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"LiFT-HRA-20K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is the dataset proposed in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\". LiFT-HRA is a high-quality Human Preference Annotation dataset that can be used to train video-text-to-text reward models. All videos in the LiFT-HRA dataset have resolutions of at least 512√ó512.\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/LiFT-HRA-20K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"emova-asr-tts-eval","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-asr-tts-eval","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-ASR-TTS-Eval\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-ASR-TTS-Eval is a dataset designed for evaluating the ASR and TTS performance of Omni-modal LLMs. It is derived from the test-clean set of the LibriSpeech dataset. This dataset is part of the EMOVA-Datasets collection. We extract the speech units using the EMOVA Speech Tokenizer.\n\n\t\n\t\t\n\t\tStructure\n\t\n\nThis‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-asr-tts-eval.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\n\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\n\nSora\nHunyouan\nPika 2.0\nRunway ML Alpha\nLuma Ray 2\n\nExplore our latest model rankings on our website.\nIf you get value from this dataset and would‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences.","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"tiny_webvid_latents","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tensorkelechi/tiny_webvid_latents","creator_name":"kelechic","creator_url":"https://huggingface.co/tensorkelechi","description":"\n\t\n\t\t\n\t\ttiny_webvid_latents\n\t\n\nThis is a tiny processed split of Doubiiu/webvid10m_motion.\n\nresized to 224x224\nTrimmed to 5s and 4fps\nEncoded to video latents with hunyuanvideo-community/HunyuanVideo's video/3D VAE\ntext/caption encoded with google-t5/t5-base\n\n","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-anime-images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-anime-images","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\tTest-HunyuanVideo-Anime-Stills\n\t\n\nA small dataset of AI-generated anime-themed images designed for general anime text-to-image (T2I) training debug or testing Hunyuan Video. This dataset provides a balanced distribution of subjects and aims to align large pretrained models with anime aesthetics in terms of visual appeal and text faithfulness.\n\n\t\n\t\t\n\t\tSubject Selection\n\t\n\nThe subject distributions (other than the 50% anime girls) are selected based on the policy outlined in Meta's Emu‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-anime-images.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-anime-images","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-anime-images","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\tTest-HunyuanVideo-Anime-Stills\n\t\n\nA small dataset of AI-generated anime-themed images designed for general anime text-to-image (T2I) training debug or testing Hunyuan Video. This dataset provides a balanced distribution of subjects and aims to align large pretrained models with anime aesthetics in terms of visual appeal and text faithfulness.\n\n\t\n\t\t\n\t\tSubject Selection\n\t\n\nThe subject distributions (other than the 50% anime girls) are selected based on the policy outlined in Meta's Emu‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-anime-images.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"cs2-highlights","keyword":"text-to-video","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/cs2-highlights","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Counter-Strike 2 Highlight Clips\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 8,369 high-quality gameplay highlight clips primarily from Counter-Strike 2, with a small portion from Counter-Strike: Global Offensive. The clips focus on key gameplay moments such as kills, bomb interactions, and grenade usage. The clips are collected from competitive platforms like Faceit and in-game competitive modes (Premier, Matchmaking) across various skill levels, making it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/cs2-highlights.","first_N":5,"first_N_keywords":["video-classification","text-to-video","image-to-video","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"cc3m","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WeiChow/cc3m","creator_name":"zhou wei","creator_url":"https://huggingface.co/WeiChow","description":"This repo is CC3M's unofficial huggingface repo. \nHowever, for the large picture, we process it as follow and then upload:\nif pil_image.width > 1024 or pil_image.height > 1024:\n    pil_image = pil_image.resize((1024, 1024), Image.BICUBIC)\n\n","first_N":5,"first_N_keywords":["text-to-image","image-to-image","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"klingai","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/klingai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for KLING AI\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 12,782 AI-generated media items (images and videos) created using KLING AI's generative tools. The content includes metadata and original files for various AI generations, encompassing both still images and motion videos created through text-to-image and image-to-video transformations.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English (en), with prompts and metadata in English.\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/klingai.","first_N":5,"first_N_keywords":["text-to-image","image-to-video","machine-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"sakugabooru2025","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/sakugabooru2025","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\tSakugabooru2025: Curated Animation Clips from Enthusiasts\n\t\n\nSakugabooru.com is a booru-style imageboard dedicated to collecting and sharing noteworthy animation clips, emphasizing Japanese anime but open to creators worldwide. Over the years, it has amassed more than 240,000 animation clips, alongside informative blog posts for anime fans everywhere.\nWith the growing interest in generative video models and AI animations, the scarcity of proper animation-related video datasets has‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/sakugabooru2025.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"sakugabooru2025","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/sakugabooru2025","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\tSakugabooru2025: Curated Animation Clips from Enthusiasts\n\t\n\nSakugabooru.com is a booru-style imageboard dedicated to collecting and sharing noteworthy animation clips, emphasizing Japanese anime but open to creators worldwide. Over the years, it has amassed more than 240,000 animation clips, alongside informative blog posts for anime fans everywhere.\nWith the growing interest in generative video models and AI animations, the scarcity of proper animation-related video datasets has‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/sakugabooru2025.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","Japanese","mit"],"keywords_longer_than_N":true},
	{"name":"LayoutSAM-eval","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tLayoutSAM-eval Benchmark\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLM‚Äôs visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR score‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Inst-It-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Bench","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\n\t\n\t\t\n\t\tInst-It Bench\n\t\n\nHomepage | Code | Paper | arXiv\nInst-It Bench is a fine-grained multimodal benchmark for evaluating LMMs at the instance-Level, which is introduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning.\n\nSize: 1,000 image QAs and 1,000 video QAs\nSplits: Image split and Video split\nEvaluation Formats: Open-Ended and Multiple-Choice\n\n\n\t\n\t\n\t\n\t\tIntroduction\n\t\n\nExisting multimodal benchmarks primarily focus on global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Bench.","first_N":5,"first_N_keywords":["multiple-choice","question-answering","visual-question-answering","video-text-to-text","image-text-to-text"],"keywords_longer_than_N":true},
	{"name":"OmniMMI","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bigai-nlco/OmniMMI","creator_name":"BIGAI NLCo","creator_url":"https://huggingface.co/bigai-nlco","description":"\n\t\n\t\t\n\t\tOmniMMI\n\t\n\nPaper: OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts\nCode\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nwe introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigai-nlco/OmniMMI.","first_N":5,"first_N_keywords":["video-text-to-text","mit","1K - 10K","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"MIG-Bench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michael4933/MIG-Bench","creator_name":"You Li","creator_url":"https://huggingface.co/Michael4933","description":"\n    \n\n\n\n\n\n\t\n\t\t\n\t\tMigician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models\n\t\n\nYou Li, Heyu Huang*, Chen Chi, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun\n\n        \nThis repository hosts the usage details of our training dataset MGrounding-630k and benchmark MIG-Bench and the training implementation of Migician, the first competitive Multi-image Grounding MLLM capable of free-form grounding.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Michael4933/MIG-Bench.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Text2Face","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/oguzhanercan/Text2Face","creator_name":"Oƒüuzhan Ercan","creator_url":"https://huggingface.co/oguzhanercan","description":"This is version 1.0 of Text2Face dataset. This dataset generated by using Flux1.dev (Nunchaku 4 bit optimization method). Facial descriptions generated with promptgen.py script. More advanced version of prompt generator is also available as promptgenv2.py. For more detailed facial descriptions, you can generate with that.\n","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"elevenlabs_multilingual_v2-technical-speech","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WpythonW/elevenlabs_multilingual_v2-technical-speech","creator_name":"Andrew","creator_url":"https://huggingface.co/WpythonW","description":"\n\t\n\t\t\n\t\tElevenLabs Multilingual V2 Technical Speech Dataset\n\t\n\nThis dataset contains automatically generated technical phrases in three domains, converted to speech using the ElevenLabs Multilingual V2 model with Adam voice.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset includes audio samples of technical phrases across three categories:\n\nMachine Learning (ML)\nScience\nTechnology\n\nEach entry contains:\n\nAudio file in MP3 format (22050Hz)\nSource text\nText length\nCategory label\n\n\n\t\n\t\t\n\t\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WpythonW/elevenlabs_multilingual_v2-technical-speech.","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ImageFXPrompts","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/shb777/ImageFXPrompts","creator_name":"SB","creator_url":"https://huggingface.co/shb777","description":"Auto-suggested prompts collected from Google ImageFX\n","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"MMathCoT-1M","keyword":"image-text-to-text","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/URSA-MATH/MMathCoT-1M","creator_name":"URSA-MATH","creator_url":"https://huggingface.co/URSA-MATH","description":"\n\t\n\t\t\n\t\tMMathCoT-1M\n\t\n\nThis repository contains the data presented in URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics.\nCode: https://github.com/URSA-MATH/URSA-MATH\nImage data can be downloaded from the following address:\n\nMAVIS: https://github.com/ZrrSkywalker/MAVIS, https://drive.google.com/drive/folders/1LGd2JCVHi1Y6IQ7l-5erZ4QRGC4L7Nol.\nMultimath: https://huggingface.co/datasets/pengshuai-rin/multimath-300k.\nGeo170k:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/URSA-MATH/MMathCoT-1M.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","gpl-3.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"FaceCaptionHQ-4M","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M","creator_name":"ddw2AIGROUP-CQUPT","creator_url":"https://huggingface.co/OpenFace-CQUPT","description":"\n\t\n\t\t\n\t\tFaceCaptionHQ-4M\n\t\n\nYou need to first download the data from here and then apply for access to the original Laion-face dataset by completing the required agreement (github). Once approved, refer to the information available on HuggingFace to obtain the corresponding image-text pairs.\n[25/06/09] ü§óThe Original Images, are Released Completing the Agreement\nFaceCaptionHQ-4M contains about 4M facial image-text pairs that cleaned from FaceCaption-15M .  \n\n\n\n\n\t\n\t\n\t\n\t\tFigure.1 Illustrations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenFace-CQUPT/FaceCaptionHQ-4M.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"game_screenshots_100","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/badigadiii/game_screenshots_100","creator_name":"Alexandr Kuznetcov","creator_url":"https://huggingface.co/badigadiii","description":"badigadiii/game_screenshots_100 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"bam-asr-early","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RobotsMali/bam-asr-early","creator_name":"RobotsMali AI4D LAB","creator_url":"https://huggingface.co/RobotsMali","description":"The **Bambara-ASR-Early Audio Dataset** is a multilingual dataset containing audio samples in Bambara, accompanied by semi-expert transcriptions and French translations. \nThe dataset includes various subsets: `jeli-asr`, `oza-mali-pense`, and `rt-data-collection`. Each audio file is aligned with Bambara transcriptions or French translations, making it suitable for tasks such as automatic speech recognition (ASR) and translation. \nData sources include all publicly available collections of audio with Bambara transcriptions as of December 2024, organized for accessibility and usability.\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","audio-language-identification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"prl_dark_love_style","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paralaif/prl_dark_love_style","creator_name":"paralaif","creator_url":"https://huggingface.co/paralaif","description":"\n\t\n\t\t\n\t\tprl_dark_love_style Dataset\n\t\n\nThis dataset is used to train my lora Crimson Maiden from Civitai.\n","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"Tarsier2-Recap-585K","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K","creator_name":"omni-research","creator_url":"https://huggingface.co/omni-research","description":"\n\t\n\t\t\n\t\tDataset Card for Tarsier2-Recap-585K\n\t\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\n‚ú®Tarsier2-Recap-585K‚ú® consists of 585K distinct video clips, lasting for 1972 hours in total, from open-source datasets (e.g. VATEX, TGIF, LSMDC, etc.) and each one with a detailed video description annotated by Tarsier2-7B, which beats GPT-4o in generating detailed and accurate video descriptions for video clips of 5~20 seconds (See the DREAM-1K Leaderboard). Experiments demonstrate its effectiveness in enhancing the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","Video","arxiv:2501.07888"],"keywords_longer_than_N":true},
	{"name":"awesome-text2video-prompts","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/awesome-text2video-prompts","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Preference Dataset\n\t\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset contains prompts for video generation for 14 different categories. They were collected with a combination of manual prompting and ChatGPT 4o. We provide one example sora video generation for each video.  \n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tCategories and Comments\n\t\n\n\nObject Interactions Scenes: Basic scenes with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/awesome-text2video-prompts.","first_N":5,"first_N_keywords":["text-to-video","video-classification","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"my_dataset_2","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kashif314/my_dataset_2","creator_name":"kashif","creator_url":"https://huggingface.co/kashif314","description":"kashif314/my_dataset_2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Urdu","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"emova-sft-speech-eval","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Emova-ollm/emova-sft-speech-eval","creator_name":"EMOVA Hugging Face","creator_url":"https://huggingface.co/Emova-ollm","description":"\n\t\n\t\t\n\t\tEMOVA-SFT-Speech-Eval\n\t\n\n\n\n\nü§ó EMOVA-Models | ü§ó EMOVA-Datasets | ü§ó EMOVA-Demo \nüìÑ Paper | üåê Project-Page | üíª Github | üíª EMOVA-Speech-Tokenizer-Github\n\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nEMOVA-SFT-Speech-Eval is an evaluation dataset curated for omni-modal instruction tuning and emotional spoken dialogue. This dataset is created by converting existing text and visual instruction datasets via Text-to-Speech (TTS) tools. EMOVA-SFT-Speech-Eval is part of EMOVA-Datasets collection, and the training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Emova-ollm/emova-sft-speech-eval.","first_N":5,"first_N_keywords":["audio-to-audio","automatic-speech-recognition","text-to-speech","English","Chinese"],"keywords_longer_than_N":true},
	{"name":"VisNumBench","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/GML-FMGroup/VisNumBench","creator_name":"Foundation Model Group at Guangming Laboratory","creator_url":"https://huggingface.co/GML-FMGroup","description":"This dataset is designed for research in Deep Learning for Geometry Problem Solving (DL4GPS) and accompanies the survey paper A Survey of Deep Learning for Geometry Problem Solving. It aims to provide a structured resource for evaluating and training AI models, particularly multimodal large language models (MLLMs), on mathematical reasoning tasks involving geometric contexts.\nThe dataset provides a collection of geometry problems, each consisting of a textual question and a corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GML-FMGroup/VisNumBench.","first_N":5,"first_N_keywords":["image-text-to-text","mit","1K - 10K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"minimal_video_pairs","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/minimal_video_pairs","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\n\t\n\t\t\n\t\tMinimal Video Pairs\n\t\n\nA shortcut-aware benchmark for spatio-temporal and intuitive physics video understanding (VideoQA) using minimally different video pairs.\n\nGithub\n\n\n  \n\n\nFor legal reasons, we are unable to upload the videos directly to Huggingface. However, we provide scripts in this repository for downloading the videos in our github repository. Our benchmark is built on top of videos source from 9 domains:\n\n\t\n\t\t\nSubset\nData sources\n\n\n\t\t\nHuman object interactions\nPerceptionTest‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/minimal_video_pairs.","first_N":5,"first_N_keywords":["question-answering","video-text-to-text","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Live-CC-5M","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chenjoya/Live-CC-5M","creator_name":"Joya Chen","creator_url":"https://huggingface.co/chenjoya","description":"\n\t\n\t\t\n\t\tDataset Card for Live-CC-5M\n\t\n\n\n\n\t\n\t\t\n\t\tUses\n\t\n\nThis dataset is used for LiveCC-7B-Base model pre-training. We only allow the use of this dataset for academic research and educational purposes. For OpenAI GPT-4o generated user prompts, we recommend users check the OpenAI Usage Policy.\n\nProject Page: https://showlab.github.io/livecc\nPaper: https://huggingface.co/papers/2504.16030\n\n\n\t\n\t\t\n\t\n\t\n\t\tLive-CC-5M Dataset\n\t\n\n\nStatistics: 5,047,208 YouTube Video-CC 30~240s samples.\n\n\nAnnotation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/chenjoya/Live-CC-5M.","first_N":5,"first_N_keywords":["video-text-to-text","English","apache-2.0","1M<n<10M","arxiv:2504.16030"],"keywords_longer_than_N":true},
	{"name":"ReflectiVA-Data","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aimagelab/ReflectiVA-Data","creator_name":"AImageLab","creator_url":"https://huggingface.co/aimagelab","description":"In this datasets space, you will find the data of Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering.\nFor more information, visit our ReflectiVA repository, our project page and model space.\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you make use of our work, please cite our repo:\n@inproceedings{cocchi2024augmenting,\n  title={{Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering}},\n  author={Cocchi, Federico and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aimagelab/ReflectiVA-Data.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","arxiv:2411.16863","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"VPO","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CCCCCC/VPO","creator_name":"Jiale Cheng","creator_url":"https://huggingface.co/CCCCCC","description":"\n\t\n\t\t\n\t\tDataset Card for VPO\n\t\n\n\n\t\n\t\t\n\t\tData Summary\n\t\n\nTo bridge the gap between training and inference in video generation models, we present VPO‚Äîa principle-driven framework designed to generate harmless, accurate, and helpful prompts for high-quality video generation.\nWe release an SFT dataset containing 10k samples constructed using gpt-4o. In addition, we provide the DPO datasets derived from CogVideoX-2B and CogVideoX-5B.\nPlease refer to our paper for further details.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CCCCCC/VPO.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"WISA-80K","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/qihoo360/WISA-80K","creator_name":"Âåó‰∫¨Â•áËôéÁßëÊäÄÊúâÈôêÂÖ¨Âè∏","creator_url":"https://huggingface.co/qihoo360","description":"\n\t\n\t\t\n\t\tWISA-80K\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation\n\nJing Wang*, Ao Ma*, Ke Cao*, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng‚Ä°, Yuhui Yin, Xiaodan Liang‚Ä°(*Equal Contribution, ‚Ä°Corresponding Authors)\n\n\n  \n\n\n  \n\n\n  \n\n\n\t\n\t\n\t\n\t\tBibTeX\n\t\n\n@article{wang2025wisa,\n  title={WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation},\n  author={Wang, Jing and Ma, Ao and Cao‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qihoo360/WISA-80K.","first_N":5,"first_N_keywords":["text-to-video","English","apache-2.0","10K - 100K","Video"],"keywords_longer_than_N":true},
	{"name":"VIDGEN-1K","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K","creator_name":"Fudan-FUXI","creator_url":"https://huggingface.co/Fudan-FUXI","description":"\n\t\n\t\t\n\t\tLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\t\n\n\n\t\n\t\t\n\t\tSummary\n\t\n\nThis is a video-text-to-text dataset used in our paper \"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\".\nProject: https://codegoat24.github.io/LiFT/\nCode: https://github.com/CodeGoat24/LiFT\n\n\t\n\t\t\n\t\tUsage\n\t\n\n\n\t\n\t\t\n\t\tInstallation\n\t\n\n\nClone the github repository and navigate to LiFT folder\n\ngit clone https://github.com/CodeGoat24/LiFT.git\ncd LiFT\n\n\nInstall packages\n\nbash‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Fudan-FUXI/VIDGEN-1K.","first_N":5,"first_N_keywords":["video-text-to-text","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"RefChartQA","keyword":"image-text-to-text","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/omoured/RefChartQA","creator_name":"Omar Moured","creator_url":"https://huggingface.co/omoured","description":"\n\t\n\t\t\n\t\tüß† About\n\t\n\nRefChartQA is a large-scale benchmark for visual grounding in chart-based question answering. It extends the ChartQA and TinyChart-PoT datasets by adding explicit bounding box annotations that link each answer to supporting visual elements in the chart. RefChartQA contains 73,702 annotated samples, including:\n\n55,789 training,\n6,223 validation,\n11,690 testing instances.\n\nFor details, see our paper and GitHub repository.\n\n  \n\n\n\n\t\n\t\n\t\n\t\tüõ†Ô∏è Usage\n\t\n\n\n\t\n\t\n\t\n\t\tüì¶ Environment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/omoured/RefChartQA.","first_N":5,"first_N_keywords":["table-question-answering","visual-question-answering","image-text-to-text","English","agpl-3.0"],"keywords_longer_than_N":true},
	{"name":"minecraft-skins-legacy","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/deepwaterhorizon/minecraft-skins-legacy","creator_name":"Dylan Hawley","creator_url":"https://huggingface.co/deepwaterhorizon","description":"\n\t\n\t\t\n\t\tMinecraft Skin Img2Img Dataset (Legacy Format)\n\t\n\n\n\t\n\t\t\n\t\tData Overview\n\t\n\n\n[!NOTE]\nThis dataset contains skins in the pre-1.8 format, with an image size of 64px x 32px instead of the newer 64px x 64px.\n\nThis dataset contains character images paired with a Minecraft skin texture and their descriptions. Character images were generated by DALL-E, prompted with renderings of the Minecraft skin file. All images in this dataset are 1024px x 1024px.\n\n\t\n\t\t\n\t\tUsage\n\t\n\nYou can load this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/deepwaterhorizon/minecraft-skins-legacy.","first_N":5,"first_N_keywords":["image-to-image","text-to-image","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"sdf_dataset_zh","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/minghanw/sdf_dataset_zh","creator_name":"Minghan Wang","creator_url":"https://huggingface.co/minghanw","description":"\n\t\n\t\t\n\t\tSpeechDialogueFactory Dataset\n\t\n\n\n\t\n\t\t\n\t\tBackground\n\t\n\nThis dataset is part of the SpeechDialogueFactory project, a comprehensive framework for generating high-quality speech dialogues at scale. Speech dialogue datasets are essential for developing and evaluating Speech-LLMs, but existing datasets face limitations including high collection costs, privacy concerns, and lack of conversational authenticity. This dataset addresses these challenges by providing synthetically generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/minghanw/sdf_dataset_zh.","first_N":5,"first_N_keywords":["text-generation","text-to-speech","audio-to-audio","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Hypa_Fleurs","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hypaai/Hypa_Fleurs","creator_name":"Hypa-Intelligence","creator_url":"https://huggingface.co/hypaai","description":"\n\t\n\t\t\n\t\tHypa_Fleurs\n\t\n\nHypa_Fleurs is an open-source multilingual, multi-modal dataset with a long term vision of advancing speech and language technology for low-resource African languages by leveraging the English split of the Google Fleurs dataset to create parallel speech and text datasets for a wide range of low-resource African languages. In this initial release, professional AfroVoices experts translated the original English texts into three under-resourced African languages: Igbo (ig)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hypaai/Hypa_Fleurs.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","translation","text-classification","AfroVoices"],"keywords_longer_than_N":true},
	{"name":"KS-Gen","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MCG-NJU/KS-Gen","creator_name":"Multimedia Computing Group-Nanjing University","creator_url":"https://huggingface.co/MCG-NJU","description":"\n\t\n\t\t\n\t\tüé¨ KS-Gen Dataset\n\t\n\n\n\t\n\t\t\n\t\tüìã Overview\n\t\n\nKS-Gen (also known as SkillVid) is a benchmark dataset for Key-Step Generation (KS-Gen) of human skills in the wild. This dataset is designed to support the generation of human skill videos at key-step levels, enabling researchers to develop models that can generate coherent sequences of skill demonstrations based on textual descriptions.\n\n\t\n\t\t\n\t\tüì¶ Contents\n\t\n\n\nAnnotation Files üè∑Ô∏è: We provide comprehensive annotations in parquet format for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MCG-NJU/KS-Gen.","first_N":5,"first_N_keywords":["image-to-video","text-to-video","image-to-image","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"KS-Gen","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MCG-NJU/KS-Gen","creator_name":"Multimedia Computing Group-Nanjing University","creator_url":"https://huggingface.co/MCG-NJU","description":"\n\t\n\t\t\n\t\tüé¨ KS-Gen Dataset\n\t\n\n\n\t\n\t\t\n\t\tüìã Overview\n\t\n\nKS-Gen (also known as SkillVid) is a benchmark dataset for Key-Step Generation (KS-Gen) of human skills in the wild. This dataset is designed to support the generation of human skill videos at key-step levels, enabling researchers to develop models that can generate coherent sequences of skill demonstrations based on textual descriptions.\n\n\t\n\t\t\n\t\tüì¶ Contents\n\t\n\n\nAnnotation Files üè∑Ô∏è: We provide comprehensive annotations in parquet format for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MCG-NJU/KS-Gen.","first_N":5,"first_N_keywords":["image-to-video","text-to-video","image-to-image","text-to-image","English"],"keywords_longer_than_N":true},
	{"name":"Flux-Ultimate","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/thejagstudio/Flux-Ultimate","creator_name":"Jagrat Patel","creator_url":"https://huggingface.co/thejagstudio","description":"thejagstudio/Flux-Ultimate dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"swahili-tts-dataset","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jacksonwambali/swahili-tts-dataset","creator_name":"Jackson Wambali","creator_url":"https://huggingface.co/jacksonwambali","description":"\n\t\n\t\t\n\t\tSwahili TTS Dataset\n\t\n\nThis dataset contains 10 Swahili audio clips with corresponding transcripts, used to train a Text-to-Speech (TTS) model using Coqui TTS.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nwavs/: Contains WAV audio files (One.wav to Ten.wav).\nmetadata.csv: Contains the following columns:\nfile_name: Name of the audio file.\naudio_path: Path to the audio file.\ntranscript: Swahili transcript of the audio.\nduration: Duration of the audio in seconds.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tUsage\n\t\n\nThis dataset can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jacksonwambali/swahili-tts-dataset.","first_N":5,"first_N_keywords":["Swahili","cc-by-4.0","Audio","üá∫üá∏ Region: US","audio"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Luma Ray2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Luma Ray2 Human Preference\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MM-RLHF","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/MM-RLHF","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n\n[üìñ arXiv Paper] \n[üìä Training Code] \n[üìù Homepage] \n[üèÜ Reward Model] \n[üîÆ MM-RewardBench] \n[üîÆ MM-SafetyBench] \n[üìà Evaluation Suite] \n\n\n\n\n\t\n\t\t\n\t\tThe Next Step Forward in Multimodal LLM Alignment\n\t\n\n[2025/02/10] üî• We are proud to open-source MM-RLHF, a comprehensive project for aligning Multimodal Large Language Models (MLLMs) with human preferences. This release includes:\n\nA high-quality MLLM alignment dataset.\nA strong Critique-Based MLLM reward model and its training algorithm.\nA novel‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/MM-RLHF.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ScImage","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/casszhao/ScImage","creator_name":"casszhao","creator_url":"https://huggingface.co/casszhao","description":"The prompt for ICLR2025 paper \nScImage: HOW GOOD ARE MULTIMODAL LARGE LANGUAGE MODELS AT SCIENTIFIC TEXT-TO-IMAGE GENERATION?\nThe prompt template and the object list will be added soon.\n@inproceedings{\nscimage2025,\ntitle={ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?},\nauthor={Zhang, Leixin and Cheng, Yinjie and Zhai, Weihe and Eger, Steffen and Belouadi, Jonas and Moafian, Fahimeh and Zhao, Zhixue},\nbooktitle={The Thirteenth International‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/casszhao/ScImage.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"ScImage","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/casszhao/ScImage","creator_name":"casszhao","creator_url":"https://huggingface.co/casszhao","description":"The prompt for ICLR2025 paper \nScImage: HOW GOOD ARE MULTIMODAL LARGE LANGUAGE MODELS AT SCIENTIFIC TEXT-TO-IMAGE GENERATION?\nThe prompt template and the object list will be added soon.\n@inproceedings{\nscimage2025,\ntitle={ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?},\nauthor={Zhang, Leixin and Cheng, Yinjie and Zhai, Weihe and Eger, Steffen and Belouadi, Jonas and Moafian, Fahimeh and Zhao, Zhixue},\nbooktitle={The Thirteenth International‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/casszhao/ScImage.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Rich Human Feedback Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \n\nPrompt - Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\n\t\n\t\t\n\t\tRapidata Video Generation Rich Human Feedback Dataset\n\t\n\n\n\n\n\n\n\n\n\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\n\n\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \n\nPrompt - Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback.","first_N":5,"first_N_keywords":["video-classification","text-to-video","text-classification","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"CommonVoices20_ro","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro","creator_name":"Transfer Rapid","creator_url":"https://huggingface.co/TransferRapid","description":"\n\t\n\t\t\n\t\tCommon Voices Corpus 20.0 (Romanian)\n\t\n\n\nCommon Voices is an open-source dataset of speech recordings created by \nMozilla to improve speech recognition technologies. \nIt consists of crowdsourced voice samples in multiple languages, contributed by volunteers worldwide.\n\n\n\nChallenges: The raw dataset included numerous recordings with incorrect transcriptions \nor those requiring adjustments, such as sampling rate modifications, conversion to .wav format, and other refinements \nessential‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-speech","text-to-audio","Romanian"],"keywords_longer_than_N":true},
	{"name":"CommonVoices20_ro","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro","creator_name":"Transfer Rapid","creator_url":"https://huggingface.co/TransferRapid","description":"\n\t\n\t\t\n\t\tCommon Voices Corpus 20.0 (Romanian)\n\t\n\n\nCommon Voices is an open-source dataset of speech recordings created by \nMozilla to improve speech recognition technologies. \nIt consists of crowdsourced voice samples in multiple languages, contributed by volunteers worldwide.\n\n\n\nChallenges: The raw dataset included numerous recordings with incorrect transcriptions \nor those requiring adjustments, such as sampling rate modifications, conversion to .wav format, and other refinements \nessential‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TransferRapid/CommonVoices20_ro.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-speech","text-to-audio","Romanian"],"keywords_longer_than_N":true},
	{"name":"pick-double-caption","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DualCPO/pick-double-caption","creator_name":"DCPO-T2I","creator_url":"https://huggingface.co/DualCPO","description":"\n\t\n\t\t\n\t\tDual Caption Preference Optimization for Diffusion Models\n\t\n\n\n\n\n\nWe propose DCPO, a new paradigm to improve the alignment performance of text-to-image diffusion models. For more details on the technique, please refer to our paper here. \n\n\t\n\t\t\n\t\n\t\n\t\tDeveloped by\n\t\n\n\nAmir Saeidi*\nYiran Luo*\nAgneet Chatterjee\nShamanthak Hegde\nBimsara Pathiraja\nYezhou Yang\nChitta Baral\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset\n\t\n\nThis dataset is Pick-Double Caption, a modified version of the Pick-a-Pic V2 dataset. We generated‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DualCPO/pick-double-caption.","first_N":5,"first_N_keywords":["text-to-image","mit","10K - 100K","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"Images-for-examples","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Felguk/Images-for-examples","creator_name":"Alex Felguk","creator_url":"https://huggingface.co/Felguk","description":"Yes this is example of images for image generator gr.Examples\nThere are three of them for now but I will add many images soon.\nSub me\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"TOSD","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tamazight-NLP/TOSD","creator_name":"Tamazight NLP","creator_url":"https://huggingface.co/Tamazight-NLP","description":"\n\t\n\t\t\n\t\tDataset Card for Tamazight Open Speech Dataset\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tamazight-NLP/TOSD.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Standard Moroccan Tamazight","ber","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MARIO-6M","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/stzhao/MARIO-6M","creator_name":"steve z","creator_url":"https://huggingface.co/stzhao","description":"This datasets is curated by TextDiffuser team in their work: \nTextDiffuser: Diffusion Models as Text Painters (NeurIPS 2023)\nMARIO-6M contains 6M images with text rendered on, filtered from LAION-400M.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"english-tts-eval","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ntt123/english-tts-eval","creator_name":"Th√¥ng Nguy·ªÖn","creator_url":"https://huggingface.co/ntt123","description":"\n\t\n\t\t\n\t\tEnglish Text-to-Speech Evaluation Dataset\n\t\n\nThe english-tts-eval dataset evaluates the performance of Text-to-Speech (TTS) systems. It includes a diverse collection of English text samples covering a wide range of use cases, such as news updates, navigation assistance, customer service, and storytelling.\nThere are 100 examples, each of which includes:\n\nText: The original text sample.\nNormalized Text: The spoken form of the text, with numbers and abbreviations expanded.\nCategory: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ntt123/english-tts-eval.","first_N":5,"first_N_keywords":["text-to-speech","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"allstar","keyword":"video-text-to-text","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/allstar","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Allstar.gg Clips\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains information about 47,896 video clips from the gaming platform allstar.gg. The clips primarily focus on Counter-Strike 2 gameplay moments and include detailed metadata such as player information, game statistics, view counts, and related media URLs.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is primarily in English (en).\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThis dataset includes the following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/allstar.","first_N":5,"first_N_keywords":["video-classification","video-text-to-text","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-pixelart-images","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-images","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\ttrojblue/test-HunyuanVideo-pixelart-images\n\t\n\nHey there! üëã Heads up‚Äîthis repository is just a PARTIAL dataset. For the full pixelart-images dataset, make sure to grab both parts:\n\nImages Part (this repo)\nVideo Part\n\nThis dataset is a collection of anime-style pixel art images and is perfect for debugging general anime text-to-image (T2I) training or testing Hunyuan Video models. üé®\n\n\t\n\t\n\t\n\t\tWhat's in the Dataset?\n\t\n\nThis dataset is all about anime-styled pixel art images that have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-images.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-pixelart-images","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-images","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\n\t\n\t\t\n\t\ttrojblue/test-HunyuanVideo-pixelart-images\n\t\n\nHey there! üëã Heads up‚Äîthis repository is just a PARTIAL dataset. For the full pixelart-images dataset, make sure to grab both parts:\n\nImages Part (this repo)\nVideo Part\n\nThis dataset is a collection of anime-style pixel art images and is perfect for debugging general anime text-to-image (T2I) training or testing Hunyuan Video models. üé®\n\n\t\n\t\n\t\n\t\tWhat's in the Dataset?\n\t\n\nThis dataset is all about anime-styled pixel art images that have‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-images.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-pixelart-videos","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-videos","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\t\n\t\t\n\t\ttrojblue/test-HunyuanVideo-pixelart-images\n\t\n\nüëã Heads up‚Äîthis repository is just a PARTIAL dataset. For the full pixelart-images dataset, make sure to grab both parts:\n\nImages Part \nVideo Part (this repo)\n\n\n\t\n\t\t\n\t\n\t\n\t\tWhat's in the Dataset?\n\t\n\nThis dataset is all about anime-styled pixel art images that have been carefully selected to make your models shine. Here‚Äôs what makes these images special:\n\nRich in detail: Pixelated, yes‚Äîbut still full of life and not overly simplified.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-videos.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"test-HunyuanVideo-pixelart-videos","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-videos","creator_name":"trojblue","creator_url":"https://huggingface.co/trojblue","description":"\n\t\n\t\t\n\t\ttrojblue/test-HunyuanVideo-pixelart-images\n\t\n\nüëã Heads up‚Äîthis repository is just a PARTIAL dataset. For the full pixelart-images dataset, make sure to grab both parts:\n\nImages Part \nVideo Part (this repo)\n\n\n\t\n\t\t\n\t\n\t\n\t\tWhat's in the Dataset?\n\t\n\nThis dataset is all about anime-styled pixel art images that have been carefully selected to make your models shine. Here‚Äôs what makes these images special:\n\nRich in detail: Pixelated, yes‚Äîbut still full of life and not overly simplified.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/trojblue/test-HunyuanVideo-pixelart-videos.","first_N":5,"first_N_keywords":["text-to-image","text-to-video","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"prl_crimson_maiden_style","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paralaif/prl_crimson_maiden_style","creator_name":"paralaif","creator_url":"https://huggingface.co/paralaif","description":"\n\t\n\t\t\n\t\tprl_crimson_maiden_style Dataset\n\t\n\nThis dataset is used to train my lora Crimson Maiden from Civitai.\n","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"text-to-audio","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"RSL_Maran","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran.","first_N":5,"first_N_keywords":["token-classification","table-question-answering","question-answering","text-classification","zero-shot-classification"],"keywords_longer_than_N":true},
	{"name":"vf-eval","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/songtingyu/vf-eval","creator_name":"Tingyu Song","creator_url":"https://huggingface.co/songtingyu","description":"\n\t\n\t\t\n\t\tDataset Card for VF-Eval Benchmark\n\t\n\nRepository: sighingsnow/vf-eval\nFor the usage of this dataset, please refer to the github repo. \nIf you find this repository helpful, feel free to cite our paper:\n@misc{song2025vfeval,\n      title={VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos}, \n      author={Tingyu Song and Tongyan Hu and Guo Gan and Yilun Zhao},\n      year={2025},\n      eprint={2505.23693},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/songtingyu/vf-eval.","first_N":5,"first_N_keywords":["video-text-to-text","visual-question-answering","mit","1K<n<10K","Video"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"Unite-Instruct-Retrieval-Train","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train","creator_name":"Kong","creator_url":"https://huggingface.co/friedrichor","description":"\n\t\n\t\t\n\t\tModality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval\n\t\n\n\n\n\n\n\n\n\t\t\n\t\tStatistics\n\t\n\n\n    \n\n\n\n\t\n\t\t\n\t\tAccessing Images and Videos\n\t\n\n\n2025-06-19: We've updated the compressed archives for all image and video files to enable faster extraction.If you've already downloaded the previous files, there's no need to redownload them ‚Äî the content remains exactly the same. The only difference lies in the compression method, which now allows for quicker‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/friedrichor/Unite-Instruct-Retrieval-Train.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","text-retrieval","image-feature-extraction","video-text-to-text"],"keywords_longer_than_N":true},
	{"name":"GuardReasoner-VLTrain","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain","creator_name":"yueliu1999","creator_url":"https://huggingface.co/yueliu1999","description":"\n\t\n\t\t\n\t\tGuardReasoner-VLTrain\n\t\n\nGuardReasoner-VLTrain is the training data for R-SFT of GuardReasoner-VL, as described in the paper GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.\nCode: https://github.com/yueliu1999/GuardReasoner-VL/\n\n\t\n\t\t\n\t\tUsage\n\t\n\nfrom datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\nds = load_dataset(\"yueliu1999/GuardReasoner-VLTrain\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this dataset, please cite our paper.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yueliu1999/GuardReasoner-VLTrain.","first_N":5,"first_N_keywords":["image-text-to-text","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"diffusiondb_captions","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hz2475/diffusiondb_captions","creator_name":"Haotian Zhang","creator_url":"https://huggingface.co/hz2475","description":"\n\t\n\t\t\n\t\tDiffusionDB Captions\n\t\n\nCaptions corpora processed from DiffusionDB dataset, the goal is to create a dataset similar to flickr30k but much larger\nFiltered from diffusionDB + diffusionDB_large, total 14M data, into ~1M\npre-processing includes:\n\nremove style prompts\nremove too long and specific prompts\nremove prompts with many specific names\n\n\n\t\n\t\t\n\t\n\t\n\t\tCitation\n\t\n\nIf you find this curated dataset helpful, feel free to give us a cite.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hz2475/diffusiondb_captions.","first_N":5,"first_N_keywords":["text-to-image","cc0-1.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"jedi","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rronan-h/jedi","creator_name":"Ronan Riochet","creator_url":"https://huggingface.co/rronan-h","description":"\n\t\n\t\t\n\t\tJEDI\n\t\n\nThe JEDI Dataset consists of four carefully designed categories:\n\nIcon\nComponent\nLayout\nRefusal\n\nThis repository includes all the textures and images for these components.\nAdditionally, JEDI processes and improves the data from AGUVIS, calling it AGUVIS++. This repository contains the texture portion of AGUVIS++. For images, please refer to the original repository.\n\n\t\n\t\t\n\t\n\t\n\t\tüìÑ Citation\n\t\n\nIf you find this work useful, please consider citing our paper:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rronan-h/jedi.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"miami_beach_hologram2_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robb-0/miami_beach_hologram2_dataset","creator_name":"robbie","creator_url":"https://huggingface.co/robb-0","description":"\n  \n  \n\n\nDataset of images used for my Miami Beach Hologram version 2.\nIt is a .zip file which includes 60 images and 60 text files with caption (captioned automatically on Civitai)\nMiami Beach Hologram 2 Dataset ¬© 2025 by Robb-0 is licensed under CC BY 4.0 \n","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VisCode-200K","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/VisCode-200K","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\n\t\n\t\t\n\t\tVisCode-200K\n\t\n\nüè† Project Page | üíª GitHub | üìñ Paper | ü§ó VisCoder-3B | ü§ó VisCoder-7B\nVisCode-200K is a large-scale instruction-tuning dataset for training language models to generate and debug executable Python visualization code.\n\n\t\n\t\t\n\t\n\t\n\t\tüß† Overview\n\t\n\nVisCode-200K contains over 200,000 samples for executable Python visualization tasks. Each sample includes a natural language instruction and the corresponding Python code, structured as a messages list in ChatML format.\nWe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/VisCode-200K.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"twi-words-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-words-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Words Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 413463 parallel speech-text pairs for Twi (Akan), a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Twi (Akan) - tw\nTask: Speech Recognition, Text-to-Speech\nSize: 413463 audio files >‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-words-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Twi"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"MVBench-EvalData-PixelReasoner","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JasperHaozhe/MVBench-EvalData-PixelReasoner","creator_name":"Haozhe Wang","creator_url":"https://huggingface.co/JasperHaozhe","description":"The evaluation data for the MVBench. \nThe data structure follows the evaluation code of PixelReasoner\n","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ViMUL-Bench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MBZUAI/ViMUL-Bench","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","description":"\n\t\n\t\t\n\t\tViMUL-Bench: A Culturally-diverse Multilingual Multimodal Video Benchmark\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe evaluation toolkit to be used is lmms-eval. This toolkit facilitates the evaluation of models across multiple tasks and languages.\n\n\t\n\t\n\t\n\t\tKey Features\n\t\n\n\nüåç 14 Languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, Japaneseüé≠ 15 Categories: Including 8 culturally diverse categories (lifestyles, festivals, foods‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/ViMUL-Bench.","first_N":5,"first_N_keywords":["video-text-to-text","cc-by-sa-4.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ga-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/ga-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tGa Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 98343 parallel speech-text pairs for Ga, a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Ga - gaa\nTask: Speech Recognition, Text-to-Speech\nSize: 98343 audio files > 1KB (small/corrupted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/ga-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Ga"],"keywords_longer_than_N":true},
	{"name":"textureninja","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/textureninja","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Texture Ninja\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,540 texture images from texture.ninja. It includes high-resolution textures of brick, concrete, rock, wood, metal, paint, plaster, ground materials, and other surfaces. The original images were downloaded, processed, and compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish (en):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/textureninja.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"modular_characters","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/QLeca/modular_characters","creator_name":"Quentin Leca","creator_url":"https://huggingface.co/QLeca","description":"QLeca/modular_characters dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"pbrpx","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/pbrpx","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for PBRPX Asset Library\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains metadata for 710 physically-based rendering (PBR) assets from pbrpx.com, a CC0 asset library. The dataset includes comprehensive information about 3D models, textures, and materials with detailed file listings, download URLs, texture resolutions, and categorization data. Assets include nature elements (trees, rocks), architectural materials (brick, concrete), and various other 3D models with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/pbrpx.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"ljspeech-enhanced","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thewh1teagle/ljspeech-enhanced","creator_name":"thewh1teagle","creator_url":"https://huggingface.co/thewh1teagle","description":"\n\t\n\t\t\n\t\tljspeech-enhanced\n\t\n\nThis dataset contains ~10 hours of 1 to 14 seconds audio files from LJSpeech dataset in similar format 0.txt|phonemes|speaker_id|text\nThe dataset enhanced using Adobe speech enhance v2 and phonemized using espeak-ng\nExtracted data size is ~14GB\n","first_N":5,"first_N_keywords":["text-to-speech","English","mit","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"MIDI_Art","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LunaticGhoulPiano/MIDI_Art","creator_name":"SU,PO-HSUN","creator_url":"https://huggingface.co/LunaticGhoulPiano","description":"\n\t\n\t\t\n\t\tMIDI Art images\n\t\n\nMIDI art images with human-generated description txt files\n\n\t\n\t\t\n\t\tSource\n\t\n\n./data_source.md\n\n\t\n\t\t\n\t\tType\n\t\n\n\nMIDIArt: MIDI art images\nPianoRoll: piano roll images\n\n\n\t\n\t\t\n\t\tOriginal size\n\t\n\n./Original_size\n\n\t\n\t\t\n\t\tSpecific resized width and height\n\t\n\n\nFolder naming: {width}_{height}\n\n","first_N":5,"first_N_keywords":["mit","< 1K","text","Image","Text"],"keywords_longer_than_N":true},
	{"name":"CC3M_synthetic","keyword":"text-to-image","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/timjeffrey10/CC3M_synthetic","creator_name":"ÂºµÂ∫≠Áëú","creator_url":"https://huggingface.co/timjeffrey10","description":"\n\t\n\t\t\n\t\tDataset Card for CC3M_synthetic\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a data of synthetic captions for Conceptual Captions. The alt_text is the original caption in CC3M dataset. The synthetic_captions were produced by Florence-2-large.\n\n\t\n\t\t\n\t\tProcedure\n\t\n\nWe captioned the images by Florence-2-large. We gave the model DETAILED_CAPTION task with a beam search size of 3. To ensure quality of the captions, we filtered out samples with too short (fewer than 50 characters) or excessively‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timjeffrey10/CC3M_synthetic.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-synthetic-1m-part001","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part001","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset - Part 1 of 5\n\t\n\n\n\t\n\t\t\n\t\tüéâ The Largest Speech Dataset for Twi Language\n\t\n\nThis dataset contains part 1 of the largest speech dataset for the Twi language, featuring 1 million speech-to-text pairs split across 5 parts (approximately 200,000 samples each). This represents a groundbreaking resource for Twi (Akan), a language spoken primarily in Ghana.\n\n\t\n\t\t\n\t\tüöÄ Breaking the Low-Resource Language Barrier\n\t\n\nThis publication demonstrates that African‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part001.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Akan"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-synthetic-1m-part003","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part003","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset - Part 3 of 5\n\t\n\n\n\t\n\t\t\n\t\tüéâ The Largest Speech Dataset for Twi Language\n\t\n\nThis dataset contains part 3 of the largest speech dataset for the Twi language, featuring 1 million speech-to-text pairs split across 5 parts (approximately 200,000 samples each). This represents a groundbreaking resource for Twi (Akan), a language spoken primarily in Ghana.\n\n\t\n\t\t\n\t\tüöÄ Breaking the Low-Resource Language Barrier\n\t\n\nThis publication demonstrates that African‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part003.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Akan"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-synthetic-1m-part004","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part004","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset - Part 4 of 5\n\t\n\n\n\t\n\t\t\n\t\tüéâ The Largest Speech Dataset for Twi Language\n\t\n\nThis dataset contains part 4 of the largest speech dataset for the Twi language, featuring 1 million speech-to-text pairs split across 5 parts (approximately 200,000 samples each). This represents a groundbreaking resource for Twi (Akan), a language spoken primarily in Ghana.\n\n\t\n\t\t\n\t\tüöÄ Breaking the Low-Resource Language Barrier\n\t\n\nThis publication demonstrates that African‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part004.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Akan"],"keywords_longer_than_N":true},
	{"name":"arabic-tts-wav-24k","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NeoBoy/arabic-tts-wav-24k","creator_name":"Sharjeel Abid Butt","creator_url":"https://huggingface.co/NeoBoy","description":"\n\t\n\t\t\n\t\tArabic TTS WAV 24k Dataset\n\t\n\nA high-quality, open-source dataset for Arabic Text-to-Speech (TTS) research, containing paired audio and text samples from both male and female speakers. All audio is provided in 24kHz WAV format, with rich metadata and phonetic transcriptions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is designed for training and evaluating neural TTS systems in Modern Standard Arabic. It includes:\n\nAudio: Clean, studio-quality WAV files at 24,000 Hz.\nText: Original Arabic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NeoBoy/arabic-tts-wav-24k.","first_N":5,"first_N_keywords":["text-to-speech","Arabic","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"arabic_myanmar_quran_voices","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices","creator_name":"Wynn","creator_url":"https://huggingface.co/freococo","description":"\n\t\n\t\t\n\t\tüìñ Arabic-Myanmar Quran Voice Dataset\n\t\n\n\n\t\n\t\t\n\t\tüïå Overview\n\t\n\nThis dataset contains high-quality MP3 audio recordings of the entire Holy Qur‚Äôan with:\n\nArabic recitation of each verse\nFollowed immediately by its Myanmar (Burmese) translation\n\nIt is the first complete Arabic-Myanmar Quran audio interpretation of its kind publicly released in Myanmar. The goal is to make the Qur‚Äôan more accessible to:\n\nElderly persons\nBlind or visually impaired people\nMyanmar speakers who wish to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/arabic_myanmar_quran_voices.","first_N":5,"first_N_keywords":["audio-classification","audio-to-audio","automatic-speech-recognition","text-to-speech","human-annotated"],"keywords_longer_than_N":true},
	{"name":"amazing_miami_holograms_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/semiosphere/amazing_miami_holograms_dataset","creator_name":"semiosphere","creator_url":"https://huggingface.co/semiosphere","description":"\n\t\n\t\t\n\t\tOkay, you wanted holograms, right? I, at least, did,...\n\t\n\nPlease ignore everything I said about MIAMI BEACH HOLOGRAM VERSION 1, it's actually insanely amazing. The point is that it's a tiny LoCON. IT IS NOT A LoRA!!!!!\nWhich means that it too can help writing small words, and it gives somehow more freedom than the second version.\nBut since it's so small, it's very recommendable to use a CFG BOOSTER. You can find them on Civitai and on TensorArt. With CFG Booster at 0.5, with only‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/semiosphere/amazing_miami_holograms_dataset.","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CommonVoicesDelta21_ro","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ionut-visan/CommonVoicesDelta21_ro","creator_name":"Ionut Visan","creator_url":"https://huggingface.co/ionut-visan","description":"\n\t\n\t\t\n\t\tCommon Voices Delta 21.0 (Romanian)\n\t\n\n\nCommon Voices is an open-source dataset of speech recordings created by \nMozilla to improve speech recognition technologies. \nIt consists of crowdsourced voice samples in multiple languages, contributed by volunteers worldwide.\n\n\n\nChallenges: The raw dataset included numerous recordings with incorrect transcriptions \nor those requiring adjustments, such as sampling rate modifications, conversion to .wav format, and other refinements \nessential‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ionut-visan/CommonVoicesDelta21_ro.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-speech","text-to-audio","Romanian"],"keywords_longer_than_N":true},
	{"name":"CommonVoicesDelta21_ro","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ionut-visan/CommonVoicesDelta21_ro","creator_name":"Ionut Visan","creator_url":"https://huggingface.co/ionut-visan","description":"\n\t\n\t\t\n\t\tCommon Voices Delta 21.0 (Romanian)\n\t\n\n\nCommon Voices is an open-source dataset of speech recordings created by \nMozilla to improve speech recognition technologies. \nIt consists of crowdsourced voice samples in multiple languages, contributed by volunteers worldwide.\n\n\n\nChallenges: The raw dataset included numerous recordings with incorrect transcriptions \nor those requiring adjustments, such as sampling rate modifications, conversion to .wav format, and other refinements \nessential‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ionut-visan/CommonVoicesDelta21_ro.","first_N":5,"first_N_keywords":["automatic-speech-recognition","audio-classification","text-to-speech","text-to-audio","Romanian"],"keywords_longer_than_N":true},
	{"name":"the_artist_flux_dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hawkwind/the_artist_flux_dataset","creator_name":"Galen Ironheart","creator_url":"https://huggingface.co/Hawkwind","description":"Dataset related to\nThe Artist | Flux edition.\nAuto-tagged on Civitai.\n","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"MasriSpeech","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NightPrince/MasriSpeech","creator_name":"Yahya Muhammad Alnwsany","creator_url":"https://huggingface.co/NightPrince","description":"\n\t\n\t\t\n\t\tüó£Ô∏è MasriSpeech: Egyptian Arabic Speech Dataset\n\t\n\nA large-scale, high-quality Egyptian Arabic speech dataset for Automatic Speech Recognition (ASR), curated and released by Yahya Muhammad Alnwsany. MasriSpeech is designed to empower research and development in Arabic ASR, dialect modeling, and linguistic studies.\n\n  \n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tüåü Mission & Vision\n\t\n\nMasriSpeech aims to:\n\nAdvance the state of Egyptian Arabic ASR and dialectal NLP.\nEnable researchers, developers, and students to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NightPrince/MasriSpeech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","found","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"emilia-en-snac","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nytopop/emilia-en-snac","creator_name":"Eric Izoita","creator_url":"https://huggingface.co/nytopop","description":"\n\t\n\t\t\n\t\tStats (EN)\n\t\n\nEmilia: 46,349 hours\nEmilia-YODAS: 87,258 hours\nTotal: 133,607 hours\n\n\t\n\t\t\n\t\tLicense\n\t\n\nThe Emilia subset is licensed under CC BY-NC 4.0.\nThe Emilia-YODAS subset is licensed under CC BY 4.0.\n\n\t\n\t\t\n\t\tReference\n\t\n\n@inproceedings{emilialarge,\n    author={He, Haorui and Shang, Zengqiang and Wang, Chaoren and Li, Xuyuan and Gu, Yicheng and Hua, Hua and Liu, Liwei and Yang, Chen and Li, Jiaqi and Shi, Peiyang and Wang, Yuancheng and Chen, Kai and Zhang, Pengyuan and Wu‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nytopop/emilia-en-snac.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","English","cc-by-4.0","100M - 1B"],"keywords_longer_than_N":true},
	{"name":"ToneBooksPlus","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vikhrmodels/ToneBooksPlus","creator_name":"Vikhr models","creator_url":"https://huggingface.co/Vikhrmodels","description":"\n\t\n\t\t\n\t\tToneBooksPlus\n\t\n\nToneBooksPlus ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ Vikhrmodels/ToneBooks, –Ω–æ –±–µ–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏. –í –¥–∞—Ç–∞—Å–µ—Ç–µ 179.16 —á–∞—Å–æ–≤ –∞—É–¥–∏–æ –¥–ª—è train —Å–ø–ª–∏—Ç–∞ –∏ 9.42 —á–∞—Å–∞ –¥–ª—è validation.\n–ë–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ its5Q –∑–∞ –ø–æ–º–æ—â—å –≤ —Å–±–æ—Ä–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n\n\n\t\n\t\t\n\t\n\t\n\t\t–û–ø–∏—Å–∞–Ω–∏–µ\n\t\n\n–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—É–¥–∏–æ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–±—Ä–∞–Ω—ã:\n\n–°—Å—ã–ª–∫–∞ –Ω–∞ MP3-—Ñ–∞–π–ª (audio)\n–¢–µ–∫—Å—Ç–æ–≤–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ (text)\n–ò–º—è –≥–æ–ª–æ—Å–∞ (voice_name) ‚Äî –æ–¥–Ω–æ –∏–∑ –∏–º—ë–Ω –¥–∏–∫—Ç–æ—Ä–æ–≤:\nAleksandr Kotov  \nAleksandr Zbarovskii  \nAlina Archibasova  \nDaniel Che‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vikhrmodels/ToneBooksPlus.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Russian","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"MSRVTT-Personalization","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LIMinghan/MSRVTT-Personalization","creator_name":"LIMinghan","creator_url":"https://huggingface.co/LIMinghan","description":"\n\t\n\t\t\n\t\tMSRVTT-Personalization\n\t\n\nFollow instruction to get the msrvtt-personalization data.\n\n\t\n\t\t\n\t\tLICENSE\n\t\n\nSee License of MSRVTT-Personalization\n","first_N":5,"first_N_keywords":["text-to-video","image-to-video","English","mit","100M<n<1B"],"keywords_longer_than_N":true},
	{"name":"KC-MMbench","keyword":"video-text-to-text","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kwai-Keye/KC-MMbench","creator_name":"Kwai-Keye","creator_url":"https://huggingface.co/Kwai-Keye","description":"  [üçé Home Page] [üìñ Technical Report] [\\ud83d\\udcca Models] [\\ud83d\\ude80 Demo] \nThis repository contains KC-MMBench, a new benchmark dataset meticulously tailored for real-world short-video scenarios, as presented in the paper \"Kwai Keye-VL Technical Report\". Constructed from Kuaishou short video data, KC-MMBench comprises 6 distinct datasets designed to evaluate the performance of Vision-Language Models (VLMs) like Kwai Keye-VL-8B, Qwen2.5-VL, and InternVL in comprehending dynamic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kwai-Keye/KC-MMbench.","first_N":5,"first_N_keywords":["video-text-to-text","Chinese","English","cc-by-sa-4.0","arxiv:2507.01949"],"keywords_longer_than_N":true},
	{"name":"audio_data_russian_annotated","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kijjjj/audio_data_russian_annotated","creator_name":"fgfd","creator_url":"https://huggingface.co/kijjjj","description":"\n\t\n\t\t\n\t\tDataset Audio Russian Annotated\n\t\n\nThis is a dataset with Russian annotated audio data, split into train for tasks like text-to-speech, speech recognition, and speaker identification.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\ntext: Audio transcription (string).\nspeaker_name: Speaker identifier (string).\naudio: Audio file.\nutterance_pitch_mean: The average pitch of the speech utterance (float64).\nutterance_pitch_std: The standard deviation of pitch, representing variability in intonation (float64)\nsnr:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kijjjj/audio_data_russian_annotated.","first_N":5,"first_N_keywords":["text-to-speech","Russian","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"french-lot-department-captioned-photos","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tLot Department, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from the Lot region of France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from three notable locations in France's Lot department: Rocamadour, Autoire, and Padirac. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/french-lot-department-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"CoF-SFT-Data-5.4k","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k","creator_name":"Xintong Zhang","creator_url":"https://huggingface.co/xintongzhang","description":"Overview.\nThis dataset is used for supervised fine-tuning (SFT) in training Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL,\nDetails.\nThis dataset includes 5.4k reasoning samples: 2.4k involve zoom-in behavior, and 3k are text-only reasoning cases. All samples were generated using our Visual Agent and span a variety of image resolutions.\nTraining Code: The SFT code can be found at https://github.com/xtong-zhang/Chain-of-Focus\nProject page:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","Image","arxiv:2505.15436"],"keywords_longer_than_N":true},
	{"name":"albi-captioned-photos","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos","creator_name":"No√© Flandre","creator_url":"https://huggingface.co/NoeFlandre","description":"\n\t\n\t\t\n\t\tAlbi, France Image Dataset\n\t\n\nA collection of high-resolution scenic photographs from Albi, France with AI-generated descriptive captions.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains scenic photographs from Albi, France, including the city center, the Toulouse Lautrec museum, and the Sainte-C√©cile Cathedral. All images were captured using a Sony A6600 camera and are paired with detailed English captions generated by Mistral AI's Pixtral-Large model.\nKey Features:\n\nHigh-resolution‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NoeFlandre/albi-captioned-photos.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","image-classification","object-detection","English"],"keywords_longer_than_N":true},
	{"name":"som_tts","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zakihassan/som_tts","creator_name":"Zakarie Hassan Abdi","creator_url":"https://huggingface.co/zakihassan","description":"\n\t\n\t\t\n\t\tSomali TTS Dataset\n\t\n\nThis dataset contains high-quality Somali speech recordings with corresponding transcriptions.It is designed for training Text-to-Speech (TTS) models such as Coqui XTTS v2, FastSpeech, or VITS for the Somali language.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nFormat: .wav audio + metadata.csv (tab-separated: wav_path, transcription)\nLanguage: Somali\nTotal Samples: ~20,000 clips\nSampling Rate: 22050 Hz\n\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo train with Coqui TTS:\ntts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zakihassan/som_tts.","first_N":5,"first_N_keywords":["text-to-speech","Somali","apache-2.0","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"OseDitionary_Voice","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MakarMD/OseDitionary_Voice","creator_name":"Mariya","creator_url":"https://huggingface.co/MakarMD","description":"MakarMD/OseDitionary_Voice dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["translation","text-to-audio","cc-by-4.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"TTS_eval_datasets","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/k2-fsa/TTS_eval_datasets","creator_name":"k2-fsa","creator_url":"https://huggingface.co/k2-fsa","description":"\n\t\n\t\t\n\t\tTTS evaluation datasets\n\t\n\nThis repository contains three testsets for zero-shot TTS models:\n\ndialog_testset: Chinese and English testsets for spoken dialogue generation models, introduced in paper ZipVoice-Dialog.\nlibrispeech_pc_testset: English testset for zero-shot TTS models, introduced in paper F5-TTS.\nseedtts_testset: Chinese and English testsets for zero-shot TTS models, introduced in paper Seed-TTS.\n\n","first_N":5,"first_N_keywords":["text-to-speech","Chinese","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"TikTok_MostComment_Video_Transcription_Example","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_MostComment_Video_Transcription_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_MostComment_Video_Transcription_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","feature-extraction","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"TikTok_MostComment_Video_Transcription_Example","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_MostComment_Video_Transcription_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_MostComment_Video_Transcription_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","feature-extraction","text-to-speech"],"keywords_longer_than_N":true},
	{"name":"TikTok_Hottest_Video_Transcript_Example","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_Hottest_Video_Transcript_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_Hottest_Video_Transcript_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","translation","summarization"],"keywords_longer_than_N":true},
	{"name":"TikTok_Hottest_Video_Transcript_Example","keyword":"text-to-audio","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MasaFoundation/TikTok_Hottest_Video_Transcript_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","description":"\n\t\n\t\t\n\t\tüì≤ Example Dataset: TikTok Scraper Tool\n\t\n\nüëâ Start Scraping TikTok: TikTok Scraper Tool\n\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Instant Transcription ‚Äì Turn any TikTok video into an AI-ready transcript  \nüéØ Metadata ‚Äì Get the title, language description, and video hashtags  \nüîó URL-Based Access ‚Äì Just drop in a TikTok video URL to start scraping  \nüß© LLM-Ready Output ‚Äì Receive clean JSON ready for agents, RAG, or AI tools  \nüí∏ Free Tier ‚Äì Use up to 100 queries during the beta period  \nüí´ Easy‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/TikTok_Hottest_Video_Transcript_Example.","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","translation","summarization"],"keywords_longer_than_N":true},
	{"name":"Notcrowy","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mobinx/Notcrowy","creator_name":"Mobin Chowdhury","creator_url":"https://huggingface.co/mobinx","description":"\n\t\n\t\t\n\t\tAudio Dataset Statistics\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal audio files\n556,667\n\n\nTotal duration\n1,024.71 hours (3,688,949 seconds)\n\n\nAverage duration\n6.63 seconds\n\n\nShortest clip\n0.41 seconds\n\n\nLongest clip\n44.97 seconds\n\n\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tSpeaker Breakdown\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tTop 10 Speakers by Clip Count\n\t\n\n\n\t\n\t\t\nSpeaker\nClips\nDuration\n% of Total\n\n\n\t\t\nDespina\n60,150\n118.07 hours\n11.5%\n\n\nSulafat\n31,593\n58.15 hours\n5.7%\n\n\nAchernar29,889\n54.53 hours\n5.3%\n\n\nAutonoe\n27,897‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mobinx/Notcrowy.","first_N":5,"first_N_keywords":["text-to-speech","English","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"wan_walking_forward","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_walking_forward","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"Mechanicalpart","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alirezzaa13/Mechanicalpart","creator_name":"alireza_tavakol","creator_url":"https://huggingface.co/alirezzaa13","description":"alirezzaa13/Mechanicalpart dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"wan_wrap_effect","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_wrap_effect","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"wan_yawning","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linoyts/wan_yawning","creator_name":"Linoy Tsaban","creator_url":"https://huggingface.co/linoyts","description":"This dataset contains videos generated using Wan 2.1 T2V 14B.\n","first_N":5,"first_N_keywords":["apache-2.0","< 1K","Text","Video","Datasets"],"keywords_longer_than_N":true},
	{"name":"vocalno","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/t0bi4s/vocalno","creator_name":"sujiuheng","creator_url":"https://huggingface.co/t0bi4s","description":"\n\t\n\t\t\n\t\tTobias Chinese TTS Dataset\n\t\n\nËøôÊòØ‰∏Ä‰∏™‰∏≠ÊñáÊñáÊú¨ËΩ¨ËØ≠Èü≥(TTS)Êï∞ÊçÆÈõÜÔºåÂåÖÂê´Á∫¶997‰∏™È´òË¥®ÈáèÁöÑ‰∏≠ÊñáÈü≥È¢ë-ÊñáÊú¨ÂØπ„ÄÇ\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜ‰ø°ÊÅØ\n\t\n\n\nËØ≠Ë®Ä: ‰∏≠Êñá (Chinese)\n‰ªªÂä°: ÊñáÊú¨ËΩ¨ËØ≠Èü≥ (Text-to-Speech)\nÊ†∑Êú¨Êï∞Èáè: ~997‰∏™Èü≥È¢ë-ÊñáÊú¨ÂØπ\nÈü≥È¢ëÊ†ºÂºè: WAV, 16kHzÈááÊ†∑Áéá\nËÆ∏ÂèØËØÅ: MIT\nÂèëË®Ä‰∫∫: Âçï‰∏ÄÂèëË®Ä‰∫∫\n\n\n\t\n\t\t\n\t\tÊï∞ÊçÆÈõÜÁªìÊûÑ\n\t\n\nfrom datasets import load_dataset\n\n# Âä†ËΩΩÂÆåÊï¥Êï∞ÊçÆÈõÜ\ndataset = load_dataset(\"your_username/tobias-tts-chinese\")\n\n# Âè™Âä†ËΩΩËÆ≠ÁªÉÈõÜ\ntrain_dataset = load_dataset(\"your_username/tobias-tts-chinese\", split=\"train\")\n\n# Âè™Âä†ËΩΩÈ™åËØÅÈõÜ\nvalidation_dataset = load_dataset(\"your_username/tobias-tts-chinese\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/t0bi4s/vocalno.","first_N":5,"first_N_keywords":["text-to-speech","Chinese","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"clker-svg","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/clker-svg","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for Clker.com SVG Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 255,758 public domain SVG vector clipart images collected from Clker.com. Clker.com hosts user-shared vector clip art that is explicitly released into the public domain (CC0). The dataset includes the SVG content itself along with metadata such as titles and tags associated with each image. The SVG files in this dataset have been minified using tdewolff/minify to reduce file size while preserving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/clker-svg.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"ViCA-thinking-2.68k","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k","creator_name":"nkkbr","creator_url":"https://huggingface.co/nkkbr","description":"\n\t\n\t\t\n\t\tViCA-Thinking-2.68K\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuickstart\n\t\n\nYou can load our dataset using the following code:\nfrom datasets import load_dataset\nvica_thinking = load_dataset(\"nkkbr/ViCA-thinking-2.68k\")\n\n\n\t\n\t\n\t\n\t\tOverview\n\t\n\nThis is the dataset we created to further fine-tune the ViCA model. Our motivation stems from the observation that, after being trained on large-scale visuospatial instruction data (e.g., ViCA-322K), ViCA tends to output final answers directly without any intermediate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k.","first_N":5,"first_N_keywords":["visual-question-answering","image-to-text","video-text-to-text","machine-generated","machine-generated"],"keywords_longer_than_N":true},
	{"name":"openclipart","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/openclipart","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for OpenClipart.org SVG Images\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 178,604 public domain SVG vector clipart images collected from OpenClipart.org. OpenClipart.org is a community-driven platform where artists share vector clip art explicitly released into the public domain (CC0). The dataset includes the SVG content along with comprehensive metadata such as titles, descriptions, artist names, creation dates, tags, and image URLs. The SVG files in this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/openclipart.","first_N":5,"first_N_keywords":["image-classification","zero-shot-image-classification","text-to-image","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"LongBench-T2I","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/YCZhou/LongBench-T2I","creator_name":"Yucheng Zhou","creator_url":"https://huggingface.co/YCZhou","description":"\n\t\n\t\t\n\t\tLongBench-T2I\n\t\n\nLongBench-T2I is a benchmark dataset introduced in the paper Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation.It is a standalone dataset designed specifically for evaluating text-to-image (T2I) generation models under long and compositionally rich prompts.\n\n\t\n\t\t\n\t\n\t\n\t\tüì¶ Dataset Summary\n\t\n\nThis dataset contains 500 samples, each composed of:\n\nA long-form instruction (complex natural language prompt).\nA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YCZhou/LongBench-T2I.","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"CreatiDesign_dataset","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuiZhang0812/CreatiDesign_dataset","creator_name":"HuiZhang","creator_url":"https://huggingface.co/HuiZhang0812","description":"\n\t\n\t\t\n\t\tCreatiDesign Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nCreatiDesign Dataset is a large-scale, high-quality dataset for multi-conditional graphic design generation. It contains 400,000 synthetic graphic design samples, each richly annotated with multiple conditions‚Äîincluding primary visual elements (main subject images), secondary visual elements (decorative objects with spatial and semantic annotations), and textual elements (such as slogans or titles with layout information).\nThis dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuiZhang0812/CreatiDesign_dataset.","first_N":5,"first_N_keywords":["text-to-image","apache-2.0","100K - 1M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"ACON","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jiwan-chung/ACON","creator_name":"Jiwan Chung","creator_url":"https://huggingface.co/jiwan-chung","description":"\n\t\n\t\t\n\t\tDataset Card for ACON Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nData from: Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?\n@inproceedings{chung2025are,\n  title={Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?},\n  author={Chung, Jiwan and Yoon, Janghan and Park, Junhyeong and Lee, Sangeyl and Yang, Joowon and Park, Sooyeon and Yu, Youngjae},\n  booktitle={Proceedings of the 63rd Annual Meeting of the Association for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jiwan-chung/ACON.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"livesqlbench-base-lite","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/birdsql/livesqlbench-base-lite","creator_name":"The BIRD Team","creator_url":"https://huggingface.co/birdsql","description":"\n\t\n\t\t\n\t\tüöÄ LiveSQLBench-Base-Lite\n\t\n\nA dynamic, contamination‚Äëfree benchmark for evaluating LLMs on complex, real‚Äëworld text‚Äëto‚ÄëSQL tasks.\nüåê Website ‚Ä¢ üìÑ Paper (coming soon) ‚Ä¢ üíª GitHub\nMaintained by the ü¶ú BIRD Team @ HKU & ‚òÅÔ∏è Google Cloud\n\n\t\n\t\t\n\t\n\t\n\t\tüìä LiveSQLBench Overview\n\t\n\nLiveSQLBench (BIRD-SQL Pro v0.5) is a contamination-free, continuously evolving benchmark designed to evaluate LLMs on complex, real-world text-to-SQL tasks, featuring diverse real-world user queries, including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birdsql/livesqlbench-base-lite.","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"LLM_Dys","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tong0/LLM_Dys","creator_name":"huanpm","creator_url":"https://huggingface.co/tong0","description":"\n\t\n\t\t\n\t\tüîä LLM-Dys Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLLM-Dys is an innovative dataset that leverages large language models to help realistic dysfluent speech synthesis. This comprehensive dataset supports multiple types of dysfluency at different linguistic levels, enabling advanced research in speech synthesis and dysfluency analysis.\n\n\t\n\t\t\n\t\tüîç Dysfluency Types\n\t\n\nOur dataset supports multiple types of dysfluency at both word and phoneme levels:\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\nNatural and authentic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tong0/LLM_Dys.","first_N":5,"first_N_keywords":["text-to-speech","audio-classification","automatic-speech-recognition","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"LLM_Dys","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tong0/LLM_Dys","creator_name":"huanpm","creator_url":"https://huggingface.co/tong0","description":"\n\t\n\t\t\n\t\tüîä LLM-Dys Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nLLM-Dys is an innovative dataset that leverages large language models to help realistic dysfluent speech synthesis. This comprehensive dataset supports multiple types of dysfluency at different linguistic levels, enabling advanced research in speech synthesis and dysfluency analysis.\n\n\t\n\t\t\n\t\tüîç Dysfluency Types\n\t\n\nOur dataset supports multiple types of dysfluency at both word and phoneme levels:\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\nNatural and authentic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tong0/LLM_Dys.","first_N":5,"first_N_keywords":["text-to-speech","audio-classification","automatic-speech-recognition","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"PCogAlignBench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/YongqiLi/PCogAlignBench","creator_name":"Yongqi Li","creator_url":"https://huggingface.co/YongqiLi","description":"\n\t\n\t\t\n\t\tAligning VLM Assistants with Personalized Situated Cognition (ACL 2025 main)\n\t\n\n\n\n\nThis repository contains the constructed benchmark in our ACL 2025 main paper \"Aligning VLM Assistants with Personalized Situated Cognition\". \n\n‚ö†Ô∏è This project is for academic research only and not intended for commercial use.\n\n\n\t\n\t\n\t\n\t\tAbstract\n\t\n\nVision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YongqiLi/PCogAlignBench.","first_N":5,"first_N_keywords":["image-text-to-text","cc-by-4.0","Image","arxiv:2506.00930","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"vlms-are-biased","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anvo25/vlms-are-biased","creator_name":"An Vo","creator_url":"https://huggingface.co/anvo25","description":"\n\t\n\t\t\n\t\tVision Language Models are Biased \n\t\n\n    \n  by \n    An Vo1*,\n    Khai-Nguyen Nguyen2*,\n    Mohammad Reza Taesiri3, \n    Vy Tuong Dang1,\n    Anh Totti Nguyen4‚Ä†,\n    Daeyoung Kim1‚Ä†\n  \n  \n    *Equal contribution¬†¬†¬†¬†‚Ä†Equal advising\n    1KAIST, 2College of William and Mary, 3University of Alberta, 4Auburn University\n  \n\n\n\n\n\n \n\n\n\n\n\nTLDR: State-of-the-art Vision Language Models (VLMs) perform perfectly on counting tasks with original images but fail catastrophically (e.g., 100% ‚Üí 17.05%‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anvo25/vlms-are-biased.","first_N":5,"first_N_keywords":["visual-question-answering","image-text-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"process_dataset_mini","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doublesizebed/process_dataset_mini","creator_name":"chong","creator_url":"https://huggingface.co/doublesizebed","description":"doublesizebed/process_dataset_mini dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Malay","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"aidealab-videojp-eval","keyword":"text-to-video","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aidealab/aidealab-videojp-eval","creator_name":"AIdeaLab","creator_url":"https://huggingface.co/aidealab","description":"\n\t\n\t\t\n\t\tAIdeaLab VideoJP Ë©ï‰æ°ÂÜçÁèæÁî®„Éá„Éº„Çø\n\t\n\n\n\t\n\t\t\n\t\t„ÅØ„Åò„ÇÅ„Å´\n\t\n\n„Åì„ÅÆ„É™„Éù„Ç∏„Éà„É™„ÅØAIdeaLab VideoJP„ÅÆFVD„ÇíÊ∏¨ÂÆö„Åô„Çã„Åü„ÇÅ„ÅÆ„Éá„Éº„Çø„Çí\nÈõÜ„ÇÅ„Åæ„Åó„Åü„ÄÇÂÜçÁèæÊâãÈ†Ü„ÇíÊ¨°„ÅÆ„Å®„Åä„Çä„Å´Á§∫„Åó„Åæ„Åô„ÄÇ\n\n\t\n\t\t\n\t\tË©ï‰æ°ÊñπÊ≥ï\n\t\n\n„Åæ„Åö„ÄÅË©ï‰æ°Áî®„É©„Ç§„Éñ„É©„É™„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ\ngit clone https://github.com/JunyaoHu/common_metrics_on_video_quality\n\n„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åß„Åç„Åü„Çâ„ÄÅ„É©„Ç§„Éñ„É©„É™„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´ÊâãÈ†Ü„ÇíË∏è„Çì„Åß„ÄÅ„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Åæ„Åô„ÄÇ\n„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Åü„Çâ„ÄÅÂêå„Åò„Éá„Ç£„É¨„ÇØ„Éà„É™„Å´Ê¨°„ÅÆ„Éï„Ç°„Ç§„É´„Çí„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n\nevaluate_videos.py\nvideos.tar\ngen_ja.tar\n\n„Ç≥„Éî„Éº„Åß„Åç„Åü„Çâ„ÄÅvideos.tar„Å®gen_ja.tar„ÇíÂ±ïÈñã„Åó„Åæ„Åô„ÄÇ\ntar xf videos.tar\ntar xf gen_ja.tar\n\nÊúÄÂæå„Å´evaluate_videos.py„ÇíÂÆüË°å„Åô„Çã„Å®„ÄÅFVD„ÅåË°®Á§∫„Åï„Çå„Çã„ÅØ„Åö„Åß„Åô„ÄÇ\n\n\t\n\t\t\n\t\n\t\n\t\t„Åä„Åæ„ÅëÔºö Ë©ï‰æ°Áî®Êò†ÂÉè„ÅÆ‰Ωú„ÇäÊñπ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aidealab/aidealab-videojp-eval.","first_N":5,"first_N_keywords":["text-to-video","Japanese","apache-2.0","1K - 10K","webdataset"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-multispeaker","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-multispeaker","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 21138 parallel speech-text pairs for Twi (Akan), a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Twi (Akan) - tw\nTask: Speech Recognition, Text-to-Speech\nSize: 21138 audio files > 1KB‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-multispeaker.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Twi"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-tarsier2_recaption","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-tarsier2_recaption","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents (Tarsier2 Re-captioned)\n\t\n\nThis dataset contains only prompt_embeds for the videos from the OpenVid HD Wan2.1 Latents Dataset. These embeddings were generated using the Tarsier2-Recap-7b model from Tarsier, which significantly improves caption quality for video generation tasks.\nEach .pth file contains:\n\n'prompt_embeds': the improved prompt embedding vector\n\nThe embeddings correspond to the same videos as in the original dataset, and can be used as a direct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-tarsier2_recaption.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-tarsier2_recaption","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-tarsier2_recaption","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents (Tarsier2 Re-captioned)\n\t\n\nThis dataset contains only prompt_embeds for the videos from the OpenVid HD Wan2.1 Latents Dataset. These embeddings were generated using the Tarsier2-Recap-7b model from Tarsier, which significantly improves caption quality for video generation tasks.\nEach .pth file contains:\n\n'prompt_embeds': the improved prompt embedding vector\n\nThe embeddings correspond to the same videos as in the original dataset, and can be used as a direct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-tarsier2_recaption.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","1M<n<10M","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"iReason","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rippleripple/iReason","creator_name":"Jackie Y","creator_url":"https://huggingface.co/rippleripple","description":"Paper: Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models\nPaper: https://arxiv.org/abs/2506.00258\nWebsite: https://jackie-2000.github.io/iReason.github.io/\nGitHub: https://github.com/eric-ai-lab/iReason\niReason: Designed to probe MLLMs‚Äô implicit reasoning by evaluating their ability to detect subtle flaws in seemingly valid instructions. Covers 643 real-world scenarios spanning four failure types‚Äîobject absence, ambiguity, contradiction, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rippleripple/iReason.","first_N":5,"first_N_keywords":["image-text-to-text","mit","< 1K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Ranjan-Hindi33min","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BBSRguy/Ranjan-Hindi33min","creator_name":"Rashmi Ranjan Dash","creator_url":"https://huggingface.co/BBSRguy","description":"\n\t\n\t\t\n\t\tRanjan-Hindi33min\n\t\n\nOwner: @BBSRguyCreated: 2025-06-03Year: 2025Language: Hindi üáÆüá≥Region Focus: Odisha, IndiaSample Rate Variants: 16 kHz, 24 kHz, 32 kHzTotal Files: 29 pairs (speech + text)Duration: Approximately 33 minutes of speech  \n\n\n\t\n\t\t\n\t\n\t\n\t\tüìú Description\n\t\n\nRanjan-Hindi33min is a meticulously curated dataset comprising high-quality Hindi speech samples and their corresponding textual transcriptions. This dataset is designed to support various speech processing tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BBSRguy/Ranjan-Hindi33min.","first_N":5,"first_N_keywords":["text-to-speech","Hindi","mit","< 1K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"ibero-characters-es","keyword":"video-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es","creator_name":"Hackathon SomosNLP 2025","creator_url":"https://huggingface.co/somosnlp-hackathon-2025","description":"\n\t\n\t\t\n\t\tConjunto de datos de personajes de mitos y leyendas iberoamericanos.\n\t\n\n\n‚ö†Ô∏è Este dataset se encuentra en desarrollo activo. Se planea expandir significativamente el n√∫mero de registros y mejorar la cobertura de im√°genes.\n\n\n\t\n\t\t\n\t\tüìö Descripci√≥n\n\t\n\nDataset de personajes m√≠ticos y legendarios de Iberoam√©rica, dise√±ado para preservar y promover el patrimonio cultural a trav√©s de la inteligencia artificial.\n\n\t\n\t\t\n\t\tüåü Motivaci√≥n e Impacto\n\t\n\n\nüì± Preservaci√≥n Digital: Conservaci√≥n del‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp-hackathon-2025/ibero-characters-es.","first_N":5,"first_N_keywords":["video-text-to-text","text-generation","Spanish","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"audiobooks","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yasalma/audiobooks","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","description":"170 hours of aligned audiobooks taken from tatkniga.ru. There are 4 speakers with 17+ hours of audio and 20 speakers in total. All the books are in free access and most of them in public domain.\n","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","Tatar","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"R1-Reward-RL","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL","creator_name":"Yi-Fan Zhang","creator_url":"https://huggingface.co/yifanzhang114","description":"\n  \n\n\n[üìñ arXiv Paper] \n[üìä R1-Reward Code] \n[üìù R1-Reward Model] \n\n\n\n\t\n\t\t\n\t\n\t\n\t\tTraining Multimodal Reward Model Through Stable Reinforcement Learning\n\t\n\nüî• We are proud to open-source R1-Reward, a comprehensive project for improve reward modeling through reinforcement learning. This release includes:\n\nR1-Reward Model: A state-of-the-art (SOTA) multimodal reward model demonstrating substantial gains (Voting@15):\n13.5% improvement on VL Reward-Bench.3.5% improvement on MM-RLHF Reward-Bench.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"miami-hologram-dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robb-0/miami-hologram-dataset","creator_name":"robbie","creator_url":"https://huggingface.co/robb-0","description":"\nDataset of images used for my Miami Beach Hologram\nIt is a .zip file which includes 14 images and 14 .tex files with caption (captioned automatically on Civitai)\nMiami Hologram Dataset ¬© 2025 by Robb-0 is licensed under CC BY 4.0 \n","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","1M<n<10M"],"keywords_longer_than_N":true},
	{"name":"ljs-mos-120","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stefantaubert/ljs-mos-120","creator_name":"Stefan Taubert","creator_url":"https://huggingface.co/stefantaubert","description":"\n\t\n\t\t\n\t\tLJS-MOS-120: Human MOS Ratings for 120 Samples of the LJ Speech Dataset\n\t\n\nLJS-MOS-120 provides Mean Opinion Score (MOS) ratings for 120 text-to-speech (TTS) samples based on the LJ Speech dataset. Each sample was rated by human annotators for intelligibility and naturalness across four experimental TTS conditions. The dataset follows the Tidy data format, with one row per rating per dimension.\nRatings were collected via Amazon Mechanical Turk (MTurk) in 2022. Each entry includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stefantaubert/ljs-mos-120.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K - 100K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"ljs-mos-120","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stefantaubert/ljs-mos-120","creator_name":"Stefan Taubert","creator_url":"https://huggingface.co/stefantaubert","description":"\n\t\n\t\t\n\t\tLJS-MOS-120: Human MOS Ratings for 120 Samples of the LJ Speech Dataset\n\t\n\nLJS-MOS-120 provides Mean Opinion Score (MOS) ratings for 120 text-to-speech (TTS) samples based on the LJ Speech dataset. Each sample was rated by human annotators for intelligibility and naturalness across four experimental TTS conditions. The dataset follows the Tidy data format, with one row per rating per dimension.\nRatings were collected via Amazon Mechanical Turk (MTurk) in 2022. Each entry includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stefantaubert/ljs-mos-120.","first_N":5,"first_N_keywords":["text-to-speech","English","mit","10K - 100K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"billys-vintage-cars-dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robb-0/billys-vintage-cars-dataset","creator_name":"robbie","creator_url":"https://huggingface.co/robb-0","description":"\n\t\n\t\t\n\t\tBilly's Vintage Cards\n\t\n\n\nThis is a .zip image dataset with .txt files autotagged on Civitai.\nI used this dataset to create my image LoRA model \"Billy's Vintage Cars\"\nAll images were AI-Generated, which means I am NOT affiliated with any Company/Manufacturer/Brand, and the dataset should be used for research and historical reasons.\nEnjoy!\nBilly's Vintage Cars Dataset ¬© 2025 by Robb-0 is licensed under CC BY 4.0\n","first_N":5,"first_N_keywords":["text-to-image","image-classification","English","cc-by-4.0","10M<n<100M"],"keywords_longer_than_N":true},
	{"name":"malay-tts-tags","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/doublesizebed/malay-tts-tags","creator_name":"chong","creator_url":"https://huggingface.co/doublesizebed","description":"doublesizebed/malay-tts-tags dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","Malay","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"GenRef","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jianqunZ/GenRef","creator_name":"Jianqun Zhou","creator_url":"https://huggingface.co/jianqunZ","description":"jianqunZ/GenRef dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"sql_bi__b_db","keyword":"text-to-sql","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/timbossm/sql_bi__b_db","creator_name":"Timur","creator_url":"https://huggingface.co/timbossm","description":"\n\t\n\t\t\n\t\t–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö: timbossm/sql_bi__b_db\n\t\n\n–≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç SQL-–∑–∞–ø—Ä–æ—Å—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Å–æ–±–∏—è \"–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º –ø–æ —è–∑—ã–∫—É SQL: –ø—Ä–∞–∫—Ç–∏–∫—É–º\" (—Å–æ—Å—Ç. –¢. –ú. –ë–æ—Å–µ–Ω–∫–æ, –Æ.–í. –§—Ä–æ–ª–æ–≤. ‚Äì –ú.: –ú–ì–ü–£, 2025. ‚Äì 101 —Å.).\n–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç 25 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–∞–∫—Ç–∏–∫–∏ –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ–º—ã –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö —Ä–∞–±–æ—Ç:\n\n–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ1. –ò–∑—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥ DDL.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timbossm/sql_bi__b_db.","first_N":5,"first_N_keywords":["question-answering","table-question-answering","text-generation","Russian","English"],"keywords_longer_than_N":true},
	{"name":"audio_data_russian_backup","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kijjjj/audio_data_russian_backup","creator_name":"fgfd","creator_url":"https://huggingface.co/kijjjj","description":"\n\t\n\t\t\n\t\tDataset Audio Russian Backup\n\t\n\nThis is a backup dataset with Russian audio data, split into train_0 to train_49 for tasks like text-to-speech, speech recognition, and speaker identification.\n\n\t\n\t\t\n\t\tFeatures\n\t\n\n\ntext: Audio transcription (string).\nspeaker_name: Speaker identifier (string).\n\n\naudio: Audio file.\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nLoad the dataset like this:\nfrom datasets import load_dataset\ndataset = load_dataset(\"kijjjj/audio_data_russian_backup\", split=\"train_0\")  # Or any train_X‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kijjjj/audio_data_russian_backup.","first_N":5,"first_N_keywords":["text-to-speech","Russian","mit","100K<n<1M","Audio"],"keywords_longer_than_N":true},
	{"name":"CompoundPrompts","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/etaisella/CompoundPrompts","creator_name":"Etai  Sella","creator_url":"https://huggingface.co/etaisella","description":"\n\t\n\t\t\n\t\tCompoundPrompts\n\t\n\nThis benchmark is introduced in the InstanceGen (SIGGRAPH 2025) paper, and is aimed at estimating the performance of image-to-text models in generating complex prompts involving multiple objects, instance level attributes and spatial relationships.\n\nThe process used to evalute performance on the benchmark is explained in the paper, but we also plan to release the code that does this in the near future, stay tuned!\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"hailuoai","keyword":"text-to-video","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/hailuoai","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for HailuoAI Video Metadata\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 544,646 entries of video metadata collected from HailuoAI, a platform that offers AI-powered image-to-video generation services. Each entry includes detailed information about AI-generated videos, such as video URLs, dimensions, creation parameters, model IDs, and associated tags. This collection represents a diverse range of AI-generated videos that can be used for multimodal analysis, video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/hailuoai.","first_N":5,"first_N_keywords":["image-to-text","text-to-image","text-to-video","image-to-video","found"],"keywords_longer_than_N":true},
	{"name":"video-dataset","keyword":"text-to-video","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProgramerSalar/video-dataset","creator_name":"ProgramerSalar","creator_url":"https://huggingface.co/ProgramerSalar","description":"\n\t\n\t\t\n\t\tVideo Dataset on Hugging Face\n\t\n\nThis repository hosts the  video dataset, a widely used benchmark dataset for human action recognition in videos. The dataset has been processed and uploaded to the Hugging Face Hub for easy access, sharing, and integration into machine learning workflows.\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe  dataset is a large-scale video dataset designed for action recognition tasks. It contains 13,320 video clips across 101 action categories, making it one of the most‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProgramerSalar/video-dataset.","first_N":5,"first_N_keywords":["text-to-video","video-classification","video-text-to-text","voice-activity-detection","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"video-dataset","keyword":"video-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ProgramerSalar/video-dataset","creator_name":"ProgramerSalar","creator_url":"https://huggingface.co/ProgramerSalar","description":"\n\t\n\t\t\n\t\tVideo Dataset on Hugging Face\n\t\n\nThis repository hosts the  video dataset, a widely used benchmark dataset for human action recognition in videos. The dataset has been processed and uploaded to the Hugging Face Hub for easy access, sharing, and integration into machine learning workflows.\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nThe  dataset is a large-scale video dataset designed for action recognition tasks. It contains 13,320 video clips across 101 action categories, making it one of the most‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ProgramerSalar/video-dataset.","first_N":5,"first_N_keywords":["text-to-video","video-classification","video-text-to-text","voice-activity-detection","visual-question-answering"],"keywords_longer_than_N":true},
	{"name":"billys-cardboard-cars-dataset","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robb-0/billys-cardboard-cars-dataset","creator_name":"robbie","creator_url":"https://huggingface.co/robb-0","description":"\n\n\t\n\t\t\n\t\tBilly's Cardboard Cars Dataset\n\t\n\nThis is the dataset made to train Billy's Cardboard Cars image LoRA model. All images were AI-Generated.\nDataset Information (Shared with the model):\n\nSize: 30 images in a .zip file with .txt captions.\n\nCaptioning: Automatically captioned by Civitai.\n\n\nBilly's Cardboard Cars Dataset ¬© 2025 by Robb-0 is licensed under CC BY 4.0\n","first_N":5,"first_N_keywords":["image-classification","text-to-image","English","cc-by-4.0","10M<n<100M"],"keywords_longer_than_N":true},
	{"name":"audiobooks","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gaydmi/audiobooks","creator_name":"Dmitry Gaynullin","creator_url":"https://huggingface.co/gaydmi","description":"\n\t\n\t\t\n\t\tCrimean Tatar Audiobooks\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nCrimean Tatar Audiobooks is a speech dataset sourced from different sources (public radio stations/youtube channels) containing audiobooks in Crimean Tatar. The dataset comprises recordings of different native fiction books, all read by a single female speaker (for now). The dataset is intended for text-to-speech (TTS) research and development in the Crimean Tatar language. \n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nParts:The dataset contains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gaydmi/audiobooks.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","Crimean Tatar","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"russian_librispeech","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/istupakov/russian_librispeech","creator_name":"Ilya Stupakov","creator_url":"https://huggingface.co/istupakov","description":"\n\t\n\t\t\n\t\tRussian LibriSpeech (RuLS)\n\t\n\nIdentifier: SLR96 from openslr.org\nSummary: This dataset is based on LibriVox audiobooks\nCategory: Speech\nLicense: The dataset is Public Domain in the USA.\nAbout this resource:\nRussian LibriSpeech (RuLS) dataset is based on LibriVox's public domain audio books (see BOOKS.TXT for the list of included books) and contains about 98 hours of audio data.\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Russian","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"bird-interact-lite","keyword":"text-to-sql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/birdsql/bird-interact-lite","creator_name":"The BIRD Team","creator_url":"https://huggingface.co/birdsql","description":"\n\t\n\t\t\n\t\tüß∏ Overview\n\t\n\nBIRD-INTERACT, an interactive text-to-SQL benchmark, re-imagines Text-to-SQL evaluation via lens of dynamic interactions.\nThe environment blends a hierarchical knowledge base, database documentation and a function-driven user simulator to recreate authentic enterprise environments across full CRUD operations.\nIt offers two rigorous test modes: (1) passive Conversational Interaction and (2) active Agentic Interaction, spanning 600 annotated tasks including Business‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/birdsql/bird-interact-lite.","first_N":5,"first_N_keywords":["cc-by-4.0","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-wan-latents-81frames-tarsier2_recaption","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames-tarsier2_recaption","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents with Tarsier2 Prompts\n\t\n\nThis dataset contains VAE-encoded latent representations and high-quality prompt embeddings extracted from the OpenVid HD video dataset. The latents were encoded using the Wan2.1 VAE model, and the prompts were re-captioned using the Tarsier2-Recap-7b model for improved semantic quality.\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nSource Dataset: Enderfga/openvid-hd (~433k videos)\nThis Dataset: ~270k VAE latents + Tarsier2 captions\nVAE Model: Wan2.1 VAE‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames-tarsier2_recaption.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"openvid-hd-wan-latents-81frames-tarsier2_recaption","keyword":"text-to-video","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames-tarsier2_recaption","creator_name":"Guian Fang","creator_url":"https://huggingface.co/Enderfga","description":"\n\t\n\t\t\n\t\tOpenVid HD Latents with Tarsier2 Prompts\n\t\n\nThis dataset contains VAE-encoded latent representations and high-quality prompt embeddings extracted from the OpenVid HD video dataset. The latents were encoded using the Wan2.1 VAE model, and the prompts were re-captioned using the Tarsier2-Recap-7b model for improved semantic quality.\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nSource Dataset: Enderfga/openvid-hd (~433k videos)\nThis Dataset: ~270k VAE latents + Tarsier2 captions\nVAE Model: Wan2.1 VAE‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Enderfga/openvid-hd-wan-latents-81frames-tarsier2_recaption.","first_N":5,"first_N_keywords":["text-to-video","English","cc-by-4.0","100K - 1M","webdataset"],"keywords_longer_than_N":true},
	{"name":"texturecan","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/texturecan","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for TextureCan Textures\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 4,037 texture images from texturecan.com. It includes textures of various materials such as brick, paper, fabric, metal, wood, stone, and other surfaces. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression (90%) to reduce file size while maintaining good quality.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe dataset is monolingual:\n\nEnglish‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/texturecan.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"ambientcg","keyword":"text-to-image","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nyuuzyou/ambientcg","creator_name":"nyuuzyou","creator_url":"https://huggingface.co/nyuuzyou","description":"\n\t\n\t\t\n\t\tDataset Card for AmbientCG Textures and HDRIs\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 14,202 high-quality texture images and HDRI environments from ambientcg.com. It includes a comprehensive collection of materials such as fabric, metal, wood, stone, concrete, nature elements, and HDRI skyboxes for 3D rendering and computer graphics applications. The original archives were downloaded, unpacked, and images were compressed using PNG optimization and JPEG quality compression‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nyuuzyou/ambientcg.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-to-image","found"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-synthetic-1m-part002","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part002","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset - Part 2 of 5\n\t\n\n\n\t\n\t\t\n\t\tüéâ The Largest Speech Dataset for Twi Language\n\t\n\nThis dataset contains part 2 of the largest speech dataset for the Twi language, featuring 1 million speech-to-text pairs split across 5 parts (approximately 200,000 samples each). This represents a groundbreaking resource for Twi (Akan), a language spoken primarily in Ghana.\n\n\t\n\t\t\n\t\tüöÄ Breaking the Low-Resource Language Barrier\n\t\n\nThis publication demonstrates that African‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part002.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Akan"],"keywords_longer_than_N":true},
	{"name":"twi-speech-text-parallel-synthetic-1m-part005","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part005","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tTwi Speech-Text Parallel Dataset - Part 5 of 5\n\t\n\n\n\t\n\t\t\n\t\tüéâ The Largest Speech Dataset for Twi Language\n\t\n\nThis dataset contains part 5 of the largest speech dataset for the Twi language, featuring 1 million speech-to-text pairs split across 5 parts (approximately 200,000 samples each). This represents a groundbreaking resource for Twi (Akan), a language spoken primarily in Ghana.\n\n\t\n\t\t\n\t\tüöÄ Breaking the Low-Resource Language Barrier\n\t\n\nThis publication demonstrates that African‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/twi-speech-text-parallel-synthetic-1m-part005.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Akan"],"keywords_longer_than_N":true},
	{"name":"nb-librivox","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/nb-librivox","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\n\t\n\t\t\n\t\tüìÑ NB-LibriVox\n\t\n\nA high-quality Norwegian text-to-speech (TTS) dataset derived from LibriVox public domain audiobooks. It includes audio clips with pseudo-aligned transcripts and punctuation, curated by the National Library of Norway for speech synthesis and ASR research.\n\n\n\t\n\t\t\n\t\tüìÇ Dataset Overview\n\t\n\n\n\t\n\t\t\nField\nDescription\n\n\n\t\t\nfile_name\nAudio file name in .wav format\n\n\nid\nUnique identifier for each utterance/sample\n\n\ntext\nTranscript automatically generated using NB-Whisper Large‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NbAiLab/nb-librivox.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","Norwegian","Norwegian Bokm√•l","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"InstructTTSEval","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval","creator_name":"Kexin Huang","creator_url":"https://huggingface.co/CaasiHUANG","description":"\n\t\n\t\t\n\t\tInstructTTSEval\n\t\n\nInstructTTSEval is a comprehensive benchmark designed to evaluate Text-to-Speech (TTS) systems' ability to follow complex natural-language style instructions. The dataset provides a hierarchical evaluation framework with three progressively challenging tasks that test both low-level acoustic control and high-level style generalization capabilities.\n\nGithub Repository: https://github.com/KexinHUANG19/InstructTTSEval\nPaper: InstructTTSEval: Benchmarking Complex‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval.","first_N":5,"first_N_keywords":["text-to-speech","English","Chinese","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PhysUniBench","keyword":"image-text-to-text","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PrismaX/PhysUniBench","creator_name":"PrismaX","creator_url":"https://huggingface.co/PrismaX","description":"\n\t\n\t\t\n\t\tPhysUniBench\n\t\n\nAn Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models\nPaper: https://arxiv.org/abs/2506.17667\nRepository: https://github.com/PrismaX-Team/PhysUniBenchmark\nProject page: https://prismax-team.github.io/PhysUniBenchmark/\nPhysUniBench is the first large-scale multimodal physics benchmark specifically designed for undergraduate-level understanding, reasoning, and problem-solving. It provides a valuable testbed for advancing multimodal large language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PrismaX/PhysUniBench.","first_N":5,"first_N_keywords":["image-text-to-text","English","Chinese","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"swahili-speech","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/stem-content-ai-project/swahili-speech","creator_name":"Harnessing AI Project","creator_url":"https://huggingface.co/stem-content-ai-project","description":"\n\t\n\t\t\n\t\tSwahili Speech-to-Text Dataset\n\t\n\nThis dataset contains paired audio and text data for training and evaluating speech-to-text models in Swahili. The audio files have been processed to remove silence, converted to 44.1kHz mono FLAC format, and are paired with corresponding transcriptions.\n\n\t\n\t\t\n\t\tStructure\n\t\n\n\naudio_*.flac: Audio files in FLAC format, named by their corresponding text corpus ID.\nmetadata.jsonl: JSON Lines file with metadata for each audio-text pair. Each line is a JSON‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stem-content-ai-project/swahili-speech.","first_N":5,"first_N_keywords":["text-to-speech","Swahili","mit","1K - 10K","soundfolder"],"keywords_longer_than_N":true},
	{"name":"ASMR-Archive-Processed","keyword":"text-to-speech","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OmniAICreator/ASMR-Archive-Processed","creator_name":"OmniAICreator","creator_url":"https://huggingface.co/OmniAICreator","description":"\n\t\n\t\t\n\t\tASMR-Archive-Processed (WIP)\n\t\n\n\nWork in Progress ‚Äî expect breaking changes while the pipeline and data layout stabilize.\n\nThis dataset contains ASMR audio data sourced from DeliberatorArchiver/asmr-archive-data-01 and DeliberatorArchiver/asmr-archive-data-02, which has undergone the following preprocessing steps:\n\n\n\t\n\t\t\n\t\n\t\n\t\tPreprocessing Steps\n\t\n\n\nLow-Quality Data Filtering:\nAudio files are filtered to remove low-quality samples. This process checks for:\n\nUndesirable codecs (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OmniAICreator/ASMR-Archive-Processed.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","Japanese","agpl-3.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"swahili-words-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/swahili-words-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tSwahili Words Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 411048 parallel speech-text pairs for Swahili, a widely spoken language in East Africa. The dataset consists of audio recordings paired with corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Swahili - sw\nTask: Speech Recognition, Text-to-Speech\nSize: 411048 audio files > 1KB‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/swahili-words-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Swahili"],"keywords_longer_than_N":true},
	{"name":"Dogs-images-text-pair","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair","creator_name":"Mikdad Mrhij","creator_url":"https://huggingface.co/MikdadMrhij","description":"This dataset is curated from subsets of public datasets such as MS COCO, LAION-ART, and SBU Captions.\nIt specifically filters for samples featuring dog-related content.\nThe goal of this dataset is to support text-to-image generation models focused on dog objects and related scenes. \nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/SBUcaptions.md\nhttps://github.com/rom1504/img2dataset/blob/main/dataset_examples/mscoco.md‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MikdadMrhij/Dogs-images-text-pair.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"t2i-diversity-gender-neutral-captions","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIML-TUDA/t2i-diversity-gender-neutral-captions","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","description":"This dataset contains different synthetic captions for our image samples. \nWe have selected the best-performing caption set from our experiments, the random-length captions. Then, we have used Gemma-2-9b-it and instructed it to remove different genders from the captions. We obtained three sets from the original set, namly (i) all genders neutralized, (ii) only female gender neutralized, and (iii) only male gender neutralized. To this end, we have removed all gender indicative words such as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/t2i-diversity-gender-neutral-captions.","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"MindCube","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MLL-Lab/MindCube","creator_name":"MLL Lab","creator_url":"https://huggingface.co/MLL-Lab","description":"\n\t\n\t\t\n\t\tMindCube: Spatial Mental Modeling from Limited Views\n\t\n\nMindCube is a novel benchmark designed to evaluate how well Vision Language Models (VLMs) can form robust spatial mental models from limited visual views. It comprises 21,154 questions across 3,268 images, assessing capabilities such as cognitive mapping (representing positions), perspective-taking (orientations), and mental simulation (dynamics for \"what-if\" movements). The dataset aims to expose critical gaps in existing VLMs'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLL-Lab/MindCube.","first_N":5,"first_N_keywords":["question-answering","image-text-to-text","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"vctk","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jspaulsen/vctk","creator_name":"Jacob Paulsen","creator_url":"https://huggingface.co/jspaulsen","description":"\n\t\n\t\t\n\t\tVCTK\n\t\n\nThis is a processed clone of the VCTK dataset with leading and trailing silence removed using Silero VAD. A fixed 25‚ÄØms of padding has been added to both ends of each audio clip to (hopefully) imrprove training and finetuning.\nThe original dataset is available at: https://datashare.ed.ac.uk/handle/10283/3443.\n\n\t\n\t\t\n\t\tReproducing\n\t\n\nThis repository notably lacks a requirements.txt file. There's likely a missing dependency or two, but roughly:\npydub\ntqdm\ntorch\ntorchaudio‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jspaulsen/vctk.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","text-to-audio","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vctk","keyword":"text-to-audio","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jspaulsen/vctk","creator_name":"Jacob Paulsen","creator_url":"https://huggingface.co/jspaulsen","description":"\n\t\n\t\t\n\t\tVCTK\n\t\n\nThis is a processed clone of the VCTK dataset with leading and trailing silence removed using Silero VAD. A fixed 25‚ÄØms of padding has been added to both ends of each audio clip to (hopefully) imrprove training and finetuning.\nThe original dataset is available at: https://datashare.ed.ac.uk/handle/10283/3443.\n\n\t\n\t\t\n\t\tReproducing\n\t\n\nThis repository notably lacks a requirements.txt file. There's likely a missing dependency or two, but roughly:\npydub\ntqdm\ntorch\ntorchaudio‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jspaulsen/vctk.","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","text-to-audio","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"vagla-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/vagla-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tVagla Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 48605 parallel speech-text pairs for Vagla, a language spoken primarily in Ghana. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Vagla - vag\nTask: Speech Recognition, Text-to-Speech\nSize: 48605 audio files > 1KB‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/vagla-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Vagla"],"keywords_longer_than_N":true},
	{"name":"all-words-in-english-with-pink-trombone","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mcamara/all-words-in-english-with-pink-trombone","creator_name":"Mateo C√°mara","creator_url":"https://huggingface.co/mcamara","description":"\n\t\n\t\t\n\t\tDataset Card for Pink Trombone English Phonetic & Landmark Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains audio samples of English words generated by the Pink Trombone, a popular open-source vocal tract synthesizer. The primary goal of this dataset is to provide a clean, large-scale resource linking phonetic sequences to both their acoustic realization and the underlying articulatory landmarks.\nEach sample in the dataset corresponds to a word from the Oxford English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mcamara/all-words-in-english-with-pink-trombone.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Audio"],"keywords_longer_than_N":true},
	{"name":"yoruba-speech-text-parallel","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michsethowusu/yoruba-speech-text-parallel","creator_name":"Mich-Seth Owusu","creator_url":"https://huggingface.co/michsethowusu","description":"\n\t\n\t\t\n\t\tYoruba Speech-Text Parallel Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 1647022 parallel speech-text pairs for Yoruba, a language spoken primarily in Nigeria and other West African countries. The dataset consists of audio recordings paired with their corresponding text transcriptions, making it suitable for automatic speech recognition (ASR) and text-to-speech (TTS) tasks.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nLanguage: Yoruba - yo\nTask: Speech Recognition, Text-to-Speech‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michsethowusu/yoruba-speech-text-parallel.","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","keyword-spotting","monolingual","Yoruba"],"keywords_longer_than_N":true},
	{"name":"NgocHuyenViVoice","keyword":"text-to-speech","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thangnzt/NgocHuyenViVoice","creator_name":"Thang Nguyen Duc","creator_url":"https://huggingface.co/thangnzt","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thangnzt/NgocHuyenViVoice.","first_N":5,"first_N_keywords":["text-to-speech","Vietnamese","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"dubs","keyword":"text-to-speech","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yasalma/dubs","creator_name":"Yasalma","creator_url":"https://huggingface.co/yasalma","description":"yasalma/dubs dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-speech","automatic-speech-recognition","audio-to-audio","Tatar","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Perception-R1-Dataset","keyword":"image-text-to-text","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tongxiao2002/Perception-R1-Dataset","creator_name":"tongxiao","creator_url":"https://huggingface.co/tongxiao2002","description":"Paper: arxiv.org/abs/2506.07218\nPlease refer to GitHub repo for detailed usage: https://github.com/tongxiao2002/Perception-R1\n","first_N":5,"first_N_keywords":["image-text-to-text","visual-question-answering","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"TreeBench","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeBench","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTraceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology\n\t\n\nThis repository contains the TreeBench dataset, a diagnostic benchmark for visual grounded reasoning, introduced in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTL; DR: We propose TreeBench, the first benchmark specially designed for evaluating \"thinking with images\" capabilities with traceable visual evidence, and TreeVGR, the current state-of-the-art‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeBench.","first_N":5,"first_N_keywords":["image-text-to-text","apache-2.0","< 1K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"TreeVGR-SFT-35K","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTreeBench: Traceable Evidence Enhanced Visual Grounded Reasoning Benchmark\n\t\n\nThis repository contains TreeBench, a diagnostic benchmark dataset proposed in the paper Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is designed to holistically evaluate \"thinking with images\" capabilities by dynamically referencing visual regions. It is built on three core principles:\n\nFocused visual perception of subtle targets in complex scenes.\nTraceable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-SFT-35K.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"koel-benchmark","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NisargBhavsar25/koel-benchmark","creator_name":"Nisarg Bhavsar","creator_url":"https://huggingface.co/NisargBhavsar25","description":"\n\t\n\t\t\n\t\tKoel Benchmark Suite\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Koel Benchmark Suite is a comprehensive set of evaluation datasets designed to rigorously test the real-world performance of Text-to-Speech (TTS) models for major Indian languages. The suite focuses on challenges unique to the Indian context, such as code-switching, domain-specific terminology, proper nouns, and complex numeric formats.\nThis dataset was created to help developers and researchers build more natural, accurate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NisargBhavsar25/koel-benchmark.","first_N":5,"first_N_keywords":["English","Hindi","Tamil","Telugu","Kannada"],"keywords_longer_than_N":true},
	{"name":"slr-co-male-training-piper","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/igortamara/slr-co-male-training-piper","creator_name":"Igor T√°mara","creator_url":"https://huggingface.co/igortamara","description":"Training checkpoints for colombian male voice from lessac as a base\n","first_N":5,"first_N_keywords":["text-to-speech","Spanish","cc-by-sa-4.0","Audio","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"TreeVGR-RL-37K","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K","creator_name":"HaochenWang","creator_url":"https://huggingface.co/HaochenWang","description":"\n\t\n\t\t\n\t\tTreeBench Dataset Card\n\t\n\nThis repository contains TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark designed for evaluating \"thinking with images\" capabilities with traceable visual evidence.\nThe dataset was introduced in the paper: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology.\nTreeBench is built on three core principles:\n\nFocused visual perception: of subtle targets in complex scenes.\nTraceable evidence: via bounding box‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HaochenWang/TreeVGR-RL-37K.","first_N":5,"first_N_keywords":["image-text-to-text","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Test","keyword":"text-to-speech","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/imenLa/Test","creator_name":"imen laouirine","creator_url":"https://huggingface.co/imenLa","description":"Test Dataset\n","first_N":5,"first_N_keywords":["text-to-speech","Arabic","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"T2I_4History","keyword":"text-to-image","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jianqunZ/T2I_4History","creator_name":"Jianqun Zhou","creator_url":"https://huggingface.co/jianqunZ","description":"jianqunZ/T2I_4History dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-to-image","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"sql_translator","keyword":"text-to-sql","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ty-kim/sql_translator","creator_name":"taeyeopkim","creator_url":"https://huggingface.co/ty-kim","description":"\n\t\n\t\t\n\t\tDataset Card for Text-to-SQL Conversations\n\t\n\n\n\n\n\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): ['en']\nLicense: mit\n\n\n\t\n\t\t\n\t\tDataset Sources [optional]\n\t\n\n\n\n\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\n\n\n\t\n\t\t\n\t\tUses\n\t\n\n\n\n\n\t\n\t\t\n\t\tDirect Use‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ty-kim/sql_translator.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"bibletts-asante-twi-max29secs-total9hrs-sr22050","keyword":"text-to-speech","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hci-lab-dcug/bibletts-asante-twi-max29secs-total9hrs-sr22050","creator_name":"DCS HCI LAB","creator_url":"https://huggingface.co/hci-lab-dcug","description":"\n\t\n\t\t\n\t\tBibleTTS Asante Twi Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Information\n\t\n\nThis dataset is derived from the BibleTTS corpus, specifically focusing on Asante Twi speech data. The original BibleTTS is a large, high-fidelity, multilingual, and uniquely African speech corpus.\n\nTotal Duration: {total_hours:.2f} hours ({total_hours*60:.1f} minutes)\nNumber of Files: {file_count:,}\nSample Rate: {sample_rate:,} Hz\nMax File Duration: {max_duration:.1f} seconds\nFormat: WAV files with corresponding‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hci-lab-dcug/bibletts-asante-twi-max29secs-total9hrs-sr22050.","first_N":5,"first_N_keywords":["text-to-speech","Akan","Twi","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"cv-22-de","keyword":"text-to-speech","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fidoriel/cv-22-de","creator_name":"fidoriel","creator_url":"https://huggingface.co/fidoriel","description":"German split of Common Voice 22. cc0 license\n","first_N":5,"first_N_keywords":["automatic-speech-recognition","text-to-speech","German","cc0-1.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"jewelry-design-dataset","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sidd707/jewelry-design-dataset","creator_name":"Siddharth patel","creator_url":"https://huggingface.co/sidd707","description":"\n\t\n\t\t\n\t\tüíé Jewelry Text-to-Image Fine-Tuning Dataset\n\t\n\nThis dataset is created for fine-tuning Stable Diffusion XL (SDXL) models to generate realistic and diverse images of jewelry ‚Äî including rings, bracelets, necklaces, and earrings.\n\n\n\t\n\t\t\n\t\tüì¶ Dataset Summary\n\t\n\nThe dataset includes over 6,100 high-resolution images of jewelry, organized into four categories. It was collected from various sources and manually curated. Each image is associated with a simple or structured caption describing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sidd707/jewelry-design-dataset.","first_N":5,"first_N_keywords":["text-to-image","English","mit","1K - 10K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"NeoBabel-Eval","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mderakhshani/NeoBabel-Eval","creator_name":"Mohammad Mahdi Derakhshani","creator_url":"https://huggingface.co/mderakhshani","description":"\n\t\n\t\t\n\t\tOfficial Multilingual Evaluation Suite of NeoBabel\n\t\n\nThis repository contains the full evaluation datasets used for benchmarking multilingual text-to-image generation in NeoBabel. We release two benchmarks: m-GenEval and m-DPG, both provided as .zip archives.\n\n\t\n\t\t\n\t\tm-GenEval\n\t\n\nThis dataset is a multilingual extension of the original GenEval benchmark. Each English prompt from GenEval has been translated into five additional languages: Chinese, Dutch, French, Hindi, and Persian.Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mderakhshani/NeoBabel-Eval.","first_N":5,"first_N_keywords":["text-to-image","English","Dutch","French","Persian"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"text-to-image","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://sdiffusers.com/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\n‚îú‚îÄ‚îÄ üìÇ images/\n‚îÇ   ‚îú‚îÄ‚îÄ 1.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 2.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 3.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ ....\n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"civitai-top-sfw-images-with-metadata","keyword":"image-text-to-text","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata","creator_name":"Wallstone","creator_url":"https://huggingface.co/wallstoneai","description":"\n\t\n\t\t\n\t\tCivitAI Top SFW Images Dataset\n\t\n\nThis dataset contains 12k+ top SFW images from CivitAI filtered using top reactions. The dataset contains prompt & nsfw level metadata in prompts.json file. The nsfw levels are: Soft, Mature & X.\n\n\t\n\t\t\n\t\tOriginal forum post:\n\t\n\nhttps://sdiffusers.com/Thread-CivitAI-Top-SFW-Images-Dataset-12k-images\n\n\t\n\t\t\n\t\tDataset collection date\n\t\n\nJuly 2025\n\n\t\n\t\t\n\t\tDataset structure:\n\t\n\n‚îú‚îÄ‚îÄ üìÇ images/\n‚îÇ   ‚îú‚îÄ‚îÄ 1.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 2.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ 3.jpeg\n‚îÇ   ‚îú‚îÄ‚îÄ ....\n‚îú‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wallstoneai/civitai-top-sfw-images-with-metadata.","first_N":5,"first_N_keywords":["image-classification","image-to-text","text-to-image","image-text-to-text","image-to-image"],"keywords_longer_than_N":true},
	{"name":"bird-sql-portuguese","keyword":"text-to-sql","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boakpe/bird-sql-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","description":"\n\t\n\t\t\n\t\tBIRD-SQL - Vers√£o em Portugu√™s\n\t\n\nEste reposit√≥rio cont√©m a tradu√ß√£o para portugu√™s da parti√ß√£o de treino e desenvolvimento do benchmark BIRD-SQL, um benchmark para a tarefa de Text-to-SQL.\n","first_N":5,"first_N_keywords":["table-question-answering","question-answering","Portuguese","cc-by-sa-4.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"kuyen-dog-sdxl","keyword":"text-to-image","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/t8ix/kuyen-dog-sdxl","creator_name":"t8ix","creator_url":"https://huggingface.co/t8ix","description":"\n\t\n\t\t\n\t\tkuyen-dog-sdxl\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a DreamBooth training dataset for SDXL fine-tuning. The dataset contains 27 image-text pairs.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\nimage: Training images in RGB format\ntext: Corresponding captions/prompts for each image\n\n\n\t\n\t\t\n\t\tUsage\n\t\n\nThis dataset is designed for use with SDXL DreamBooth training scripts. You can load it using:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"t8ix/kuyen-dog-sdxl\")\n\n\n\t\n\t\t\n\t\tTraining Details‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/t8ix/kuyen-dog-sdxl.","first_N":5,"first_N_keywords":["text-to-image","English","mit","< 1K","parquet"],"keywords_longer_than_N":true}
]
;
