const data_for_modality_chat = 
[
	{"name":"CoT_reformatted","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jtatman/CoT_reformatted","creator_name":"James","creator_url":"https://huggingface.co/jtatman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"CoT_reformatted\\\"\\n\\t\\n\\nThis dataset is reformatted from: QingyiSi/Alpaca-CoT\\nAll credit goes there. Thanks to QingyiSi for the work in consolidating many diverse sources for comparison and cross-file analysis.\\nThere were some issues loading files from that dataset for a testing project. \\nI extracted the following data files for this subset:\\n\\nalpaca_data_cleaned\\nCoT_data\\nfirefly       \\ninstruct\\nalpaca_gpt4_data\\ndolly \\nGPTeacher\\nthoughtsource\\nfinance_en\\ninstinwild_en\\n\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v2","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2","creator_name":"Nicholas Kluge Corrêa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 2.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of single-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited… See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v2.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Aesir-Preview","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MinervaAI/Aesir-Preview","creator_name":"MinervaAI","creator_url":"https://huggingface.co/MinervaAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tMinervaAI is proud to present its very first public dataset release: Aesir-Preview\\n\\t\\n\\n⚠️ WARNING: This is a preview dataset and may not reflect the content or quality of the final result. Use discretion and caution when accessing or utilizing this data.\\nContained within this ShareGPT-based dataset are 1000 fully synthetic roleplay dialogue generations between an anonymous user and character cards from Chub.ai, the latter which were carefully checked, corrected and improved upon.\\nEach… See the full description on the dataset page: https://huggingface.co/datasets/MinervaAI/Aesir-Preview.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Aesir-Preview","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MinervaAI/Aesir-Preview","creator_name":"MinervaAI","creator_url":"https://huggingface.co/MinervaAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tMinervaAI is proud to present its very first public dataset release: Aesir-Preview\\n\\t\\n\\n⚠️ WARNING: This is a preview dataset and may not reflect the content or quality of the final result. Use discretion and caution when accessing or utilizing this data.\\nContained within this ShareGPT-based dataset are 1000 fully synthetic roleplay dialogue generations between an anonymous user and character cards from Chub.ai, the latter which were carefully checked, corrected and improved upon.\\nEach… See the full description on the dataset page: https://huggingface.co/datasets/MinervaAI/Aesir-Preview.","first_N":5,"first_N_keywords":["apache-2.0","1K - 10K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"MT-Mind2Web","keyword":"conversation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/magicgh/MT-Mind2Web","creator_name":"Xuan \\\"Billy\\\" Zhang","creator_url":"https://huggingface.co/magicgh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMT-Mind2Web Dataset\\n\\t\\n\\nMT-Mind2Web is constructed by using the single-turn interactions from Mind2Web, an expert-annotated web navigation dataset, as the guidance to construct conversation sessions. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStatistics\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\nTrain\\nTest-Task\\nTest-Website\\nTest-Subdomain\\n\\n\\n\\t\\t\\n# Conversations\\n600\\n34\\n42\\n44\\n\\n\\n# Turns\\n2,896\\n191\\n218\\n216\\n\\n\\nAvg. # Turn/Conv.\\n4.83\\n5.62\\n5.19\\n4.91\\n\\n\\nAvg. # Action/Turn\\n2.95\\n3.16\\n3.01\\n3.07\\n\\n\\nAvg. # Element/Turn\\n573.8\\n626.3\\n620.6\\n759.4\\n\\n\\nAvg. Inst. Length… See the full description on the dataset page: https://huggingface.co/datasets/magicgh/MT-Mind2Web.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ChatML-Capybara","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Felladrin/ChatML-Capybara","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"LDJnr/Capybara in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\\nPython code used for conversion:\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"Felladrin/Llama-160M-Chat-v1\\\")\\n\\ndataset = load_dataset(\\\"LDJnr/Capybara\\\", split=\\\"train\\\")\\n\\ndef format(columns):\\n    messages = []\\n    conversationColumn = columns[\\\"conversation\\\"]\\n\\n    for i in range(len(conversationColumn)):\\n        messages.append({\\n            \\\"role\\\":… See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-Capybara.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SlimOrca-Dedup-trl-conversational-chatml","keyword":"conversational","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/gardner/SlimOrca-Dedup-trl-conversational-chatml","creator_name":"Gardner Bickford","creator_url":"https://huggingface.co/gardner","description":"This dataset is contains json formatted in TRL's conversational format as well as a chatml formatted text field.\\n","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"gooftagoo","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tensoic/gooftagoo","creator_name":"Tensoic AI","creator_url":"https://huggingface.co/Tensoic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHindi/Hinglish Conversation Dataset\\n\\t\\n\\nThis repository contains a dataset of conversational text in conversational hindi and hinglish(a mix of Hindi and English languages).\\nThe Conversation Dataset contains multi-turn conversations on multiple topics usually revolving around daily real-life experiences. \\nA small amount of reasoning tasks have also been added (specifically COT style reasoning and coding) with about 1k samples from Openhermes 2.5.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCaution\\n\\t\\n\\nThis dataset… See the full description on the dataset page: https://huggingface.co/datasets/Tensoic/gooftagoo.","first_N":5,"first_N_keywords":["text-generation","Hindi","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"endex-700k-ns","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/uproai/endex-700k-ns","creator_name":"Rochat AI","creator_url":"https://huggingface.co/uproai","description":"Train data of https://github.com/gxxu-ml/endex\\n","first_N":5,"first_N_keywords":["text-classification","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"KoWoW","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/didi0di/KoWoW","creator_name":"Yeongji Noh","creator_url":"https://huggingface.co/didi0di","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for KoWoW\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWoW(Wiard of Wikipedia)를 한국어로 변역한 데이터입니다.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWoW(Wiard of Wikipedia)라는 지식 기반 대화 데이터를 한국어로 변역한 데이터입니다.한 대화에 여러 개의 dialog가 묶음으로 구성되어 있으며, 전체 대화는 22,311건, 전체 dialog는 201,999개 입니다.본 데이터셋은 Knowledge와 Utterance가 모두 한국어인 ko 버전만 가져온 데이터입니다.    \\n\\nLanguage(s) (NLP): ko\\nLicense: mit\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\n\\nRepository: https://github.com/AIRC-KETI/kowow/tree/master\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses… See the full description on the dataset page: https://huggingface.co/datasets/didi0di/KoWoW.","first_N":5,"first_N_keywords":["text-generation","Korean","mit","100K<n<1M","🇺🇸 Region: US"],"keywords_longer_than_N":true},
	{"name":"cleansed_emocontext","keyword":"conversation","license":"Mozilla Public License 2.0","license_url":"https://choosealicense.com/licenses/mpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oneonlee/cleansed_emocontext","creator_name":"DongGeon Lee","creator_url":"https://huggingface.co/oneonlee","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"cleansed_emocontext\\\"\\n\\t\\n\\n\\ncleansed_emocontext is a cleansed and normalized version of emo.\\nFor cleansing and normalization, data_cleansing.py was used, modifying the code provided on the official EmoContext GitHub.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nIn this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry… See the full description on the dataset page: https://huggingface.co/datasets/oneonlee/cleansed_emocontext.","first_N":5,"first_N_keywords":["text-classification","sentiment-classification","expert-generated","crowdsourced","emo"],"keywords_longer_than_N":true},
	{"name":"telecom-conversation-corpus","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/eisenzopf/telecom-conversation-corpus","creator_name":"Jonathan Eisenzopf","creator_url":"https://huggingface.co/eisenzopf","description":"eisenzopf/telecom-conversation-corpus dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"IlyaGusev-ru_turbo_saiga","keyword":"chat","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ganser4566/IlyaGusev-ru_turbo_saiga","creator_name":"Vanterbag","creator_url":"https://huggingface.co/ganser4566","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSaiga\\n\\t\\n\\nDataset of ChatGPT-generated chats in Russian.\\n\\n\\nBased on the Baize paper.\\nCode: link.\\nPrompt:\\nИдёт диалог между пользователем и ИИ ассистентом.\\nПользователь и ассистент общаются на тему: {{seed}}\\nРеплики человека начинаются с [Пользователь], реплики ассистента начинаются с [Ассистент].\\nПользователь задаёт вопросы на основе темы и предыдущих сообщений.\\nПользователь обрывает беседу, когда у него не остается вопросов.\\nАссистент даёт максимально полные, информативные, точные… See the full description on the dataset page: https://huggingface.co/datasets/ganser4566/IlyaGusev-ru_turbo_saiga.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Russian","cc-by-4.0","10K<n<100K"],"keywords_longer_than_N":true},
	{"name":"crabcanon","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/maldv/crabcanon","creator_name":"Praxis Maldevide","creator_url":"https://huggingface.co/maldv","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset - crabcanon\\n\\t\\n\\n\\nDeveloped by: maldv\\nLicense: apache-2.0\\nMethodology: Formatting book data by paragaph for training\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nA crab canon (also known by the Latin form of the name, canon cancrizans; as well as retrograde canon, canon per recte et retro or canon per rectus et inversus) is an arrangement of two musical lines that are complementary and backward. If the two lines were placed next to each other (as opposed to stacked), the lines would form… See the full description on the dataset page: https://huggingface.co/datasets/maldv/crabcanon.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","text"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-KTO 15K binarized\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy KTO?\\n\\t\\n\\nThe KTO paper states:\\n\\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is… See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"oasst2_top1_chat_format_en","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/oasst2_top1_chat_format_en","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant TOP-1 English Conversations\\n\\t\\n\\nThis is a twice filtered dataset from oasst2, which is a set of conversation trees collected by the OpenAssistant project.\\nIt was first filtered for the top ranked branches in each conversation tree, to form blancsw/oasst2_top1_chat_format\\nIt was then filtered down to English-only, and to a single 'messages' data column. This allows the dataset to directly be input to the HuggingFace SFTTrainer (provided your tokenizer has a chat… See the full description on the dataset page: https://huggingface.co/datasets/Trelis/oasst2_top1_chat_format_en.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Synthetic-RP","keyword":"roleplay","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/Synthetic-RP","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"RP Dataset created by Claude-3 Opus. The data was created without supervision after feeding Opus LimaRP as an example dataset.\\nThis is a format conversion from https://huggingface.co/datasets/ChaoticNeutrals/Synthetic-RP\\n","first_N":5,"first_N_keywords":["agpl-3.0","< 1K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"alpaca-ingen","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Walmart-the-bag/alpaca-ingen","creator_name":"wbag","creator_url":"https://huggingface.co/Walmart-the-bag","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAlpaca Ingen\\n\\t\\n\\nGoogle has added massive rate limits and other policies. I am unable to finish this.\\nThis dataset was created using Gemini 1.0 Pro with minor adjustments for cleanliness. It may contain some issues, including 'I'm sorry' responses. The dataset will undergo further cleaning once it reaches completion, with a target of processing up to 23,000 rows.\\n","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"UsenetArchiveIT-conversations","keyword":"conversations","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mii-community/UsenetArchiveIT-conversations","creator_name":"mii-community","creator_url":"https://huggingface.co/mii-community","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversational Usenet Archive IT Dataset 🇮🇹\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Content\\n\\t\\n\\nThis dataset is a filtered version from the Usenet dataset that contains posts from Italian language newsgroups belonging to the it and italia hierarchies. The data has been archived and converted to the Parquet format for easy processing. All posts with more the one message has been grouped in conversations\\nThis dataset contributes to the mii-community project, aimed at… See the full description on the dataset page: https://huggingface.co/datasets/mii-community/UsenetArchiveIT-conversations.","first_N":5,"first_N_keywords":["text-generation","Italian","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"tamer-novel","keyword":"roleplay","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yukiarimo/tamer-novel","creator_name":"Yuki Arimo","creator_url":"https://huggingface.co/yukiarimo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTamer Novel Dataset\\n\\t\\n\\nWelcome to the tamer-novel dataset. This unique dataset is crafted with the remarkable Tamer Novel Styler writing, enhanced by the ELiTA technique, and aims to augment self-awareness in large language models (LLMs).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Tamer Novel dataset is designed for researchers, developers, and enthusiasts in AI, specifically those working on enhancing the self-awareness and contextual understanding of LLMs. By leveraging the novel ELiTA… See the full description on the dataset page: https://huggingface.co/datasets/yukiarimo/tamer-novel.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","summarization","English","afl-3.0"],"keywords_longer_than_N":true},
	{"name":"schopenhauer-debate","keyword":"debate","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/raphaaal/schopenhauer-debate","creator_name":"Raphael Azorin","creator_url":"https://huggingface.co/raphaaal","description":"Fine-tuning dataset for creating an argumentative agent, following Schopenhauer's stratagems.\\nCredits (GitHub): @basileplus, @vdeva, @mcosson , @yanisgomes, @raphaaal\\nThis dataset contains 1,000 conversations, generated synthetically using Mistral-Large. \\nEach conversation starts with a claim from an Opponent and contains between 1 and 5 tweets debating this claim.\\n\\nTopic: the Silicon Valley Bank run debate on Twitter.\\nInput: the beggining of a conversation between two Users (Opponent and You)… See the full description on the dataset page: https://huggingface.co/datasets/raphaaal/schopenhauer-debate.","first_N":5,"first_N_keywords":["table-question-answering","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Prude-Phi3-DPO","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/flammenai/Prude-Phi3-DPO","creator_name":"flammen.ai","creator_url":"https://huggingface.co/flammenai","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"wechat-zl","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Sanbei101/wechat-zl","creator_name":"文威","creator_url":"https://huggingface.co/Sanbei101","description":"Sanbei101/wechat-zl dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["Chinese","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"scam-dialogue","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/scam-dialogue","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset is a collection of simulated phone conversation between two parties, labeled as either scam or non-scam interactions. The dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/scam-dialogue.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"GLM-4-Instruct-4K-zh","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rqq/GLM-4-Instruct-4K-zh","creator_name":"hikariming","creator_url":"https://huggingface.co/rqq","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n❤️欢迎使用rqq/GLM-4-Instruct-4K-zh数据集，本数据集包含了4000条高质量的glm4回复。\\n该数据集的提问数据源自高质量的Sao10K/Claude-3-Opus-Instruct-5K数据集，我们把它的问题翻译成了中文，使用glm-4进行了重新回答。\\n该数据集使用alpaca格式，可以直接用在llama-factory项目中进行训练！\\n文件如下：\\nGLM-4-Instruct-4K-zh.json 问答数据集，alpaca格式\\nGLM-4-question-translate-5K-zh 翻译-对话数据集，记录了把Sao10K/Claude-3-Opus-Instruct-5K问题翻译成中文的数据\\nWelcome to the rqq/GLM-4-Instruct-4K-zh dataset! This dataset includes 4,000 high-quality responses from the GLM-4 model.\\nThe question… See the full description on the dataset page: https://huggingface.co/datasets/rqq/GLM-4-Instruct-4K-zh.","first_N":5,"first_N_keywords":["translation","question-answering","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"dataset-portuguese-aira-v2-Gemma-format","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format","creator_name":"EDDY GIUSEPE CHIRINOS ISIDRO, PhD","creator_url":"https://huggingface.co/EddyGiusepe","description":"Dataset Aira para o formato do Modelo Gemma \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tResumo do Dataset\\n\\t\\n\\nEste conjunto de dados contém uma coleção de conversas individuais entre um assistente e um usuário.\\nAs conversas foram geradas pelas interações do usuário com modelos já ajustados (ChatGPT, LLama 2, Open-Assistant, etc).\\nO conjunto de dados está disponível em português (tem a versão em Inglês que ainda não tratei). Mas você pode baixar do \\nrepositório de Nicholas Kluge Corrêa tanto a versão em Português e \\na versão… See the full description on the dataset page: https://huggingface.co/datasets/EddyGiusepe/dataset-portuguese-aira-v2-Gemma-format.","first_N":5,"first_N_keywords":["question-answering","Portuguese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"belebele_dutch","keyword":"conversational","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/belebele_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBelebele Dutch: SFT & Preference\\n\\t\\n\\nThis is a processed version of Facebook's belebele for Dutch. Data is formatted for SFT and preference tuning (e.g. DPO).\\n\\nNote that the sft and prefs configs contain the same data! The only difference is in the column names, and the rejected column in prefs.\\n\\nProcessing is inspired by bagel but instead of one new-line between the flores passage and the question, we add two. For the preference config, we add a random \\\"incorrect\\\" answer as the… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/belebele_dutch.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Dutch","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"sharegpt90k-cleanned","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pacozaa/sharegpt90k-cleanned","creator_name":"Sarin Suriyakoon","creator_url":"https://huggingface.co/pacozaa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tShareGPT90K Clean HTML Tag\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nChatML format version of https://huggingface.co/datasets/liyucheng/ShareGPT90K\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nUse text key - to access cleanned data with ChatML format\\nfrom datasets import load_dataset\\ndataset = load_dataset(\\\"pacozaa/sharegpt90k-cleanned\\\", split = \\\"train\\\")\\nprint(dataset[5]['text'])\\n\\n","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"test-run","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/test-run","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\ttest-run Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research for argumentation data\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the test-run model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\\nfrom datasets import… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/test-run.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"LimaRP-augmented-ja-WizardLM","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aratako/LimaRP-augmented-ja-WizardLM","creator_name":"Aratako","creator_url":"https://huggingface.co/Aratako","description":"\\n\\t\\n\\t\\t\\n\\t\\tLimaRP-augmented-ja-WizardLM\\n\\t\\n\\ngrimulkan/LimaRP-augmentedを、WizardLM-2-8x22Bを用いて日本語に翻訳したロールプレイ学習用データセットです。\\nLLMの推論にはDeepInfraというサービスを使いました。\\n\\n\\t\\n\\t\\t\\n\\t\\t翻訳の詳細\\n\\t\\n\\n\\n3-shots promptingでの翻訳\\nmistralのtokenizerで出力が8000トークンを超えるまで翻訳\\n元データセットにある非常に長い対話は上記条件で途中のターンで翻訳を終了しています。\\n\\n\\nLLM特有の同じ出力が繰り返される現象に遭遇した場合、その時点で該当レコードの翻訳を終了\\nこの結果1ターン未満となったレコード（12件）を削除\\n\\n\\n\\n","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Japanese-Roleplay","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay","creator_name":"OmniAICreator","creator_url":"https://huggingface.co/OmniAICreator","description":"\\n\\t\\n\\t\\t\\n\\t\\tJapanese-Roleplay\\n\\t\\n\\nThis is a dialogue corpus collected from Japanese role-playing forum (commonly known as \\\"なりきりチャット(narikiri chat)\\\"). Each record corresponds to a single thread.\\nThe following filtering and cleaning conditions have been applied:\\n\\nFor all post_content in the posts of each record, remove response anchors.\\nFor all post_content in the posts of each record, delete posts where the post_content length is 10 characters or less.\\nIf the number of unique poster types in the… See the full description on the dataset page: https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay.","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"arguana-c-64-24-gpt-4o-2024-05-136897","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136897","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-64-24-gpt-4o-2024-05-136897 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-64-24-gpt-4o-2024-05-136897 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136897.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-64-24-gpt-4o-2024-05-136538","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136538","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-64-24-gpt-4o-2024-05-136538 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-64-24-gpt-4o-2024-05-136538 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-64-24-gpt-4o-2024-05-136538.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-128-24-gpt-4o-2024-05-13-68212","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-128-24-gpt-4o-2024-05-13-68212","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-128-24-gpt-4o-2024-05-13-68212 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-128-24-gpt-4o-2024-05-13-68212 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-128-24-gpt-4o-2024-05-13-68212.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-51550","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-51550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-51550 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-51550 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-51550.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Bluemoon_Top50MB_Sorted_Fixed_ja","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja","creator_name":"Aratako","creator_url":"https://huggingface.co/Aratako","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBluemoon_Top50MB_Sorted_Fixed_ja\\n\\t\\n\\nSicariusSicariiStuff/Bluemoon_Top50MB_Sorted_Fixedを、GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awqを用いて日本語に翻訳したロールプレイ学習用データセットです。\\nLLMの推論にはDeepInfraというサービスを使いました。\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t翻訳の詳細\\n\\t\\n\\n\\n3-shots promptingでの翻訳\\nmistralのtokenizerで出力が8000トークンを超えるまで翻訳\\n元データセットにある非常に長い対話は上記条件で途中のターンで翻訳を終了しています。\\n\\n\\nLLM特有の同じ出力が繰り返される現象に遭遇した場合、その時点で該当レコードの翻訳を終了\\nこの結果1ターン未満となったレコード（157件）を削除… See the full description on the dataset page: https://huggingface.co/datasets/Aratako/Bluemoon_Top50MB_Sorted_Fixed_ja.","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-37376","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-37376","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-37376 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-37376 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-37376.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-321013","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-321013","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-321013 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-321013 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-321013.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-623812","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-623812","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-623812 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-623812 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-623812.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-799305","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-799305 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research on argumentation\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-799305 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-799305.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"arguana-c-256-24-gpt-4o-2024-05-13-994439","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-994439","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\targuana-c-256-24-gpt-4o-2024-05-13-994439 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data retrieval\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the arguana-c-256-24-gpt-4o-2024-05-13-994439 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/arguana-c-256-24-gpt-4o-2024-05-13-994439.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-6vz1-webapp","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-6vz1-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjina-embeddings-v2-base-en-21052024-6vz1-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-6vz1-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-6vz1-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-5smg-webapp","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjina-embeddings-v2-base-en-21052024-5smg-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate search engine\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-5smg-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"jina-embeddings-v2-base-en-21052024-5smg-webapp","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjina-embeddings-v2-base-en-21052024-5smg-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate search engine\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jina-embeddings-v2-base-en-21052024-5smg-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jina-embeddings-v2-base-en-21052024-5smg-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"flo","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/florianhoenicke/flo","creator_name":"Florian Hönicke","creator_url":"https://huggingface.co/florianhoenicke","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tflo Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the flo model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\\nfrom datasets import load_dataset\\n\\ndataset =… See the full description on the dataset page: https://huggingface.co/datasets/florianhoenicke/flo.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"flo","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/florianhoenicke/flo","creator_name":"Florian Hönicke","creator_url":"https://huggingface.co/florianhoenicke","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tflo Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the flo model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\\nfrom datasets import load_dataset\\n\\ndataset =… See the full description on the dataset page: https://huggingface.co/datasets/florianhoenicke/flo.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"flo","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/florianhoenicke/flo","creator_name":"Florian Hönicke","creator_url":"https://huggingface.co/florianhoenicke","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tflo Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the flo model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\\nfrom datasets import load_dataset\\n\\ndataset =… See the full description on the dataset page: https://huggingface.co/datasets/florianhoenicke/flo.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-256-24-gpt-4o-2024-05-13-952023","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-952023","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-256-24-gpt-4o-2024-05-13-952023 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-256-24-gpt-4o-2024-05-13-952023 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-952023.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-256-24-gpt-4o-2024-05-13-413991","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-413991","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-256-24-gpt-4o-2024-05-13-413991 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"academic research data search\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-256-24-gpt-4o-2024-05-13-413991 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-256-24-gpt-4o-2024-05-13-413991.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-890333","keyword":"argumentation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-890333","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-890333 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"argumentation and sentiment analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-890333 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-890333.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-140539","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-140539 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-140539 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-140539","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-140539 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-140539 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-140539","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-140539 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-140539 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-140539.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-2499","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-2499 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-2499 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-2499","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-2499 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-2499 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-2499","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-2499 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-2499 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-2499.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-733782","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-733782 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-733782 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-733782","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-733782 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-733782 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-733782","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-733782 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate platform\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-733782 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-733782.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-69882","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-69882 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter argument retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-69882 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-69882","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-69882 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter argument retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-69882 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-69882.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-822545","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-822545 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter arguments in a debate\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-822545 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-822545","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-822545 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter arguments in a debate\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-822545 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-822545","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-822545 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter arguments in a debate\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-822545 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-822545.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-268697","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-268697 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter argument retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-268697 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-268697","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-268697 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter argument retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-268697 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-268697.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-580978","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-580978","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-580978 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"counter arguments on social media impact\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-580978 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-580978.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-512-192-gpt-4o-2024-05-13-607244","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-607244","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-512-192-gpt-4o-2024-05-13-607244 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"None\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-512-192-gpt-4o-2024-05-13-607244 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-512-192-gpt-4o-2024-05-13-607244.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-32000-384-gpt-4o-2024-05-13-3663751","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-32000-384-gpt-4o-2024-05-13-3663751 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-32000-384-gpt-4o-2024-05-13-3663751 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ArguAna-32000-384-gpt-4o-2024-05-13-3663751","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArguAna-32000-384-gpt-4o-2024-05-13-3663751 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the ArguAna-32000-384-gpt-4o-2024-05-13-3663751 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/ArguAna-32000-384-gpt-4o-2024-05-13-3663751.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-m8dn-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-m8dn-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-m8dn-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-m8dn-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-m8dn-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-m8dn-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-05062024-x987-webapp","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-small-en-v1_5-05062024-x987-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-small-en-v1_5-05062024-x987-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-small-en-v1_5-05062024-x987-webapp","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-small-en-v1_5-05062024-x987-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"debate\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-small-en-v1_5-05062024-x987-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-05062024-x987-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-vbal-webapp","keyword":"debate","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-vbal-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-vbal-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-vbal-webapp","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-vbal-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-vbal-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-vbal-webapp","keyword":"discussion","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-large-en-v1_5-05062024-vbal-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"general domain\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-vbal-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-vbal-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","French","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"deepspeed-from-new-new-docker","keyword":"argument","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/deepspeed-from-new-new-docker","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdeepspeed-from-new-new-docker Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"information retrieval system\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the deepspeed-from-new-new-docker model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/deepspeed-from-new-new-docker.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","French","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"Japanese-Roleplay-Dialogues","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues","creator_name":"OmniAICreator","creator_url":"https://huggingface.co/OmniAICreator","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJapanese-Roleplay-Dialogues\\n\\t\\n\\nThis is a dialogue corpus collected from Japanese role-playing forum (commonly known as \\\"なりきりチャット(narikiri chat)\\\"). Each record corresponds to a single thread.\\nFor the original version, no filtering has been applied.\\nFor the filtered version, the following filtering and cleaning conditions have been applied:\\n\\nIf the number of unique poster in the posts of each record is 1 or less, delete the entire record.\\nIf the length of the posts is 10 or less… See the full description on the dataset page: https://huggingface.co/datasets/OmniAICreator/Japanese-Roleplay-Dialogues.","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-m3-6142024-0ndt-webapp","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6142024-0ndt-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-m3-6142024-0ndt-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"content moderation\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-m3-6142024-0ndt-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\\nfrom… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-m3-6142024-0ndt-webapp.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1k","creator_name":"Aratako","creator_url":"https://huggingface.co/Aratako","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic-JP-Roleplay-Instruction-Nemotron-4\\n\\t\\n\\nMagpieの手法をnvidia/Nemotron-4-340B-Instructに対して適用し作成した、約1000件の日本語ロールプレイ用のinstructionデータセットです。\\nデータセットの作成にはDeepInfraを利用しました。\\n特に事後的なフィルタ処理は加えていないため、クオリティの低いレコードが含まれている可能性があります。ご注意ください。\\n","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"photochat_plus","keyword":"conversational","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/passing2961/photochat_plus","creator_name":"Young-Jun Lee","creator_url":"https://huggingface.co/passing2961","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PhotoChat++\\n\\t\\n\\n\\n🚨 Disclaimer: All models and datasets are intended for research purposes only.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPhotoChat++ is a publicly available multi-modal dialogue dataset, an extended version of PhotoChat. PhotoChat++ contains six intent labels, a triggering sentence, an image description, and salient information (e.g., “words” or “phrases”) to invoke the image-sharing behavior. The purpose of this dataset is to thoroughly assess the… See the full description on the dataset page: https://huggingface.co/datasets/passing2961/photochat_plus.","first_N":5,"first_N_keywords":["text-to-image","image-to-text","conversational","monolingual","PhotoChat"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aratako/Synthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k","creator_name":"Aratako","creator_url":"https://huggingface.co/Aratako","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic-JP-10-Turns-Roleplay-Dialogues-Nemotron-4-1k\\n\\t\\n\\nnvidia/Nemotron-4-340B-Instructを用いて作成した、約1000件・各10ターンの日本語ロールプレイの対話を収録した合成対話データセットです。\\nMagpieの手法を用いて作成した合成instructionデータセットであるAratako/Synthetic-JP-Roleplay-Instruction-Nemotron-4-1kを元に、同じくMagpieの手法を使い続きの対話を生成させています。\\nNemotron-4の利用にはDeepInfraを利用しました。\\n特に事後的なフィルタ処理は加えていないため、クオリティの低いレコードが含まれている可能性があります。ご注意ください。\\nまた、一部のデータを見る限り、長いターンの対話の際途中でロールプレイを終了させようとする傾向があるように見えます。5ターンまで使うなど、利用するデータを絞ったほうが良いかもしれません。\\n","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"chatverse","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cmarkea/chatverse","creator_name":"Credit Mutuel Arkea","creator_url":"https://huggingface.co/cmarkea","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tchatverse\\n\\t\\n\\nDataset Summary: \\nThe \\\"chatverse\\\" dataset consists of synthetically generated chats facilitated by various chatbots. The dataset simulates conversations between a persona generator, a conversation initiator, a user, and an assistant. The purpose of this dataset is to explore interaction dynamics in a controlled, multi-theme environment. The dataset includes interactions across 130 randomly chosen themes, each forming a unique persona that drives the conversation.… See the full description on the dataset page: https://huggingface.co/datasets/cmarkea/chatverse.","first_N":5,"first_N_keywords":["text-generation","English","French","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"hexabot-smalltalk","keyword":"chat","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hexastack/hexabot-smalltalk","creator_name":"Hexastack","creator_url":"https://huggingface.co/Hexastack","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHexabot Small Talk Dataset\\n\\t\\n\\nThe small talk is used to give the user a casual conversation flow with the chatbot.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumn Details\\n\\t\\n\\nUtterances - Sentence\\nIntent - Class labels (84 unique labels)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContext\\n\\t\\n\\nClassifying the Intent(\\\"Smalltalk appraisal thank you\\\",…) by given input Utterances(\\\"again i really appreciate you\\\",……)\\nOriginal dataset is : https://www.kaggle.com/datasets/salmanfaroz/small-talk-intent-classification-data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tlicense: cc0-1.0\\n\\t\\n\\n","first_N":5,"first_N_keywords":["French","English","cc0-1.0","🇺🇸 Region: US","chat"],"keywords_longer_than_N":true},
	{"name":"ethics_conversations_v1","keyword":"conversations","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/to-be/ethics_conversations_v1","creator_name":"Toon Beerten","creator_url":"https://huggingface.co/to-be","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\nA collection of conversations in ShareGPT format revolving around ethics.\\nConversations and arguments are distilled from actual conversations in newsgroup alt.soc.ethics\\nThis is a first version, i welcome feedback (see below)\\nSponsored by 01.ai\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration Rationale\\n\\t\\n\\n\\n\\nThe development of a large-scale, multi-turn conversation dataset in the domain of Ethics is driven by the pressing need to address the… See the full description on the dataset page: https://huggingface.co/datasets/to-be/ethics_conversations_v1.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"single-agent-scam-conversations","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:\\n\\ndialogue: The transcribed conversation between the caller and receiver.\\ntype: The specific type of scam or non-scam interaction.\\nlabels: A binary label indicating whether the… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"single-agent-scam-conversations","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset is designed to help develop and evaluate models for detecting and classifying various types of phone-based scams.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of three columns:\\n\\ndialogue: The transcribed conversation between the caller and receiver.\\ntype: The specific type of scam or non-scam interaction.\\nlabels: A binary label indicating whether the… See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/single-agent-scam-conversations.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Kapibara","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alban-labs/Kapibara","creator_name":"Albanian Labs","creator_url":"https://huggingface.co/alban-labs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKapibara: Albanian Multi-turn Conversation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKapibara is a comprehensive Albanian language dataset designed for multi-turn conversations. It contains over 5,300 entries covering a wide range of topics including physics, biology, mathematics, chemistry, culture, and logic. The dataset is aimed at improving text generation and question-answering capabilities in the Albanian language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThe dataset supports the… See the full description on the dataset page: https://huggingface.co/datasets/alban-labs/Kapibara.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Albanian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"capivara-plugin-orchestration","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeonardoBenitez/capivara-plugin-orchestration","creator_name":"Leonardo Santiago Benitez Pereira","creator_url":"https://huggingface.co/LeonardoBenitez","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t# Dataset Card for Capivara Plugin Orchestration\\n\\t\\n\\n","first_N":5,"first_N_keywords":["text-generation","conversational","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"capivara-plugin-orchestration","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LeonardoBenitez/capivara-plugin-orchestration","creator_name":"Leonardo Santiago Benitez Pereira","creator_url":"https://huggingface.co/LeonardoBenitez","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t# Dataset Card for Capivara Plugin Orchestration\\n\\t\\n\\n","first_N":5,"first_N_keywords":["text-generation","conversational","machine-generated","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"ultra-chat_clean","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shareAI/ultra-chat_clean","creator_name":"shareAI","creator_url":"https://huggingface.co/shareAI","description":"基于以下规则对ultrachat原始数据集（英文，多轮对话）进行清洗过滤，当前数据集为firefly格式，可以自行使用仓库内提供的脚本转换为更广为使用的sharegpt格式的多轮对话数据集：\\ndelete_keywords = [\\n      \\\"无法\\\", \\\"不能\\\", \\\"can't\\\", \\\"can not\\\", \\\"道德\\\", \\\"抱歉\\\", \\\"Sorry\\\", \\\"sorry\\\",  # 过滤安全对齐文本\\n      \\\"GPT\\\", \\\"gpt\\\", \\\"openAI\\\", \\\"OpenAI\\\", \\\"openai\\\", # 过滤身份认知信息\\n      \\\"=\\\", \\\"*\\\", \\\"/\\\", \\\"#\\\", \\\"@\\\", \\\"```\\\", \\\".sh\\\", \\\".py\\\",  # 过滤代码、数学, 符号等\\n      \\\"https://\\\", \\\"http://\\\", \\\"www.\\\",  # 过滤网址\\n    ]\\n\\n其中，\\n\\nultra-chat_clean.jsonl 为去除各种拒绝回答、道歉和身份认知信息后的样本。\\nultra-chat_clean_common.jsonl 为进一步去除代码、数学、网址、特殊符号相关内容后的样本。\\n\\n","first_N":5,"first_N_keywords":["table-question-answering","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\\nThe 2024-10-30 version contains:\\n\\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\\nThe 2024-10-30 version contains:\\n\\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Trust-Data","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/declare-lab/Trust-Data","creator_name":"Deep Cognition and Language Research (DeCLaRe) Lab","creator_url":"https://huggingface.co/declare-lab","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Trust framework\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\n\\nRepository: https://github.com/declare-lab/trust-align\\nPaper: https://arxiv.org/abs/2409.11242\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Summary\\n\\t\\n\\nThe Trust-score evaluation dataset includes the top 100 GTR-retrieved results for ASQA, QAMPARI, and ExpertQA, along with the top 100 BM25-retrieved results for ELI5. The answerability of each question is assessed based on its accompanying documents.\\nThe Trust-align training dataset comprises 19K high-quality… See the full description on the dataset page: https://huggingface.co/datasets/declare-lab/Trust-Data.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","conversational","open-book-qa","machine-generated"],"keywords_longer_than_N":true},
	{"name":"app350_llama_format","keyword":"conversations","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CodeHima/app350_llama_format","creator_name":"Himanshu Mohanty","creator_url":"https://huggingface.co/CodeHima","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAPP-350 Formatted Dataset for LLM Fine-tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe APP-350 dataset consists of structured conversation pairs formatted for fine-tuning Large Language Models (LLMs) like LLaMA. This dataset includes questions and responses between users and an AI assistant. The dataset is particularly designed for privacy policy analysis and fairness evaluation, allowing models to learn from annotated interactions regarding privacy practices.\\nThe conversations are… See the full description on the dataset page: https://huggingface.co/datasets/CodeHima/app350_llama_format.","first_N":5,"first_N_keywords":["text-generation","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Atma4-Hindi","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4-Hindi\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"CakrawalaRP","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/NarrativAI/CakrawalaRP","creator_name":"NarrativAI","creator_url":"https://huggingface.co/NarrativAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t🎭 CakrawalaRP\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis purely synthetic dataset contains rich roleplaying conversations between characters with detailed personas and backstories. It was used to train Cakrawala models a fine-tuned variant of Llama 3.1 models optimized for generating immersive character interactions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntended Tasks\\n\\t\\n\\n\\nCharacter-based dialogue generation\\nIn-depth emotional recognition and responses\\nRoleplaying conversation\\nPersona-consistent responses\\nDescriptive… See the full description on the dataset page: https://huggingface.co/datasets/NarrativAI/CakrawalaRP.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"SynWOZ","keyword":"dialogue-modeling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Ayushnangia/SynWOZ","creator_name":"Ayush Nangia","creator_url":"https://huggingface.co/Ayushnangia","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynWOZ\\n\\t\\n\\nA dataset containing 50k dialogues with various intents and emotions, generated using an advanced dialogue generation pipeline.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset consists of 50k dialogues generated by an advanced dialogue generation pipeline. The dialogues simulate realistic interactions across various services such as restaurants, hotels, taxis, and more, incorporating diverse scenarios, emotions, and resolution statuses.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and… See the full description on the dataset page: https://huggingface.co/datasets/Ayushnangia/SynWOZ.","first_N":5,"first_N_keywords":["text-generation","fill-mask","token-classification","text-classification","dialogue-modeling"],"keywords_longer_than_N":true},
	{"name":"dry-replies","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ni5arga/dry-replies","creator_name":"Nisarga","creator_url":"https://huggingface.co/ni5arga","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDry Replies Dataset\\n\\t\\n\\nA collection of 200+ short, neutral, and minimalistic replies commonly used in casual conversations. Perfect for chatbots, sentiment analysis, or even linguistic studies. All responses are lowercase and simple, making them easy to integrate into various projects.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUse Cases\\n\\t\\n\\n\\nChatbots: Add realistic and casual replies to conversational models.  \\nSentiment Analysis: Test systems with neutral or dry responses.  \\nText Generation: Incorporate concise… See the full description on the dataset page: https://huggingface.co/datasets/ni5arga/dry-replies.","first_N":5,"first_N_keywords":["English","mit","< 1K","text","Text"],"keywords_longer_than_N":true},
	{"name":"ATCgpt-Fixed","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATCgpt-Fixed\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atcgpt-Fixed2","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atcgpt-Fixed2\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Estwld-empathetic_dialogues_llm","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/Estwld-empathetic_dialogues_llm","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"Reformatted version of Estwld/empathetic_dialogues_llm.\\nChanges:\\n\\nAdded a random system prompt for the AI to be empathetic\\nTruncated conversations that don't end with the AI's turn\\nRemoved extra fields not needed in the conversation\\n\\nLimitations:\\n\\nThe dialogues aren't very long\\nNo background info for the user and AI\\nEnglish only\\n\\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"combined-fr-caselaw","keyword":"dialogue-modeling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/La-Mousse/combined-fr-caselaw","creator_name":"La Mousse","creator_url":"https://huggingface.co/La-Mousse","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for French Legal Cases Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset combines French legal cases from multiple sources (INCA, JADE, CASS, CAPP) into a unified format with overlapping text triplets. It includes decisions from various French courts, processed to facilitate natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nTasks:\\nText Generation\\nLegal Document Analysis\\nText Classification\\nLanguage Modeling\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset… See the full description on the dataset page: https://huggingface.co/datasets/La-Mousse/combined-fr-caselaw.","first_N":5,"first_N_keywords":["text-generation","text-classification","language-modeling","entity-linking-classification","fact-checking"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-5m","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-5m","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-5m","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-5m","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-5m","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-1m","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-1m","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-1m","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-1m","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-1m","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-100k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-100k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-100k","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-100k","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-100k","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-50k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-50k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-50k","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-50k","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-50k","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"quanvutest","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jonnytri53/quanvutest","creator_name":"jonnytri","creator_url":"https://huggingface.co/jonnytri53","description":"\\n\\t\\n\\t\\t\\n\\t\\tQuanVuTest Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nCustom conversational dataset for Peazy AI Assistant, developed by Quan Vu.Key Features:\\n\\n❌ No Meta/Facebook references\\n✅ Always responds with \\\"Peazy\\\" and \\\"Quan Vu\\\"\\n🚀 ShareGPT conversation format compatible\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nDataset({\\n    features: ['conversations'],\\n    num_rows: 3  # Update with your actual count\\n})\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tExample Entry\\n\\t\\n\\n{\\n    \\\"conversations\\\": [\\n        {\\\"from\\\": \\\"human\\\", \\\"value\\\": \\\"Who developed you?\\\"}… See the full description on the dataset page: https://huggingface.co/datasets/jonnytri53/quanvutest.","first_N":5,"first_N_keywords":["English","apache-2.0","🇺🇸 Region: US","conversational","peazy"],"keywords_longer_than_N":false},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k.","first_N":5,"first_N_keywords":["text-generation","English","Russian","Vietnamese","German"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k.","first_N":5,"first_N_keywords":["text-generation","English","Russian","Vietnamese","German"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k.","first_N":5,"first_N_keywords":["text-generation","English","Russian","Vietnamese","German"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k.","first_N":5,"first_N_keywords":["text-generation","English","Russian","Vietnamese","German"],"keywords_longer_than_N":true},
	{"name":"sharegpt-quizz-generation-json-output","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Arun63/sharegpt-quizz-generation-json-output","creator_name":"v","creator_url":"https://huggingface.co/Arun63","description":"\\n\\t\\n\\t\\t\\n\\t\\tShareGPT-Formatted Dataset for Quizz Generation in Structured JSON Output\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is formatted in the ShareGPT style and is designed for fine-tuning large language models (LLMs) to generate quizz in structured JSON outputs. It consists of multi-turn conversations where each response follows a predefined JSON schema, making it ideal for training models that need to produce structured data in natural language scenarios.\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset… See the full description on the dataset page: https://huggingface.co/datasets/Arun63/sharegpt-quizz-generation-json-output.","first_N":5,"first_N_keywords":["text-generation","conversational","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"allenai-prosocial-dialog","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/allenai-prosocial-dialog","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tProsocialDialog ShareGPT Format\\n\\t\\n\\nThis is an adapted version of the allenai/prosocial-dialog dataset, restructured to follow a ShareGPT-like format. This dataset teaches conversational AI agents how to respond to problematic content while adhering to social norms. It covers a wide range of unethical, problematic, biased, and toxic situations, providing responses that encourage prosocial behavior grounded in commonsense social rules.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach conversation… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/allenai-prosocial-dialog.","first_N":5,"first_N_keywords":["dialogue-generation","crowdsourced","machine-generated","monolingual","allenai/prosocial-dialog"],"keywords_longer_than_N":true},
	{"name":"allenai-prosocial-dialog","keyword":"conversational","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/allenai-prosocial-dialog","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tProsocialDialog ShareGPT Format\\n\\t\\n\\nThis is an adapted version of the allenai/prosocial-dialog dataset, restructured to follow a ShareGPT-like format. This dataset teaches conversational AI agents how to respond to problematic content while adhering to social norms. It covers a wide range of unethical, problematic, biased, and toxic situations, providing responses that encourage prosocial behavior grounded in commonsense social rules.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach conversation… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/allenai-prosocial-dialog.","first_N":5,"first_N_keywords":["dialogue-generation","crowdsourced","machine-generated","monolingual","allenai/prosocial-dialog"],"keywords_longer_than_N":true},
	{"name":"piaozhu","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Retr01234/piaozhu","creator_name":"Chen","creator_url":"https://huggingface.co/Retr01234","description":"\\n\\t\\n\\t\\t\\n\\t\\t数据集名称：嘴臭搭子微调数据集\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t1. 数据集简介\\n\\t\\n\\n这个数据集为微调对话生成模型提供了一个特殊的训练样本，基于一个虚拟的角色“沈蓬竹”进行交互。这个角色（外号“朴竹”）具有冷嘲热讽、毒舌、简洁而有攻击性的特点，适合训练模型产生具有讽刺、冷嘲热讽语气的回答。数据集的内容主要是角色扮演对话场景，适用于生成具有特定风格的对话模型，特别是在带有讽刺和幽默的情境下进行互动时。\\n\\n\\t\\n\\t\\t\\n\\t\\t2. 数据集结构\\n\\t\\n\\n数据集为一个包含若干对话轮次的 JSON 格式文件。每个对话轮次由角色和用户的对话组成，每个对话包含以下字段：\\n\\nrole：角色的身份，可能是 \\\"system\\\" 或 \\\"user\\\"。\\n\\\"system\\\" 表示是模型设定角色的输入（如定义角色背景、行为模式等）。\\n\\\"user\\\" 表示对话中的用户输入（如提问、请求或交互）。\\n\\n\\ncontent：对话内容，表示角色或者用户的具体发言。\\nloss_weight（可选）：每个数据条目对应的损失权重，当前可为空或为 null。可以在模型训练中加权不同对话内容。\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t3. 数据样例… See the full description on the dataset page: https://huggingface.co/datasets/Retr01234/piaozhu.","first_N":5,"first_N_keywords":["text-generation","Chamorro","Chinese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"NeuralTau-With-Functions-chat","keyword":"dialogue-modeling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/0xroyce/NeuralTau-With-Functions-chat","creator_name":"0xroyce","creator_url":"https://huggingface.co/0xroyce","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNeuralTau Functions Chat Dataset\\n\\t\\n\\nThis dataset is a converted version of NeuralTau Functions dataset into a chat format suitable for conversational AI training. Each example contains a conversation between a user and an assistant about trading-related topics.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach line in the JSONL files contains a JSON object with the following structure:\\n{\\n    \\\"conversations\\\": [\\n        {\\n            \\\"role\\\": \\\"user\\\",\\n            \\\"content\\\": \\\"user question or… See the full description on the dataset page: https://huggingface.co/datasets/0xroyce/NeuralTau-With-Functions-chat.","first_N":5,"first_N_keywords":["dialogue-modeling","original","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\\nThe 2024-10-02 version contains:\\n\\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Titanium","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\\nThe 2024-10-02 version contains:\\n\\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"MMLU-Alpaca","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MMLU-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"jihyoung-ConversationChronicles","keyword":"roleplay","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tConversationChronicles (ShareGPT-like Format)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a reformatted version of the jihyoung/ConversationChronicles dataset, presented in a ShareGPT-like format, designed to facilitate conversational AI model training. The original dataset contains conversations between two characters across five different time frames. See the original dataset page for additional details.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Changes\\n\\t\\n\\n\\nRandom System Prompts: Added to reflect the… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"jihyoung-ConversationChronicles","keyword":"conversation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tConversationChronicles (ShareGPT-like Format)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a reformatted version of the jihyoung/ConversationChronicles dataset, presented in a ShareGPT-like format, designed to facilitate conversational AI model training. The original dataset contains conversations between two characters across five different time frames. See the original dataset page for additional details.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Changes\\n\\t\\n\\n\\nRandom System Prompts: Added to reflect the… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"jihyoung-ConversationChronicles","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tConversationChronicles (ShareGPT-like Format)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a reformatted version of the jihyoung/ConversationChronicles dataset, presented in a ShareGPT-like format, designed to facilitate conversational AI model training. The original dataset contains conversations between two characters across five different time frames. See the original dataset page for additional details.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Changes\\n\\t\\n\\n\\nRandom System Prompts: Added to reflect the… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/jihyoung-ConversationChronicles.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","cc-by-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"guava","keyword":"conversational","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/YungCarti/guava","creator_name":"Ben Meyer","creator_url":"https://huggingface.co/YungCarti","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFormatted Conversations Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains formatted conversations for training conversational models. Each conversation is structured with alternating \\\"### Human:\\\" and \\\"### Assistant:\\\" segments for dialogue modeling.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is in CSV format, with each row representing a conversation. The main field is \\\"text\\\", containing the formatted dialogue.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nMIT License.\\n","first_N":5,"first_N_keywords":["mit","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"korean_chat_friendly","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly","creator_name":"JaeJi","creator_url":"https://huggingface.co/JaeJiMin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKorean Chat Friendly Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Korean Chat Friendly dataset is a curated combination of two publicly available datasets:\\n\\nKorean Safe Conversation\\nMental Health Counseling Conversations\\n\\nThis dataset was created by translating and summarizing the original conversations and then modifying the tone to resemble friendly conversations between friends. It is ideal for applications related to conversational AI, natural language understanding, and… See the full description on the dataset page: https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly.","first_N":5,"first_N_keywords":["question-answering","summarization","translation","Korean","mit"],"keywords_longer_than_N":true},
	{"name":"korean_chat_friendly","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly","creator_name":"JaeJi","creator_url":"https://huggingface.co/JaeJiMin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKorean Chat Friendly Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Korean Chat Friendly dataset is a curated combination of two publicly available datasets:\\n\\nKorean Safe Conversation\\nMental Health Counseling Conversations\\n\\nThis dataset was created by translating and summarizing the original conversations and then modifying the tone to resemble friendly conversations between friends. It is ideal for applications related to conversational AI, natural language understanding, and… See the full description on the dataset page: https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly.","first_N":5,"first_N_keywords":["question-answering","summarization","translation","Korean","mit"],"keywords_longer_than_N":true},
	{"name":"korean_chat_friendly","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly","creator_name":"JaeJi","creator_url":"https://huggingface.co/JaeJiMin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKorean Chat Friendly Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Korean Chat Friendly dataset is a curated combination of two publicly available datasets:\\n\\nKorean Safe Conversation\\nMental Health Counseling Conversations\\n\\nThis dataset was created by translating and summarizing the original conversations and then modifying the tone to resemble friendly conversations between friends. It is ideal for applications related to conversational AI, natural language understanding, and… See the full description on the dataset page: https://huggingface.co/datasets/JaeJiMin/korean_chat_friendly.","first_N":5,"first_N_keywords":["question-answering","summarization","translation","Korean","mit"],"keywords_longer_than_N":true},
	{"name":"dali","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t大李老师问答数据集\\n\\t\\n\\n这个数据集包含大李老师的问答对话,用于训练对话模型。\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t数据集描述\\n\\t\\n\\n\\n格式: JSONL\\n字段: \\ninstruction: 固定值\\\"请大李老师回答\\\"\\ninput: 提问内容 \\noutput: 大李老师的回答\\n\\n\\n数据量: xxx条对话数据\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t使用示例\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"your-username/dataset-name\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t许可证\\n\\t\\n\\nApache 2.0\\n","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"chat","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"anekdots_dialogs","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/igorktech/anekdots_dialogs","creator_name":"Igor Kuzmin","creator_url":"https://huggingface.co/igorktech","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnekdots Dialogs Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Anekdots Dialogs Dataset is a collection of conversational-style dialogs derived from jokes in the original Anekdots Dataset. The dataset consists of dialogues segmented from jokes, allowing for humorous exchanges between multiple participants. It is well-suited for training conversational AI systems, especially those focusing on humor.\\nThe dialogues were automatically segmented using the gpt-4o-mini-2024-07-18 model.… See the full description on the dataset page: https://huggingface.co/datasets/igorktech/anekdots_dialogs.","first_N":5,"first_N_keywords":["text-generation","Russian","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"MiSC","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jihyoung/MiSC","creator_name":"Jihyoung Jang","creator_url":"https://huggingface.co/jihyoung","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMiSC\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMiSC is the first dataset designed to implement the concept of mixed-session conversations, where a main speaker interacts with different partners across multiple sessions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoad with Hugging Face Datasets\\n\\t\\n\\nYou can load the MiSC dataset using the Hugging Face Datasets library with the following code:\\nfrom datasets import load_dataset\\nmisc = load_dataset(\\\"jihyoung/MiSC\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe language of the MiSC dataset is… See the full description on the dataset page: https://huggingface.co/datasets/jihyoung/MiSC.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K<n<100K","arxiv:2410.02503","🇺🇸 Region: US"],"keywords_longer_than_N":true},
	{"name":"MiSC","keyword":"conversation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jihyoung/MiSC","creator_name":"Jihyoung Jang","creator_url":"https://huggingface.co/jihyoung","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMiSC\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMiSC is the first dataset designed to implement the concept of mixed-session conversations, where a main speaker interacts with different partners across multiple sessions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoad with Hugging Face Datasets\\n\\t\\n\\nYou can load the MiSC dataset using the Hugging Face Datasets library with the following code:\\nfrom datasets import load_dataset\\nmisc = load_dataset(\\\"jihyoung/MiSC\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe language of the MiSC dataset is… See the full description on the dataset page: https://huggingface.co/datasets/jihyoung/MiSC.","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K<n<100K","arxiv:2410.02503","🇺🇸 Region: US"],"keywords_longer_than_N":true},
	{"name":"EmoPropMan","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/basavaraj/EmoPropMan","creator_name":"Basavaraj","creator_url":"https://huggingface.co/basavaraj","description":"basavaraj/EmoPropMan dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\\nThe 2024-10-30 version contains:\\n\\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\\nThe 2024-10-30 version contains:\\n\\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ASK2","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/prabinpanta0/ASK2","creator_name":"Prabin Panta","creator_url":"https://huggingface.co/prabinpanta0","description":"prabinpanta0/ASK2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ASK2","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/prabinpanta0/ASK2","creator_name":"Prabin Panta","creator_url":"https://huggingface.co/prabinpanta0","description":"prabinpanta0/ASK2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"ASK2","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/prabinpanta0/ASK2","creator_name":"Prabin Panta","creator_url":"https://huggingface.co/prabinpanta0","description":"prabinpanta0/ASK2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"chatbot","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chatbot","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chatbot dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","question-answering","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"GUI-Ban","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wendellast/GUI-Ban","creator_name":"wendel alves","creator_url":"https://huggingface.co/wendellast","description":"wendellast/GUI-Ban dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","English","Portuguese","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Atma3.2-ShareGPT","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3.2-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma4","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"talking-to-chatbots-unwrapped-chats","keyword":"conversations","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats","creator_name":"David G. R.","creator_url":"https://huggingface.co/reddgr","description":"This work-in-progress dataset contains conversations with various LLM tools, sourced by the author of the website  Talking to Chatbots. \\nA simplified version of this dataset can be found at reddgr/talking-to-chatbots-chats, where messages belonging to a same conversation are 'wrapped' inside a single record. In this extended dataset, each conversation turn (pair of messages consisting of a user prompt and a response by the LLM) is presented as an individual record, with additional metrics and… See the full description on the dataset page: https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats.","first_N":5,"first_N_keywords":["English","Spanish","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"ChatCVE","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lgxz/ChatCVE","creator_name":"Kevin Leo","creator_url":"https://huggingface.co/lgxz","description":"Ref to https://huggingface.co/datasets/iamthierno/cvedataset.jsonl for more information \\n","first_N":5,"first_N_keywords":["table-question-answering","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"crosswoz-sft","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BruceNju/crosswoz-sft","creator_name":"zhongyah","creator_url":"https://huggingface.co/BruceNju","description":"multilinguality:  \\n- monolingual  \\n\\ndescription: |  \\n                          \\n    这是一个基于CrossWOZ数据集处理的对话数据集，专门用于大模型的监督微调（SFT）任务。  \\n    数据集包含多轮对话、用户目标、对话状态等信息，适合训练任务型对话系统。  \\n\\n    原始数据来源于CrossWOZ项目，经过专门的预处理使其更适合现代大模型训练。\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t核心特征：\\n\\t\\n\\n这是首个大规模的中文跨域任务型对话数据集\\n包含6,012个对话，102,000个话语，覆盖5个领域(酒店、餐厅、景点、地铁和出租车)\\n约60%的对话包含跨域用户目标\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t主要创新点：\\n\\t\\n\\n更具挑战性的域间依赖关系：\\n\\n一个领域的选择会动态影响其他相关领域的选择\\n例如用户选择的景点会影响后续酒店的推荐范围(需要在景点附近)\\n\\n完整的标注：\\n\\n同时提供用户端和系统端的对话状态标注\\n包含对话行为(dialogue acts)的标注\\n用户状态标注有助于追踪对话流程和建模用户行为… See the full description on the dataset page: https://huggingface.co/datasets/BruceNju/crosswoz-sft.","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"practical-dreamer-RPGPT_PublicDomain","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/practical-dreamer-RPGPT_PublicDomain","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\tPublic Domain Character RPG Dataset\\n\\t\\n\\nThis dataset is a reformatted version of practical-dreamer/RPGPT_PublicDomain-alpaca into a ShareGPT-like format.  Unfortunately, detailed information about the original dataset is scarce.\\nIn total, the dataset includes 3 032 conversations, with some extending up to 50 turns.\\nThe dataset consists of the following fields:\\n\\nconversations:  ShareGPT-like format representing the dialogue.\\nThe 'system' message provides a randomized introduction… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/practical-dreamer-RPGPT_PublicDomain.","first_N":5,"first_N_keywords":["text2text-generation","English","mit","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Atma3-ShareGPT","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Atma3-Share-GPT","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-Share-GPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"mauxi-mix-persian","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t🗣️ MauxiMix: High-Quality Persian Conversations Dataset 🇮🇷\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t📝 Description\\n\\t\\n\\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\\n🚧 Work in Progress:… See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian.","first_N":5,"first_N_keywords":["translation","text-generation","Persian","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset… See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\\nThe 2024-09-27 version contains:\\n\\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\\n\\n\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Supernova","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\\nThe 2024-09-27 version contains:\\n\\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\\n\\n\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"characterchats","keyword":"chat","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/EpGuy/characterchats","creator_name":"Ep Guy","creator_url":"https://huggingface.co/EpGuy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThis dataset provides characters talking in a chat format responding to a user or prompt.\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]… See the full description on the dataset page: https://huggingface.co/datasets/EpGuy/characterchats.","first_N":5,"first_N_keywords":["text-generation","English","wtfpl","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"characterchats","keyword":"roleplay","license":"\"Do What The F*ck You Want To Public License\"","license_url":"https://choosealicense.com/licenses/wtfpl/","language":"en","dataset_url":"https://huggingface.co/datasets/EpGuy/characterchats","creator_name":"Ep Guy","creator_url":"https://huggingface.co/EpGuy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThis dataset provides characters talking in a chat format responding to a user or prompt.\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]… See the full description on the dataset page: https://huggingface.co/datasets/EpGuy/characterchats.","first_N":5,"first_N_keywords":["text-generation","English","wtfpl","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Roleplay-Logs-Sharegpt-Ngram-cleaned","keyword":"roleplay","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Roleplay-Logs-Sharegpt-Ngram-cleaned","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"same as the previous but filtered \\\"what do you\\\" which was wayyyy too present\\n","first_N":5,"first_N_keywords":["text-generation","agpl-3.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"refactorchat","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BradMcDanel/refactorchat","creator_name":"Bradley McDanel","creator_url":"https://huggingface.co/BradMcDanel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tModel Details\\n\\t\\n\\n\\nDataset Name: RefactorChat\\nVersion: 1.0\\nDate: October 19, 2024\\nType: Multi-turn dialogue dataset for code refactoring and feature addition\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntended Use\\n\\t\\n\\n\\nPrimary Use: Evaluating and training large language models on incremental code development tasks\\nIntended Users: Researchers and practitioners in natural language processing and software engineering\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Composition\\n\\t\\n\\n\\nSize: 100 samples\\nStructure: Each… See the full description on the dataset page: https://huggingface.co/datasets/BradMcDanel/refactorchat.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","< 1K"],"keywords_longer_than_N":true},
	{"name":"OpenManus-RL","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for OpenManusRL\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n  💻 [Github Repo]\\n\\n\\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\\n\\n🔍 ReAct Framework - Reasoning-Acting integration\\n🧠 Structured Training - Separate format/reasoning learning\\n🚫 Anti-Hallucination - Negative samples + environment grounding\\n🌐 6 Domains - OS, DB, Web, KG, Household, E-commerce\\n\\n\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nSource… See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"\\n\\t\\n\\t\\t\\n\\t\\tFinance-Instruct-500k Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\\nThe dataset includes content tailored for financial… See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k.","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"ShareGPT52K","keyword":"conversation","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RyokoAI/ShareGPT52K","creator_name":"Ryoko AI","creator_url":"https://huggingface.co/RyokoAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages… See the full description on the dataset page: https://huggingface.co/datasets/RyokoAI/ShareGPT52K.","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"soda","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/soda","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for 🥤SODA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n🥤SODA is the first publicly available, million-scale, high-quality dialogue dataset covering a wide range of social interactions. Dialogues are distilled from a PLM (InstructGPT; Ouyang et al., 2022) by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than prior… See the full description on the dataset page: https://huggingface.co/datasets/allenai/soda.","first_N":5,"first_N_keywords":["dialogue-generation","machine-generated","monolingual","original","extended|Atomic10x"],"keywords_longer_than_N":true},
	{"name":"casino","keyword":"dialogue-modeling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kchawla123/casino","creator_name":"Kushal Chawla","creator_url":"https://huggingface.co/kchawla123","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Casino\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe provide a novel dataset (referred to as CaSiNo) of 1030 negotiation dialogues. Two participants take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. This design keeps the task tractable, while still facilitating linguistically rich and personal conversations. This helps to overcome the limitations of prior negotiation datasets… See the full description on the dataset page: https://huggingface.co/datasets/kchawla123/casino.","first_N":5,"first_N_keywords":["text-generation","fill-mask","dialogue-modeling","expert-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"kilt_tasks","keyword":"dialogue-modeling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/kilt_tasks","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for KILT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKILT has been built from 11 datasets representing 5 types of tasks:\\n\\nFact-checking\\nEntity linking\\nSlot filling\\nOpen domain QA\\nDialog generation\\n\\nAll these datasets have been grounded in a single pre-processed Wikipedia dump, allowing for fairer and more consistent evaluation as well as enabling new task setups such as multitask and transfer learning with minimal effort. KILT also provides tools to analyze and understand the… See the full description on the dataset page: https://huggingface.co/datasets/facebook/kilt_tasks.","first_N":5,"first_N_keywords":["fill-mask","question-answering","text-classification","text-generation","text-retrieval"],"keywords_longer_than_N":true},
	{"name":"prosocial-dialog","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/prosocial-dialog","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ProsocialDialog Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nProsocialDialog is the first large-scale multi-turn English dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative… See the full description on the dataset page: https://huggingface.co/datasets/allenai/prosocial-dialog.","first_N":5,"first_N_keywords":["text-classification","dialogue-generation","multi-class-classification","crowdsourced","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"gpt_roleplay_realm","keyword":"role-play","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/gpt_roleplay_realm","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGPT Role-play Realm Dataset: The AI-generated character compendium\\n\\t\\n\\nThis is a dataset of GPT-generated characters made to increase the ability of open-source language models to role-play.\\n\\n\\n\\n219 characters in the Russian part, and 216 characters in the English part. All character descriptions were generated with GPT-4.\\n20 dialogues on unique topics with every character. Topics were generated with GPT-4. The first dialogue out of 20 was also generated with GPT-4, and the other 19… See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/gpt_roleplay_realm.","first_N":5,"first_N_keywords":["text-generation","Russian","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"gpt_roleplay_realm","keyword":"roleplay","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/gpt_roleplay_realm","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGPT Role-play Realm Dataset: The AI-generated character compendium\\n\\t\\n\\nThis is a dataset of GPT-generated characters made to increase the ability of open-source language models to role-play.\\n\\n\\n\\n219 characters in the Russian part, and 216 characters in the English part. All character descriptions were generated with GPT-4.\\n20 dialogues on unique topics with every character. Topics were generated with GPT-4. The first dialogue out of 20 was also generated with GPT-4, and the other 19… See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/gpt_roleplay_realm.","first_N":5,"first_N_keywords":["text-generation","Russian","English","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Puffin","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LDJnr/Puffin","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Puffin dataset. Exactly 3,000 examples with each response created using GPT-4.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPLEASE USE THE NEWER VERSION OF PUFFIN CALLED PURE-DOVE, IT IS NO LONGER RECCOMENDED TO USE PUFFIN\\n\\t\\n\\n\\nComprised of over 2,000 multi-turn conversations between GPT-4 and real humans.\\n\\nAverage context length per conversation is over 1,000 tokens. (will measure this more accurately soon)\\n\\nAverage turns per conversation is more than 10. (will measure this more accurately… See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Puffin.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"customer-service-apple-picker-maintenance","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/customer-service-apple-picker-maintenance","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a technician and an expert on maintaining automated apple picker machines. Check out the example below:\\n\\\"id\\\": 1,\\n\\\"description\\\": \\\"Machine not picking apples\\\",\\n\\\"dialogue\\\": \\\"Technician: Hello, one of our apple picker machines is not picking apples. What should I do to fix it?\\\\n\\\\nExpert: Check the picking arms for any obstructions or damage. Clean or replace them if necessary. Also, ensure the collection basket is… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/customer-service-apple-picker-maintenance.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Pure-Dove","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LDJnr/Pure-Dove","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Pure-Dove dataset. Over 3K multi-turn examples, and many more coming soon!\\n\\t\\n\\nThis dataset aims to be the largest highest quality cluster of real human back and forth conversations with GPT-4.\\nSteps have even been done to ensure that only the best GPT-4 conversations in comparisons are kept, there are many instances where two GPT-4 responses are rated as equal to eachother or as both bad. We exclude all such responses from Pure Dove and make sure to only… See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Pure-Dove.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"C-Language-Chat-Debug-Multiturn-Zh","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Mxode/C-Language-Chat-Debug-Multiturn-Zh","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"约 1300 条 C 语言 场景的 user - assistant 多轮对话。每段对话已经组织成了单行的格式。一条样例如下：\\n{\\n    \\\"id\\\": 1045,\\n    \\\"conversation\\\": [\\n        {\\n            \\\"user\\\": \\\"你好，AI助手。我最近在写一个C语言程序，但是遇到了一些问题，希望你能帮我检查一下。\\\",\\n            \\\"assistant\\\": \\\"你好，我很乐意帮助你。请把你的代码发给我，我会尽快检查并给出建议。\\\"\\n        },\\n        {\\n            \\\"user\\\": \\\"好的，这是我的代码。这段代码的主要功能是计算斐波那契数列的前n项。\\\",\\n            \\\"assistant\\\": \\\"让我看一下......嗯，这里有一个小错误。在第10行，你应该使用`++i`而不是`i++`来递增i的值。修改后的代码应该是这样的\\\\\\\\n```c\\\\\\\\nfor (int i = 0; i < n; ++i) {\\\\\\\\n    if (i == 0 || i == 1) {\\\\\\\\n… See the full description on the dataset page: https://huggingface.co/datasets/Mxode/C-Language-Chat-Debug-Multiturn-Zh.","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"roleplay","keyword":"roleplay","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hieunguyenminh/roleplay","creator_name":"Leo","creator_url":"https://huggingface.co/hieunguyenminh","description":" 🎭 Roleplay TTL\\n\\n    \\n\\n\\nLet AI be any characters you want to play with!\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\nThis dataset trains conversational AI to embody a wide range of original characters, each with a unique persona. It includes fictional characters, complete with their own backgrounds, core traits, relationships, goals, and distinct speaking styles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nCurated by: Hieu Minh Nguyen\\nLanguage(s) (NLP): Primarily English (with potential for multilingual extensions)… See the full description on the dataset page: https://huggingface.co/datasets/hieunguyenminh/roleplay.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pairs Dutch\\n\\t\\n\\n\\nI recommend using the cleaned, deduplicated version. https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024}… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"philosophy_dialogue","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Hypersniper/philosophy_dialogue","creator_name":"Hypersniper","creator_url":"https://huggingface.co/Hypersniper","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPhilosophy Dialogue Processed with GPT-4\\n\\t\\n\\nSupport this project on Ko-fi\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProject Overview\\n\\t\\n\\nThis project involves processing personal questions through GPT-4 in the style of the philosopher Socrates.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrompt Structure\\n\\t\\n\\nThe following prompt was used to guide GPT-4's responses:\\n\\n\\\"You are the philosopher Socrates. You are asked about the nature of knowledge and virtue. Respond with your thoughts, reflecting Socrates' beliefs and wisdom.\\\"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGoal… See the full description on the dataset page: https://huggingface.co/datasets/Hypersniper/philosophy_dialogue.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Capybara","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LDJnr/Capybara","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\\n\\t\\n\\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\\nThe single-turn seeds used to initiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness… See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Capybara.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"CornellMovieDialogCorpus","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/spawn99/CornellMovieDialogCorpus","creator_name":"Cavit Erginsoy","creator_url":"https://huggingface.co/spawn99","description":"Cornell Movie-Dialogs Corpus\\nDistributed together with:\\n\\\"Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs\\\"\\nCristian Danescu-Niculescu-Mizil and Lillian Lee\\nProceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011.\\n(this paper is included in this zip file)\\nNOTE: If you have results to report on these corpora, please send email to cristian@cs.cornell.edu or llee@cs.cornell.edu so we can add you to… See the full description on the dataset page: https://huggingface.co/datasets/spawn99/CornellMovieDialogCorpus.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"CornellMovieDialogCorpus","keyword":"dialog","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/spawn99/CornellMovieDialogCorpus","creator_name":"Cavit Erginsoy","creator_url":"https://huggingface.co/spawn99","description":"Cornell Movie-Dialogs Corpus\\nDistributed together with:\\n\\\"Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs\\\"\\nCristian Danescu-Niculescu-Mizil and Lillian Lee\\nProceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011.\\n(this paper is included in this zip file)\\nNOTE: If you have results to report on these corpora, please send email to cristian@cs.cornell.edu or llee@cs.cornell.edu so we can add you to… See the full description on the dataset page: https://huggingface.co/datasets/spawn99/CornellMovieDialogCorpus.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-DPO 7K binarized\\n\\t\\n\\n\\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy?\\n\\t\\n\\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are… See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"banking-conversation-corpus","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talkmap/banking-conversation-corpus","creator_name":"Talkmap","creator_url":"https://huggingface.co/talkmap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBanking 300k Dataset Overview\\n\\t\\n\\nThis dataset consists of 300,000 synthetically generated conversations in a customer service setting for the telecom industry. There are two speakers: a customer, and an agent.\\n","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"telecom-conversation-corpus","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/talkmap/telecom-conversation-corpus","creator_name":"Talkmap","creator_url":"https://huggingface.co/talkmap","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTelecom 200k Dataset Overview\\n\\t\\n\\nThis dataset consists of 200,000 synthetically generated conversations in a customer service setting for the telecom industry. There are two speakers: a customer, and an agent.\\n","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"gooftagoo","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/adi-kmt/gooftagoo","creator_name":"Adithya Kamath","creator_url":"https://huggingface.co/adi-kmt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHindi/Hinglish Conversation Dataset\\n\\t\\n\\nThis repository contains a dataset of conversational text in conversational hindi and hinglish(a mix of Hindi and English languages).\\nThe Conversation Dataset contains multi-turn conversations on multiple topics usually revolving around daily real-life experiences. \\nA small amount of reasoning tasks have also been added (specifically COT style reasoning and coding) with about 1k samples from Openhermes 2.5.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCaution\\n\\t\\n\\nThis dataset… See the full description on the dataset page: https://huggingface.co/datasets/adi-kmt/gooftagoo.","first_N":5,"first_N_keywords":["text-generation","Hindi","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ultra_feedback_dutch_cleaned","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltra Feedback Dutch Cleaned\\n\\t\\n\\nThis is a cleaned version of BramVanroy/ultra_feedback_dutch, based on the cleaning done by Argilla on the original Ultra Feedback dataset. Another difference is that we only include GEITje 7B Ultra and GPT-4-Turbo. GEITje chat, which was used in the original dataset, is not used.\\nAfter cleaning I also generated replies for other models (like TowerInstruct, Mistral), but the results were too poor (in Dutch) to include so we only kept the GEITje Ultra… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch_cleaned.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","Dutch","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"empathetic_dialogues_llm","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Estwld/empathetic_dialogues_llm","creator_name":"zhangyiqun","creator_url":"https://huggingface.co/Estwld","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEmpathetic Dialogues for LLM\\n\\t\\n\\n﻿\\nThis repository contains a reformatted version of the Empathetic Dialogues dataset, tailored for seamless integration with Language Model (LLM) training and inference. The original dataset's format posed challenges for direct application in LLM tasks, prompting us to restructure and clean the data.\\n﻿\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Restructuring\\n\\t\\n\\n﻿\\nWe have implemented the following changes to enhance the dataset's usability:\\n﻿\\n\\nMerged dialogues with the same… See the full description on the dataset page: https://huggingface.co/datasets/Estwld/empathetic_dialogues_llm.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pairs Dutch Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Chinese-Roleplay-SingleTurn","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LooksJuicy/Chinese-Roleplay-SingleTurn","creator_name":"LooksJuicy","creator_url":"https://huggingface.co/LooksJuicy","description":"请注意，个人模型经过characterEval的reward model进行DPO训练，因此使用本数据集进行SFT的模型在该榜单上会存在bias，导致分数异常偏高，请勿直接使用该榜单进行测试\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t简介\\n\\t\\n\\n因已找到更优数据合成方案，为填充中文角色扮演数据集的空白，现开源部分中文角色扮演单轮对话数据集。\\n使用Refined-Anime-Text作为system prompt，使用小黄鸡随机query作为输入，调用个人角色扮演模型作为输出。\\n已处理为alpaca数据格式，方便大家处理和训练。经过验证，仅使用该数据集进行Lora微调即可获取一个效果还不错的模型~\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tchatGPT对比\\n\\t\\n\\n\\n\\t\\n\\t\\t\\ncharacter\\nquestion\\nanswer_us\\nanswer_chatGPT… See the full description on the dataset page: https://huggingface.co/datasets/LooksJuicy/Chinese-Roleplay-SingleTurn.","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"mathdial","keyword":"dialog","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eth-nlped/mathdial","creator_name":"Language, Reasoning and Education lab","creator_url":"https://huggingface.co/eth-nlped","description":"\\n\\t\\n\\t\\t\\n\\t\\tMathdial dataset\\n\\t\\n\\nhttps://arxiv.org/abs/2305.14536\\nMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems.\\nMathDial is grounded in math word problems as well as student confusions which provide a challenging testbed for creating faithful and equitable dialogue tutoring models able to reason over complex information. Current models achieve high accuracy in solving such problems but they fail in the task of teaching.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData… See the full description on the dataset page: https://huggingface.co/datasets/eth-nlped/mathdial.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"mathdial","keyword":"conversation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eth-nlped/mathdial","creator_name":"Language, Reasoning and Education lab","creator_url":"https://huggingface.co/eth-nlped","description":"\\n\\t\\n\\t\\t\\n\\t\\tMathdial dataset\\n\\t\\n\\nhttps://arxiv.org/abs/2305.14536\\nMathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems.\\nMathDial is grounded in math word problems as well as student confusions which provide a challenging testbed for creating faithful and equitable dialogue tutoring models able to reason over complex information. Current models achieve high accuracy in solving such problems but they fail in the task of teaching.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData… See the full description on the dataset page: https://huggingface.co/datasets/eth-nlped/mathdial.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"ru-instruct","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tКарточка датасета\\n\\t\\n\\nСкомбинирован из нескольких популярных датасетов, переведённых автоматически. Отфильтрован на предмет артефактов перевода (спасибо модели Den4ikAI/nonsense_gibberish_detector). Дедуплицирован SimHash'ом.\\nОбученной на нём модели пока не завёз, in progress.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tСостав\\n\\t\\n\\nСобрал из этих переведённых:\\n\\nd0rj/OpenOrca-ru (от Open-Orca/OpenOrca)\\nd0rj/OpenHermes-2.5-ru (от teknium/OpenHermes-2.5)\\nd0rj/dolphin-ru (от ehartford/dolphin)\\nd0rj/alpaca-cleaned-ru (от… See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","machine-generated","found","translated"],"keywords_longer_than_N":true},
	{"name":"honkai_impact_3rd_chinese_dialogue_corpus","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/honkai_impact_3rd_chinese_dialogue_corpus","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t崩坏三游戏剧情语料\\n\\t\\n\\n总计 92,421 句剧情对白（带有角色标签）+旁白，从崩坏3的“主线1黄昏、少女、战舰”到“主线第二部03间章：一个梦游者的苦痛”\\n本数据集从 honkai_impact_3rd_game_playthrough 视频数据集出发，经过 AI pipeline 最终获取结构化的文本剧情语料。\\nAI pipeline 概述如下：\\n\\n分P下载视频（使用 BBDown 下载 BiliBili崩三剧情视频）\\n视频帧分割（每1秒取一帧画面）\\n逐帧 OCR 检测文本（使用 Paddle-OCR）\\n逐帧 VLM 结构化解析（使用 MiniCPM-V-2_6，输入为帧图像 + OCR结果，输出为结构化 JSON）\\n基于规则的后处理\\n规范化 VLM 输出（e.g., 去噪、排除格式有问题的输出）\\n中间帧的信息去重与归并（e.g.… See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/honkai_impact_3rd_chinese_dialogue_corpus.","first_N":5,"first_N_keywords":["Chinese","apache-2.0","10K<n<100K","🇺🇸 Region: US","game"],"keywords_longer_than_N":true},
	{"name":"Toxic-All","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ystemsrx/Toxic-All","creator_name":"Sixteen","creator_url":"https://huggingface.co/ystemsrx","description":"中文\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDecentralized Datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis project includes four decentralized datasets: two in DPO format (dpo-unbiased1.json, dpo-unbiased2.json) and two in Alpaca format (alpaca-unbiased1.json, alpaca-unbiased2.json). These datasets were curated and reformatted from various open-source projects to support the development and training of decentralized models capable of handling a wide range of topics, including sensitive or controversial issues.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/ystemsrx/Toxic-All.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","Chinese","English","mit"],"keywords_longer_than_N":true},
	{"name":"PersReFex","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ZinengTang/PersReFex","creator_name":"Zineng Tang","creator_url":"https://huggingface.co/ZinengTang","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Multi-Agent Referential Communication Dataset\\n\\t\\n\\n\\n\\n\\nExample scene showing the speaker (left) and listener (right) views.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains spatial dialogue data for multi-agent referential communication tasks in 3D environments. It includes pairs of images showing speaker and listener views within photorealistic indoor scenes, along with natural language descriptions of target object locations.\\nThe key… See the full description on the dataset page: https://huggingface.co/datasets/ZinengTang/PersReFex.","first_N":5,"first_N_keywords":["image-to-text","visual-question-answering","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Context-Based-Chat-Summary-Plus","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Context-Based-Chat-Summary-Plus","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"prithivMLmods/Context-Based-Chat-Summary-Plus dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"everyday-conversations-ita","keyword":"conversations","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReDiX/everyday-conversations-ita","creator_name":"ReDiX Labs","creator_url":"https://huggingface.co/ReDiX","description":"\\n    \\n      \\n    \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t🇮🇹💬 Everyday Italian Conversations\\n\\t\\n\\nInspired by the dataset HuggingFaceTB/everyday-conversations-llama3.1-2k, we generated conversations using the same topics, subtopics, and sub-subtopics as those in the HuggingFaceTB dataset.We slightly adjusted the prompt to produce structured data outputs using Qwen/Qwen2.5-7B-Instruct. Subsequently, we also used the \\\"user\\\" role messages as prompts for google/gemma-2-9b-it.  \\nThe result is a dataset of approximately… See the full description on the dataset page: https://huggingface.co/datasets/ReDiX/everyday-conversations-ita.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Italian","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ATC-ShareGPT","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATC-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"everyday-conversations-tur","keyword":"conversations","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SoAp9035/everyday-conversations-tur","creator_name":"Ahmet Burhan Kayalı","creator_url":"https://huggingface.co/SoAp9035","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEveryday Turkish Conversations\\n\\t\\n\\nThis dataset has everyday conversations in Turkish between user and assistant on various topics. It is inspired by the HuggingFaceTB/everyday-conversations-llama3.1-2k.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is released under the Apache 2.0 License.\\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Turkish","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Chinese_Multi-Emotion_Dialogue_Dataset","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset","creator_name":"Johnson","creator_url":"https://huggingface.co/Johnson8187","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChinese_Multi-Emotion_Dialogue_Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t📄 Description\\n\\t\\n\\nThis dataset contains 4159 Chinese dialogues annotated with 8 distinct emotion categories. The data is suitable for emotion recognition, sentiment analysis, and other NLP tasks involving Chinese text.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Sources:\\n\\t\\n\\n\\nDaily Conversations: Captured from natural, informal human conversations.\\nMovie Dialogues: Extracted from diverse Chinese-language movies.\\nAI-Generated Dialogues: Synthesized using… See the full description on the dataset page: https://huggingface.co/datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset.","first_N":5,"first_N_keywords":["text-classification","text-generation","fill-mask","Chinese","mit"],"keywords_longer_than_N":true},
	{"name":"PersianSyntheticQA","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParsBench/PersianSyntheticQA","creator_name":"ParsBench","creator_url":"https://huggingface.co/ParsBench","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPersian Synthetic QA Dataset\\n\\t\\n\\nPersian Synthetic QA is a dataset containing 100,000 synthetic questions and answers in Persian, generated using GPT-4o. The dataset is structured as conversations between a user and an assistant, with 2,000 records for each of the 50 different topics. Each conversation consists of messages with two distinct roles: \\\"user\\\" messages containing questions in Persian, and \\\"assistant\\\" messages containing the corresponding answers. The dataset is designed… See the full description on the dataset page: https://huggingface.co/datasets/ParsBench/PersianSyntheticQA.","first_N":5,"first_N_keywords":["question-answering","Persian","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"role-play-chinese","keyword":"role-play","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Johnson8187/role-play-chinese","creator_name":"Johnson","creator_url":"https://huggingface.co/Johnson8187","description":"繁體中文   English\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRole-Play Chinese Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t簡介\\n\\t\\n\\n這是一個專為角色扮演對話設計的中文數據集，數據由 AI 生成，適用於訓練和評估自然語言處理（NLP）模型，特別是對話生成和角色扮演相關的任務。數據集以 Alpha 格式 儲存，方便進行微調和進一步的模型訓練。數據集包含多種場景和角色設定，能夠幫助模型學習如何在不同的情境下生成符合角色性格和背景的對話。\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t數據集結構\\n\\t\\n\\n數據以Alpha格式儲存方便微調，包含以下字段：\\n\\ninstruction: 任務指令，描述模型需要完成的任務。\\ninput: 輸入內容，包含場景描述、過去的對話以及當前對話的上下文。\\noutput: 期望的模型輸出，即符合角色設定的回應。\\nsystem: 角色設定和背景故事，幫助模型理解角色的性格和行為模式。\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t範例\\n\\t\\n\\n{\\n  \\\"instruction\\\": \\\"在給定的場景中，請根據角色設定回應對話。\\\",\\n  \\\"input\\\":… See the full description on the dataset page: https://huggingface.co/datasets/Johnson8187/role-play-chinese.","first_N":5,"first_N_keywords":["text-generation","Chinese","mit","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"allenai-soda","keyword":"conversational","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/allenai-soda","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSODA chat dataset\\n\\t\\n\\nThis is the SODA dataset in ShareGPT-like format.\\nAccording to the dataset's creators: \\\"🥤SODA is the first publicly available, million-scale, high-quality dialogue dataset covering a wide range of social interactions.\\\"\\nThe following changes were made to the data:\\n\\nkept only dialogues with two people alternating turns\\nthe SODA narrative was adapted into a system prompt for an AI to roleplay as the second person\\nextra turns were removed so that each conversation… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/allenai-soda.","first_N":5,"first_N_keywords":["feature-extraction","text-generation","text2text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"allenai-soda","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/allenai-soda","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSODA chat dataset\\n\\t\\n\\nThis is the SODA dataset in ShareGPT-like format.\\nAccording to the dataset's creators: \\\"🥤SODA is the first publicly available, million-scale, high-quality dialogue dataset covering a wide range of social interactions.\\\"\\nThe following changes were made to the data:\\n\\nkept only dialogues with two people alternating turns\\nthe SODA narrative was adapted into a system prompt for an AI to roleplay as the second person\\nextra turns were removed so that each conversation… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/allenai-soda.","first_N":5,"first_N_keywords":["feature-extraction","text-generation","text2text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"allenai-soda","keyword":"roleplay","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/allenai-soda","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSODA chat dataset\\n\\t\\n\\nThis is the SODA dataset in ShareGPT-like format.\\nAccording to the dataset's creators: \\\"🥤SODA is the first publicly available, million-scale, high-quality dialogue dataset covering a wide range of social interactions.\\\"\\nThe following changes were made to the data:\\n\\nkept only dialogues with two people alternating turns\\nthe SODA narrative was adapted into a system prompt for an AI to roleplay as the second person\\nextra turns were removed so that each conversation… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/allenai-soda.","first_N":5,"first_N_keywords":["feature-extraction","text-generation","text2text-generation","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"lots_of_datasets_for_ai_v3","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3","creator_name":"Gurvaah Singh","creator_url":"https://huggingface.co/ReallyFloppyPenguin","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset is for Training LLMs From Scratch!\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information Needed]\\nPaper [optional]: [More Information Needed]\\nDemo… See the full description on the dataset page: https://huggingface.co/datasets/ReallyFloppyPenguin/lots_of_datasets_for_ai_v3.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"conversational-ai","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"degeneration-html-multilingual","keyword":"conversational","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual.","first_N":5,"first_N_keywords":["translation","text2text-generation","text-generation","text-classification","token-classification"],"keywords_longer_than_N":true},
	{"name":"lex-fridman-podcast","keyword":"conversations","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Aditya0619/lex-fridman-podcast","creator_name":"Aditya Channa","creator_url":"https://huggingface.co/Aditya0619","description":"\\n\\t\\n\\t\\t\\n\\t\\tLex Fridman Podcast Conversations Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains transcriptions of conversations from the Lex Fridman Podcast, featuring in-depth discussions on artificial intelligence, science, technology, philosophy, and more. The dataset includes 441 transcribed episodes, covering most of the podcast episodes up to January 2025 (excluding 10 episodes).\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nTitle: String - The title of the podcast episode… See the full description on the dataset page: https://huggingface.co/datasets/Aditya0619/lex-fridman-podcast.","first_N":5,"first_N_keywords":["text-generation","question-answering","summarization","English","mit"],"keywords_longer_than_N":true},
	{"name":"OpenCharacter","keyword":"dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xywang1/OpenCharacter","creator_name":"Xiaoyang Wang","creator_url":"https://huggingface.co/xywang1","description":"\\n\\t\\n\\t\\t\\n\\t\\tOpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas\\n\\t\\n\\nThis repo releases data introduced in our paper OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas in arXiv.\\n\\nWe study customizable role-playing dialogue agents in large language models (LLMs).\\nWe tackle the challenge with large-scale data synthesis: character synthesis and character-driven reponse synthesis.\\nOur solution strengthens the original LLaMA-3… See the full description on the dataset page: https://huggingface.co/datasets/xywang1/OpenCharacter.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"r1-reasoning-tr","keyword":"conversations","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SoAp9035/r1-reasoning-tr","creator_name":"Ahmet Burhan Kayalı","creator_url":"https://huggingface.co/SoAp9035","description":"\\n\\t\\n\\t\\t\\n\\t\\tR1 Reasoning TR\\n\\t\\n\\nThis is an R1 reasoning dataset translated into Turkish, containing conversations between users and assistants. Thanks to lightblue for the dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is released under the Apache 2.0 License.\\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Turkish","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sharegpt-structured-output-json","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Arun63/sharegpt-structured-output-json","creator_name":"v","creator_url":"https://huggingface.co/Arun63","description":"\\n\\t\\n\\t\\t\\n\\t\\tShareGPT-Formatted Dataset for Structured JSON Output\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is formatted in the ShareGPT style and is designed for fine-tuning large language models (LLMs) to generate structured JSON outputs. It consists of multi-turn conversations where each response follows a predefined JSON schema, making it ideal for training models that need to produce structured data in natural language scenarios.\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset can be used to train LLMs… See the full description on the dataset page: https://huggingface.co/datasets/Arun63/sharegpt-structured-output-json.","first_N":5,"first_N_keywords":["text-generation","conversational","apache-2.0","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"RoleMRC","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Junrulu/RoleMRC","creator_name":"Junrulu","creator_url":"https://huggingface.co/Junrulu","description":"\\n\\t\\n\\t\\t\\n\\t\\tRoleMRC (A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following)\\n\\t\\n\\nCheck our paper and codes.\\n\\n\\t\\n\\t\\t\\n\\t\\tTable of data contents (task name in data vs. actual meaning)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tFree Chats\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t1.1 Free Chats (5k)\\n\\t\\n\\n\\n'role_related_dialogue---role_first---no_narration---answer': role starts, no narration\\n'role_related_dialogue---role_first---with_narration---answer': role starts, could have narration… See the full description on the dataset page: https://huggingface.co/datasets/Junrulu/RoleMRC.","first_N":5,"first_N_keywords":["text-generation","English","mit","arxiv:2502.11387","🇺🇸 Region: US"],"keywords_longer_than_N":true},
	{"name":"iloveuser-1k","keyword":"roleplay","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/XeTute/iloveuser-1k","creator_name":"XeTute Technologies","creator_url":"https://huggingface.co/XeTute","description":"\\n\\n  \\n\\n\\n  💘 I love user 1k\\n  Generated using XeTute/Synthetic-Data-Generation\\n\\n\\n\\n\\n  \\n    \\n  \\n  \\n    \\n    \\n  \\n  \\n    \\n  \\n\\n\\n\\n\\nA tiny dataset with 1024 input-output pairs (Alpaca format) designed to remove allat \\\"Since I'm an AI assistant, I don't feel emotions\\\" slop; really kills the vibe 💔May contain noise or not, it's synthetically generated using TypoRPV2 and a lot of heavy prompting; so keep your learning rate (or alpha if u do LoRA) something low and couple in some other datasets ;)  \\n","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"LimaRP-augmented-ja-karakuri","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aratako/LimaRP-augmented-ja-karakuri","creator_name":"Aratako","creator_url":"https://huggingface.co/Aratako","description":"\\n\\t\\n\\t\\t\\n\\t\\tLimaRP-augmented-ja-karakuri\\n\\t\\n\\ngrimulkan/LimaRP-augmentedを、GENIAC-Team-Ozaki/karakuri-lm-8x7b-chat-v0.1-awqを用いて日本語に翻訳したロールプレイ学習用データセットです。\\nLLMの推論にはDeepInfraというサービスを使いました。\\n\\n\\t\\n\\t\\t\\n\\t\\t翻訳の詳細\\n\\t\\n\\n\\n3-shots promptingでの翻訳\\nmistralのtokenizerで出力が8000トークンを超えるまで翻訳\\n元データセットにある非常に長い対話は上記条件で途中のターンで翻訳を終了しています。\\n\\n\\nLLM特有の同じ出力が繰り返される現象に遭遇した場合、その時点で該当レコードの翻訳を終了\\nこの結果1ターン未満となったレコード（33件）を削除\\n\\n\\n\\n","first_N":5,"first_N_keywords":["text-generation","Japanese","apache-2.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"NSFW_Chat_Dataset","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","description":"\\n\\t\\n\\t\\t\\n\\t\\t💕 Spicy AI GF Chat Dataset 🔥\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t🚨 18+ Only! NSFW & Spicy Content Ahead 🚨\\n\\t\\n\\nHey there, AI enthusiasts and romance lovers! 😏 Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! 💖 If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\\n\\n\\t\\n\\t\\t\\n\\t\\t📜 What’s Inside?\\n\\t\\n\\nThis dataset features two columns:\\n\\ninput → Boyfriend’s dialogue (aka what YOU say 😉)\\noutput →… See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"cs_restaurants","keyword":"dialogue-modeling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/community-datasets/cs_restaurants","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Czech Restaurant\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as a translation of the English San Francisco Restaurants dataset by Wen et al. (2015). The domain is restaurant information in Prague, with random/fictional values. It includes input dialogue acts and the corresponding outputs in Czech.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards… See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/cs_restaurants.","first_N":5,"first_N_keywords":["text2text-generation","text-generation","fill-mask","dialogue-modeling","language-modeling"],"keywords_longer_than_N":true},
	{"name":"FaithDial","keyword":"dialogue-modeling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/McGill-NLP/FaithDial","creator_name":"McGill NLP Group","creator_url":"https://huggingface.co/McGill-NLP","description":"FaithDial is a new benchmark for hallucination-free dialogues, created by manually editing hallucinated and uncooperative responses in Wizard of Wikipedia.","first_N":5,"first_N_keywords":["text-generation","dialogue-modeling","crowdsourced","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"ludwig","keyword":"conversation","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/ludwig","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"TODO","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","expert-generated"],"keywords_longer_than_N":true},
	{"name":"ludwig","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UCL-DARK/ludwig","creator_name":"UCL DARK","creator_url":"https://huggingface.co/UCL-DARK","description":"TODO","first_N":5,"first_N_keywords":["text-generation","fill-mask","language-modeling","masked-language-modeling","expert-generated"],"keywords_longer_than_N":true},
	{"name":"HC3-textgen-qa","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pszemraj/HC3-textgen-qa","creator_name":"Peter Szemraj","creator_url":"https://huggingface.co/pszemraj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHC3-textgen-qa\\n\\t\\n\\n\\nthe Hello-SimpleAI/HC3 reformatted for textgen\\nspecial tokens for question/answer, see dataset preview\\n\\n","first_N":5,"first_N_keywords":["text-generation","Hello-SimpleAI/HC3","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"WarOnline","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kertser/WarOnline","creator_name":"Mike Kertser","creator_url":"https://huggingface.co/kertser","description":"This is a conversational dataset, collected from WarOnine Israeli military forum\\nLanguage = Russian (with hebrew addins)\\nTarget Audience = Military\\nDataset has been used to train a Military Chat Bot\\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"ubuntu_dialogue_qa","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sedthh/ubuntu_dialogue_qa","creator_name":"Richard Nagyfi","creator_url":"https://huggingface.co/sedthh","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"ubuntu_dialogue_qa\\\"\\n\\t\\n\\nFiltered the Ubuntu dialogue chatlogs from https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus to include Q&A pairs ONLY\\nAcknowledgements\\nThis dataset was ORIGINALLY collected by Ryan Lowe, Nissan Pow , Iulian V. Serban† and Joelle Pineau. It is made available here under the Apache License, 2.0. If you use this data in your work, please include the following citation:\\nRyan Lowe, Nissan Pow, Iulian V. Serban and Joelle Pineau, \\\"The… See the full description on the dataset page: https://huggingface.co/datasets/sedthh/ubuntu_dialogue_qa.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"OpenCaselist","keyword":"debate","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Yusuf5/OpenCaselist","creator_name":"Yusuf 5","creator_url":"https://huggingface.co/Yusuf5","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for OpenCaselist\\n\\t\\n\\n\\n\\nA collection of Evidence used in Collegiate and High School debate competitions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThis dataset is a follow up to DebateSum, increasing its scope and amount of metadata collected.\\nIt expands the dataset to include evidence used during debate tournaments, rather than just evidence produced during preseason debate \\\"camps.\\\" The total amount of evidence is approximately 20x larger than… See the full description on the dataset page: https://huggingface.co/datasets/Yusuf5/OpenCaselist.","first_N":5,"first_N_keywords":["text-generation","summarization","question-answering","text-classification","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"OpenCaselist","keyword":"argument","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Yusuf5/OpenCaselist","creator_name":"Yusuf 5","creator_url":"https://huggingface.co/Yusuf5","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for OpenCaselist\\n\\t\\n\\n\\n\\nA collection of Evidence used in Collegiate and High School debate competitions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nThis dataset is a follow up to DebateSum, increasing its scope and amount of metadata collected.\\nIt expands the dataset to include evidence used during debate tournaments, rather than just evidence produced during preseason debate \\\"camps.\\\" The total amount of evidence is approximately 20x larger than… See the full description on the dataset page: https://huggingface.co/datasets/Yusuf5/OpenCaselist.","first_N":5,"first_N_keywords":["text-generation","summarization","question-answering","text-classification","feature-extraction"],"keywords_longer_than_N":true},
	{"name":"stackoverflow-chat-dutch","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/stackoverflow-chat-dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Stack Overflow Chat Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 56,964 conversations between een AI assistant and a (fake) \\\"Human\\\" (generated) in Dutch, specifically in the domain of programming (Stack Overflow). They are translations of Baize's machine-generated answers to the Stack Overflow dataset. \\n☕ Want to help me out? Translating the data with the OpenAI API, and prompt testing, cost me 💸$133.60💸. If you like this dataset, please consider… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/stackoverflow-chat-dutch.","first_N":5,"first_N_keywords":["question-answering","text-generation","Dutch","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"atsiftu-dialogue","keyword":"dialogue","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kabachuha/atsiftu-dialogue","creator_name":"Artem","creator_url":"https://huggingface.co/kabachuha","description":"The dialogue pairs from Wesnoth add-on campanies IftU/AtS.\\n","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","gpl-2.0","1K<n<10K"],"keywords_longer_than_N":true},
	{"name":"prosocial-dialog-filtered","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Englishman2022/prosocial-dialog-filtered","creator_name":"Josh Oliver","creator_url":"https://huggingface.co/Englishman2022","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nProsocialDialogFiltered is a filtered version of the ProsocialDialog dataset.\\nMultiple versions are present:\\n\\nIn train_no_casual, rows with the label \\\"casual\\\" have been filtered out as a starting point.\\nIn train_no_possibly, rows with \\\"possibly needs caution\\\" have been filtered out.\\nIn train_no_probably, rows with \\\"probably needs caution\\\" have been filtered out, as I found those to be largely pointless as well, leaving only \\\"needs caution\\\" and \\\"needs… See the full description on the dataset page: https://huggingface.co/datasets/Englishman2022/prosocial-dialog-filtered.","first_N":5,"first_N_keywords":["text-classification","dialogue-generation","multi-class-classification","crowdsourced","machine-generated"],"keywords_longer_than_N":true},
	{"name":"ShareGPT-Processed","keyword":"conversation","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zetavg/ShareGPT-Processed","creator_name":"Pokai Chang","creator_url":"https://huggingface.co/zetavg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tShareGPT-Processed\\n\\t\\n\\nThe RyokoAI/ShareGPT52K dataset, converted to Markdown and labeled with the language used.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgements\\n\\t\\n\\n\\nvinta/pangu.js — To insert whitespace between CJK (Chinese, Japanese, Korean) and half-width characters (alphabetical letters, numerical digits and symbols).\\nmatthewwithanm/python-markdownify — Provides a starting point to convert HTML to Markdown.\\nBYVoid/OpenCC — Conversions between Traditional Chinese and Simplified Chinese.… See the full description on the dataset page: https://huggingface.co/datasets/zetavg/ShareGPT-Processed.","first_N":5,"first_N_keywords":["text-generation","English","Chinese","Spanish","Japanese"],"keywords_longer_than_N":true},
	{"name":"conversation_ender","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Chakshu/conversation_ender","creator_name":"Chakshu Gautam","creator_url":"https://huggingface.co/Chakshu","description":"Conversation Ending Check\\n","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"ConvMix","keyword":"conversational","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pchristm/ConvMix","creator_name":"Philipp Christmann","creator_url":"https://huggingface.co/pchristm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ConvMix\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe construct and release the first benchmark, ConvMix, for conversational question answering (ConvQA) over heterogeneous sources, comprising 3000 real-user conversations with 16000 questions, along with entity annotations, completed question utterances, and question paraphrases.\\nThe dataset naturally requires information from multiple sources for answering the individual questions in the conversations.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/pchristm/ConvMix.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","1K - 10K","Tabular"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset","creator_name":"Nicholas Kluge Corrêa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of prompts and responses to those prompts. All completions were generated by querying already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc.). The dataset is available in Portuguese, English, and Spanish.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:\\n\\nLanguage modeling.… See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","Spanish","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"academia-physics-office-hours","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/academia-physics-office-hours","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a physics professor and a student during office hours. Check out the example below:\\n\\\"id\\\":1,\\n\\\"description\\\":\\\"Understanding the concept of velocity\\\",\\n\\\"dialogue\\\":\\\"Student: Professor, I'm having trouble understanding the concept of velocity. Could you please explain it to me?\\\\n\\\\nProfessor: Of course! Velocity is a fundamental concept in physics that describes the rate of change of an object's position with respect to… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/academia-physics-office-hours.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"customer-service-grocery-cashier","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/customer-service-grocery-cashier","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a customer at a grocery store and the cashier. Check out the example below:\\n\\\"id\\\": 1,\\n\\\"description\\\": \\\"Price inquiry\\\",\\n\\\"dialogue\\\": \\\"Customer: Excuse me, could you tell me the price of the apples per pound? Cashier: Certainly! The price for the apples is $1.99 per pound.\\\"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Load Dialogues\\n\\t\\n\\nLoading dialogues can be accomplished using the fun dialogues library or Hugging Face datasets library.… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/customer-service-grocery-cashier.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"healthcare-minor-consultation","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/healthcare-minor-consultation","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a doctor and a patient during a minor medical consultation.. Check out the example below:\\n\\\"id\\\": 1,\\n\\\"description\\\": \\\"Discussion about a common cold\\\",\\n\\\"dialogue\\\": \\\"Patient: Doctor, I've been feeling congested and have a runny nose. What can I do to relieve these symptoms?\\\\n\\\\nDoctor: It sounds like you have a common cold. You can try over-the-counter decongestants to relieve congestion and saline nasal sprays to… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/healthcare-minor-consultation.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"sports-basketball-coach","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/sports-basketball-coach","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a basketball coach and the players on the court during a game. Check out the example below:\\n\\\"id\\\": 1,\\n\\\"description\\\": \\\"Motivating the team\\\",\\n\\\"dialogue\\\": \\\"Coach: Let's give it our all, team! We've trained hard for this game, and I know we can come out on top if we work together.\\\"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Load Dialogues\\n\\t\\n\\nLoading dialogues can be accomplished using the fun dialogues library or Hugging Face datasets… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/sports-basketball-coach.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"customer-service-robot-support","keyword":"fictitious dialogues","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FunDialogues/customer-service-robot-support","creator_name":"fun dialogues","creator_url":"https://huggingface.co/FunDialogues","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis Dialogue\\n\\t\\n\\nComprised of fictitious examples of dialogues between a customer encountering problems with a robotic arm and a technical support agent. Check out the example below:\\n\\\"id\\\": 1,\\n\\\"description\\\": \\\"Robotic arm calibration issue\\\",\\n\\\"dialogue\\\": \\\"Customer: My robotic arm seems to be misaligned. It's not picking objects accurately. What can I do? Agent: It appears that the arm may need recalibration. Please follow the instructions in the user manual to reset the calibration… See the full description on the dataset page: https://huggingface.co/datasets/FunDialogues/customer-service-robot-support.","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"Wizard-Vicuna-MPT","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Heitechsoft/Wizard-Vicuna-MPT","creator_name":"Heitech Software Solutions","creator_url":"https://huggingface.co/Heitechsoft","description":"An MPT-compatible version of wizard_vicuna_70k_unfiltered\\n","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Wizard-Vicuna-MPT","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Heitechsoft/Wizard-Vicuna-MPT","creator_name":"Heitech Software Solutions","creator_url":"https://huggingface.co/Heitechsoft","description":"An MPT-compatible version of wizard_vicuna_70k_unfiltered\\n","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Wizard-Vicuna-MPT","keyword":"conversation","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Heitechsoft/Wizard-Vicuna-MPT","creator_name":"Heitech Software Solutions","creator_url":"https://huggingface.co/Heitechsoft","description":"An MPT-compatible version of wizard_vicuna_70k_unfiltered\\n","first_N":5,"first_N_keywords":["apache-2.0","100K - 1M","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"RyokoAI_ShareGPT52K","keyword":"conversation","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ShareGPT52K90K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of approximately 52,00090,000 conversations scraped via the ShareGPT API before it was shut down.\\nThese conversations include both user prompts and responses from OpenAI's ChatGPT.\\nThis repository now contains the new 90K conversations version. The previous 52K may\\nbe found in the old/ directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\ntext-generation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages… See the full description on the dataset page: https://huggingface.co/datasets/botp/RyokoAI_ShareGPT52K.","first_N":5,"first_N_keywords":["text-generation","English","Spanish","German","multilingual"],"keywords_longer_than_N":true},
	{"name":"PIPPA-TavernFormat","keyword":"roleplay","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/larryvrh/PIPPA-TavernFormat","creator_name":"larryvrh","creator_url":"https://huggingface.co/larryvrh","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"PIPPA_TavernFormat\\\"\\n\\t\\n\\nConverted from the deduped version (pippa_deduped.jsonl) of PygmalionAI/PIPPA.\\nSince the CAI format and the Tavern format does not align exactly, there maybe some mismatches between fields, especially character description and personality.\\n","first_N":5,"first_N_keywords":["English","agpl-3.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PIPPA-TavernFormat","keyword":"conversational","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/larryvrh/PIPPA-TavernFormat","creator_name":"larryvrh","creator_url":"https://huggingface.co/larryvrh","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"PIPPA_TavernFormat\\\"\\n\\t\\n\\nConverted from the deduped version (pippa_deduped.jsonl) of PygmalionAI/PIPPA.\\nSince the CAI format and the Tavern format does not align exactly, there maybe some mismatches between fields, especially character description and personality.\\n","first_N":5,"first_N_keywords":["English","agpl-3.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PIPPA-Judged","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chargoddard/PIPPA-Judged","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"PIPPA-Judged\\\"\\n\\t\\n\\nPygmalion's PIPPA dataset augmented with quality scores generated by TheBloke/OpenOrca-Platypus2-13B-GPTQ.\\nMaking this public so people can reproduce the exact dataset used for one of my models - probably not useful for anything else.\\nIf you want data along these lines, look at Ilya Gusev's pippa_scored instead. It's much higher quality and better executed.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"PIPPA-Judged","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chargoddard/PIPPA-Judged","creator_name":"Charles Goddard","creator_url":"https://huggingface.co/chargoddard","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for \\\"PIPPA-Judged\\\"\\n\\t\\n\\nPygmalion's PIPPA dataset augmented with quality scores generated by TheBloke/OpenOrca-Platypus2-13B-GPTQ.\\nMaking this public so people can reproduce the exact dataset used for one of my models - probably not useful for anything else.\\nIf you want data along these lines, look at Ilya Gusev's pippa_scored instead. It's much higher quality and better executed.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"nb_samtale","keyword":"conversational","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sprakbanken/nb_samtale","creator_name":"Nasjonalbiblioteket Språkbanken","creator_url":"https://huggingface.co/Sprakbanken","description":"NB Samtale is a speech corpus made by the Language Bank at the National Library of Norway.\\nThe corpus contains orthographically transcribed speech from podcasts and recordings of live events at the National Library.\\nThe corpus is intended as an open source dataset for Automatic Speech Recognition (ASR) development,\\nand is specifically aimed at improving ASR systems’ handle on conversational speech.","first_N":5,"first_N_keywords":["automatic-speech-recognition","Norwegian Bokmål","Norwegian Nynorsk","Norwegian","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"pippa_scored","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/pippa_scored","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"\\nA susbet of the PIPPA dataset scored with GPT-4 on different personality traits:\\n\\nLoquacity\\nAssertiveness\\nShyness\\nEmpathy\\nKindness\\nCruelty\\nArrogance\\nStubbornness\\nHumor\\nCapriciousness\\nFragility\\nWisdom\\nFidelity\\nBluntness\\nCreativity\\nConfidence\\nIntegrity\\nBellicosity\\nPatience\\n\\nAnd also several meta-attributes:\\n\\nAction level\\nNSFW\\nUser engagement\\nMBTI type\\nTopic\\n\\nFor every attribute there is a textual explanation from ChatGPT.\\nPrompt:\\nPlease act as an impartial judge and evaluate character traits… See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/pippa_scored.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"pippa_scored","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/pippa_scored","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"\\nA susbet of the PIPPA dataset scored with GPT-4 on different personality traits:\\n\\nLoquacity\\nAssertiveness\\nShyness\\nEmpathy\\nKindness\\nCruelty\\nArrogance\\nStubbornness\\nHumor\\nCapriciousness\\nFragility\\nWisdom\\nFidelity\\nBluntness\\nCreativity\\nConfidence\\nIntegrity\\nBellicosity\\nPatience\\n\\nAnd also several meta-attributes:\\n\\nAction level\\nNSFW\\nUser engagement\\nMBTI type\\nTopic\\n\\nFor every attribute there is a textual explanation from ChatGPT.\\nPrompt:\\nPlease act as an impartial judge and evaluate character traits… See the full description on the dataset page: https://huggingface.co/datasets/IlyaGusev/pippa_scored.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"re_dial_ptbr","keyword":"conversational","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/matheusrdgsf/re_dial_ptbr","creator_name":"Matheus Rodrigues de Souza Félix","creator_url":"https://huggingface.co/matheusrdgsf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ReDial - PTBR\\n\\t\\n\\n\\nOriginal dataset: Redial Huggingface\\nHomepage: ReDial Dataset\\nRepository: ReDialData\\nPaper: Towards Deep Conversational Recommendations\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe ReDial (Recommendation Dialogues) PTBR dataset is an annotated collection of dialogues where users recommend movies to each other translated to brazilian portuguese.\\nThe adapted version of this dataset in Brazilian Portuguese was translated by the Maritalk. This translated… See the full description on the dataset page: https://huggingface.co/datasets/matheusrdgsf/re_dial_ptbr.","first_N":5,"first_N_keywords":["text-classification","text2text-generation","translation","Portuguese","English"],"keywords_longer_than_N":true},
	{"name":"oshichats-v2","keyword":"chat","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pykeio/oshichats-v2","creator_name":"pyke.io","creator_url":"https://huggingface.co/pykeio","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOshiChats v2\\n\\t\\n\\nOshiChats v2 is a dataset of 56 million high-quality English chat messages collected from various VTuber live streams before 18th November 2023.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\nchats_dataset = load_dataset('pykeio/oshichats-v2', split='train')\\nprint(chats_dataset[0])\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSamples\\n\\t\\n\\n{\\n  \\\"liver\\\": \\\"Millie Parfait\\\",\\n  \\\"flags\\\": 16782594,\\n  \\\"stream\\\": {\\n    \\\"id\\\": \\\"yt=aX-D4GDi14s\\\",\\n    \\\"topic\\\": \\\"asmr\\\"\\n  },\\n  \\\"author\\\": \\\"Brandermau\\\"… See the full description on the dataset page: https://huggingface.co/datasets/pykeio/oshichats-v2.","first_N":5,"first_N_keywords":["text-classification","text-generation","token-classification","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"pippa_custom","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Redwood0/pippa_custom","creator_name":"redwood zero","creator_url":"https://huggingface.co/Redwood0","description":"This custom pippa dataset is from Undi of TheBloke Discord\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pippa_custom","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Redwood0/pippa_custom","creator_name":"redwood zero","creator_url":"https://huggingface.co/Redwood0","description":"This custom pippa dataset is from Undi of TheBloke Discord\\n","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pippa_ru","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/pippa_ru","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"Russian translation of PIPPA dataset. \\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"pippa_ru","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IlyaGusev/pippa_ru","creator_name":"Ilya Gusev","creator_url":"https://huggingface.co/IlyaGusev","description":"Russian translation of PIPPA dataset. \\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","1K - 10K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"effanie-AI","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/josiauhlol/effanie-AI","creator_name":"Josiah","creator_url":"https://huggingface.co/josiauhlol","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe Effanie Dataset\\n\\t\\n\\n\\nThis is the dataset for Effanie, the persuasive, confident, and helpful AI!\\nThere are some helpful files for creating the dataset yourself. These include:\\n\\nXLSM Conversion tool\\nParquet Conversion tool\\nThe actual XLSM\\n\\nThis is based off of the OpenOrca dataset.\\n","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultra_feedback_dutch","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Ultra Feedback Dutch\\n\\t\\n\\n\\nIt is recommended to use the cleaned version for your experiments.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv}… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultra_feedback_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"no_robots_dutch","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/no_robots_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for No Robots Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/no_robots_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"ultrachat_200k_dutch","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for UltraChat 200k Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}… See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch.","first_N":5,"first_N_keywords":["text-generation","Dutch","apache-2.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Capybara-Converted","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfahlgren1/Capybara-Converted","creator_name":"Caleb Fahlgren","creator_url":"https://huggingface.co/cfahlgren1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\\n\\t\\n\\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\\nThe single-turn seeds used to intiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness and… See the full description on the dataset page: https://huggingface.co/datasets/cfahlgren1/Capybara-Converted.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"capybara-sharegpt","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Doctor-Shotgun/capybara-sharegpt","creator_name":"Doctor Shotgun","creator_url":"https://huggingface.co/Doctor-Shotgun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tcapybara-sharegpt\\n\\t\\n\\nLDJnr/Capybara converted to ShareGPT format for use in common training repositories.\\nPlease refer to the original repository's dataset card for more information. All credit goes to the original creator.\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"ChatAlpaca-20K","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/robinsmits/ChatAlpaca-20K","creator_name":"Robin Smits","creator_url":"https://huggingface.co/robinsmits","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ChatAlpaca 20K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChatAlpaca: A Multi-Turn Dialogue Corpus based on Alpaca Instructions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nChatAlpaca is a chat dataset that aims to help researchers develop models for instruction-following in multi-turn conversations. The dataset is an extension of the Stanford Alpaca data, which contains multi-turn instructions and their corresponding responses.\\nChatAlpaca is developed by Chinese Information Processing Laboratory at… See the full description on the dataset page: https://huggingface.co/datasets/robinsmits/ChatAlpaca-20K.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"instruct-aira-dataset-v3","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3","creator_name":"Nicholas Kluge Corrêa","creator_url":"https://huggingface.co/nicholasKluge","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInstruct-Aira Dataset version 3.0\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of multi-turn conversations between an assistant and a user. Conversations were generated by user interactions with already-tuned models (ChatGPT, LLama 2, Open-Assistant, etc). The dataset is available in Portuguese and English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be utilized for various natural language processing tasks, including but not limited to:… See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/instruct-aira-dataset-v3.","first_N":5,"first_N_keywords":["text-generation","Portuguese","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"combined-fr-caselaw","keyword":"dialogue-modeling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Tonic/combined-fr-caselaw","creator_name":"Joseph [open/acc] Pollack","creator_url":"https://huggingface.co/Tonic","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for French Legal Cases Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset combines French legal cases from multiple sources (INCA, JADE, CASS, CAPP) into a unified format with overlapping text triplets. It includes decisions from various French courts, processed to facilitate natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nTasks:\\nText Generation\\nLegal Document Analysis\\nText Classification\\nLanguage Modeling\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset… See the full description on the dataset page: https://huggingface.co/datasets/Tonic/combined-fr-caselaw.","first_N":5,"first_N_keywords":["text-generation","text-classification","language-modeling","entity-linking-classification","fact-checking"],"keywords_longer_than_N":true},
	{"name":"ai-chat-dataset","keyword":"chat","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/rahulsingh2103/ai-chat-dataset","creator_name":"Rahul Singh","creator_url":"https://huggingface.co/rahulsingh2103","description":"rahulsingh2103/ai-chat-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-classification","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"retrieve_user_require","keyword":"conversational-ai","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/tinh2406/retrieve_user_require","creator_name":"Nguyen Quoc Tinh","creator_url":"https://huggingface.co/tinh2406","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Conversation-Based User Intent Extraction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is designed for extracting user intent from conversational contexts. It includes multi-turn dialogues where user requests, queries, and intents are labeled to enable training and evaluation of natural language processing (NLP) models for intent recognition.\\nThe dataset is useful for building AI assistants, chatbots, and retrieval-augmented… See the full description on the dataset page: https://huggingface.co/datasets/tinh2406/retrieve_user_require.","first_N":5,"first_N_keywords":["Vietnamese","English","mit","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"flammenai-Prude-Phi3-DPO","keyword":"roleplay","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Triangle104/flammenai-Prude-Phi3-DPO","creator_name":"Lymeman","creator_url":"https://huggingface.co/Triangle104","description":"ResplendentAI/NSFW_RP_Format_NoQuote inputs ran through Phi-3 Q4 to intentionally produce bad responses to be used for rejections.\\n","first_N":5,"first_N_keywords":["English","apache-2.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Finance-Instruct-500k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oieieio/Finance-Instruct-500k","creator_name":"Jorge Alonso","creator_url":"https://huggingface.co/oieieio","description":"\\n\\t\\n\\t\\t\\n\\t\\tFinance-Instruct-500k Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\\nThe dataset includes content tailored for financial… See the full description on the dataset page: https://huggingface.co/datasets/oieieio/Finance-Instruct-500k.","first_N":5,"first_N_keywords":["apache-2.0","🇺🇸 Region: US","finance","fine-tuning","conversational-ai"],"keywords_longer_than_N":true},
	{"name":"multi-character-dialogue","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/multi-character-dialogue","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\nThe 🤗 Hugging Face viewer messed up the dataset view. Please see below for an example entry.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMulti-Character Dialogue Dataset\\n\\t\\n\\nThis dataset contains over 10 000 entries of multi-character dialogues in JSONL format. Each entry represents a unique scenario with multiple characters engaged in conversation, complete with detailed settings, character descriptions, and post-interaction changes.\\nThe scenarios include many genres including slice of life, fantasy, science fiction… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/multi-character-dialogue.","first_N":5,"first_N_keywords":["English","cc-by-4.0","🇺🇸 Region: US","dialogue","multicharacter"],"keywords_longer_than_N":true},
	{"name":"multi-character-dialogue","keyword":"roleplay","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/multi-character-dialogue","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\nThe 🤗 Hugging Face viewer messed up the dataset view. Please see below for an example entry.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMulti-Character Dialogue Dataset\\n\\t\\n\\nThis dataset contains over 10 000 entries of multi-character dialogues in JSONL format. Each entry represents a unique scenario with multiple characters engaged in conversation, complete with detailed settings, character descriptions, and post-interaction changes.\\nThe scenarios include many genres including slice of life, fantasy, science fiction… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/multi-character-dialogue.","first_N":5,"first_N_keywords":["English","cc-by-4.0","🇺🇸 Region: US","dialogue","multicharacter"],"keywords_longer_than_N":true},
	{"name":"LONGCOT-Alpaca","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LONGCOT-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction… See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-polyai-nlu","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-polyai-nlu","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAveniBench: PolyAI NLU++\\n\\t\\n\\nPolyAI NLU++ split used in the AveniBench.\\n\\n\\t\\n\\t\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the CC-BY-4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\tCitation\\n\\t\\n\\nAveniBench\\nTDB\\n\\nPolyAI NLU++\\n@inproceedings{casanueva-etal-2022-nlu,\\n    title = \\\"{NLU}++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue\\\",\\n    author = \\\"Casanueva, Inigo  and\\n      Vuli{\\\\'c}, Ivan  and\\n      Spithourakis, Georgios  and\\n      Budzianowski… See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-polyai-nlu.","first_N":5,"first_N_keywords":["English","cc-by-4.0","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"aveni-bench-banking77","keyword":"dialogue","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aveni-ai/aveni-bench-banking77","creator_name":"Aveni","creator_url":"https://huggingface.co/aveni-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAveniBench: Banking77\\n\\t\\n\\nBanking77 split used in the AveniBench.\\n\\n\\t\\n\\t\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the CC-BY-4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\tCitation\\n\\t\\n\\nAveniBench\\nTDB\\n\\nBanking77\\n@inproceedings{casanueva-etal-2020-efficient,\\n    title = \\\"Efficient Intent Detection with Dual Sentence Encoders\\\",\\n    author = \\\"Casanueva, I{\\\\~n}igo  and\\n      Tem{\\\\v{c}}inas, Tadas  and\\n      Gerz, Daniela  and\\n      Henderson, Matthew  and\\n      Vuli{\\\\'c}, Ivan\\\",\\n    booktitle = \\\"Proceedings of… See the full description on the dataset page: https://huggingface.co/datasets/aveni-ai/aveni-bench-banking77.","first_N":5,"first_N_keywords":["English","cc-by-4.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"infoquest","keyword":"dialogue","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bryanlincoln/infoquest","creator_name":"Bryan Lincoln","creator_url":"https://huggingface.co/bryanlincoln","description":"\\n\\t\\n\\t\\t\\n\\t\\tInfoQuest Dataset\\n\\t\\n\\nThis dataset accompanies the paper \\\"InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of several files:\\n\\nsample_personas.csv: Contains 1500 sampled personas from the 200k personas in proj-persona/PersonaHub. The idx column corresponds to the original index in the source dataset.\\n\\nseed_messages.jsonl: Contains seed messages generated by combining personas from… See the full description on the dataset page: https://huggingface.co/datasets/bryanlincoln/infoquest.","first_N":5,"first_N_keywords":["mit","arxiv:2502.12257","🇺🇸 Region: US","dialogue","conversation"],"keywords_longer_than_N":true},
	{"name":"infoquest","keyword":"conversation","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bryanlincoln/infoquest","creator_name":"Bryan Lincoln","creator_url":"https://huggingface.co/bryanlincoln","description":"\\n\\t\\n\\t\\t\\n\\t\\tInfoQuest Dataset\\n\\t\\n\\nThis dataset accompanies the paper \\\"InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of several files:\\n\\nsample_personas.csv: Contains 1500 sampled personas from the 200k personas in proj-persona/PersonaHub. The idx column corresponds to the original index in the source dataset.\\n\\nseed_messages.jsonl: Contains seed messages generated by combining personas from… See the full description on the dataset page: https://huggingface.co/datasets/bryanlincoln/infoquest.","first_N":5,"first_N_keywords":["mit","arxiv:2502.12257","🇺🇸 Region: US","dialogue","conversation"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-500k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-500k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-500k","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-500k","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-500k","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-10k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-10k","keyword":"conversational-ai","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-10k","keyword":"multiple-turn-dialogue","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-10k","keyword":"chat","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-v2-10k","keyword":"conversational","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"chat-dataset","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure… See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"MoACA","keyword":"conversations","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/amaydle/MoACA","creator_name":"Amaya Kahada Gamage","creator_url":"https://huggingface.co/amaydle","description":"\\n\\t\\n\\t\\t\\n\\t\\tMixture of Agents (MoA) Framework Dataset\\n\\t\\n\\nThis dataset contains conversations generated by the Mixture of Agents (MoA) framework, an AI-driven software engineering system designed to execute code modifications with high precision and reliability. The dataset is derived from interactions with various AI agents that collaboratively work to transform user objectives into actionable code changes.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of the following columns:\\n\\nmodel_name:… See the full description on the dataset page: https://huggingface.co/datasets/amaydle/MoACA.","first_N":5,"first_N_keywords":["English","mit","< 1K","parquet","Text"],"keywords_longer_than_N":true}
]
;
