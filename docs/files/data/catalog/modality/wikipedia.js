const data_for_modality_wikipedia = 
[
	{"name":"sql-parsed","keyword":"wikisql","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"wiki_toxic","keyword":"wikipedia","description":"Jigsaw Toxic Comment Challenge dataset. This dataset was the basis of a Kaggle competition run by Jigsaw","url":"https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic","creator_name":"OxAI Safety Hub Active Learning with Large Language Models Labs Team","creator_url":"https://huggingface.co/OxAISH-AL-LLM","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","hate-speech-detection","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"Korean_Wikipedia_Dataset_for_GPT2_August_2022","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for korean_wikipedia_dataset_for_GPT2\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEntire Korean language Wikipedia data for GPT-2 training as of August 1st, 2022.\nemail: oscar.eaglewatch@gmail.com\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is to make a pre-trained GPT-2 Korean model\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nKorean\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nTrain wikipedia article count: 334420\nvalidation wikipedia article count: 83605\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n'text'\n\n\t\n\t\t\n\t\tData Splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022.","url":"https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022","creator_name":"Yongwoo Jeong","creator_url":"https://huggingface.co/eaglewatch","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","translation","visual-question-answering","open-domain-qa","closed-domain-qa"],"keywords_longer_than_N":true},
	{"name":"wikidata-enwiki-categories-and-statements","keyword":"wikipedia","description":"derenrich/wikidata-enwiki-categories-and-statements dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/derenrich/wikidata-enwiki-categories-and-statements","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-3.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"wikidata-enwiki-categories-and-statements","keyword":"wikidata","description":"derenrich/wikidata-enwiki-categories-and-statements dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/derenrich/wikidata-enwiki-categories-and-statements","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["text-classification","English","cc-by-3.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"autoregressive-paraphrase-dataset","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for [Dataset Name]\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSource Data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset.","url":"https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"tamil_sentences_sample","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"tamil_combined_sentences\"\n\t\n\nMore Information needed\n","url":"https://huggingface.co/datasets/AnanthZeke/tamil_sentences_sample","creator_name":"Ananth","creator_url":"https://huggingface.co/AnanthZeke","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","zero-shot-classification","Tamil","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"OneOS","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tOneOS Dataset\n\t\n\nThe OneOS dataset is a collection of text data for the OneOS project. It consists of a large number of text samples that can be used for training and evaluating natural language processing models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nNumber of Samples: 13,068\nLicense: CC0*\nLanguage: English, French\n\n  * Only unlicensed sentences generated manually fall under CreativeCommon-0. Sentences already licensed under different terms, such as nl2bash or samantha-data, remain subject to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wasertech/OneOS.","url":"https://huggingface.co/datasets/wasertech/OneOS","creator_name":"Danny Waser","creator_url":"https://huggingface.co/wasertech","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","French","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"tydiqa_xtreme","keyword":"extended|wikipedia","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).\n\nWe also include \"translate-train\" and \"translate-test\" splits for each non-English languages from XTREME (Hu et al., 2020). These splits are the automatic translations from English to each target language used in the XTREME paper [https://arxiv.org/abs/2003.11080]. The \"translate-train\" split purposefully ignores the non-English TyDiQA-GoldP training data to simulate the transfer learning scenario where original-language data is not available and system builders must rely on labeled English data plus existing machine translation systems.","url":"https://huggingface.co/datasets/juletxara/tydiqa_xtreme","creator_name":"Julen Etxaniz","creator_url":"https://huggingface.co/juletxara","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"entity_popularity","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tEntity Popularity Dataset\n\t\n\nThis dataset contains information for about 26,000 entities, including the Wikipedia article title, QID, and the annual article view count for the year 2021. \nThe annual article view count can be considered as an indicator of the popularity of a entity.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is composed in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"masaki-sakata/entity_popularity\")[\"en\"]\n\nprint(dataset)\n#‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/entity_popularity.","url":"https://huggingface.co/datasets/masaki-sakata/entity_popularity","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"squad-augmented-v2","keyword":"extended|wikipedia","description":"christti/squad-augmented-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/christti/squad-augmented-v2","creator_name":"Christoph Timmermann","creator_url":"https://huggingface.co/christti","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"MultiSim","keyword":"wikipedia","description":"MultiSim is a growing collection of Text Simplfication datasets in multiple languages.  Each dataset is a set of complex and simple sentence pairs.","url":"https://huggingface.co/datasets/MichaelR207/MultiSim","creator_name":"Michael Ryan","creator_url":"https://huggingface.co/MichaelR207","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["summarization","text-generation","English","French","Russian"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-copy","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tFork of b-mc2/sql-create-context\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy.","url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"wiki-entity-similarity","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWiki Entity Similarity\n\t\n\nUsage:\nfrom datasets import load_dataset\n\ncorpus = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20corpus', split='train')\nassert corpus[0] == {'article': 'A1000 road', 'link_text': 'A1000', 'is_same': 1}\n\npairs = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20pairs', split='train')\nassert corpus[0] == {'article': 'Rhinobatos', 'link_text': 'Ehinobatos beurleni', 'is_same': 1}\nassert len(corpus) == 4_793_180\n\n\n\t\n\t\t\n\t\tCorpus (name=*corpus)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Exr0n/wiki-entity-similarity.","url":"https://huggingface.co/datasets/Exr0n/wiki-entity-similarity","creator_name":"exr0n","creator_url":"https://huggingface.co/Exr0n","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["found","found","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-instruction","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction.","url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_pa","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Punjabi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Panjabi"],"keywords_longer_than_N":true},
	{"name":"wiki-talks","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWiki-Talks\n\t\n\nThe Wiki-Talks dataset is a collection of conversational threads extracted from the talk pages on Wikipedia.\nThis dataset captures collaborative dialogue, discussion patterns, and consensus-building among Wikipedia contributors.\nIt is useful for NLP research focused on dialogue, sentiment analysis, and community dynamics.\n\n\t\n\t\t\n\t\tDetails\n\t\n\nCurrently due to PyArrow incompatibility to the long recursive structures in the dataset there is an intrinsic incompatibility‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lflage/wiki-talks.","url":"https://huggingface.co/datasets/lflage/wiki-talks","creator_name":"Lucas Fonseca Lage","creator_url":"https://huggingface.co/lflage","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","German","Portuguese","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"Bitcoin_Wikipedia_Webscraper_Example","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tüåê Web Scraper: Turn Any URL into AI-Ready Data\n\t\n\nConvert any public web page into clean, structured JSON in one click. Just paste a URL and this tool scrapes, cleans, and formats the content‚Äîready to be used in any AI or content pipeline.\nWhether you're building datasets for LLMs or feeding fresh content into agents, this no-code tool makes it effortless to extract high-quality data from the web.\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Scrape Any Public Page ‚Äì Works on blogs, websites, docs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasaFoundation/Bitcoin_Wikipedia_Webscraper_Example.","url":"https://huggingface.co/datasets/MasaFoundation/Bitcoin_Wikipedia_Webscraper_Example","creator_name":"MasaAI","creator_url":"https://huggingface.co/MasaFoundation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"WikiFinance","keyword":"wikipedia","description":"This dataset comprises Wikipedia articles focused on financial and economic topics, systematically curated using Wikipedia's native category system. The collection process involved identifying Wikipedia categories directly related to money, finance, business, and monetary systems, then filtering articles based on their categorical overlap with these financial domains. The current dataset contains only articles that appear in three or more of these finance-related Wikipedia categories, creating‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/WikiFinance.","url":"https://huggingface.co/datasets/Akhil-Theerthala/WikiFinance","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mr","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Marathi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Marathi"],"keywords_longer_than_N":true},
	{"name":"en_wikidata_5M_entities","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\ten_wikidata_5M_entities\n\t\n\nHugging Face dataset card for a large, English-only Wikidata slice with optional Wikipedia links and Wikimedia Commons image URLs.\n\nOne file, five million entities.\nFilename: en_wikidata_5M_entities.jsonl.gz\n\n\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\n\nFormat: JSON Lines, gzip-compressed (.jsonl.gz)\nRows: 5,000,000 entities (one JSON object per line)\nLanguage: English labels/descriptions\nFields: qid, label, description, enwiki_title, wikipedia_url, images (list of URLs), has_image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vijaysr4/en_wikidata_5M_entities.","url":"https://huggingface.co/datasets/Vijaysr4/en_wikidata_5M_entities","creator_name":"VIJAY VENKATESH M","creator_url":"https://huggingface.co/Vijaysr4","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","English","cc0-1.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"en_wikidata_5M_entities","keyword":"wikidata","description":"\n\t\n\t\t\n\t\ten_wikidata_5M_entities\n\t\n\nHugging Face dataset card for a large, English-only Wikidata slice with optional Wikipedia links and Wikimedia Commons image URLs.\n\nOne file, five million entities.\nFilename: en_wikidata_5M_entities.jsonl.gz\n\n\n\n\t\n\t\t\n\t\tTL;DR\n\t\n\n\nFormat: JSON Lines, gzip-compressed (.jsonl.gz)\nRows: 5,000,000 entities (one JSON object per line)\nLanguage: English labels/descriptions\nFields: qid, label, description, enwiki_title, wikipedia_url, images (list of URLs), has_image‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Vijaysr4/en_wikidata_5M_entities.","url":"https://huggingface.co/datasets/Vijaysr4/en_wikidata_5M_entities","creator_name":"VIJAY VENKATESH M","creator_url":"https://huggingface.co/Vijaysr4","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","English","cc0-1.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ksd","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kashmiri (Devanagari script) version of the NanoDBPedia dataset, specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kashmiri"],"keywords_longer_than_N":true},
	{"name":"AnimeMangaCharacters-247K","keyword":"wiki","description":"\n\t\n\t\t\n\t\tAnime Manga Characters Dataset\n\t\n\nThis dataset is a metafile containing information about 247,034 anime and manga characters sourced from 2,372 fandom wiki sites. Each entry represents a character along with its associated metadata. The dataset has been deduplicated based on the url field to avoid redundancy, although a single character may still have multiple associated URLs.\n\n\t\n\t\t\n\t\tPotential Applications\n\t\n\n\nMultimodal Data Creation: Use the URLs to download the respective wiki‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K.","url":"https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"GreekWikipedia","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tGreekWikipedia\n\t\n\nA Greek abstractive summarization dataset collected from the Greek part of Wikipedia, which contains 93,432 articles, their titles and summaries.\nThis dataset has been used to train our best-performing model GreekWiki-umt5-base as part of our upcoming research article:Giarelis, N., Mastrokostas, C., & Karacapilidis, N. (2024) Greek Wikipedia: A Study on Abstractive Summarization.For information about dataset creation, limitations etc. see the original article.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IMISLab/GreekWikipedia.","url":"https://huggingface.co/datasets/IMISLab/GreekWikipedia","creator_name":"IMIS Lab UPatras","creator_url":"https://huggingface.co/IMISLab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","Greek","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"sinhala-articles","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tSinhala Articles Dataset\n\t\n\nA large-scale, high-quality Sinhala text corpus curated from diverse sources including news articles, Wikipedia entries, and general web content. This dataset is designed to support a wide range of Sinhala Natural Language Processing (NLP) tasks.\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nName: Navanjana/sinhala-articles\nTotal Samples:  2,148,688\nLanguages: Sinhala (si)\nFeatures:\ntext: A single column containing Sinhala text passages.\n\n\nSize: Approximately 1M < n <‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/sinhala-articles.","url":"https://huggingface.co/datasets/Navanjana/sinhala-articles","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Sinhala","apache-2.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"sinhala-articles","keyword":"wiki","description":"\n\t\n\t\t\n\t\tSinhala Articles Dataset\n\t\n\nA large-scale, high-quality Sinhala text corpus curated from diverse sources including news articles, Wikipedia entries, and general web content. This dataset is designed to support a wide range of Sinhala Natural Language Processing (NLP) tasks.\n\n\n\t\n\t\t\n\t\tüìä Dataset Overview\n\t\n\n\nName: Navanjana/sinhala-articles\nTotal Samples:  2,148,688\nLanguages: Sinhala (si)\nFeatures:\ntext: A single column containing Sinhala text passages.\n\n\nSize: Approximately 1M < n <‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/sinhala-articles.","url":"https://huggingface.co/datasets/Navanjana/sinhala-articles","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Sinhala","apache-2.0","1M - 10M","csv"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-20250820","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Label Maps 2025-08-20\n\t\n\nLabel maps extracted from the 2025-08-20 Wikidata dump.Use these to resolve Q and P identifiers to English labels quickly.\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nentity_map.parquet - columns: id, label, descriptionQ items. 77.4M rows.\nprop_map.parquet - columns: id, label, description, datatypeP items. 11,568 rows.\n\nAll files are Parquet with Zstandard compression.\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\n\t\n\t\t\n\t\tA) Hugging Face snapshot to a local folder\n\t\n\nfrom huggingface_hub import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-20250820.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-20250820","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","English","cc0-1.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-20250820","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Label Maps 2025-08-20\n\t\n\nLabel maps extracted from the 2025-08-20 Wikidata dump.Use these to resolve Q and P identifiers to English labels quickly.\n\n\t\n\t\t\n\t\tFiles\n\t\n\n\nentity_map.parquet - columns: id, label, descriptionQ items. 77.4M rows.\nprop_map.parquet - columns: id, label, description, datatypeP items. 11,568 rows.\n\nAll files are Parquet with Zstandard compression.\n\n\t\n\t\t\n\t\tDownload Options\n\t\n\n\n\t\n\t\t\n\t\tA) Hugging Face snapshot to a local folder\n\t\n\nfrom huggingface_hub import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-20250820.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-20250820","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","English","cc0-1.0","10M - 100M"],"keywords_longer_than_N":true},
	{"name":"CUBE-MT","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tCUBE-MT: A Cultural Benchmark for Multimodal Knowledge Graph Construction with Generative Models\n\t\n\n\nCUBE-MT (CUltural BEnchmark with Multimodal Transformations) is an extension to the CUltural BEnchmark for Text-to-Image models (CUBE). CUBE contains 300K cultural artifacts across 8 countries (Brazil, France, India, Italy, Japan, Nigeria, Turkey, and USA) and 3 domains (cuisine, landmarks, art) extracted from Wikidata; and 1K text-to-image generation prompts that enable evaluation of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/albertmeronyo/CUBE-MT.","url":"https://huggingface.co/datasets/albertmeronyo/CUBE-MT","creator_name":"Albert Mero√±o Pe√±uela","creator_url":"https://huggingface.co/albertmeronyo","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["translation","English","cc-by-4.0","n<1K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"wikipedia_passages","keyword":"wikipedia","description":"There are 21M Wikipedia passages proposed by DPR and used in QPaug.","url":"https://huggingface.co/datasets/Minsang/wikipedia_passages","creator_name":"MinsangKim","creator_url":"https://huggingface.co/Minsang","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"Scifi4TopicModel","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Scifi4TopicModel.","url":"https://huggingface.co/datasets/zli12321/Scifi4TopicModel","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"rightnow-arabic-llm-corpus","keyword":"wikipedia","description":"\n\n\t\n\t\t\n\t\tRightNow Arabic LLM Corpus\n\t\n\nThe largest and highest-quality Arabic language model training dataset, featuring 743,288 meticulously cleaned articles with 244 million words of professional Arabic text.\n\n\t\n\t\t\n\t\tAbout RightNow AI\n\t\n\nThis dataset was collected by the RightNow AI team, creators of the #1 GPU-powered AI code editor. Visit us at https://rightnowai.co/\n\n\t\n\t\t\n\t\tDataset Statistics\n\t\n\n\n\t\n\t\t\nMetric\nValue\n\n\n\t\t\nTotal Articles\n743,288\n\n\nTotal Words\n244,000,000+\n\n\nDataset Size\n8.7‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/rightnow-arabic-llm-corpus.","url":"https://huggingface.co/datasets/Jr23xd23/rightnow-arabic-llm-corpus","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","token-classification","question-answering","summarization"],"keywords_longer_than_N":true},
	{"name":"wikipedia_definitions","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWikipedia Definitions Dataset\n\t\n\nwikipedia_definitions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the definition sentence(s) that open each Wikipedia article.The corpus contains 25 449 entities.\nLead-paragraph definitions give a slightly richer, stylistically uniform overview of an entity than the short Wikidata description, making them useful as lightweight contextual signals for tasks such as entity linking, retrieval, question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions.","url":"https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikipedia_definitions","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikipedia Definitions Dataset\n\t\n\nwikipedia_definitions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the definition sentence(s) that open each Wikipedia article.The corpus contains 25 449 entities.\nLead-paragraph definitions give a slightly richer, stylistically uniform overview of an entity than the short Wikidata description, making them useful as lightweight contextual signals for tasks such as entity linking, retrieval, question answering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions.","url":"https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Polish-wikipedia-selected-topics","keyword":"wikipedia","description":"categories:\n    \"Nauki przyrodnicze\",\n    \"Nauki humanistyczne\",\n    \"Nauki biologiczne\", \n    \"Biologia\",\n    \"Metodologia nauk przyrodniczych\",\n    \"Polska\",\n    \"Nauka w Polsce\",\n    \"Jƒôzyki Polski\",\n    \"Nauki humanistyczne\",\n    \"Jƒôzykoznawstwo\",\n    \"Kultuta\",\n    \"Kultura jƒôzyka\",\n    \"Myrmekologia\",\n    \"Mr√≥wkowate\",\n    \"Ekologia\",\n    \"Prawo w Polsce\",\n    \"Jƒôzykoznawstwo\",\n    \"Dialektologia\",\n    \"Odmiany i style jƒôzykowe\",\n    \"Futurologia\"\n\nevery example has max 2000 words in it. \n","url":"https://huggingface.co/datasets/Igorrr0/Polish-wikipedia-selected-topics","creator_name":"Igor Paw≈Çowicz","creator_url":"https://huggingface.co/Igorrr0","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Polish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"NewQA","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"squad\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/badokorach/NewQA.","url":"https://huggingface.co/datasets/badokorach/NewQA","creator_name":"brenda Adokorach","creator_url":"https://huggingface.co/badokorach","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"WikiConvert","keyword":"extended|wikipedia","description":"Language Modelling with Cardinal Number Annotations.","url":"https://huggingface.co/datasets/usc-isi/WikiConvert","creator_name":"USC Information Sciences Institute","creator_url":"https://huggingface.co/usc-isi","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["fill-mask","other","text-generation","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"wikiomnia","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"Wikiomnia\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWe present the WikiOmnia dataset, a new publicly available set of QA-pairs and corresponding Russian Wikipedia article summary sections, composed with a fully automated generative pipeline. The dataset includes every available article from Wikipedia for the Russian language. The WikiOmnia pipeline is available open-source and is also tested for creating SQuAD-formatted QA on other domains, like news texts, fiction, and social‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RussianNLP/wikiomnia.","url":"https://huggingface.co/datasets/RussianNLP/wikiomnia","creator_name":"Natural Language Processing in Russian","creator_url":"https://huggingface.co/RussianNLP","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","Russian","apache-2.0","1M<n<10M","arxiv:2204.08009"],"keywords_longer_than_N":true},
	{"name":"qa_squadshifts_synthetic","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"lmqg/qa_squadshifts_synthetic\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a synthetic QA dataset generated with fine-tuned QG models over lmqg/qa_squadshifts, made for question-answering based evaluation (QAE) for question generation model proposed by Zhang and Bansal, 2019.\nThe test split is the original validation set of lmqg/qa_squadshifts, where the model should be evaluate on.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nquestion-answering\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic.","url":"https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"qald_9_plus","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tQALD-9-plus Dataset Description\n\t\n\nQALD-9-plus is the dataset for Knowledge Graph Question Answering (KGQA) based on well-known QALD-9.\nQALD-9-plus enables to train and test KGQA systems over DBpedia and Wikidata using questions in 9 different languages: English, German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian.\nSome of the questions have several alternative writings in particular languages which enables to evaluate the robustness of KGQA systems and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/qald_9_plus.","url":"https://huggingface.co/datasets/casey-martin/qald_9_plus","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","Bashkir","Belarusian","German","English"],"keywords_longer_than_N":true},
	{"name":"wikitoxic","keyword":"wikipedia","description":"This dataset has been created as an artefact of the paper AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets (Lesci and Vlachos, 2024).\nMore info about this dataset in the appendix of the paper. \nThis is the same dataset as OxAISH-AL-LLM/wiki_toxic.\nThe only differences are:\n\nAddition of a unique identifier, uid.\n\nAddition of the indices, that is, 3 columns with the embeddings of 3 different sentence-transformers\n\nall-mpnet-base-v2\nmulti-qa-mpnet-base-dot-v1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pietrolesci/wikitoxic.","url":"https://huggingface.co/datasets/pietrolesci/wikitoxic","creator_name":"Pietro Lesci","creator_url":"https://huggingface.co/pietrolesci","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","hate-speech-detection","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-zh-mnbvc","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tzhwiki-mnbvc\n\t\n\nÂàÜÈ°πÁõÆÔºöÁà¨ÂèñÂπ∂Â§ÑÁêÜ‰∏≠ÊñáÁª¥Âü∫ÁôæÁßëËØ≠Êñô\nÊï∞ÊçÆÊó∂Èó¥Ôºö202302-202305 ÔºàÊåÅÁª≠Êõ¥Êñ∞Ôºâ\n‰∏ªÈ°πÁõÆÔºöMNBVC(Massive Never-ending BT Vast Chinese corpus)Ë∂ÖÂ§ßËßÑÊ®°‰∏≠ÊñáËØ≠ÊñôÈõÜ https://github.com/esbatmop/MNBVC\nËØ•È°πÁõÆÊ∏ÖÊ¥óÊµÅÁ®ã‰∏ªË¶ÅÂèÇËÄÉÔºöhttps://kexue.fm/archives/4176/comment-page-1\nÂπ∂‰∏î‰ΩøÁî®ÁªÑÂëòÂºÄÂèëÁöÑÂéªÈáçÂ∑•ÂÖ∑ËøõË°åÊï∞ÊçÆÊ†ºÂºèÂåñ„ÄÇ\nÊÄªË°åÊï∞ÔºàÊ†∑Êú¨Ôºâ: 10,754,146\n‰∏Ä‰∏™Á§∫‰æãÔºö\n{\n  \"Êñá‰ª∂Âêç\": \"cleaned/zhwiki-20230420/folder_0/723712.txt\",\n  \"ÊòØÂê¶ÂæÖÊü•Êñá‰ª∂\": false,\n  \"ÊòØÂê¶ÈáçÂ§çÊñá‰ª∂\": false,\n  \"Êñá‰ª∂Â§ßÂ∞è\": 558,\n  \"simhash\": 14363740497821204542,\n  \"ÊúÄÈïøÊÆµËêΩÈïøÂ∫¶\": 142,\n  \"ÊÆµËêΩÊï∞\": 6,\n  \"ÂéªÈáçÊÆµËêΩÊï∞\": 6,\n  \"‰ΩéË¥®ÈáèÊÆµËêΩÊï∞\": 0,\n  \"ÊÆµËêΩ\": [\n    {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc.","url":"https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"qa_harvesting_from_wikipedia","keyword":"extended|wikipedia","description":"QA pairs generated in https://aclanthology.org/P18-1177/","url":"https://huggingface.co/datasets/lmqg/qa_harvesting_from_wikipedia","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"wikidata-en-descriptions-small","keyword":"wikipedia","description":"derenrich/wikidata-en-descriptions-small dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions-small","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["English","cc-by-3.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikidata-en-descriptions-small","keyword":"wikidata","description":"derenrich/wikidata-en-descriptions-small dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions-small","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["English","cc-by-3.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"TALI","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"TALI\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim of inspiring‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI.","url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["zero-shot-classification","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"quac","keyword":"extended|wikipedia","description":"Question Answering in Context is a dataset for modeling, understanding,\nand participating in information seeking dialog. Data instances consist\nof an interactive dialog between two crowd workers: (1) a student who\nposes a sequence of freeform questions to learn as much as possible\nabout a hidden Wikipedia text, and (2) a teacher who answers the questions\nby providing short excerpts (spans) from the text. QuAC introduces\nchallenges not found in existing machine comprehension datasets: its\nquestions are often more open-ended, unanswerable, or only meaningful\nwithin the dialog context.","url":"https://huggingface.co/datasets/allenai/quac","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":null,"first_N":5,"first_N_keywords":["question-answering","text-generation","fill-mask","dialogue-modeling","extractive-qa"],"keywords_longer_than_N":true},
	{"name":"spanextract","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"squad\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Lexi/spanextract.","url":"https://huggingface.co/datasets/Lexi/spanextract","creator_name":"Liu","creator_url":"https://huggingface.co/Lexi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_as","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Assamese version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Assamese"],"keywords_longer_than_N":true},
	{"name":"scholarly-article-citations-in-wikipedia","keyword":"wikipedia","description":"This dataset includes a list of citations to scholarly articles from a 2015 version of English Wikipedia.\nCitations are in the form of PubMed IDs (pmid) and PubMedCentral IDs (pmcid).\nDigital Object Identifiers (doi)\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach row in the dataset represents a citation as a (Wikipedia article, scholarly article) pair. Metadata about when the citation was first added is included.\npage_id: The identifier of the Wikipedia article (int), e.g. 1325125\npage_title: The title of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia-community/scholarly-article-citations-in-wikipedia.","url":"https://huggingface.co/datasets/wikimedia-community/scholarly-article-citations-in-wikipedia","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","cc0-1.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"nano_wiki","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"nano_wiki\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nnano_wiki is a synthetic encyclopedia-style text dataset generated using Google's Gemma 3 27B language model. It contains 9,107 articles in simple English, covering essential human knowledge based on the Wikipedia list of articles all languages should have (expanded version).\nEach entry was generated using a consistent prompt designed to produce very simple, readable language suitable for small-scale language model pretraining.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sixf0ur/nano_wiki.","url":"https://huggingface.co/datasets/sixf0ur/nano_wiki","creator_name":"David","creator_url":"https://huggingface.co/sixf0ur","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"nano_wiki","keyword":"wiki","description":"\n\t\n\t\t\n\t\tDataset Card for \"nano_wiki\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nnano_wiki is a synthetic encyclopedia-style text dataset generated using Google's Gemma 3 27B language model. It contains 9,107 articles in simple English, covering essential human knowledge based on the Wikipedia list of articles all languages should have (expanded version).\nEach entry was generated using a consistent prompt designed to produce very simple, readable language suitable for small-scale language model pretraining.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sixf0ur/nano_wiki.","url":"https://huggingface.co/datasets/sixf0ur/nano_wiki","creator_name":"David","creator_url":"https://huggingface.co/sixf0ur","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mni","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Manipuri version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Manipuri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_hi","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Hindi version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Hindi"],"keywords_longer_than_N":true},
	{"name":"chinese_moegirl_wiki_corpus_raw","keyword":"wiki","description":"\n\t\n\t\t\n\t\tChinese Moegirl ACG Corpus (Raw Data)\n\t\n\nMoegirl ÊòØ‰∏™‰∏≠Êñá‰∫åÊ¨°ÂÖÉ wiki ÁΩëÁ´ô\nÊú¨È°πÁõÆÂØπ 20230814 wiki dump for wiki-zh.moegirl.org.cn Âè™ËøõË°å‰∫ÜÁÆÄÂçïÁöÑÊï∞ÊçÆÊ†ºÂºèÂ§ÑÁêÜÔºàxml -> jsonl datasetÔºâÔºåÂêéÁª≠Â¶ÇÊÉ≥‰Ωú‰∏∫ LLM È¢ÑËÆ≠ÁªÉËØ≠ÊñôÔºåÂä°ÂøÖËøõË°åÂêÑÁßçÊñáÊú¨Ê∏ÖÊ¥ó„ÄÇ\nÁÆÄÂçï‰ΩøÁî®Ê≠£ÂàôÁªôÊØèÊù°Êï∞ÊçÆÂ¢ûÂä†‰∫Ü tagÔºõÁõ¥Êé•ËøáÊª§ÊéâÊâÄÊúâÂ∏¶Êúâ \"#REDIRECT\" ÂÜÖÂÆπÁöÑÈáçÂÆöÂêëÊù°ÁõÆ„ÄÇ\nMoegirl is a well-known Chinese wiki website for ACG.\nThis datasets is a raw text version of the 20230814 wiki dump for wiki-zh.moegirl.org.cn reformatted into jsonl dataset. You must perform further data processing for LLM (continual) pretraining.\nSimply‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw.","url":"https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","arrow"],"keywords_longer_than_N":true},
	{"name":"wikibio","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWikiBio @ ACL 2023\n\t\n\nThis is the repository of the WikiBio corpus, which is described in the following paper:\n\nWikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events\n\nPlease use this reference to cite our work\n\n@inproceedings{stranisci-etal-2023-wikibio,    title = \"{W}iki{B}io: a Semantic Resource for the Intersectional Analysis of Biographical Events\",    author = \"Stranisci, Marco Antonio  and\n      Damiano, Rossana  and\n      Mensa, Enrico  and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marco-stranisci/wikibio.","url":"https://huggingface.co/datasets/marco-stranisci/wikibio","creator_name":"Marco Stranisci","creator_url":"https://huggingface.co/marco-stranisci","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_bn","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Bengali version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Bengali"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ne","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Nepali version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Nepali"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 12,802,615 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"AshwinSankar/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/AshwinSankar/Indic-Rag-Suite","creator_name":"Ashwin Sankar","creator_url":"https://huggingface.co/AshwinSankar","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"english-wikipedia-pageviews-by-second","keyword":"wikipedia","description":"This file contains a count of pageviews to the English-language Wikipedia from 2015-03-16T00:00:00 to 2015-04-25T15:59:59, grouped by timestamp (down to a one-second resolution level) and site (mobile or desktop).\nThe smallest number of events in a group is 645; because of this, we are confident there should not be privacy implications of releasing this data.\n","url":"https://huggingface.co/datasets/wikimedia-community/english-wikipedia-pageviews-by-second","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["cc0-1.0","1M - 10M","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"wikidata_triple_en","keyword":"wikidata","description":"RJZ/wikidata_triple_en dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/RJZ/wikidata_triple_en","creator_name":"JZ","creator_url":"https://huggingface.co/RJZ","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","cc0-1.0","10M - 100M","json","Text"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for Text360 Sample Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains text samples from two sources (arXiv and Wikipedia) organized in a hierarchical directory structure. Each sample includes a text field and a subset identifier.\n\n\t\n\t\t\n\t\tData Files Structure\n\t\n\nThe dataset maintains its original directory structure:\n.\n‚îú‚îÄ‚îÄ dir1/\n‚îÇ   ‚îî‚îÄ‚îÄ subdir1/\n‚îÇ       ‚îî‚îÄ‚îÄ sample1.jsonl  # Contains arXiv samples\n‚îî‚îÄ‚îÄ dir2/\n    ‚îî‚îÄ‚îÄ subdir2/\n        ‚îî‚îÄ‚îÄ sample2.jsonl  # Contains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nikhilranjan/test.","url":"https://huggingface.co/datasets/nikhilranjan/test","creator_name":"Nikhil Ranjan","creator_url":"https://huggingface.co/nikhilranjan","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","no-annotation","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mag","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Magahi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Magahi"],"keywords_longer_than_N":true},
	{"name":"MultiSimV2","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for MultiSim Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MultiSim benchmark is a growing collection of text simplification datasets targeted at sentence simplification in several languages.  Currently, the benchmark spans 12 languages.\n\n\n\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\n\nSentence Simplification\n\n\n\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importload_dataset\n\ndataset = load_dataset(\"MichaelR207/MultiSimV2\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this benchmark, please cite our paper:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MichaelR207/MultiSimV2.","url":"https://huggingface.co/datasets/MichaelR207/MultiSimV2","creator_name":"Michael Ryan","creator_url":"https://huggingface.co/MichaelR207","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","text2text-generation","text-generation","English","French"],"keywords_longer_than_N":true},
	{"name":"query-reformulation","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nQRKB-16k (Synthetic Query Reformulation for Knowledge Graphs) is a dataset of 16,384 synthetic query reformulation pairs designed to facilitate research in query understanding and retrieval-augmented generation (RAG) using knowledge graphs. \nEach pair consists of a natural language query and a set of corresponding subqueries, with each subquery structured as a partial semantic triple, suitable for retrieval from knowledge graphs like DBpedia and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexdong/query-reformulation.","url":"https://huggingface.co/datasets/alexdong/query-reformulation","creator_name":"Alex Dong","creator_url":"https://huggingface.co/alexdong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","question-answering","sentence-similarity","English","mit"],"keywords_longer_than_N":true},
	{"name":"autoencoder-paraphrase-dataset","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for Machine Paraphrase Dataset (MPC)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Autoencoder Paraphrase Corpus (APC) consists of ~200k examples of original, and paraphrases using three neural language models.\nIt uses three models (BERT, RoBERTa, Longformer) on three source texts (Wikipedia, arXiv, student theses).\nThe examples are aligned, i.e., we sample the same paragraphs for originals and paraphrased versions.\n\n\t\n\t\t\n\t\tHow to use it\n\t\n\nYou can load the dataset using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset.","url":"https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-tydiqa","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/neulab/PangeaBench-tydiqa.","url":"https://huggingface.co/datasets/neulab/PangeaBench-tydiqa","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"axya-tech-websearch","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDhivehi Combined Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset combines three 30K Dhivehi language corpora from the Leipzig Corpora Collection into a single unified CSV file containing 90,000 sentences. The dataset provides a comprehensive resource for Dhivehi language processing, combining data from Wikipedia, news sources, and web crawls.\n\n\t\n\t\t\n\t\tDataset Composition\n\t\n\nThe dataset consists of three distinct sources:\n\nWikipedia (2021): 30,000 sentences from Dhivehi Wikipedia\nNews‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/axmeeabdhullo/axya-tech-websearch.","url":"https://huggingface.co/datasets/axmeeabdhullo/axya-tech-websearch","creator_name":"Azmy","creator_url":"https://huggingface.co/axmeeabdhullo","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","summarization","translation","Divehi"],"keywords_longer_than_N":true},
	{"name":"Wikipedia-it-Trame-di-Film","keyword":"wikipedia","description":"Collection of plots of historical films and adventure films from Italian Wikipedia (April 2024)\nRaccolta di trame di film storici e film di avventura da Wikipedia italiana (Aprile 2024)\n","url":"https://huggingface.co/datasets/scribis/Wikipedia-it-Trame-di-Film","creator_name":"Fabio Martines","creator_url":"https://huggingface.co/scribis","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Italian","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"wikimovies","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWikipedia Movies Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains 58.1k movie information scraped from Wikipedia's \"List of films\" pages. The data includes basic movie metadata, infobox information, and introductory text from individual movie Wikipedia pages.\nNOTE: This is an uncleaned dataset containing raw scraped data. The content is sourced from Wikipedia and is not owned by the dataset creator. All content remains under Wikipedia's licensing terms.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashassnadig/wikimovies.","url":"https://huggingface.co/datasets/yashassnadig/wikimovies","creator_name":"Yashas Nadig","creator_url":"https://huggingface.co/yashassnadig","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","summarization","English","mit"],"keywords_longer_than_N":true},
	{"name":"enwiki_anchor_pos_negative_490K_qwen2-5_en","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tenwiki_anchor_pos_negative_490K Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 495,470 entries derived from English Wikipedia, meticulously crafted for training and evaluating embedding models. It's particularly suited for fine-tuning models using techniques like MultipleNegativesRankingLoss.\nImportant Note: Each 'positive' text in this dataset is a chunk of information from a Wikipedia article, limited to a maximum of 512 tokens (\"tokenizer_class\": \"XLMRobertaTokenizer\"). This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en.","url":"https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en","creator_name":"CCR","creator_url":"https://huggingface.co/CCRss","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["sentence-similarity","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ml","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Malayalam version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Malayalam"],"keywords_longer_than_N":true},
	{"name":"georgian-text-pairs","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tGeorgian Text Pairs\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains Georgian language text pairs designed for natural language processing tasks. Each entry consists of two related Georgian text segments: a shorter \"positive\" text and a longer \"anchor\" text that provides additional context or elaboration.\n\n\t\n\t\t\n\t\tData Sources\n\t\n\nThe data for this dataset was gathered from multiple sources:\n\nGeorgian Wikipedia: Articles and encyclopedic content\nGeorgian websites: Various web sources‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sithet/georgian-text-pairs.","url":"https://huggingface.co/datasets/sithet/georgian-text-pairs","creator_name":"Irakli Khutsishvili","creator_url":"https://huggingface.co/sithet","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","sentence-similarity","text-classification","Georgian","mit"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ta","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Tamil version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Tamil"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_awa","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Awadhi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Awadhi"],"keywords_longer_than_N":true},
	{"name":"faquad","keyword":"extended|wikipedia","description":"Academic secretaries and faculty members of higher education institutions face a common problem: \n  the abundance of questions sent by academics \n  whose answers are found in available institutional documents. \nThe official documents produced by Brazilian public universities are vast and disperse, \n  which discourage students to further search for answers in such sources.\nIn order to lessen this problem, we present FaQuAD: \n  a novel machine reading comprehension dataset \n  in the domain of Brazilian higher education institutions. \nFaQuAD follows the format of SQuAD (Stanford Question Answering Dataset) [Rajpurkar et al. 2016]. \nIt comprises 900 questions about 249 reading passages (paragraphs), \n  which were taken from 18 official documents of a computer science college \n  from a Brazilian federal university \n  and 21 Wikipedia articles related to Brazilian higher education system. \nAs far as we know, this is the first Portuguese reading comprehension dataset in this format.","url":"https://huggingface.co/datasets/eraldoluis/faquad","creator_name":"Eraldo R. Fernandes","creator_url":"https://huggingface.co/eraldoluis","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"qa_squad","keyword":"extended|wikipedia","description":"SQuAD with the train/validation/test split used in SQuAD QG","url":"https://huggingface.co/datasets/lmqg/qa_squad","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"SyReC","keyword":"wiki","description":"\n\n\t\n\t\t\n\t\tSyReC: The Syntactic Reconstruction Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nSyReC (Syntactic Reconstruction Corpus) is a large-scale, synthetically generated dataset designed to benchmark and train language models on the task of syntactic and semantic reconstruction. The core challenge presented by this dataset is to reconstruct a coherent, grammatically correct paragraph from a \"bag of words\"‚Äîa collection of its constituent words that have been deliberately stripped of their‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ambrosfitz/SyReC.","url":"https://huggingface.co/datasets/ambrosfitz/SyReC","creator_name":"Christopher Smith","creator_url":"https://huggingface.co/ambrosfitz","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"answerable_tydiqa","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"answerable-tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nAnswerable TyDi QA is an extension of the GoldP subtask of the original TyDi QA dataset to also include unanswertable questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains a train and a validation set, with 116067 and 13325 examples, respectively. Access them with\nfrom datasets import load_dataset\ndataset =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/answerable_tydiqa.","url":"https://huggingface.co/datasets/copenlu/answerable_tydiqa","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"qa_squadshifts","keyword":"extended|wikipedia","description":"[SQuAD Shifts](https://modestyachts.github.io/squadshifts-website/index.html) dataset for question answering task with custom split.","url":"https://huggingface.co/datasets/lmqg/qa_squadshifts","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"cmu_wiki_qa","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"cmu_wiki_qa\"\n\t\n\nA filtered / cleaned version of the http://www.cs.cmu.edu/~ark/QA-data/ Q&A dataset, which provides manually-generated factoid questions from Wikipedia articles.\nAcknowledgments\nThese data were collected by Noah Smith, Michael Heilman, Rebecca Hwa, Shay Cohen, Kevin Gimpel, and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010.\nTheir research project was supported by NSF IIS-0713265 (to Smith), an NSF‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sedthh/cmu_wiki_qa.","url":"https://huggingface.co/datasets/sedthh/cmu_wiki_qa","creator_name":"Richard Nagyfi","creator_url":"https://huggingface.co/sedthh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","summarization","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"custom_squad","keyword":"extended|wikipedia","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","url":"https://huggingface.co/datasets/lhoestq/custom_squad","creator_name":"Quentin Lhoest","creator_url":"https://huggingface.co/lhoestq","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"qas","keyword":"extended|wikipedia","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","url":"https://huggingface.co/datasets/fedryanto/qas","creator_name":"Fedryanto Dartiko","creator_url":"https://huggingface.co/fedryanto","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"tydiqa-primary","keyword":"extended|wikipedia","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).","url":"https://huggingface.co/datasets/khalidalt/tydiqa-primary","creator_name":"Khalid Almubarak","creator_url":"https://huggingface.co/khalidalt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"qa_harvesting_from_wikipedia_pseudo","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"lmqg/qa_harvesting_from_wikipedia_pseudo\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a synthetic QA dataset generated with fine-tuned QG models over lmqg/qa_harvesting_from_wikipedia, 1 million paragraph and answer pairs collected in Du and Cardie, 2018, made for question-answering based evaluation (QAE) for question generation model proposed by Zhang and Bansal, 2019.\nThe train split is the synthetic data and the validation split is the original validation set of SQuAD‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmqg/qa_harvesting_from_wikipedia_pseudo.","url":"https://huggingface.co/datasets/lmqg/qa_harvesting_from_wikipedia_pseudo","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"ropes","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for ROPES\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s) (e.g., \"animal pollinators increase efficiency of fertilization in flowers\"), a novel situation that uses this background, and questions that require reasoning about effects of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/allenai/ropes.","url":"https://huggingface.co/datasets/allenai/ropes","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"rebel-dataset-de","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for German REBEL Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\nThe Wikipedia Version is from November 2022. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nGerman\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\"docid\": \"9400003\",\n \"title\": \"Odin-Gletscher\",\n \"uri\": \"Q7077818\",\n \"text\": \"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er flie√üt von den Westh√§ngen des Mount Odin in der‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de.","url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","German","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"rebel-dataset-de","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tDataset Card for German REBEL Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\nThe Wikipedia Version is from November 2022. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nGerman\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\"docid\": \"9400003\",\n \"title\": \"Odin-Gletscher\",\n \"uri\": \"Q7077818\",\n \"text\": \"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er flie√üt von den Westh√§ngen des Mount Odin in der‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de.","url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","German","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"turkish-wikipedia-dataset","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tT√ºrk√ße Kamu Kurumlarƒ± ve Tarih Sohbet Veri Seti\n\t\n\nBu veri seti, T√ºrkiye'deki kamu kurumlarƒ±, bakanlƒ±klar, devlet organlarƒ±, resmi semboller ve tarihi fig√ºrler hakkƒ±nda yapƒ±landƒ±rƒ±lmƒ±≈ü T√ºrk√ße sohbet verileri i√ßermektedir. Veriler, g√ºvenilir ve tarafsƒ±z bir kaynak olan T√ºrk√ße Vikipedi'den otomatik olarak √ßƒ±karƒ±lmƒ±≈ü ve b√ºy√ºk dil modellerini (LLM) ince ayar (fine-tuning) i√ßin uygun bir formata d√∂n√º≈üt√ºr√ºlm√º≈üt√ºr.\nHer bir √∂rnek, bir \"sistem\" talimatƒ±, bir \"kullanƒ±cƒ±\" sorgusu ve bir‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset.","url":"https://huggingface.co/datasets/kaan39/turkish-wikipedia-dataset","creator_name":"Kaan K√∂se","creator_url":"https://huggingface.co/kaan39","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Turkish","mit","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"medwiki","keyword":"extended|wikipedia","description":"MedWiki is a large-scale sentence dataset collected from Wikipedia with medical entity (UMLS) annotations. This dataset is intended for pretraining.","url":"https://huggingface.co/datasets/mvarma/medwiki","creator_name":"Maya Varma","creator_url":"https://huggingface.co/mvarma","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","entity-linking-retrieval","machine-generated","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"Bitcoin_Wikipedia_Webscraper_Example","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tüåê Web Scraper: Turn Any URL into AI-Ready Data\n\t\n\nConvert any public web page into clean, structured JSON in one click. Just paste a URL and this tool scrapes, cleans, and formats the content‚Äîready to be used in any AI or content pipeline.\nWhether you're building datasets for LLMs or feeding fresh content into agents, this no-code tool makes it effortless to extract high-quality data from the web.\n\n\t\n\t\t\n\t\t‚ú® Key Features\n\t\n\n\n‚ö° Scrape Any Public Page ‚Äì Works on blogs, websites, docs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gopher-Lab/Bitcoin_Wikipedia_Webscraper_Example.","url":"https://huggingface.co/datasets/Gopher-Lab/Bitcoin_Wikipedia_Webscraper_Example","creator_name":"Gopher AI","creator_url":"https://huggingface.co/Gopher-Lab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","table-question-answering","zero-shot-classification","English","mit"],"keywords_longer_than_N":true},
	{"name":"wikipedia-article-ratings","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for Wikipedia article ratings (V4)\n\t\n\n1-year dump of English Wikipedia article ratings. The dataset includes 47,207,448 records corresponding to 11,801,862 unique ratings posted between July 22, 2011 and July 22, 2012.\nThe Wikimedia Foundation has been experimenting with a feature to capture reader quality assessments of articles since September 2010. Article Feedback v4 (AFTv4) is a tool allowing readers to rate the quality of an article along 4 different dimensions. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia-community/wikipedia-article-ratings.","url":"https://huggingface.co/datasets/wikimedia-community/wikipedia-article-ratings","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["English","cc0-1.0","10M - 100M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"ELNER-DZ","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tELNER-DZ: Algerian Arabic Dataset for Named Entity Recognition and Entity Linking\n\t\n\nThis dataset, titled ELNER-DZ, was created by Bouguettoucha Hadjer Hanine and Djouablia Ilhem as part of our Master‚Äôs thesis . It is the first large-scale dataset designed for Named Entity Recognition (NER) and Entity Linking (EL) in Algerian Arabic Dialect (Darija), including both Arabic script and Arabizi (Latin-script).\nThis dataset contains over 2 million dialectal sentences labeled with more than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HadjerHaninebgt7878/ELNER-DZ.","url":"https://huggingface.co/datasets/HadjerHaninebgt7878/ELNER-DZ","creator_name":"Hanine_Bgt","creator_url":"https://huggingface.co/HadjerHaninebgt7878","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","token-classification","feature-extraction","Arabic","French"],"keywords_longer_than_N":true},
	{"name":"Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDynamic Topic Modeling Dataset: RedPajama-1T SubSample (100k samples, 1k tokens)\n\t\n\n\n  üìùCheck out the Blog Post\n\n\nThis dataset represents a curated subset of the RedPajama-1T Sample dataset, specifically processed for dynamic topic modeling applications. It contains 100,000 \nsamples from the original dataset, with each document limited to the first 1,024 tokens for consistent processing.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\n\nName:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens.","url":"https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"parlabe-catala-sentences-1m","keyword":"wiki","description":"\n\t\n\t\t\n\t\tParlab√©: corpus de frases en catal√† (500K)\n\t\n\nAquest dataset cont√© 1.000.000 frases correctes en catal√† obtingudes a partir dels datasets p√∫blics TeCla v2 (not√≠cies) i CaWikiTC (Viquip√®dia catalana), provinents del projecte AINA.\nLes frases s‚Äôhan netejat i filtrat per garantir qualitat:\n\nEliminaci√≥ d‚Äôespais duplicats\nSeparaci√≥ en frases amb nltk.sent_tokenize\nLongitud entre 7 i 40 paraules\nEliminaci√≥ de frases amb car√†cters no catalans o erronis\n\n\n\n\t\n\t\t\n\t\n\t\n\t\t√ös principal\n\t\n\nAquest‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Oriolshhh/parlabe-catala-sentences-1m.","url":"https://huggingface.co/datasets/Oriolshhh/parlabe-catala-sentences-1m","creator_name":"Oriol Rebordosa Cots","creator_url":"https://huggingface.co/Oriolshhh","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Catalan","apache-2.0","1M - 10M","text","Text"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-id","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a fork from sql-create-context \nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id.","url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Indonesian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Wikipedia-Turkish-SimpleQA","keyword":"wikipedia","description":"kesitt/Wikipedia-Turkish-SimpleQA dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/kesitt/Wikipedia-Turkish-SimpleQA","creator_name":"Murat Tut","creator_url":"https://huggingface.co/kesitt","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["table-question-answering","Turkish","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mai","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Maithili"],"keywords_longer_than_N":true},
	{"name":"wikiformula","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tNTCIR-WFB\n\t\n\nWiki Formula Browsing Task from NTICR-12 (https://research.nii.ac.jp/ntcir/permission/ntcir-12/perm-en-MathIR.html)\n","url":"https://huggingface.co/datasets/hcju/wikiformula","creator_name":"Haocheng Ju","creator_url":"https://huggingface.co/hcju","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"German-RAG-LLM-EASY-BENCHMARK","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tGerman-RAG-LLM-EASY-BENCHMARK\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis German-RAG-LLM-BENCHMARK represents a specialized collection for evaluating language models with a focus on source citation, time difference stating in RAG-specific tasks.\nTo evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/German-RAG-LLM-EASY-BENCHMARK/\nMost of the Subsets are synthetically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK.","url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","summarization","German","English"],"keywords_longer_than_N":true},
	{"name":"wikiMIA-2024-hard","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWikiMIA-2024 Hard Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nWikiMIA_2024 Hard is a challenging dataset for membership inference attacks intorduced in the paper \"The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage\" containing temporal Wikipedia articles with different versions based on date cutoffs. \nThis dataset is designed to evaluate the robustness of privacy-preserving machine learning models against sophisticated membership inference techniques.\nIt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hallisky/wikiMIA-2024-hard.","url":"https://huggingface.co/datasets/hallisky/wikiMIA-2024-hard","creator_name":"Skyler Hallinan","creator_url":"https://huggingface.co/hallisky","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","apache-2.0","1K - 10K","json","Tabular"],"keywords_longer_than_N":true},
	{"name":"Wiki_Faiss_Indexes","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tdataset_info:\n  features:\n  - name: text\n    dtype: string\n  - name: embeddings\n    dtype: float32\n    shape: [384]\n  configs:\n  - config_name: default\n    data_files: \"*.parquet\"\n\t\n\n\n\t\n\t\t\n\t\tWikipedia IVF-OPQ-PQ Vector Database (GPU-Optimized)\n\t\n\nA high-performance, GPU-accelerated FAISS vector database built from Wikipedia articles with pre-computed embeddings. This dataset contains approximately 35 million Wikipedia articles with 384-dimensional embeddings using the all-MiniLM-L6-v2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ram-G/Wiki_Faiss_Indexes.","url":"https://huggingface.co/datasets/Ram-G/Wiki_Faiss_Indexes","creator_name":"Sri Ram Pavan Kumar Guttikonda","creator_url":"https://huggingface.co/Ram-G","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","text-retrieval","question-answering","English","mit"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-thai","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from sql-create-context.\n@misc{b-mc2_2023_sql-create-context,\n  title   = {sql-create-context Dataset},\n  author  = {b-mc2}, \n  year    = {2023},\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\n  note    = {This dataset was created by modifying data from the following sources: \\cite{zhongSeq2SQL2017, yu2018spider}.},\n}\n\n","url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Thai","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_te","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Telugu version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Telugu"],"keywords_longer_than_N":true},
	{"name":"tydi_xor_rc","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydi_xor_rc\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nXORQA is an extension of the original TyDi QA dataset to also include unanswerable questions, where context documents are only in English but questions are in 7 languages.\nXOR-AttriQA contains annotated attribution data for a sample of XORQA.\nThis dataset is a combined and simplified version of the Reading Comprehension data from XORQA and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/coastalcph/tydi_xor_rc.","url":"https://huggingface.co/datasets/coastalcph/tydi_xor_rc","creator_name":"CoAStaL NLP Group","creator_url":"https://huggingface.co/coastalcph","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"noob-wiki","keyword":"wiki","description":"\n\t\n\t\t\n\t\n\t\n\t\tNoob SDXL Wiki\n\t\n\nThis is the WIKI database for Noob SDXL Models.\n","url":"https://huggingface.co/datasets/Laxhar/noob-wiki","creator_name":"Laxhar Dream Lab","creator_url":"https://huggingface.co/Laxhar","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","üá∫üá∏ Region: US","wiki"],"keywords_longer_than_N":false},
	{"name":"seed-pretrain-decon","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\nPre-training corpus for seed models in \"Scalable Data Ablation Approximations for Language Models through Modular Training and Merging\", to be presented at EMNLP 2024.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/claran/seed-pretrain-decon.","url":"https://huggingface.co/datasets/claran/seed-pretrain-decon","creator_name":"Clara Na","creator_url":"https://huggingface.co/claran","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","English","odc-by","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"Wiki","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Wiki.","url":"https://huggingface.co/datasets/zli12321/Wiki","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"RUwiki","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\t–î–∞—Ç–∞—Å–µ—Ç –∏–∑ 1000 —Å—Ç–∞—Ç–µ–π —Å —Ä—É—Å—Å–∫–æ–π –≤–∏–∫–∏–ø–µ–¥–∏–∏\n\t\n\n\n–ë—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è ruWiki-web-scraper\n–ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è –∑–∞–∫–ª—é—á–µ–Ω–∞ –≤ —Ç–µ–≥–∏ <s_text> </s_text>\n\n","url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","text","Text"],"keywords_longer_than_N":true},
	{"name":"RUwiki","keyword":"wiki","description":"\n\t\n\t\t\n\t\t–î–∞—Ç–∞—Å–µ—Ç –∏–∑ 1000 —Å—Ç–∞—Ç–µ–π —Å —Ä—É—Å—Å–∫–æ–π –≤–∏–∫–∏–ø–µ–¥–∏–∏\n\t\n\n\n–ë—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è ruWiki-web-scraper\n–ö–∞–∂–¥–∞—è —Å—Ç–∞—Ç—å—è –∑–∞–∫–ª—é—á–µ–Ω–∞ –≤ —Ç–µ–≥–∏ <s_text> </s_text>\n\n","url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","text","Text"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_bho","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Bhojpuri version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Bhojpuri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_or","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Odia (Oriya) version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Oriya"],"keywords_longer_than_N":true},
	{"name":"machine-paraphrase-dataset","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for Machine Paraphrase Dataset (MPC)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Machine Paraphrase Corpus (MPC) consists of ~200k examples of original, and paraphrases using two online paraphrasing tools.\nIt uses two paraphrasing tools (SpinnerChief, SpinBot) on three source texts (Wikipedia, arXiv, student theses).\nThe examples are not aligned, i.e., we sample different paragraphs for originals and paraphrased versions.\n\n\t\n\t\t\n\t\tHow to use it\n\t\n\nYou can load the dataset using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset.","url":"https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"wikipedia-citation-index","keyword":"wikipedia","description":"Dataset with citation indexes as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions. Research: ArXiv\n","url":"https://huggingface.co/datasets/lewoniewski/wikipedia-citation-index","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"wikidata","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Entities Connected to Wikipedia\n\t\n\nThis dataset is a multilingual, JSON-formatted version of the Wikidata dump from September 18, 2024. It only includes Wikidata entities that are connected to a Wikipedia page in any language.\nA total of 112,467,802 entities are included in the original data dump, of which 30,072,707 are linked to a Wikipedia page (26.73% of all entities have at least one Wikipedia sitelink).\n\nCurated by: Jonathan Fraine & Philippe Saad√©, Wikimedia Deutschland‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philippesaade/wikidata.","url":"https://huggingface.co/datasets/philippesaade/wikidata","creator_name":"Philippe Saad√©","creator_url":"https://huggingface.co/philippesaade","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","cc0-1.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Bills","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Bills.","url":"https://huggingface.co/datasets/zli12321/Bills","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["other","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"OpenDataGen-factuality-en-v0.1","keyword":"wikipedia","description":"This synthetic dataset was generated using the Open DataGen Python library. (https://github.com/thoddnn/open-datagen)\n\n\t\n\t\t\n\t\tMethodology:\n\t\n\n\nRetrieve random article content from the HuggingFace Wikipedia English dataset.\nConstruct a Chain of Thought (CoT) to generate a Multiple Choice Question (MCQ).\nUtilize a Large Language Model (LLM) to score the results then filter it.\n\nAll these steps are prompted in the 'template.json' file located in the specified code folder.\nCode:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1.","url":"https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1","creator_name":"Thomas","creator_url":"https://huggingface.co/thoddnn","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"wikipedia-1k-cohere-openai-embeddings","keyword":"wikipedia","description":"Smaller version of https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings that includes Cohere as well as OpenAI embeddings (text-embedding-ada-002)\n100k version of this dataset will be released soon. \n","url":"https://huggingface.co/datasets/KShivendu/wikipedia-1k-cohere-openai-embeddings","creator_name":"Kumar Shivendu","creator_url":"https://huggingface.co/KShivendu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"enwiki-did-you-know","keyword":"wikipedia","description":"derenrich/enwiki-did-you-know dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/derenrich/enwiki-did-you-know","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["English","cc-by-3.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"rag-mini-wikipedia","keyword":"wikipedia","description":"In this huggingface discussion you can share what you used the dataset for.\nDerives from https://www.kaggle.com/datasets/rtatman/questionanswer-dataset?resource=download we generated our own subset using generate.py.\n","url":"https://huggingface.co/datasets/rag-datasets/rag-mini-wikipedia","creator_name":"RAG Datasets","creator_url":"https://huggingface.co/rag-datasets","license_name":"Creative Commons Attribution 3.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-3.0.html","language":"en","first_N":5,"first_N_keywords":["question-answering","sentence-similarity","English","cc-by-3.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"flores","keyword":"extended|wikipedia","description":"Evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English.","url":"https://huggingface.co/datasets/facebook-llama/flores","creator_name":"AstroKid","creator_url":"https://huggingface.co/facebook-llama","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["translation","found","found","translation","extended|wikipedia"],"keywords_longer_than_N":true},
	{"name":"squad_v2_sv","keyword":"extended|wikipedia","description":"SQuAD_v2_sv is a Swedish version of SQuAD2.0. Translation was done automatically by using Google Translate API but it is not so straightforward because;\n\n1. the span which determines the start and the end of the answer in the context may vary after translation,\n2. tne translated context may not contain the translated answer if we translate both independently.\n\nMore details on how to handle these will be provided in another blog post.","url":"https://huggingface.co/datasets/susumu2357/squad_v2_sv","creator_name":"Susumu Okazawa","creator_url":"https://huggingface.co/susumu2357","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","Swedish"],"keywords_longer_than_N":true},
	{"name":"SinhalaWikipediaArticles","keyword":"wikipedia","description":"KanishkaRandunu/SinhalaWikipediaArticles dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/KanishkaRandunu/SinhalaWikipediaArticles","creator_name":"Kanishka Randunu ","creator_url":"https://huggingface.co/KanishkaRandunu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Sinhala","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"tydiqa","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/tydiqa.","url":"https://huggingface.co/datasets/google-research-datasets/tydiqa","creator_name":"Google Research Datasets","creator_url":"https://huggingface.co/google-research-datasets","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"UnibQuAD","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for [Squad-UNIB]\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset create by own colecting for NLP task individual\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nAn example of 'train' looks as follows.\n{\n    \"answers\": {\n        \"answer_start\": [1],\n        \"text\": [\"This is a test text\"]\n    },\n    \"context\": \"This is a test context.\",\n    \"id\": \"1\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fedryanto/UnibQuAD.","url":"https://huggingface.co/datasets/fedryanto/UnibQuAD","creator_name":"Fedryanto Dartiko","creator_url":"https://huggingface.co/fedryanto","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"latvian-text","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tLatvian text dataset\n\t\n\nData set of latvian language texts. Intended for use in AI tool development, like speech recognition or spellcheckers\n\n\t\n\t\t\n\t\tData sources used\n\t\n\n\nLatvian Wikisource articles - https://wikisource.org/wiki/Category:Latvian\nLiterary works of Rainis - https://repository.clarin.lv/repository/xmlui/handle/20.500.12574/41\nLatvian Wikipedia articles - https://huggingface.co/datasets/joelito/EU_Wikipedias\nEuropean Parliament Proceedings Parallel Corpus -‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RaivisDejus/latvian-text.","url":"https://huggingface.co/datasets/RaivisDejus/latvian-text","creator_name":"Raivis Dejus","creator_url":"https://huggingface.co/RaivisDejus","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["automatic-speech-recognition","found","found","monolingual","extended|tilde_model"],"keywords_longer_than_N":true},
	{"name":"sql-create-context","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context.","url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"music-wiki","keyword":"wiki","description":"\n\t\n\t\t\n\t\tDataset Card for \"music-wiki\"\n\t\n\nüìöüéµ Introducing music-wiki \nüìäüé∂ Our data collection process unfolds as follows: \n\nStarting with a seed page from Wikipedia's music section, we navigate through a referenced page graph, employing recursive crawling up to a depth of 20 levels.\nSimultaneously, tapping into the rich MusicBrainz dump, we encounter a staggering 11 million unique music entities spanning 10 distinct categories. These entities serve as the foundation for utilizing the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seungheondoh/music-wiki.","url":"https://huggingface.co/datasets/seungheondoh/music-wiki","creator_name":"seungheon.doh","creator_url":"https://huggingface.co/seungheondoh","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"docee-event-classification","keyword":"wiki","description":"\n\t\n\t\t\n\t\tDataset Card for DocEE Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDocEE dataset is an English-language dataset containing more than 27k news and Wikipedia articles. Dataset is primarily annotated and collected for large-scale document-level event extraction.\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ntitle: TODO\ntext: TODO\nevent_type: TODO\ndate: TODO\nmetadata: TODO\n\nNote: this repo contains only event detection portion of the dataset.\n\n\t\n\t\t\n\t\tData Splits\n\t\n\nThe dataset has 2 splits: train and test. Train‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fkdosilovic/docee-event-classification.","url":"https://huggingface.co/datasets/fkdosilovic/docee-event-classification","creator_name":"Filip Karlo Do≈°iloviƒá","creator_url":"https://huggingface.co/fkdosilovic","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"pandas-create-context","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is built from sql-create-context, which in itself builds from WikiSQL and Spider.\nI have used GPT4 to translate the SQL schema into pandas DataFrame schem initialization statements and to translate the SQL queries into pandas queries. \nThere are 862 examples of natural language queries, pandas DataFrame creation statements, and pandas query answering the question using the DataFrame creation statement as context. This dataset was built with text-to-pandas LLMs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hiltch/pandas-create-context.","url":"https://huggingface.co/datasets/hiltch/pandas-create-context","creator_name":"Or Hiltch","creator_url":"https://huggingface.co/hiltch","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"tydiqa_copenlu","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/tydiqa_copenlu.","url":"https://huggingface.co/datasets/copenlu/tydiqa_copenlu","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"chinese-poetry","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tÂîêË©©‰∏âÁôæÈ¶ñ Dataset\n\t\n\nA structured JSON dataset of 320 classical Tang Dynasty poems, sourced from Á∂≠Âü∫ÊñáÂ∫´ÔºàWikisourceÔºâ.\nThis dataset provides clean, machine-readable text data suitable for natural language processing (NLP), classical Chinese analysis, digital humanities, and poetry generation tasks.\n\n\n\t\n\t\t\n\t\n\t\n\t\tüìò Overview\n\t\n\n\nTotal entries: 320\nSource: Wikisource (Traditional Chinese)\nFormat: JSON Lines (.jsonl)\nEncoding: UTF-8\nLanguage: Classical Chinese (ÁπÅÈ´î‰∏≠Êñá)\n\nEach entry corresponds to one‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZoneTwelve/chinese-poetry.","url":"https://huggingface.co/datasets/ZoneTwelve/chinese-poetry","creator_name":"ZoneTwelve","creator_url":"https://huggingface.co/ZoneTwelve","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text-classification","text-retrieval","human-annotated","found"],"keywords_longer_than_N":true},
	{"name":"TyDiP","keyword":"wikipedia","description":"The TyDiP dataset is a dataset of requests in conversations between wikipedia editors\nthat have been annotated for politeness. The splits available below consists of only\nrequests from the top 25 percentile (polite) and bottom 25 percentile (impolite) of\npoliteness scores. The English train set and English test set that are\nadapted from the Stanford Politeness Corpus, and test data in 9 more languages\n(Hindi, Korean, Spanish, Tamil, French, Vietnamese, Russian, Afrikaans, Hungarian) \nwas annotated by us.","url":"https://huggingface.co/datasets/Genius1237/TyDiP","creator_name":"Genius1237","creator_url":"https://huggingface.co/Genius1237","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"tydiqa-goldp","keyword":"extended|wikipedia","description":"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon‚Äôt know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).","url":"https://huggingface.co/datasets/khalidalt/tydiqa-goldp","creator_name":"Khalid Almubarak","creator_url":"https://huggingface.co/khalidalt","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"Dataset2","keyword":"extended|wikipedia","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","url":"https://huggingface.co/datasets/MajdTannous/Dataset2","creator_name":"Majd Tannous","creator_url":"https://huggingface.co/MajdTannous","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"Indic-Rag-Suite","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tüåè Multilingual Indic RAG Suite\n\t\n\nA comprehensive multilingual question-answering dataset covering 18 Indian languages with 21,439,886 total samples, designed for RAG (Retrieval-Augmented Generation) applications and multilingual NLP research.\n\n\t\n\t\t\n\t\tüöÄ Quick Start\n\t\n\nfrom datasets import load_dataset\n\n# Load specific language (recommended)\ndataset = load_dataset(\"ai4bharat/Indic-Rag-Suite\", \"as\")\ntrain_data = dataset['train']\n\nprint(f\"Loaded {len(train_data)} samples\")\n\n# Access‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite.","url":"https://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite","creator_name":"AI4Bharat","creator_url":"https://huggingface.co/ai4bharat","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","multilingual","Assamese","Bengali"],"keywords_longer_than_N":true},
	{"name":"wikidata_descriptions","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tWikidata Descriptions Dataset\n\t\n\nwikidata_descriptions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the English \"description\" available in Wikidata.The corpus contains 26 205 entities.\nWikidata descriptions are short, one-line summaries that concisely state what an entity is.They can be used as lightweight contextual information in entity linking, search, question answering, knowledge-graph completion and many other NLP / IR tasks.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions.","url":"https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikidata_descriptions","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Descriptions Dataset\n\t\n\nwikidata_descriptions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the English \"description\" available in Wikidata.The corpus contains 26 205 entities.\nWikidata descriptions are short, one-line summaries that concisely state what an entity is.They can be used as lightweight contextual information in entity linking, search, question answering, knowledge-graph completion and many other NLP / IR tasks.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions.","url":"https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikipedia_quality_wikirank","keyword":"wikipedia","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\n\n\t\n\t\t\n\t\n\t\n\t\tWhy It‚Äôs Important\n\t\n\n\nEnhances Trust: For readers and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank.","url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"W≈Çodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"QASports2","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nQASports is the first large sports-themed question answering dataset counting over 1 million questions and answers about 124k preprocessed wiki pages, using as documents the wiki of 20 of the most popular sports in the world, like Soccer, American Football, Basketball, Cricket, and so on. Each sport can be downloaded individually as a subset, with the train, test and validation splits, or all subsets can be downloaded together.\n\nüîß Processing scripts:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/leomaurodesenv/QASports2.","url":"https://huggingface.co/datasets/leomaurodesenv/QASports2","creator_name":"Leonardo Mauro","creator_url":"https://huggingface.co/leomaurodesenv","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikipedia","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wiki","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikidata","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_gu","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Gujarati version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Gujarati"],"keywords_longer_than_N":true},
	{"name":"noob-wiki","keyword":"wiki","description":"\n\t\n\t\t\n\t\tNoob SDXL Wiki\n\t\n\nThis is the WIKI database for Noob SDXL Models.\n","url":"https://huggingface.co/datasets/tirta123/noob-wiki","creator_name":"tirta","creator_url":"https://huggingface.co/tirta123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"NQ-256-24-gpt-4o-2024-05-13-803084","keyword":"wikipedia","description":"\n\t\n\t\t\n\t\tNQ-256-24-gpt-4o-2024-05-13-803084 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"question answering dataset search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NQ-256-24-gpt-4o-2024-05-13-803084 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084.","url":"https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"lez_wiki_20240920","keyword":"wiki","description":"leks-forever/lez_wiki_20240920 dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/leks-forever/lez_wiki_20240920","creator_name":"Lezghian Community","creator_url":"https://huggingface.co/leks-forever","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Lezghian","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ur","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Urdu version of the NanoDBPedia dataset, specifically adapted for information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Urdu"],"keywords_longer_than_N":true},
	{"name":"chatjsonsql","keyword":"wikisql","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql.","url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_hne","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Chhattisgarhi version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Chhattisgarhi"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-2025-all-languages","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Multilingual Label Maps 2025\n\t\n\nComprehensive multilingual label and description maps extracted from the 2025 Wikidata dump.This dataset contains labels and descriptions for Wikidata entities (Q-items and P-properties) across 613 languages.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nüìä Total Records: 725,274,530 label/description pairs\nüÜî Unique Entities: 117,229,348 (Q-items and P-properties)\nüåç Languages: 613 unique language codes\nüìù With Descriptions: 339,691,043 pairs (46.8% coverage)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","multilingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"wikidata-label-maps-2025-all-languages","keyword":"wikidata","description":"\n\t\n\t\t\n\t\tWikidata Multilingual Label Maps 2025\n\t\n\nComprehensive multilingual label and description maps extracted from the 2025 Wikidata dump.This dataset contains labels and descriptions for Wikidata entities (Q-items and P-properties) across 613 languages.\n\n\t\n\t\t\n\t\tDataset Overview\n\t\n\n\nüìä Total Records: 725,274,530 label/description pairs\nüÜî Unique Entities: 117,229,348 (Q-items and P-properties)\nüåç Languages: 613 unique language codes\nüìù With Descriptions: 339,691,043 pairs (46.8% coverage)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages.","url":"https://huggingface.co/datasets/yashkumaratri/wikidata-label-maps-2025-all-languages","creator_name":"Yash Kumar Atri","creator_url":"https://huggingface.co/yashkumaratri","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["other","wikidata","multilingual","English","cc0-1.0"],"keywords_longer_than_N":true},
	{"name":"faquad-nli","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for FaQuAD-NLI\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFaQuAD is a Portuguese reading comprehension dataset that follows the format of the Stanford Question Answering Dataset (SQuAD). It is a pioneer Portuguese reading comprehension dataset using the challenging format of SQuAD. The dataset aims to address the problem of abundant questions sent by academics whose answers are found in available institutional documents in the Brazilian higher education system. It consists of 900‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ruanchaves/faquad-nli.","url":"https://huggingface.co/datasets/ruanchaves/faquad-nli","creator_name":"Ruan Chaves Rodrigues","creator_url":"https://huggingface.co/ruanchaves","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"faquad-nli-parquet","keyword":"extended|wikipedia","description":"\n\t\n\t\t\n\t\tDataset Card for FaQuAD-NLI\n\t\n\nTHIS IS A TEMPORARY COPY OF THE ORIGINAL ruanchaves/faquad-nli.\nWHY? As of datasets==4.0, loading scripts and trust_remote_code are no longer supported. \nThis breaks things, like the lm-evaluation-harness-pt, which people who work with Portuguese LLMs need for running evals.\nAs soon as ruanchaves updates his version, I'll delete this copy.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nFaQuAD is a Portuguese reading comprehension dataset that follows the format of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nicholasKluge/faquad-nli-parquet.","url":"https://huggingface.co/datasets/nicholasKluge/faquad-nli-parquet","creator_name":"Nicholas Kluge Corr√™a","creator_url":"https://huggingface.co/nicholasKluge","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","extractive-qa","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ksa","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kashmiri (Arabic script) version of the NanoDBPedia dataset, specifically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kashmiri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_sa","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Sanskrit version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Sanskrit"],"keywords_longer_than_N":true},
	{"name":"Detect-Egyptian-Wikipedia-Articles","keyword":"wikipedia","description":" Detect Egyptian Wikipedia Template-translated Articles \n\n\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nWe release the heuristically filtered, manually processed, and automatically classified Egyptian Arabic Wikipedia articles dataset. This dataset was used to develop a web-based detection system to automatically identify the template-translated articles on the Egyptian Arabic Wikipedia edition. The system is called Egyptian Arabic Wikipedia Scanner and is hosted on Hugging Face Spaces, here:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles.","url":"https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles","creator_name":"Saied Alshahrani","creator_url":"https://huggingface.co/SaiedAlshahrani","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","Egyptian Wikipedia","Arabic","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_kn","keyword":"nanodbpedia","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kannada version of the NanoDBPedia dataset, specifically adapted for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn.","url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kannada"],"keywords_longer_than_N":true}
]
;
