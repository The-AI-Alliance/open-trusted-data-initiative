const data_for_modality_wikipedia = 
[
	{"name":"Wikipedia-Turkish-SimpleQA","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kesitt/Wikipedia-Turkish-SimpleQA","creator_name":"Murat Tut","creator_url":"https://huggingface.co/kesitt","description":"kesitt/Wikipedia-Turkish-SimpleQA dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["table-question-answering","Turkish","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"wikiformula","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hcju/wikiformula","creator_name":"Haocheng Ju","creator_url":"https://huggingface.co/hcju","description":"\n\t\n\t\t\n\t\tNTCIR-WFB\n\t\n\nWiki Formula Browsing Task from NTICR-12 (https://research.nii.ac.jp/ntcir/permission/ntcir-12/perm-en-MathIR.html)\n","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"kurdish-latin-wikipedia-sentences","keyword":"wikipedia","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zinaro/kurdish-latin-wikipedia-sentences","creator_name":"Zinar","creator_url":"https://huggingface.co/zinaro","description":"\n\t\n\t\t\n\t\tKurdish Latin Wikipedia Sentences Dataset\n\t\n\nThis dataset consists of 78,004 Kurdish sentences extracted from Wikipedia. All sentences are written in Latin script and consist of 12 to 18 words. The dataset has been carefully cleaned to remove numbers, dates, or non-textual elements.\n\n\t\n\t\t\n\t\tDataset Highlights\n\t\n\n\nSource: Wikipedia (Kurdish content)\nScript: Kurdish Latin\nContent: Pure textual sentences (no numbers, dates, or special characters)\nSentence Length: 12 to 18 words per… See the full description on the dataset page: https://huggingface.co/datasets/zinaro/kurdish-latin-wikipedia-sentences.","first_N":5,"first_N_keywords":["Kurdish","gpl-3.0","10K - 100K","text","Text"],"keywords_longer_than_N":true},
	{"name":"test","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/nikhilranjan/test","creator_name":"Nikhil Ranjan","creator_url":"https://huggingface.co/nikhilranjan","description":"\n\t\n\t\t\n\t\tDataset Card for Text360 Sample Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains text samples from two sources (arXiv and Wikipedia) organized in a hierarchical directory structure. Each sample includes a text field and a subset identifier.\n\n\t\n\t\t\n\t\tData Files Structure\n\t\n\nThe dataset maintains its original directory structure:\n.\n├── dir1/\n│   └── subdir1/\n│       └── sample1.jsonl  # Contains arXiv samples\n└── dir2/\n    └── subdir2/\n        └── sample2.jsonl  # Contains… See the full description on the dataset page: https://huggingface.co/datasets/nikhilranjan/test.","first_N":5,"first_N_keywords":["text-classification","no-annotation","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"parlabe-catala-sentences-1m","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Oriolshhh/parlabe-catala-sentences-1m","creator_name":"Oriol Rebordosa Cots","creator_url":"https://huggingface.co/Oriolshhh","description":"\n\t\n\t\t\n\t\tParlabé: corpus de frases en català (500K)\n\t\n\nAquest dataset conté 1.000.000 frases correctes en català obtingudes a partir dels datasets públics TeCla v2 (notícies) i CaWikiTC (Viquipèdia catalana), provinents del projecte AINA.\nLes frases s’han netejat i filtrat per garantir qualitat:\n\nEliminació d’espais duplicats\nSeparació en frases amb nltk.sent_tokenize\nLongitud entre 7 i 40 paraules\nEliminació de frases amb caràcters no catalans o erronis\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tÚs principal\n\t\n\nAquest… See the full description on the dataset page: https://huggingface.co/datasets/Oriolshhh/parlabe-catala-sentences-1m.","first_N":5,"first_N_keywords":["Catalan","apache-2.0","1M - 10M","text","Text"],"keywords_longer_than_N":true},
	{"name":"sinhala-articles","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Navanjana/sinhala-articles","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","description":"\n\t\n\t\t\n\t\tSinhala Articles Dataset\n\t\n\nA large-scale, high-quality Sinhala text corpus curated from diverse sources including news articles, Wikipedia entries, and general web content. This dataset is designed to support a wide range of Sinhala Natural Language Processing (NLP) tasks.\n\n\n\t\n\t\t\n\t\t📊 Dataset Overview\n\t\n\n\nName: Navanjana/sinhala-articles\nTotal Samples:  2,148,688\nLanguages: Sinhala (si)\nFeatures:\ntext: A single column containing Sinhala text passages.\n\n\nSize: Approximately 1M < n <… See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/sinhala-articles.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Sinhala","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"sinhala-articles","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Navanjana/sinhala-articles","creator_name":"Navanjana","creator_url":"https://huggingface.co/Navanjana","description":"\n\t\n\t\t\n\t\tSinhala Articles Dataset\n\t\n\nA large-scale, high-quality Sinhala text corpus curated from diverse sources including news articles, Wikipedia entries, and general web content. This dataset is designed to support a wide range of Sinhala Natural Language Processing (NLP) tasks.\n\n\n\t\n\t\t\n\t\t📊 Dataset Overview\n\t\n\n\nName: Navanjana/sinhala-articles\nTotal Samples:  2,148,688\nLanguages: Sinhala (si)\nFeatures:\ntext: A single column containing Sinhala text passages.\n\n\nSize: Approximately 1M < n <… See the full description on the dataset page: https://huggingface.co/datasets/Navanjana/sinhala-articles.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Sinhala","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"wikipedia-citation-index","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lewoniewski/wikipedia-citation-index","creator_name":"Włodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","description":"Dataset with citation indexes as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions. Research: ArXiv\n","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"wikidata_descriptions","keyword":"wikidata","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\n\t\n\t\t\n\t\tWikidata Descriptions Dataset\n\t\n\nwikidata_descriptions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the English \"description\" available in Wikidata.The corpus contains 26 205 entities.\nWikidata descriptions are short, one-line summaries that concisely state what an entity is.They can be used as lightweight contextual information in entity linking, search, question answering, knowledge-graph completion and many other NLP / IR tasks.… See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikidata_descriptions","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\n\t\n\t\t\n\t\tWikidata Descriptions Dataset\n\t\n\nwikidata_descriptions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the English \"description\" available in Wikidata.The corpus contains 26 205 entities.\nWikidata descriptions are short, one-line summaries that concisely state what an entity is.They can be used as lightweight contextual information in entity linking, search, question answering, knowledge-graph completion and many other NLP / IR tasks.… See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikidata_descriptions.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Polish-wikipedia-selected-topics","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Igorrr0/Polish-wikipedia-selected-topics","creator_name":"Igor Pawłowicz","creator_url":"https://huggingface.co/Igorrr0","description":"categories:\n    \"Nauki przyrodnicze\",\n    \"Nauki humanistyczne\",\n    \"Nauki biologiczne\", \n    \"Biologia\",\n    \"Metodologia nauk przyrodniczych\",\n    \"Polska\",\n    \"Nauka w Polsce\",\n    \"Języki Polski\",\n    \"Nauki humanistyczne\",\n    \"Językoznawstwo\",\n    \"Kultuta\",\n    \"Kultura języka\",\n    \"Myrmekologia\",\n    \"Mrówkowate\",\n    \"Ekologia\",\n    \"Prawo w Polsce\",\n    \"Językoznawstwo\",\n    \"Dialektologia\",\n    \"Odmiany i style językowe\",\n    \"Futurologia\"\n\nevery example has max 2000 words in it. \n","first_N":5,"first_N_keywords":["Polish","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"english-wikipedia-pageviews-by-second","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia-community/english-wikipedia-pageviews-by-second","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","description":"This file contains a count of pageviews to the English-language Wikipedia from 2015-03-16T00:00:00 to 2015-04-25T15:59:59, grouped by timestamp (down to a one-second resolution level) and site (mobile or desktop).\nThe smallest number of events in a group is 645; because of this, we are confident there should not be privacy implications of releasing this data.\n","first_N":5,"first_N_keywords":["cc0-1.0","1M - 10M","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"scholarly-article-citations-in-wikipedia","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia-community/scholarly-article-citations-in-wikipedia","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","description":"This dataset includes a list of citations to scholarly articles from a 2015 version of English Wikipedia.\nCitations are in the form of PubMed IDs (pmid) and PubMedCentral IDs (pmcid).\nDigital Object Identifiers (doi)\n\n\t\n\t\t\n\t\tFormat\n\t\n\nEach row in the dataset represents a citation as a (Wikipedia article, scholarly article) pair. Metadata about when the citation was first added is included.\npage_id: The identifier of the Wikipedia article (int), e.g. 1325125\npage_title: The title of the… See the full description on the dataset page: https://huggingface.co/datasets/wikimedia-community/scholarly-article-citations-in-wikipedia.","first_N":5,"first_N_keywords":["English","cc0-1.0","1M - 10M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"sql-parsed","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"pandas-create-context","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hiltch/pandas-create-context","creator_name":"Or Hiltch","creator_url":"https://huggingface.co/hiltch","description":"\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThis dataset is built from sql-create-context, which in itself builds from WikiSQL and Spider.\nI have used GPT4 to translate the SQL schema into pandas DataFrame schem initialization statements and to translate the SQL queries into pandas queries. \nThere are 862 examples of natural language queries, pandas DataFrame creation statements, and pandas query answering the question using the DataFrame creation statement as context. This dataset was built with text-to-pandas… See the full description on the dataset page: https://huggingface.co/datasets/hiltch/pandas-create-context.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"TALI","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Antreas/TALI","creator_name":"Antreas Antoniou","creator_url":"https://huggingface.co/Antreas","description":"\n\t\n\t\t\n\t\tDataset Card for \"TALI\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nTALI is a large-scale, tetramodal dataset designed to facilitate a shift from unimodal and duomodal to tetramodal research in deep learning. It aligns text, video, images, and audio, providing a rich resource for innovative self-supervised learning tasks and multimodal research. TALI enables exploration of how different modalities and data/model scaling affect downstream performance, with the aim of inspiring… See the full description on the dataset page: https://huggingface.co/datasets/Antreas/TALI.","first_N":5,"first_N_keywords":["zero-shot-classification","cc-by-4.0","1M - 10M","parquet","Image"],"keywords_longer_than_N":true},
	{"name":"squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lhoestq/squad","creator_name":"Quentin Lhoest","creator_url":"https://huggingface.co/lhoestq","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"music-wiki","keyword":"wiki","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/seungheondoh/music-wiki","creator_name":"seungheon.doh","creator_url":"https://huggingface.co/seungheondoh","description":"\n\t\n\t\t\n\t\tDataset Card for \"music-wiki\"\n\t\n\n📚🎵 Introducing music-wiki \n📊🎶 Our data collection process unfolds as follows: \n\nStarting with a seed page from Wikipedia's music section, we navigate through a referenced page graph, employing recursive crawling up to a depth of 20 levels.\nSimultaneously, tapping into the rich MusicBrainz dump, we encounter a staggering 11 million unique music entities spanning 10 distinct categories. These entities serve as the foundation for utilizing the… See the full description on the dataset page: https://huggingface.co/datasets/seungheondoh/music-wiki.","first_N":5,"first_N_keywords":["English","mit","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"entity_popularity","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/entity_popularity","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\n\t\n\t\t\n\t\tEntity Popularity Dataset\n\t\n\nThis dataset contains information for about 26,000 entities, including the Wikipedia article title, QID, and the annual article view count for the year 2021. \nThe annual article view count can be considered as an indicator of the popularity of a entity.\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThis dataset is composed in English.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"masaki-sakata/entity_popularity\")[\"en\"]\n\nprint(dataset)\n#… See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/entity_popularity.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"SinhalaWikipediaArticles","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/KanishkaRandunu/SinhalaWikipediaArticles","creator_name":"Kanishka Randunu ","creator_url":"https://huggingface.co/KanishkaRandunu","description":"KanishkaRandunu/SinhalaWikipediaArticles dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Sinhala","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Korean_Wikipedia_Dataset_for_GPT2_August_2022","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022","creator_name":"Yongwoo Jeong","creator_url":"https://huggingface.co/eaglewatch","description":"\n\t\n\t\t\n\t\tDataset Card for korean_wikipedia_dataset_for_GPT2\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nEntire Korean language Wikipedia data for GPT-2 training as of August 1st, 2022.\nemail: oscar.eaglewatch@gmail.com\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is to make a pre-trained GPT-2 Korean model\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nKorean\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\nTrain wikipedia article count: 334420\nvalidation wikipedia article count: 83605\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n'text'\n\n\t\n\t\t\n\t\tData Splits… See the full description on the dataset page: https://huggingface.co/datasets/eaglewatch/Korean_Wikipedia_Dataset_for_GPT2_August_2022.","first_N":5,"first_N_keywords":["question-answering","text2text-generation","translation","visual-question-answering","open-domain-qa"],"keywords_longer_than_N":true},
	{"name":"OneOS","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wasertech/OneOS","creator_name":"Danny Waser","creator_url":"https://huggingface.co/wasertech","description":"\n\t\n\t\t\n\t\tOneOS Dataset\n\t\n\nThe OneOS dataset is a collection of text data for the OneOS project. It consists of a large number of text samples that can be used for training and evaluating natural language processing models.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\nNumber of Samples: 13,068\nLicense: CC0*\nLanguage: English, French\n\n  * Only unlicensed sentences generated manually fall under CreativeCommon-0. Sentences already licensed under different terms, such as nl2bash or samantha-data, remain subject to… See the full description on the dataset page: https://huggingface.co/datasets/wasertech/OneOS.","first_N":5,"first_N_keywords":["English","French","cc0-1.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"wikianc","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\n\t\n\t\t\n\t\tDataset Card for WikiAnc\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \nThe code for generating the dataset can be found here.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nwikificiation: The dataset can be used to train a model for Wikification.\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in all 320… See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc.","first_N":5,"first_N_keywords":["token-classification","machine-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"wikianc","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyanic-selkie/wikianc","creator_name":"cyanic-selkie","creator_url":"https://huggingface.co/cyanic-selkie","description":"\n\t\n\t\t\n\t\tDataset Card for WikiAnc\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe WikiAnc dataset is an automatically generated dataset from Wikipedia (all languages) and Wikidata dumps (August, 2023). \nThe code for generating the dataset can be found here.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nwikificiation: The dataset can be used to train a model for Wikification.\nnamed-entity-linking: The dataset can be used to train a model for Named Entity Linking.\n\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nThe text in the dataset is in all 320… See the full description on the dataset page: https://huggingface.co/datasets/cyanic-selkie/wikianc.","first_N":5,"first_N_keywords":["token-classification","machine-generated","crowdsourced","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-instruction","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often be… See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-id","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is a fork from sql-create-context \nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from… See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Indonesian","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"squad-augmented-v2","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/christti/squad-augmented-v2","creator_name":"Christoph Timmermann","creator_url":"https://huggingface.co/christti","description":"christti/squad-augmented-v2 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"qald_9_plus","keyword":"wikidata","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/casey-martin/qald_9_plus","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","description":"\n\t\n\t\t\n\t\n\t\n\t\tQALD-9-plus Dataset Description\n\t\n\nQALD-9-plus is the dataset for Knowledge Graph Question Answering (KGQA) based on well-known QALD-9.\nQALD-9-plus enables to train and test KGQA systems over DBpedia and Wikidata using questions in 9 different languages: English, German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian.\nSome of the questions have several alternative writings in particular languages which enables to evaluate the robustness of KGQA systems… See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/qald_9_plus.","first_N":5,"first_N_keywords":["table-question-answering","text2text-generation","Bashkir","Belarusian","German"],"keywords_longer_than_N":true},
	{"name":"wikitoxic","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pietrolesci/wikitoxic","creator_name":"Pietro Lesci","creator_url":"https://huggingface.co/pietrolesci","description":"This dataset has been created as an artefact of the paper AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets (Lesci and Vlachos, 2024).\nMore info about this dataset in the appendix of the paper. \nThis is the same dataset as OxAISH-AL-LLM/wiki_toxic.\nThe only differences are:\n\nAddition of a unique identifier, uid.\n\nAddition of the indices, that is, 3 columns with the embeddings of 3 different sentence-transformers\n\nall-mpnet-base-v2\nmulti-qa-mpnet-base-dot-v1… See the full description on the dataset page: https://huggingface.co/datasets/pietrolesci/wikitoxic.","first_N":5,"first_N_keywords":["text-classification","hate-speech-detection","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rajpurkar/squad","creator_name":"Pranav R","creator_url":"https://huggingface.co/rajpurkar","description":"\n\t\n\t\t\n\t\tDataset Card for SQuAD\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\nSQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nQuestion Answering.… See the full description on the dataset page: https://huggingface.co/datasets/rajpurkar/squad.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"wit_base","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/wit_base","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\n\t\n\t\t\n\t\tDataset Card for WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nWikimedia's version of the Wikipedia-based Image Text (WIT) Dataset, a large multimodal multilingual dataset.\nFrom the official blog post:\n\nThe core training data is taken from the Wikipedia Image-Text (WIT) Dataset, a large curated set of more than 37 million image-text associations extracted from Wikipedia articles in 108 languages that was recently released by Google Research.\nThe WIT dataset offers extremely valuable data about the… See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wit_base.","first_N":5,"first_N_keywords":["image-to-text","text-retrieval","image-captioning","machine-generated","found"],"keywords_longer_than_N":true},
	{"name":"structured-wikipedia","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia/structured-wikipedia","creator_name":"Wikimedia","creator_url":"https://huggingface.co/wikimedia","description":"\n\t\n\t\t\n\t\tDataset Card for Wikimedia Structured Wikipedia\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nEarly beta release of pre-parsed English and French Wikipedia articles including infoboxes. Inviting feedback.\nThis dataset contains all articles of the English and French language editions of Wikipedia, pre-parsed and outputted as structured JSON files with a consistent schema (JSONL compressed as zip). Each JSON line holds the content of one full Wikipedia article stripped of… See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/structured-wikipedia.","first_N":5,"first_N_keywords":["language-modeling","masked-language-modeling","English","French","cc-by-sa-4.0"],"keywords_longer_than_N":true},
	{"name":"MultiSimV2","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/MichaelR207/MultiSimV2","creator_name":"Michael Ryan","creator_url":"https://huggingface.co/MichaelR207","description":"\n\t\n\t\t\n\t\tDataset Card for MultiSim Benchmark\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe MultiSim benchmark is a growing collection of text simplification datasets targeted at sentence simplification in several languages.  Currently, the benchmark spans 12 languages.\n\n\n\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\n\nSentence Simplification\n\n\n\t\n\t\n\t\n\t\tUsage\n\t\n\nfrom datasets importload_dataset\n\ndataset = load_dataset(\"MichaelR207/MultiSimV2\")\n\n\n\t\n\t\t\n\t\tCitation\n\t\n\nIf you use this benchmark, please cite our paper:… See the full description on the dataset page: https://huggingface.co/datasets/MichaelR207/MultiSimV2.","first_N":5,"first_N_keywords":["summarization","text2text-generation","text-generation","English","French"],"keywords_longer_than_N":true},
	{"name":"wim_schema_org_wiki_articles","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UWV/wim_schema_org_wiki_articles","creator_name":"UWV","creator_url":"https://huggingface.co/UWV","description":"\n\t\n\t\t\n\t\tDutch Wikipedia Aligned Articles aligned with Schema.org Classes\n\t\n\nDataset Version: 1.0 (2025-06-04)Point of Contact: UWV Netherlands (UWV organization on Hugging Face)License: CC BY-SA 4.0Dataset: UWV/wim_schema_org_wiki_articles\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset provides alignments between Schema.org classes and relevant Dutch Wikipedia articles. Each Schema.org class from a processed subset is linked to up to 20 distinct Wikipedia articles, including their full text, a… See the full description on the dataset page: https://huggingface.co/datasets/UWV/wim_schema_org_wiki_articles.","first_N":5,"first_N_keywords":["text-classification","question-answering","text-generation","feature-extraction","Dutch"],"keywords_longer_than_N":true},
	{"name":"bbaw_egyptian","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phiwi/bbaw_egyptian","creator_name":"Wiesenbach","creator_url":"https://huggingface.co/phiwi","description":"\n\t\n\t\t\n\t\tDataset Card for \"bbaw_egyptian\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset comprises parallel sentences of hieroglyphic encodings, transcription and translation as used in the paper Multi-Task Modeling of Phonographic Languages: Translating Middle Egyptian Hieroglyph. The data triples are extracted from the digital corpus of Egyptian texts compiled by the project \"Strukturen und Transformationen des Wortschatzes der ägyptischen Sprache\".\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards… See the full description on the dataset page: https://huggingface.co/datasets/phiwi/bbaw_egyptian.","first_N":5,"first_N_keywords":["translation","expert-generated","found","multilingual","extended|wikipedia"],"keywords_longer_than_N":true},
	{"name":"ropes","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenai/ropes","creator_name":"Ai2","creator_url":"https://huggingface.co/allenai","description":"\n\t\n\t\t\n\t\tDataset Card for ROPES\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s) (e.g., \"animal pollinators increase efficiency of fertilization in flowers\"), a novel situation that uses this background, and questions that require reasoning about effects of the… See the full description on the dataset page: https://huggingface.co/datasets/allenai/ropes.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/google-research-datasets/tydiqa","creator_name":"Google Research Datasets","creator_url":"https://huggingface.co/google-research-datasets","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in… See the full description on the dataset page: https://huggingface.co/datasets/google-research-datasets/tydiqa.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"wiki-entity-similarity","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Exr0n/wiki-entity-similarity","creator_name":"exr0n","creator_url":"https://huggingface.co/Exr0n","description":"\n\t\n\t\t\n\t\tWiki Entity Similarity\n\t\n\nUsage:\nfrom datasets import load_dataset\n\ncorpus = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20corpus', split='train')\nassert corpus[0] == {'article': 'A1000 road', 'link_text': 'A1000', 'is_same': 1}\n\npairs = load_dataset('Exr0n/wiki-entity-similarity', '2018thresh20pairs', split='train')\nassert corpus[0] == {'article': 'Rhinobatos', 'link_text': 'Ehinobatos beurleni', 'is_same': 1}\nassert len(corpus) == 4_793_180\n\n\n\t\n\t\t\n\t\tCorpus (name=*corpus)… See the full description on the dataset page: https://huggingface.co/datasets/Exr0n/wiki-entity-similarity.","first_N":5,"first_N_keywords":["found","found","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"few-nerd","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DFKI-SLT/few-nerd","creator_name":"Speech and Language Technology, DFKI","creator_url":"https://huggingface.co/DFKI-SLT","description":"Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, \nwhich contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities \nand 4,601,223 tokens. Three benchmark tasks are built, one is supervised: Few-NERD (SUP) and the \nother two are few-shot: Few-NERD (INTRA) and Few-NERD (INTER).","first_N":5,"first_N_keywords":["token-classification","named-entity-recognition","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"medwiki","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mvarma/medwiki","creator_name":"Maya Varma","creator_url":"https://huggingface.co/mvarma","description":"MedWiki is a large-scale sentence dataset collected from Wikipedia with medical entity (UMLS) annotations. This dataset is intended for pretraining.","first_N":5,"first_N_keywords":["text-retrieval","entity-linking-retrieval","machine-generated","crowdsourced","monolingual"],"keywords_longer_than_N":true},
	{"name":"catalan_textual_corpus","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/projecte-aina/catalan_textual_corpus","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\n\t\n\t\t\n\t\tDataset Card for Catalan Textual Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Catalan Textual Corpus is a 1760-million-token web corpus of Catalan built from several sources.\nIt consists of 1,758,388,896 tokens, 73,172,152 sentences, and 12,556,365 documents. Documents are separated by single new lines. These boundaries have been preserved as long as the license allowed it.\nThis work is licensed under a Creative Commons Attribution Share Alike 4.0 International license.… See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/catalan_textual_corpus.","first_N":5,"first_N_keywords":["fill-mask","no-annotation","found","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"WikiConvert","keyword":"extended|wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/usc-isi/WikiConvert","creator_name":"USC Information Sciences Institute","creator_url":"https://huggingface.co/usc-isi","description":"Language Modelling with Cardinal Number Annotations.","first_N":5,"first_N_keywords":["fill-mask","other","text-generation","language-modeling","masked-language-modeling"],"keywords_longer_than_N":true},
	{"name":"answerable_tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/answerable_tydiqa","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\n\t\n\t\t\n\t\tDataset Card for \"answerable-tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nAnswerable TyDi QA is an extension of the GoldP subtask of the original TyDi QA dataset to also include unanswertable questions.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset contains a train and a validation set, with 116067 and 13325 examples, respectively. Access them with\nfrom datasets import load_dataset\ndataset =… See the full description on the dataset page: https://huggingface.co/datasets/copenlu/answerable_tydiqa.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"tydiqa_copenlu","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/copenlu/tydiqa_copenlu","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in… See the full description on the dataset page: https://huggingface.co/datasets/copenlu/tydiqa_copenlu.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"wiki_toxic","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic","creator_name":"OxAI Safety Hub Active Learning with Large Language Models Labs Team","creator_url":"https://huggingface.co/OxAISH-AL-LLM","description":"Jigsaw Toxic Comment Challenge dataset. This dataset was the basis of a Kaggle competition run by Jigsaw","first_N":5,"first_N_keywords":["text-classification","hate-speech-detection","crowdsourced","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"docee-event-classification","keyword":"wiki","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fkdosilovic/docee-event-classification","creator_name":"Filip Karlo Došilović","creator_url":"https://huggingface.co/fkdosilovic","description":"\n\t\n\t\t\n\t\tDataset Card for DocEE Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nDocEE dataset is an English-language dataset containing more than 27k news and Wikipedia articles. Dataset is primarily annotated and collected for large-scale document-level event extraction.\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n\ntitle: TODO\ntext: TODO\nevent_type: TODO\ndate: TODO\nmetadata: TODO\n\nNote: this repo contains only event detection portion of the dataset.\n\n\t\n\t\t\n\t\tData Splits\n\t\n\nThe dataset has 2 splits: train and test. Train… See the full description on the dataset page: https://huggingface.co/datasets/fkdosilovic/docee-event-classification.","first_N":5,"first_N_keywords":["text-classification","multi-class-classification","monolingual","original","English"],"keywords_longer_than_N":true},
	{"name":"machine-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\n\t\n\t\t\n\t\tDataset Card for Machine Paraphrase Dataset (MPC)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Machine Paraphrase Corpus (MPC) consists of ~200k examples of original, and paraphrases using two online paraphrasing tools.\nIt uses two paraphrasing tools (SpinnerChief, SpinBot) on three source texts (Wikipedia, arXiv, student theses).\nThe examples are not aligned, i.e., we sample different paragraphs for originals and paraphrased versions.\n\n\t\n\t\t\n\t\tHow to use it\n\t\n\nYou can load the dataset using the… See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/machine-paraphrase-dataset.","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"autoencoder-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\n\t\n\t\t\n\t\tDataset Card for Machine Paraphrase Dataset (MPC)\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe Autoencoder Paraphrase Corpus (APC) consists of ~200k examples of original, and paraphrases using three neural language models.\nIt uses three models (BERT, RoBERTa, Longformer) on three source texts (Wikipedia, arXiv, student theses).\nThe examples are aligned, i.e., we sample the same paragraphs for originals and paraphrased versions.\n\n\t\n\t\t\n\t\tHow to use it\n\t\n\nYou can load the dataset using the… See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoencoder-paraphrase-dataset.","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"autoregressive-paraphrase-dataset","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset","creator_name":"Jan Philip Wahle","creator_url":"https://huggingface.co/jpwahle","description":"\n\t\n\t\t\n\t\tDataset Card for [Dataset Name]\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tLanguages\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n\n\t\n\t\t\n\t\tData Instances\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Fields\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tData Splits\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tDataset Creation\n\t\n\n\n\t\n\t\t\n\t\tCuration Rationale\n\t\n\n[More Information Needed]\n\n\t\n\t\t\n\t\tSource Data… See the full description on the dataset page: https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset.","first_N":5,"first_N_keywords":["text-classification","text-generation","machine-generated","machine-generated","monolingual"],"keywords_longer_than_N":true},
	{"name":"TyDiP","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Genius1237/TyDiP","creator_name":"Genius1237","creator_url":"https://huggingface.co/Genius1237","description":"The TyDiP dataset is a dataset of requests in conversations between wikipedia editors\nthat have been annotated for politeness. The splits available below consists of only\nrequests from the top 25 percentile (polite) and bottom 25 percentile (impolite) of\npoliteness scores. The English train set and English test set that are\nadapted from the Stanford Politeness Corpus, and test data in 9 more languages\n(Hindi, Korean, Spanish, Tamil, French, Vietnamese, Russian, Afrikaans, Hungarian) \nwas annotated by us.","first_N":5,"first_N_keywords":["text-classification","crowdsourced","found","multilingual","English"],"keywords_longer_than_N":true},
	{"name":"da-wit","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/da-wit","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images… See the full description on the dataset page: https://huggingface.co/datasets/alexandrainst/da-wit.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"danish-wit","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/severo/danish-wit","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images… See the full description on the dataset page: https://huggingface.co/datasets/severo/danish-wit.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"qa_squad","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmqg/qa_squad","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","description":"SQuAD with the train/validation/test split used in SQuAD QG","first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"skquad","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TUKE-DeutscheTelekom/skquad","creator_name":"TUKE and DTSS cooperation","creator_url":"https://huggingface.co/TUKE-DeutscheTelekom","description":"\n\t\n\t\t\n\t\tDataset Card for SkQuAD\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSK-QuAD is the first QA dataset for the Slovak language.\nIt is manually annotated, so it has no distortion caused by\nmachine translation. The dataset is thematically diverse – it\ndoes not overlap with SQuAD – it brings new knowledge.\nIt passed the second round of annotation – each question\nand the answer were seen by at least two annotators.\n\n\t\n\t\t\n\t\tSupported Tasks\n\t\n\n\nQuestion answering\n\n\n\t\n\t\t\n\t\tData Fields\n\t\n\nThe data fields are… See the full description on the dataset page: https://huggingface.co/datasets/TUKE-DeutscheTelekom/skquad.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","open-domain-qa","extractive-qa","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"qa_squadshifts_synthetic","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic","creator_name":"LMQG","creator_url":"https://huggingface.co/lmqg","description":"\n\t\n\t\t\n\t\tDataset Card for \"lmqg/qa_squadshifts_synthetic\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis is a synthetic QA dataset generated with fine-tuned QG models over lmqg/qa_squadshifts, made for question-answering based evaluation (QAE) for question generation model proposed by Zhang and Bansal, 2019.\nThe test split is the original validation set of lmqg/qa_squadshifts, where the model should be evaluate on.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks and Leaderboards\n\t\n\n\nquestion-answering\n\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages… See the full description on the dataset page: https://huggingface.co/datasets/lmqg/qa_squadshifts_synthetic.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","monolingual","extended|wikipedia","English"],"keywords_longer_than_N":true},
	{"name":"scandi-wiki","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexandrainst/scandi-wiki","creator_name":"Alexandra Institute","creator_url":"https://huggingface.co/alexandrainst","description":"ScandiWiki is a parsed and deduplicated version of the Danish, Norwegian Bokmål,\nNorwegian Nynorsk, Swedish, Icelandic and Faroese Wikipedia corpora, as of January\n2023.","first_N":5,"first_N_keywords":["fill-mask","text-generation","feature-extraction","language-modeling","multilingual"],"keywords_longer_than_N":true},
	{"name":"cmu_wiki_qa","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sedthh/cmu_wiki_qa","creator_name":"Richard Nagyfi","creator_url":"https://huggingface.co/sedthh","description":"\n\t\n\t\t\n\t\tDataset Card for \"cmu_wiki_qa\"\n\t\n\nA filtered / cleaned version of the http://www.cs.cmu.edu/~ark/QA-data/ Q&A dataset, which provides manually-generated factoid questions from Wikipedia articles.\nAcknowledgments\nThese data were collected by Noah Smith, Michael Heilman, Rebecca Hwa, Shay Cohen, Kevin Gimpel, and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010.\nTheir research project was supported by NSF IIS-0713265 (to Smith), an NSF… See the full description on the dataset page: https://huggingface.co/datasets/sedthh/cmu_wiki_qa.","first_N":5,"first_N_keywords":["question-answering","summarization","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"faquad-nli","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ruanchaves/faquad-nli","creator_name":"Ruan Chaves Rodrigues","creator_url":"https://huggingface.co/ruanchaves","description":"\n\t\n\t\t\n\t\tDataset Card for FaQuAD-NLI\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nFaQuAD is a Portuguese reading comprehension dataset that follows the format of the Stanford Question Answering Dataset (SQuAD). It is a pioneer Portuguese reading comprehension dataset using the challenging format of SQuAD. The dataset aims to address the problem of abundant questions sent by academics whose answers are found in available institutional documents in the Brazilian higher education system. It consists of 900… See the full description on the dataset page: https://huggingface.co/datasets/ruanchaves/faquad-nli.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","expert-generated","found","monolingual"],"keywords_longer_than_N":true},
	{"name":"latvian-text","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RaivisDejus/latvian-text","creator_name":"Raivis Dejus","creator_url":"https://huggingface.co/RaivisDejus","description":"\n\t\n\t\t\n\t\tLatvian text dataset\n\t\n\nData set of latvian language texts. Intended for use in AI tool development, like speech recognition or spellcheckers\n\n\t\n\t\t\n\t\tData sources used\n\t\n\n\nLatvian Wikisource articles - https://wikisource.org/wiki/Category:Latvian\nLiterary works of Rainis - https://repository.clarin.lv/repository/xmlui/handle/20.500.12574/41\nLatvian Wikipedia articles - https://huggingface.co/datasets/joelito/EU_Wikipedias\nEuropean Parliament Proceedings Parallel Corpus -… See the full description on the dataset page: https://huggingface.co/datasets/RaivisDejus/latvian-text.","first_N":5,"first_N_keywords":["automatic-speech-recognition","found","found","monolingual","extended|tilde_model"],"keywords_longer_than_N":true},
	{"name":"squad-sk","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TUKE-DeutscheTelekom/squad-sk","creator_name":"TUKE and DTSS cooperation","creator_url":"https://huggingface.co/TUKE-DeutscheTelekom","description":"        Slovak translation of Standford Question Answering Dataset","first_N":5,"first_N_keywords":["question-answering","text-retrieval","open-domain-qa","extractive-qa","document-retrieval"],"keywords_longer_than_N":true},
	{"name":"tamil_sentences_sample","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AnanthZeke/tamil_sentences_sample","creator_name":"Ananth","creator_url":"https://huggingface.co/AnanthZeke","description":"\n\t\n\t\t\n\t\tDataset Card for \"tamil_combined_sentences\"\n\t\n\nMore Information needed\n","first_N":5,"first_N_keywords":["sentence-similarity","zero-shot-classification","Tamil","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"NewQA","keyword":"extended|wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/badokorach/NewQA","creator_name":"brenda Adokorach","creator_url":"https://huggingface.co/badokorach","description":"\n\t\n\t\t\n\t\tDataset Card for \"squad\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n\n\t\n\t\t\n\t\tSupported Tasks and Leaderboards\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tLanguages\n\t\n\nMore Information Needed\n\n\t\n\t\t\n\t\tDataset… See the full description on the dataset page: https://huggingface.co/datasets/badokorach/NewQA.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","found"],"keywords_longer_than_N":true},
	{"name":"rebel-dataset-de","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","description":"\n\t\n\t\t\n\t\tDataset Card for German REBEL Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\nThe Wikipedia Version is from November 2022. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nGerman\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\"docid\": \"9400003\",\n \"title\": \"Odin-Gletscher\",\n \"uri\": \"Q7077818\",\n \"text\": \"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er fließt von den Westhängen des Mount Odin in der… See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de.","first_N":5,"first_N_keywords":["summarization","German","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"rebel-dataset-de","keyword":"wikidata","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mingaflo/rebel-dataset-de","creator_name":"Florian","creator_url":"https://huggingface.co/mingaflo","description":"\n\t\n\t\t\n\t\tDataset Card for German REBEL Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset is the German version of Babelscape/rebel-dataset. It has been generated using CROCODILE.\nThe Wikipedia Version is from November 2022. \n\n\t\n\t\t\n\t\tLanguages\n\t\n\n\nGerman\n\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\n{\"docid\": \"9400003\",\n \"title\": \"Odin-Gletscher\",\n \"uri\": \"Q7077818\",\n \"text\": \"Der Odin-Gletscher ist ein kleiner Gletscher im ostantarktischen Viktorialand. Er fließt von den Westhängen des Mount Odin in der… See the full description on the dataset page: https://huggingface.co/datasets/mingaflo/rebel-dataset-de.","first_N":5,"first_N_keywords":["summarization","German","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"sql-create-context","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column… See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-copy","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\n\t\n\t\t\n\t\tFork of b-mc2/sql-create-context\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from… See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"wikipedia-zh-mnbvc","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc","creator_name":"wangjunjie","creator_url":"https://huggingface.co/wanng","description":"\n\t\n\t\t\n\t\tzhwiki-mnbvc\n\t\n\n分项目：爬取并处理中文维基百科语料\n数据时间：202302-202305 （持续更新）\n主项目：MNBVC(Massive Never-ending BT Vast Chinese corpus)超大规模中文语料集 https://github.com/esbatmop/MNBVC\n该项目清洗流程主要参考：https://kexue.fm/archives/4176/comment-page-1\n并且使用组员开发的去重工具进行数据格式化。\n总行数（样本）: 10,754,146\n一个示例：\n{\n  \"文件名\": \"cleaned/zhwiki-20230420/folder_0/723712.txt\",\n  \"是否待查文件\": false,\n  \"是否重复文件\": false,\n  \"文件大小\": 558,\n  \"simhash\": 14363740497821204542,\n  \"最长段落长度\": 142,\n  \"段落数\": 6,\n  \"去重段落数\": 6,\n  \"低质量段落数\": 0,\n  \"段落\": [\n    {… See the full description on the dataset page: https://huggingface.co/datasets/wanng/wikipedia-zh-mnbvc.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","apache-2.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"wikipedia-1k-cohere-openai-embeddings","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/KShivendu/wikipedia-1k-cohere-openai-embeddings","creator_name":"Kumar Shivendu","creator_url":"https://huggingface.co/KShivendu","description":"Smaller version of https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings that includes Cohere as well as OpenAI embeddings (text-embedding-ada-002)\n100k version of this dataset will be released soon. \n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"wikipedia_tw","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jslin09/wikipedia_tw","creator_name":"Chun-Hsien Lin","creator_url":"https://huggingface.co/jslin09","description":"要搞自己的大型語言模型，最基本的基本，就是需要一大堆文字資料，從 Common Crawl 上頭抓回來慢慢清洗是一條路，清洗維基百科網站的週期性下載檔也是一個方法。本資料集是解析自維基百科於 20250401 發布的繁體中文版打包檔 bz2 檔案的內容，在解析出所需內容後，利用 wikitextparser 移除 Wiki 標記。解析後保留的欄位有兩個：條目名稱（title），條目內容（page article）。\n原始的打包檔條目內容簡繁混雜，所以有利用 OpenCC 進行簡轉繁處理。\n\n全部 4,635,681 個條目\n全部 1,471,195 個條目標題\n無法自動去標記的條目數: 3,164,486\n有內容的條目數: 1,471,195\n\n因為本資料集內容龐大，要塞進一般的個人電腦中進行計算，恐怕會有資源不足的情形。建議使用parquet格式下載使用。\n資料集當中有不少內容為 #REDIRECT 的條目已經嘗試移除，如果移除的不乾淨，就等以後有空推出修正版再來清洗了。\n","first_N":5,"first_N_keywords":["monolingual","wikipedia","Chinese","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"wikidata-en-descriptions","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","description":"derenrich/wikidata-en-descriptions dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["summarization","English","cc-by-sa-4.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"wikidata-en-descriptions","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/derenrich/wikidata-en-descriptions","creator_name":"Daniel Erenrich","creator_url":"https://huggingface.co/derenrich","description":"derenrich/wikidata-en-descriptions dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["summarization","English","cc-by-sa-4.0","1M - 10M","parquet"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_bn","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Bengali version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bn.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Bengali"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_hi","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Hindi version of the NanoDBPedia dataset, specifically adapted for information… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hi.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Hindi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_hne","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Chhattisgarhi version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_hne.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Chhattisgarhi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ksd","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kashmiri (Devanagari script) version of the NanoDBPedia dataset, specifically… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksd.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kashmiri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mag","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Magahi version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mag.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Magahi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ml","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Malayalam version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ml.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Malayalam"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mni","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Manipuri version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mni.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Manipuri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ne","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Nepali version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ne.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Nepali"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_or","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Odia (Oriya) version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_or.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Oriya"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ta","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Tamil version of the NanoDBPedia dataset, specifically adapted for information… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ta.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Tamil"],"keywords_longer_than_N":true},
	{"name":"chatjsonsql","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from WikiSQL and Spider.\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column… See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql.","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","English","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"wikidata","keyword":"wikidata","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philippesaade/wikidata","creator_name":"Philippe Saadé","creator_url":"https://huggingface.co/philippesaade","description":"\n\t\n\t\t\n\t\tWikidata Entities Connected to Wikipedia\n\t\n\nThis dataset is a multilingual, JSON-formatted version of the Wikidata dump from September 18, 2024. It only includes Wikidata entities that are connected to a Wikipedia page in any language.\nA total of 112,467,802 entities are included in the original data dump, of which 30,072,707 are linked to a Wikipedia page (26.73% of all entities have at least one Wikipedia sitelink).\n\nCurated by: Jonathan Fraine & Philippe Saadé, Wikimedia Deutschland… See the full description on the dataset page: https://huggingface.co/datasets/philippesaade/wikidata.","first_N":5,"first_N_keywords":["multilingual","cc0-1.0","1M - 10M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lez_wiki_20240920","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/leks-forever/lez_wiki_20240920","creator_name":"Lezghian Community","creator_url":"https://huggingface.co/leks-forever","description":"leks-forever/lez_wiki_20240920 dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text-generation","Lezghian","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"enwiki_anchor_pos_negative_490K_qwen2-5_en","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en","creator_name":"CCR","creator_url":"https://huggingface.co/CCRss","description":"\n\t\n\t\t\n\t\tenwiki_anchor_pos_negative_490K Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset contains 495,470 entries derived from English Wikipedia, meticulously crafted for training and evaluating embedding models. It's particularly suited for fine-tuning models using techniques like MultipleNegativesRankingLoss.\nImportant Note: Each 'positive' text in this dataset is a chunk of information from a Wikipedia article, limited to a maximum of 512 tokens (\"tokenizer_class\": \"XLMRobertaTokenizer\"). This… See the full description on the dataset page: https://huggingface.co/datasets/CCRss/enwiki_anchor_pos_negative_490K_qwen2-5_en.","first_N":5,"first_N_keywords":["sentence-similarity","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai… See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikidata","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai… See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"UniHGKR_Date_Text_Pairs","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs","creator_name":"Dehai Min","creator_url":"https://huggingface.co/ZhishanQ","description":"See description and preview to understand the content and structure of this corpus.\nThis dataset is from our paper: UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers.\nPlease see our github repository UniHGKR to know how to use this dataset and its format.\nIf you find this resource useful in your research, please consider giving a like and citation.\n@article{min2024unihgkr,\n  title={UniHGKR: Unified Instruction-aware Heterogeneous Knowledge Retrievers},\n  author={Min, Dehai… See the full description on the dataset page: https://huggingface.co/datasets/ZhishanQ/UniHGKR_Date_Text_Pairs.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","1M - 10M","json"],"keywords_longer_than_N":true},
	{"name":"PangeaBench-tydiqa","keyword":"extended|wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/neulab/PangeaBench-tydiqa","creator_name":"NeuLab @ LTI/CMU","creator_url":"https://huggingface.co/neulab","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydiqa\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in… See the full description on the dataset page: https://huggingface.co/datasets/neulab/PangeaBench-tydiqa.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"chinese_moegirl_wiki_corpus_raw","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tChinese Moegirl ACG Corpus (Raw Data)\n\t\n\nMoegirl 是个中文二次元 wiki 网站\n本项目对 20230814 wiki dump for wiki-zh.moegirl.org.cn 只进行了简单的数据格式处理（xml -> jsonl dataset），后续如想作为 LLM 预训练语料，务必进行各种文本清洗。\n简单使用正则给每条数据增加了 tag；直接过滤掉所有带有 \"#REDIRECT\" 内容的重定向条目。\nMoegirl is a well-known Chinese wiki website for ACG.\nThis datasets is a raw text version of the 20230814 wiki dump for wiki-zh.moegirl.org.cn reformatted into jsonl dataset. You must perform further data processing for LLM (continual) pretraining.\nSimply… See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/chinese_moegirl_wiki_corpus_raw.","first_N":5,"first_N_keywords":["text-generation","Chinese","cc-by-4.0","100K - 1M","arrow"],"keywords_longer_than_N":true},
	{"name":"test-big-dataset","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huggingface/test-big-dataset","creator_name":"Hugging Face","creator_url":"https://huggingface.co/huggingface","description":"\n\t\n\t\t\n\t\tDataset Card for Danish WIT\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nGoogle presented the Wikipedia Image Text (WIT) dataset in July\n2021, a dataset which contains\nscraped images from Wikipedia along with their descriptions. WikiMedia released\nWIT-Base in September\n2021,\nbeing a modified version of WIT where they have removed the images with empty\n\"reference descriptions\", as well as removing images where a person's face covers more\nthan 10% of the image surface, along with inappropriate images… See the full description on the dataset page: https://huggingface.co/datasets/huggingface/test-big-dataset.","first_N":5,"first_N_keywords":["image-to-text","zero-shot-image-classification","feature-extraction","image-captioning","wikimedia/wit_base"],"keywords_longer_than_N":true},
	{"name":"GreekWikipedia","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IMISLab/GreekWikipedia","creator_name":"IMIS Lab UPatras","creator_url":"https://huggingface.co/IMISLab","description":"\n\t\n\t\t\n\t\tGreekWikipedia\n\t\n\nA Greek abstractive summarization dataset collected from the Greek part of Wikipedia, which contains 93,432 articles, their titles and summaries.\nThis dataset has been used to train our best-performing model GreekWiki-umt5-base as part of our upcoming research article:Giarelis, N., Mastrokostas, C., & Karacapilidis, N. (2024) Greek Wikipedia: A Study on Abstractive Summarization.For information about dataset creation, limitations etc. see the original article.… See the full description on the dataset page: https://huggingface.co/datasets/IMISLab/GreekWikipedia.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","Greek","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"wiki-talks","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lflage/wiki-talks","creator_name":"Lucas Fonseca Lage","creator_url":"https://huggingface.co/lflage","description":"\n\t\n\t\t\n\t\tWiki-Talks\n\t\n\nThe Wiki-Talks dataset is a collection of conversational threads extracted from the talk pages on Wikipedia.\nThis dataset captures collaborative dialogue, discussion patterns, and consensus-building among Wikipedia contributors.\nIt is useful for NLP research focused on dialogue, sentiment analysis, and community dynamics.\n\n\t\n\t\t\n\t\tDetails\n\t\n\nCurrently due to PyArrow incompatibility to the long recursive structures in the dataset there is an intrinsic incompatibility… See the full description on the dataset page: https://huggingface.co/datasets/lflage/wiki-talks.","first_N":5,"first_N_keywords":["English","German","Portuguese","Spanish","French"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_as","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Assamese version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_as.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Assamese"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_awa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Awadhi version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_awa.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Awadhi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_bho","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Bhojpuri version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_bho.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Bhojpuri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_gu","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Gujarati version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_gu.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Gujarati"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_kn","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kannada version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_kn.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kannada"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ksa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Kashmiri (Arabic script) version of the NanoDBPedia dataset, specifically… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ksa.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Kashmiri"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mai","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Maithili version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mai.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Maithili"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_mr","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Marathi version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_mr.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Marathi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_pa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Punjabi version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_pa.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Panjabi"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_sa","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Sanskrit version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_sa.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Sanskrit"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_te","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Telugu version of the NanoDBPedia dataset, specifically adapted for… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_te.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Telugu"],"keywords_longer_than_N":true},
	{"name":"Bharat_NanoDBPedia_ur","keyword":"nanodbpedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur","creator_name":"Arun","creator_url":"https://huggingface.co/carlfeynman","description":"\n\t\n\t\t\n\t\tBharat-NanoBEIR: Indian Language Information Retrieval Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is part of the Bharat-NanoBEIR collection, which provides information retrieval datasets for Indian languages. It is derived from the NanoBEIR project, which offers smaller versions of BEIR datasets containing 50 queries and up to 10K documents each.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis particular dataset is the Urdu version of the NanoDBPedia dataset, specifically adapted for information… See the full description on the dataset page: https://huggingface.co/datasets/carlfeynman/Bharat_NanoDBPedia_ur.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","monolingual","NanoDBPedia","Urdu"],"keywords_longer_than_N":true},
	{"name":"wikibio","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/marco-stranisci/wikibio","creator_name":"Marco Stranisci","creator_url":"https://huggingface.co/marco-stranisci","description":"\n\t\n\t\t\n\t\tWikiBio @ ACL 2023\n\t\n\nThis is the repository of the WikiBio corpus, which is described in the following paper:\n\nWikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events\n\nPlease use this reference to cite our work\n\n@inproceedings{stranisci-etal-2023-wikibio,    title = \"{W}iki{B}io: a Semantic Resource for the Intersectional Analysis of Biographical Events\",    author = \"Stranisci, Marco Antonio  and\n      Damiano, Rossana  and\n      Mensa, Enrico  and… See the full description on the dataset page: https://huggingface.co/datasets/marco-stranisci/wikibio.","first_N":5,"first_N_keywords":["token-classification","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Bills","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zli12321/Bills","creator_name":"LZX","creator_url":"https://huggingface.co/zli12321","description":"\n\t\n\t\t\n\t\tDataset Overview\n\t\n\nThis repository contains benchmark datasets for evaluating Large Language Model (LLM)-based topic discovery methods and comparing them against traditional topic models.  These datasets provide a valuable resource for researchers studying topic modeling and LLM capabilities in this domain.  The work is described in the following paper: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs.  Original data… See the full description on the dataset page: https://huggingface.co/datasets/zli12321/Bills.","first_N":5,"first_N_keywords":["other","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"PUGG_IR-qrels","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_IR-qrels","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_IR-qrels.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"Detect-Egyptian-Wikipedia-Articles","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles","creator_name":"Saied Alshahrani","creator_url":"https://huggingface.co/SaiedAlshahrani","description":" Detect Egyptian Wikipedia Template-translated Articles \n\n\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nWe release the heuristically filtered, manually processed, and automatically classified Egyptian Arabic Wikipedia articles dataset. This dataset was used to develop a web-based detection system to automatically identify the template-translated articles on the Egyptian Arabic Wikipedia edition. The system is called Egyptian Arabic Wikipedia Scanner and is hosted on Hugging Face Spaces, here:… See the full description on the dataset page: https://huggingface.co/datasets/SaiedAlshahrani/Detect-Egyptian-Wikipedia-Articles.","first_N":5,"first_N_keywords":["text-classification","Egyptian Wikipedia","Arabic","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"wikidata_triple_en","keyword":"wikidata","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/RJZ/wikidata_triple_en","creator_name":"JZ","creator_url":"https://huggingface.co/RJZ","description":"RJZ/wikidata_triple_en dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["English","cc0-1.0","10M - 100M","json","Text"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]… See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"day_in_history","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/santhosh/day_in_history","creator_name":"Santhosh Thottingal","creator_url":"https://huggingface.co/santhosh","description":"\n\t\n\t\t\n\t\tDay in History\n\t\n\nThis is a dataset prepared out of wikipedia pages https://en.wikipedia.org/wiki/Category:Days_of_the_year.\nHistoric events are mapped against each date with reference if available.\nHere is a demo app using this dataset: https://huggingface.co/spaces/santhosh/day_in_history\nScript used for parsing wiki pages: https://github.com/santhoshtr/day-in-history. Please use that repository for tracking issues regarding adding more languages, data cleanup etc.\n","first_N":5,"first_N_keywords":["English","Malayalam","cc-by-sa-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"PUGG","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","extractive-qa","document-retrieval","expert-generated"],"keywords_longer_than_N":true},
	{"name":"PUGG","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","extractive-qa","document-retrieval","expert-generated"],"keywords_longer_than_N":true},
	{"name":"Wikipedia-it-Trame-di-Film","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/scribis/Wikipedia-it-Trame-di-Film","creator_name":"Fabio Martines","creator_url":"https://huggingface.co/scribis","description":"Collection of plots of historical films and adventure films from Italian Wikipedia (April 2024)\nRaccolta di trame di film storici e film di avventura da Wikipedia italiana (Aprile 2024)\n","first_N":5,"first_N_keywords":["Italian","apache-2.0","1K - 10K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"PUGG_KG","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the knowledge graph dedicated for \nthe KBQA (Knowledge Base Question Answering) task within the PUGG dataset. This repository does not \ncontain directly any task, but it provides the knowledge graph that can be used to solve the KBQA \ntask from the PUGG dataset.\n\n\t\n\t\t\n\t\tGraphs\n\t\n\nWe provide sampled versions of the knowledge graph based on Wikidata:\n\nWikidata1H: A subgraph created by traversing 1… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KG.","first_N":5,"first_N_keywords":["monolingual","multilingual","found","Polish","English"],"keywords_longer_than_N":true},
	{"name":"PUGG_KG","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KG","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the knowledge graph dedicated for \nthe KBQA (Knowledge Base Question Answering) task within the PUGG dataset. This repository does not \ncontain directly any task, but it provides the knowledge graph that can be used to solve the KBQA \ntask from the PUGG dataset.\n\n\t\n\t\t\n\t\tGraphs\n\t\n\nWe provide sampled versions of the knowledge graph based on Wikidata:\n\nWikidata1H: A subgraph created by traversing 1… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KG.","first_N":5,"first_N_keywords":["monolingual","multilingual","found","Polish","English"],"keywords_longer_than_N":true},
	{"name":"wikipedia-paragraph-keywords","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tWikipedia Paragraph and Keyword Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 10,693 paragraphs extracted from English Wikipedia articles, along with corresponding search-engine style keywords for each paragraph. It is designed to support tasks such as text summarization, keyword extraction, and information retrieval.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured as a collection of JSON objects, each representing a single paragraph with its associated keywords.… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords.","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","topic-classification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"wikipedia-paragraph-keywords","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tWikipedia Paragraph and Keyword Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 10,693 paragraphs extracted from English Wikipedia articles, along with corresponding search-engine style keywords for each paragraph. It is designed to support tasks such as text summarization, keyword extraction, and information retrieval.\n\n\t\n\t\t\n\t\tDataset Structure\n\t\n\nThe dataset is structured as a collection of JSON objects, each representing a single paragraph with its associated keywords.… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-keywords.","first_N":5,"first_N_keywords":["text-classification","summarization","text-retrieval","topic-classification","keyword-spotting"],"keywords_longer_than_N":true},
	{"name":"German-RAG-SFT-Alpaca-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-SFT (Supervised Fine-Tuning) Alpaca-Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-Alpaca-HESSIAN-AI.","first_N":5,"first_N_keywords":["text-classification","question-answering","summarization","German","English"],"keywords_longer_than_N":true},
	{"name":"German-RAG-LLM-EASY-BENCHMARK","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-LLM-EASY-BENCHMARK\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis German-RAG-LLM-BENCHMARK represents a specialized collection for evaluating language models with a focus on source citation, time difference stating in RAG-specific tasks.\nTo evaluate models compatible with OpenAI-Endpoints you can refer to our Github Repo: https://github.com/avemio-digital/German-RAG-LLM-EASY-BENCHMARK/\nMost of the Subsets are synthetically… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-LLM-EASY-BENCHMARK.","first_N":5,"first_N_keywords":["text-classification","question-answering","summarization","German","English"],"keywords_longer_than_N":true},
	{"name":"wikipedia-paragraph-titles","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/wikipedia-paragraph-titles","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tWikipedia Paragraphs and AI-Generated Titles Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset contains pairs of Wikipedia paragraphs and their corresponding AI-generated titles. It is designed to facilitate research and development in natural language processing tasks, particularly in the areas of text summarization, title generation, and topic modeling.\nThe dataset combines human-written content from Wikipedia with machine-generated titles, providing a unique resource for… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/wikipedia-paragraph-titles.","first_N":5,"first_N_keywords":["English","cc-by-sa-4.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens","creator_name":"Aman Priyanshu","creator_url":"https://huggingface.co/AmanPriyanshu","description":"\n\t\n\t\t\n\t\tDynamic Topic Modeling Dataset: RedPajama-1T SubSample (100k samples, 1k tokens)\n\t\n\n\n  📝Check out the Blog Post\n\n\nThis dataset represents a curated subset of the RedPajama-1T Sample dataset, specifically processed for dynamic topic modeling applications. It contains 100,000 \nsamples from the original dataset, with each document limited to the first 1,024 tokens for consistent processing.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Overview\n\t\n\n\nName:… See the full description on the dataset page: https://huggingface.co/datasets/AmanPriyanshu/Dynamic-Topic-RedPajama-Data-1T-100k-SubSample-max-1k-tokens.","first_N":5,"first_N_keywords":["summarization","text-generation","text2text-generation","English","mit"],"keywords_longer_than_N":true},
	{"name":"text-sft","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agentlans/text-sft","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\n\t\n\t\t\n\t\tText Questions and Answers Dataset\n\t\n\nThis dataset is a combination of multiple datasets.\nThe text has been cleaned and formatted into a consistent JSONL structure, containing questions and answers derived from the text.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n\nTotal entries: 41 977\nFrom agentlans/wikipedia-paragraph-sft: 21 810\nFrom agentlans/finewebedu-sft: 10 168\nFrom Cosmopedia: 9 999\n\n\n\nThe Cosmopedia dataset's web_samples_v2 train split was processed using… See the full description on the dataset page: https://huggingface.co/datasets/agentlans/text-sft.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-sa-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"10k_wiki_summary","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ambrosfitz/10k_wiki_summary","creator_name":"Christopher Smith","creator_url":"https://huggingface.co/ambrosfitz","description":"ambrosfitz/10k_wiki_summary dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["text2text-generation","English","cc-by-sa-4.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"noob-wiki","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Laxhar/noob-wiki","creator_name":"Laxhar Dream Lab","creator_url":"https://huggingface.co/Laxhar","description":"\n\t\n\t\t\n\t\n\t\n\t\tNoob SDXL Wiki\n\t\n\nThis is the WIKI database for Noob SDXL Models.\n","first_N":5,"first_N_keywords":["text-to-image","English","apache-2.0","🇺🇸 Region: US","wiki"],"keywords_longer_than_N":false},
	{"name":"AnimeMangaCharacters-247K","keyword":"wiki","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\n\t\n\t\t\n\t\tAnime Manga Characters Dataset\n\t\n\nThis dataset is a metafile containing information about 247,034 anime and manga characters sourced from 2,372 fandom wiki sites. Each entry represents a character along with its associated metadata. The dataset has been deduplicated based on the url field to avoid redundancy, although a single character may still have multiple associated URLs.\n\n\t\n\t\t\n\t\tPotential Applications\n\t\n\n\nMultimodal Data Creation: Use the URLs to download the respective wiki… See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/AnimeMangaCharacters-247K.","first_N":5,"first_N_keywords":["English","cc-by-4.0","100K - 1M","json","Image"],"keywords_longer_than_N":true},
	{"name":"German-RAG-DPO-Alpaca-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-DPO Alpaca Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where Question-Answer… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-Alpaca-HESSIAN-AI.","first_N":5,"first_N_keywords":["question-answering","German","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"German-RAG-DPO-ShareGPT-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-DPO Share-GPT Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe DPO Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge graph where… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-DPO-ShareGPT-HESSIAN-AI.","first_N":5,"first_N_keywords":["question-answering","German","cc-by-sa-4.0","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"German-RAG-SFT-ShareGPT-HESSIAN-AI","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI","creator_name":"Avemio AG","creator_url":"https://huggingface.co/avemio","description":"\n\t\n\t\t\n\t\tGerman-RAG-SFT (Supervised Fine-Tuning) Share-GPT Format\n\t\n\n\n\t\n\t\t\n\t\tGerman-RAG - German Retrieval Augmented Generation\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThe SFT Tasks Dataset represents a specialized collection for fine-tuning language models with a focus on RAG-specific capabilities. Most tasks were developed using synthetically enhanced data derived from the German Wikipedia, accessed through Cohere's dataset (wikipedia-22-12-de-embeddings). The data is structured in a training knowledge… See the full description on the dataset page: https://huggingface.co/datasets/avemio/German-RAG-SFT-ShareGPT-HESSIAN-AI.","first_N":5,"first_N_keywords":["text-classification","question-answering","summarization","German","English"],"keywords_longer_than_N":true},
	{"name":"query-reformulation","keyword":"wikidata","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexdong/query-reformulation","creator_name":"Alex Dong","creator_url":"https://huggingface.co/alexdong","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nQRKB-16k (Synthetic Query Reformulation for Knowledge Graphs) is a dataset of 16,384 synthetic query reformulation pairs designed to facilitate research in query understanding and retrieval-augmented generation (RAG) using knowledge graphs. \nEach pair consists of a natural language query and a set of corresponding subqueries, with each subquery structured as a partial semantic triple, suitable for retrieval from knowledge graphs like DBpedia and… See the full description on the dataset page: https://huggingface.co/datasets/alexdong/query-reformulation.","first_N":5,"first_N_keywords":["text2text-generation","question-answering","sentence-similarity","English","mit"],"keywords_longer_than_N":true},
	{"name":"WikiFactDiff","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Orange/WikiFactDiff","creator_name":"Orange","creator_url":"https://huggingface.co/Orange","description":"\n\t\n\t\t\n\t\tWikiFactDiff: A Realistic Dataset for Atomic Factual Knowledge Update\n\t\n\nWikiFactDiff is a dataset designed as a resource to perform realistic factual updates within language models and to evaluate them post-update.\nAvailable datasets:\n\n20210104-20230227_legacy: The recommended WikiFactDiff dataset (its creation process is in the paper)\n20210104-20230227: An improves version of WikiFactDiff in terms of verbalization quality (Work still in progress.. DO NOT USE IT)\ntriple_verbs:… See the full description on the dataset page: https://huggingface.co/datasets/Orange/WikiFactDiff.","first_N":5,"first_N_keywords":["other","English","cc-by-sa-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"sql-create-context-thai","keyword":"wikisql","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","description":"\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset builds from sql-create-context.\n@misc{b-mc2_2023_sql-create-context,\n  title   = {sql-create-context Dataset},\n  author  = {b-mc2}, \n  year    = {2023},\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\n  note    = {This dataset was created by modifying data from the following sources: \\cite{zhongSeq2SQL2017, yu2018spider}.},\n}\n\n","first_N":5,"first_N_keywords":["text-generation","question-answering","table-question-answering","Thai","cc-by-4.0"],"keywords_longer_than_N":true},
	{"name":"OpenDataGen-factuality-en-v0.1","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1","creator_name":"Thomas","creator_url":"https://huggingface.co/thoddnn","description":"This synthetic dataset was generated using the Open DataGen Python library. (https://github.com/thoddnn/open-datagen)\n\n\t\n\t\t\n\t\tMethodology:\n\t\n\n\nRetrieve random article content from the HuggingFace Wikipedia English dataset.\nConstruct a Chain of Thought (CoT) to generate a Multiple Choice Question (MCQ).\nUtilize a Large Language Model (LLM) to score the results then filter it.\n\nAll these steps are prompted in the 'template.json' file located in the specified code folder.\nCode:… See the full description on the dataset page: https://huggingface.co/datasets/thoddnn/OpenDataGen-factuality-en-v0.1.","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"kazqad-retrieval","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/issai/kazqad-retrieval","creator_name":"Institute of Smart Systems and Artificial Intelligence, Nazarbayev University","creator_url":"https://huggingface.co/issai","description":"\n\t\n\t\t\n\t\tDataset Card for KazQAD-Retrieval\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nKazQAD is a Kazakh open-domain Question Answering Dataset\nthat can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments.\nThis repository contains only the collection and relevance judgments for information retrieval task.\nShort answers and data for the reading comprehension task (extractive QA) can be found here.\nKazQAD contains just under 6,000 unique questions and… See the full description on the dataset page: https://huggingface.co/datasets/issai/kazqad-retrieval.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","original","extended|natural_questions","extended|wikipedia"],"keywords_longer_than_N":true},
	{"name":"kazqad","keyword":"extended|wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/issai/kazqad","creator_name":"Institute of Smart Systems and Artificial Intelligence, Nazarbayev University","creator_url":"https://huggingface.co/issai","description":"\n\t\n\t\t\n\t\tDataset Card for KazQAD\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nKazQAD is a Kazakh open-domain Question Answering Dataset\nthat can be used in both reading comprehension and full ODQA settings, as well as for information retrieval experiments.\nThis repository contains only the data for the reading comprehension task (extractive QA).\nCollection and relevance judgments for information retrieval can be found here.\nThe main dataset (subset kazqad) contains just under 6,000 unique questions with… See the full description on the dataset page: https://huggingface.co/datasets/issai/kazqad.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","original","extended|natural_questions","extended|wikipedia"],"keywords_longer_than_N":true},
	{"name":"PUGG_KBQA","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KBQA","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KBQA.","first_N":5,"first_N_keywords":["question-answering","open-domain-qa","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"PUGG_KBQA","keyword":"wikidata","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_KBQA","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_KBQA.","first_N":5,"first_N_keywords":["question-answering","open-domain-qa","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"PUGG_MRC","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_MRC","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_MRC.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"PUGG_IR","keyword":"wikipedia","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/clarin-pl/PUGG_IR","creator_name":"CLARIN-PL","creator_url":"https://huggingface.co/clarin-pl","description":"\n\t\n\t\t\n\t\tPUGG: KBQA, MRC, IR Dataset for Polish\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis repository contains the PUGG dataset designed for three NLP tasks in the Polish language:\n\nKBQA (Knowledge Base Question Answering)\nMRC (Machine Reading Comprehension)\nIR (Information Retrieval)\n\n\n\t\n\t\t\n\t\tPaper\n\t\n\nFor more detailed information, please refer to our research paper titled:\n\"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction\" \nAuthored by:\n\nAlbert Sawczyn\nKatsiaryna… See the full description on the dataset page: https://huggingface.co/datasets/clarin-pl/PUGG_IR.","first_N":5,"first_N_keywords":["text-retrieval","document-retrieval","expert-generated","monolingual","original"],"keywords_longer_than_N":true},
	{"name":"tydi_xor_rc","keyword":"extended|wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/coastalcph/tydi_xor_rc","creator_name":"CoAStaL NLP Group","creator_url":"https://huggingface.co/coastalcph","description":"\n\t\n\t\t\n\t\tDataset Card for \"tydi_xor_rc\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nTyDi QA is a question answering dataset covering 11 typologically diverse languages. \nXORQA is an extension of the original TyDi QA dataset to also include unanswerable questions, where context documents are only in English but questions are in 7 languages.\nXOR-AttriQA contains annotated attribution data for a sample of XORQA.\nThis dataset is a combined and simplified version of the Reading Comprehension data from XORQA and… See the full description on the dataset page: https://huggingface.co/datasets/coastalcph/tydi_xor_rc.","first_N":5,"first_N_keywords":["question-answering","extractive-qa","crowdsourced","crowdsourced","multilingual"],"keywords_longer_than_N":true},
	{"name":"NQ-256-24-gpt-4o-2024-05-13-803084","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\n\t\n\t\t\n\t\tNQ-256-24-gpt-4o-2024-05-13-803084 Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"question answering dataset search\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the NQ-256-24-gpt-4o-2024-05-13-803084 model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:… See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/NQ-256-24-gpt-4o-2024-05-13-803084.","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-hard","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-easy","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-hard","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-easy","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-hard-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-100.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-hard-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test-500.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-hard-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-hard-test.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-easy-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-easy-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-500.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-zh-easy-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-zh-easy-test-100.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","Chinese","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-easy-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-100.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-easy-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test-500.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-easy-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-easy-test.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-hard-test","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-hard-test-500","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-500","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-500.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"VCR-wiki-en-hard-test-100","keyword":"wikimedia/wit_base","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-100","creator_name":"VCR","creator_url":"https://huggingface.co/vcr-org","description":"\n\t\n\t\t\n\t\tThe VCR-Wiki Dataset for Visual Caption Restoration (VCR)\n\t\n\n🏠 Paper | 👩🏻‍💻 GitHub | 🤗 Huggingface Datasets | 📏 Evaluation with lmms-eval\nThis is the official Hugging Face dataset for VCR-Wiki, a dataset for the Visual Caption Restoration (VCR) task.\nVCR is designed to measure vision-language models' capability to accurately restore partially obscured texts using pixel-level hints within images. text-based processing becomes ineffective in VCR as accurate text restoration depends… See the full description on the dataset page: https://huggingface.co/datasets/vcr-org/VCR-wiki-en-hard-test-100.","first_N":5,"first_N_keywords":["visual-question-answering","wikimedia/wit_base","English","cc-by-sa-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"CUBE-MT","keyword":"wikidata","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/albertmeronyo/CUBE-MT","creator_name":"Albert Meroño Peñuela","creator_url":"https://huggingface.co/albertmeronyo","description":"\n\t\n\t\t\n\t\tCUBE-MT: A Cultural Benchmark for Multimodal Knowledge Graph Construction with Generative Models\n\t\n\n\nCUBE-MT (CUltural BEnchmark with Multimodal Transformations) is an extension to the CUltural BEnchmark for Text-to-Image models (CUBE). CUBE contains 300K cultural artifacts across 8 countries (Brazil, France, India, Italy, Japan, Nigeria, Turkey, and USA) and 3 domains (cuisine, landmarks, art) extracted from Wikidata; and 1K text-to-image generation prompts that enable evaluation of… See the full description on the dataset page: https://huggingface.co/datasets/albertmeronyo/CUBE-MT.","first_N":5,"first_N_keywords":["translation","English","cc-by-4.0","< 1K","text"],"keywords_longer_than_N":true},
	{"name":"wikipedia_passages","keyword":"wikipedia","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Minsang/wikipedia_passages","creator_name":"MinsangKim","creator_url":"https://huggingface.co/Minsang","description":"There are 21M Wikipedia passages proposed by DPR and used in QPaug.","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","10M - 100M","json"],"keywords_longer_than_N":true},
	{"name":"wikipedia_definitions","keyword":"wikidata","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\n\t\n\t\t\n\t\tWikipedia Definitions Dataset\n\t\n\nwikipedia_definitions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the definition sentence(s) that open each Wikipedia article.The corpus contains 25 449 entities.\nLead-paragraph definitions give a slightly richer, stylistically uniform overview of an entity than the short Wikidata description, making them useful as lightweight contextual signals for tasks such as entity linking, retrieval, question answering… See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikipedia_definitions","keyword":"wikipedia","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions","creator_name":"Masaki Sakata","creator_url":"https://huggingface.co/masaki-sakata","description":"\n\t\n\t\t\n\t\tWikipedia Definitions Dataset\n\t\n\nwikipedia_definitions pairs English Wikipedia article titles (wiki_title) and their Wikidata IDs (qid) with the definition sentence(s) that open each Wikipedia article.The corpus contains 25 449 entities.\nLead-paragraph definitions give a slightly richer, stylistically uniform overview of an entity than the short Wikidata description, making them useful as lightweight contextual signals for tasks such as entity linking, retrieval, question answering… See the full description on the dataset page: https://huggingface.co/datasets/masaki-sakata/wikipedia_definitions.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"wikipedia-article-ratings","keyword":"wikipedia","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wikimedia-community/wikipedia-article-ratings","creator_name":"Unofficial Wikimedia Community","creator_url":"https://huggingface.co/wikimedia-community","description":"\n\t\n\t\t\n\t\tDataset Card for Wikipedia article ratings (V4)\n\t\n\n1-year dump of English Wikipedia article ratings. The dataset includes 47,207,448 records corresponding to 11,801,862 unique ratings posted between July 22, 2011 and July 22, 2012.\nThe Wikimedia Foundation has been experimenting with a feature to capture reader quality assessments of articles since September 2010. Article Feedback v4 (AFTv4) is a tool allowing readers to rate the quality of an article along 4 different dimensions. The… See the full description on the dataset page: https://huggingface.co/datasets/wikimedia-community/wikipedia-article-ratings.","first_N":5,"first_N_keywords":["English","cc0-1.0","10M - 100M","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"wikipedia_quality_wikirank","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank","creator_name":"Włodzimierz Lewoniewski","creator_url":"https://huggingface.co/lewoniewski","description":"Datasets with WikiRank quality score as of 1 August 2024 for 47 million Wikipedia articles in 55 language versions (also simplified version for each language in separate files).\nThe WikiRank quality score is a metric designed to assess the overall quality of a Wikipedia article. Although its specific algorithm can vary depending on the implementation, the score typically combines several key features of the Wikipedia article.\n\n\t\n\t\t\n\t\n\t\n\t\tWhy It’s Important\n\t\n\n\nEnhances Trust: For readers and… See the full description on the dataset page: https://huggingface.co/datasets/lewoniewski/wikipedia_quality_wikirank.","first_N":5,"first_N_keywords":["Arabic","Azerbaijani","Belarusian","Bulgarian","Catalan"],"keywords_longer_than_N":true},
	{"name":"RUwiki","keyword":"wiki","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"\n\t\n\t\t\n\t\tДатасет из 1000 статей с русской википедии\n\t\n\n\nБыла использована модификация ruWiki-web-scraper\nКаждая статья заключена в теги <s_text> </s_text>\n\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","text","Text"],"keywords_longer_than_N":true},
	{"name":"RUwiki","keyword":"wikipedia","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DataSynGen/RUwiki","creator_name":"DataGen","creator_url":"https://huggingface.co/DataSynGen","description":"\n\t\n\t\t\n\t\tДатасет из 1000 статей с русской википедии\n\t\n\n\nБыла использована модификация ruWiki-web-scraper\nКаждая статья заключена в теги <s_text> </s_text>\n\n","first_N":5,"first_N_keywords":["Russian","apache-2.0","10K - 100K","text","Text"],"keywords_longer_than_N":true}
]
;
