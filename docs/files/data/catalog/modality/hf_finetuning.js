const data_for_modality_finetuning = 
[
	{"name":"budapest-v0.1-hun","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun","creator_name":"Balazs Toldi","creator_url":"https://huggingface.co/Bazsalanszky","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBudapest-v0.1 Dataset README\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Budapest-v0.1 dataset is a cutting-edge resource specifically designed for fine-tuning large language models (LLMs). Created using GPT-4, this dataset is presented in a message-response format, making it particularly suitable for a variety of natural language processing tasks. The primary focus of Budapest-v0.1 is to aid in the development and enhancement of algorithms capable of performing summarization, question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bazsalanszky/budapest-v0.1-hun."},
	{"name":"CultriX-dpo","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/CultriX-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/CultriX-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"alpaca-cleaned-bn","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn","creator_name":"fahim abrar","creator_url":"https://huggingface.co/abrarfahim","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned-bn\\n\\t\\n\\n\\n\\nThis is a cleaned bengali translated version of the original Alpaca Dataset released by Stanford. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nimport datasets\\ndataset = datasets.load_dataset(\\\"abrarfahim/alpaca-cleaned-bn\\\")\\nprint(dataset[0])\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{'system_prompt': 'You are a virtual assistant, deliver a comprehensive response.',\\n 'qas_id': 'YY9S5K',\\n 'question_text': '\\\"‡¶∏‡¶®‡ßç‡¶¶‡ßá‡¶π\\\" ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∂‡¶¨‡ßç‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',\\n 'orig_answer_texts':‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abrarfahim/alpaca-cleaned-bn."},
	{"name":"Kannada-Dataset-v03","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/charanhu/Kannada-Dataset-v03","creator_name":"Charan H U","creator_url":"https://huggingface.co/charanhu","description":"charanhu/Kannada-Dataset-v03 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"OpenHermes-2.5-sv","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv","creator_name":"Tim Isbister","creator_url":"https://huggingface.co/timpal0l","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenHermes-2.5-sv\\n\\t\\n\\nThis is a machine translated instruct dataset from OpenHermes-2.5. \\nThe facebook/seamless-m4t-v2-large was used, and some post filtering is done to remove repetitive texts that occurred due to translation errors.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExample data:\\n\\t\\n\\n[\\n   {\\n      \\\"from\\\":\\\"human\\\",\\n      \\\"value\\\":\\\"Vilket naturfenomen, som orsakas av att ljus reflekteras och bryts genom vattendroppar, resulterar i en f√§rgglad b√•ge p√• himlen?\\\",\\n      \\\"weight\\\":null\\n   },\\n   {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timpal0l/OpenHermes-2.5-sv."},
	{"name":"Hindi-Instruct-Gemma-Prompt-formate","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate","creator_name":"CL","creator_url":"https://huggingface.co/CognitiveLab","description":"CognitiveLab/Hindi-Instruct-Gemma-Prompt-formate dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"hibo-function-calling-v1","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1","creator_name":"Thibaud Perrin","creator_url":"https://huggingface.co/thibaud-perrin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\thibo-function-calling-v1\\n\\t\\n\\n\\n    \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìñ Dataset Description\\n\\t\\n\\nThis dataset, named \\\"hibo-function-calling-v1\\\", is designed to facilitate the fine-tuning of Large Language Models (LLMs) for function calling tasks. It comprises a single 'train' split containing 323,271 data points across three columns: 'dataset_origin', 'system', and 'chat'. \\nThe dataset is a result of merging two distinct sources: gathnex/Gath_baize and glaiveai/glaive-function-calling-v2, with an aim to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1."},
	{"name":"Truth","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/abhishekbisaria/Truth","creator_name":"Abhishek Bisaria","creator_url":"https://huggingface.co/abhishekbisaria","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abhishekbisaria/Truth."},
	{"name":"TemplateGSM","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/TemplateGSM","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tTraining and Evaluating Language Models with Template-based Data Generation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tTemplateGSM Dataset\\n\\t\\n\\nThe TemplateGSM dataset is a large-scale collection of over 7 million (with potential for unlimited generation) grade school math problems, each paired with both code-based and natural language solutions.  Designed to advance mathematical reasoning in language models, this dataset presents a diverse range of challenges to assess and improve model capabilities in solving‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/TemplateGSM."},
	{"name":"LOGIC-701","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 Benchmark\\n\\t\\n\\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThis benchmark is also part of an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701."},
	{"name":"Capybara-fi-deepl-translated-sft","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft","creator_name":"Finnish-NLP","creator_url":"https://huggingface.co/Finnish-NLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Finnish-NLP/Capybara-deepl-translated-sft\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCreation process\\n\\t\\n\\n\\nLoad data from LDJnr/Capybara\\nFilter only samples that contain one input/output pair\\nDo zero shot classification with facebook/bart-large-mnli with the following prompt:\\n\\npreds =  pipe(f'{row[\\\"input\\\"]} is a question about:', candidate_labels=[\\\"USA related question\\\", \\\"Math related question\\\", \\\"General question\\\", \\\"Coding related question\\\"])\\n\\n\\nFilter out rows with too high scores in following‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Finnish-NLP/Capybara-fi-deepl-translated-sft."},
	{"name":"synthetic-confidential-information-injected-business-excerpts","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts","creator_name":"Rohit D","creator_url":"https://huggingface.co/Rohit-D","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Confidential Information Injected Business Excerpts\\n\\t\\n\\nThis dataset aims to provide business report excerpts which contain relevant confidential/sensitive information.\\nThis includes mentions of :\\n  1. Internal Marketing Strategies.\\n  2. Proprietary Product Composition.\\n  3. License Internals.\\n  4. Internal Sales Projections.\\n  5. Confidential Patent Details.\\n  6. others.\\n\\nThe dataset contains around 1k business excerpt - Reasons pairs. The Reason field contains the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rohit-D/synthetic-confidential-information-injected-business-excerpts."},
	{"name":"CoIN","keyword":"instruction tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Zacks-Chen/CoIN","creator_name":"Cheng_Chen","creator_url":"https://huggingface.co/Zacks-Chen","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContinuaL Instruction Tuning Dataset Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset sources\\n\\t\\n\\nThis dataset is constructed using publicly available and commonly used instruction tuning datasets, including VQAv2, VizWiz, ScienceQA, TextVQA, GQA, and OCR-VQA. \\nAdditionally, to enhance diversity, we introduce the classification task and referring expression comprehension task into CoIN with ImageNet, RefCOCO, RefCOCO+, and RefCOCOg.\\nBefore proceeding with instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zacks-Chen/CoIN."},
	{"name":"Confession-Subreddit-Top500","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Oguzz07/Confession-Subreddit-Top500","creator_name":"Oƒüuzhan Yƒ±ldƒ±rƒ±m","creator_url":"https://huggingface.co/Oguzz07","description":"This dataset was prepared by taking into account the 500 most popular posts of all time in the confession subreddit and the comments with the most votes on these posts.\\n"},
	{"name":"pandora-rlhf","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-rlhf","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora RLHF\\n\\t\\n\\nA Reinforcement Learning from Human Feedback (RLHF) dataset for Direct Preference Optimization (DPO) fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the anthropic/hh-rlhf dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"pandora-instruct","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Instruct\\n\\t\\n\\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the existing datasets:\\n\\nteknium/openhermes\\nise-uiuc/magicoder-evol-instruct-110k\\nise-uiuc/magicoder-oss-instruct-75k\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"pandora-instruct","keyword":"instruct","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Instruct\\n\\t\\n\\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the existing datasets:\\n\\nteknium/openhermes\\nise-uiuc/magicoder-evol-instruct-110k\\nise-uiuc/magicoder-oss-instruct-75k\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"pandora-instruct","keyword":"sft","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-instruct","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Instruct\\n\\t\\n\\nAn instruction dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the existing datasets:\\n\\nteknium/openhermes\\nise-uiuc/magicoder-evol-instruct-110k\\nise-uiuc/magicoder-oss-instruct-75k\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"pandora-tool-calling","keyword":"fine-tuning","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Tool Calling\\n\\t\\n\\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"ultrafeedback-binarized-preferences-cleaned-kto","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned) KTO\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved UltraFeedback Binarized Preferences Cleaned, the preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\\nRead more‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned-kto."},
	{"name":"pandora-tool-calling","keyword":"sft","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Tool Calling\\n\\t\\n\\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n"},
	{"name":"KnowCoder-Schema-Following-Data","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data","creator_name":"ICT-Golaxy","creator_url":"https://huggingface.co/golaxy","description":"\\n   \\n\\n\\n KnowCoder: Coding Structured Knowledge into LLMs for Universal\\nInformation Extraction \\n\\n\\n\\nüìÉ Paper\\n|\\nü§ó Resource (Schema ‚Ä¢ Data ‚Ä¢ Model)\\n|\\nüöÄ Try KnowCoder (coming soon)!\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema Following Data\\n\\t\\n\\nThe schema following data is constructed on UniversalNER, InstructIE, and LSEE. The statistics of schema following data are presented as follows.\\n\\n   \\n\\n\\n\\nThe cases of schema following data are shown here.\\nDue to data protection concerns, here we provide only 100 data samples for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/golaxy/KnowCoder-Schema-Following-Data."},
	{"name":"aya_dataset_dutch_example","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_dutch_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"data-is-better-together/aya_dataset_dutch_example dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Capybara-Preferences-Filtered","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Capybara-Preferences-Filtered\\n\\t\\n\\nThis dataset has been created with distilabel, plus some extra post-processing steps described below.\\n\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is built on top of argilla/Capybara-Preferences, but applies a further in detail filtering.\\nThe filtering approach has been proposed and shared by @LDJnr, and applies the following:\\n\\nRemove responses from the assistant, not only in the last turn, but also in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences-Filtered."},
	{"name":"code-action-sociale-familles","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'action sociale et des familles, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-action-sociale-familles."},
	{"name":"code-aviation-civile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'aviation civile, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-aviation-civile."},
	{"name":"code-cinema-image-animee","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du cin√©ma et de l'image anim√©e, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-cinema-image-animee."},
	{"name":"code-communes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des communes, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes."},
	{"name":"code-communes-nouvelle-caledonie","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des communes de la Nouvelle-Cal√©donie, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-communes-nouvelle-caledonie."},
	{"name":"code-defense","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-defense","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la d√©fense, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-defense."},
	{"name":"code-deontologie-architectes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de d√©ontologie des architectes, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-deontologie-architectes."},
	{"name":"code-disciplinaire-penal-marine-marchande","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode disciplinaire et p√©nal de la marine marchande, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-disciplinaire-penal-marine-marchande."},
	{"name":"code-domaine-etat","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du domaine de l'Etat, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat."},
	{"name":"code-domaine-etat-collectivites-mayotte","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du domaine de l'Etat et des collectivit√©s publiques applicable √† la collectivit√© territoriale de Mayotte, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-etat-collectivites-mayotte."},
	{"name":"code-domaine-public-fluvial-navigation-interieure","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du domaine public fluvial et de la navigation int√©rieure, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-domaine-public-fluvial-navigation-interieure."},
	{"name":"code-douanes-mayotte","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des douanes de Mayotte, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes-mayotte."},
	{"name":"code-electoral","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-electoral","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode √©lectoral, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-electoral."},
	{"name":"code-energie","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-energie","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'√©nergie, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-energie."},
	{"name":"code-entree-sejour-etrangers-droit-asile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'entr√©e et du s√©jour des √©trangers et du droit d'asile, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-entree-sejour-etrangers-droit-asile."},
	{"name":"code-expropriation-utilite-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'expropriation pour cause d'utilit√© publique, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-expropriation-utilite-publique."},
	{"name":"code-famille-aide-sociale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la famille et de l'aide sociale, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-famille-aide-sociale."},
	{"name":"code-forestier-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode forestier (nouveau), non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier-nouveau."},
	{"name":"code-fonction-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral de la fonction publique, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-fonction-publique."},
	{"name":"code-propriete-personnes-publiques","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral de la propri√©t√© des personnes publiques, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-personnes-publiques."},
	{"name":"code-collectivites-territoriales","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des collectivit√©s territoriales, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-collectivites-territoriales."},
	{"name":"code-impots","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des imp√¥ts, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots."},
	{"name":"code-impots-annexe-i","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des imp√¥ts, annexe I, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-i."},
	{"name":"code-impots-annexe-ii","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des imp√¥ts, annexe II, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-ii."},
	{"name":"code-impots-annexe-iii","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des imp√¥ts, annexe III, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iii."},
	{"name":"code-impots-annexe-iv","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode g√©n√©ral des imp√¥ts, annexe IV, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impots-annexe-iv."},
	{"name":"code-impositions-biens-services","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des impositions sur les biens et services, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-impositions-biens-services."},
	{"name":"code-instruments-monetaires-medailles","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des instruments mon√©taires et des m√©dailles, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-instruments-monetaires-medailles."},
	{"name":"code-juridictions-financieres","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des juridictions financi√®res, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-juridictions-financieres."},
	{"name":"code-justice-militaire-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de justice militaire (nouveau), non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-militaire-nouveau."},
	{"name":"code-justice-penale-mineurs","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la justice p√©nale des mineurs, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-penale-mineurs."},
	{"name":"code-legion-honneur-medaille-militaire-ordre-national-merite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la L√©gion d'honneur, de la M√©daille militaire et de l'ordre national du M√©rite, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-legion-honneur-medaille-militaire-ordre-national-merite."},
	{"name":"livre-procedures-fiscales","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tLivre des proc√©dures fiscales, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/livre-procedures-fiscales."},
	{"name":"code-minier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode minier, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier."},
	{"name":"code-minier-nouveau","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode minier (nouveau), non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-minier-nouveau."},
	{"name":"code-organisation-judiciaire","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'organisation judiciaire, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-organisation-judiciaire."},
	{"name":"code-patrimoine","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du patrimoine, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-patrimoine."},
	{"name":"code-penitentiaire","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode p√©nitentiaire, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penitentiaire."},
	{"name":"code-pensions-civiles-militaires-retraite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des pensions civiles et militaires de retraite, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-civiles-militaires-retraite."},
	{"name":"code-pensions-retraite-marins-francais-commerce-peche-plaisance","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des pensions de retraite des marins fran√ßais du commerce, de p√™che ou de plaisance, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-retraite-marins-francais-commerce-peche-plaisance."},
	{"name":"code-pensions-militaires-invalidite-victimes-guerre","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des pensions militaires d'invalidit√© et des victimes de guerre, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-pensions-militaires-invalidite-victimes-guerre."},
	{"name":"code-ports-maritimes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des ports maritimes, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-ports-maritimes."},
	{"name":"code-procedure-penale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de proc√©dure p√©nale, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-penale."},
	{"name":"code-recherche","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-recherche","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la recherche, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-recherche."},
	{"name":"code-rural-ancien","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode rural (ancien), non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-ancien."},
	{"name":"code-service-national","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-service-national","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du service national, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-service-national."},
	{"name":"code-tourisme","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-tourisme","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du tourisme, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-tourisme."},
	{"name":"code-travail-maritime","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du travail maritime, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail-maritime."},
	{"name":"code-voirie-routiere","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la voirie routi√®re, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-voirie-routiere."},
	{"name":"alpaca-ingen","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Walmart-the-bag/alpaca-ingen","creator_name":"wbag","creator_url":"https://huggingface.co/Walmart-the-bag","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAlpaca Ingen\\n\\t\\n\\nGoogle has added massive rate limits and other policies. I am unable to finish this.\\nThis dataset was created using Gemini 1.0 Pro with minor adjustments for cleanliness. It may contain some issues, including 'I'm sorry' responses. The dataset will undergo further cleaning once it reaches completion, with a target of processing up to 23,000 rows.\\n"},
	{"name":"Alpaca_french_mixtral","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca_french_mixtral\\n\\t\\n\\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nrobinjo\\n"},
	{"name":"grad_school_math_instructions_fr_Mixtral","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/grad_school_math_instructions_fr_Mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for grad_school_math_instructions_fr_Mixtral\\n\\t\\n\\nThis dataset was made thanks to the instruction of the vigogne's dataset but the output were generated with Mixtral-8x7B-Instruct instead of GPT3.5 to make it open-source.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nrobinjo\\n"},
	{"name":"Alpaca_french_mixtral","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/Alpaca_french_mixtral","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca_french_mixtral\\n\\t\\n\\nThis dataset was made by reusing the french alpaca instruction with Mixtral-8x7B-Instruct to make the output open-source.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nrobinjo\\n"},
	{"name":"alpaca-cleaned-fr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TPM-28/alpaca-cleaned-fr","creator_name":"TPM-28","creator_url":"https://huggingface.co/TPM-28","description":"TPM-28/alpaca-cleaned-fr dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"alpaca_astronomy_bg","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca_astronomy_bg","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"vislupus/alpaca_astronomy_bg dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"argilla-ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/csarron/argilla-ultrafeedback-binarized-preferences-cleaned","creator_name":"Qingqing Cao","creator_url":"https://huggingface.co/csarron","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback (Cleaned)\\n\\t\\n\\nThis dataset combines the train split of argilla/ultrafeedback-binarized-preferences-cleaned,\\nand test split of HuggingFaceH4/ultrafeedback_binarized.\\n"},
	{"name":"VIS","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/weiwei888/VIS","creator_name":"shiweiwei","creator_url":"https://huggingface.co/weiwei888","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/weiwei888/VIS."},
	{"name":"wangwei","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/guilty1987/wangwei","creator_name":"FAN","creator_url":"https://huggingface.co/guilty1987","description":"guilty1987/wangwei dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned","creator_name":"Farouk","creator_url":"https://huggingface.co/pharaouk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pharaouk/ultrafeedback-binarized-preferences-cleaned."},
	{"name":"alpaca-cleaned-gpt4-turbo","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo."},
	{"name":"AB1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MagedGaman/AB1","creator_name":"Maged Gaman","creator_url":"https://huggingface.co/MagedGaman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MagedGaman/AB1."},
	{"name":"alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Kushala/alpaca","creator_name":"Mummigatti","creator_url":"https://huggingface.co/Kushala","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kushala/alpaca."},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned","creator_name":"Cristian Velasquez (Entreprenerdly.com)","creator_url":"https://huggingface.co/Entreprenerdly","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Entreprenerdly/alpaca-cleaned."},
	{"name":"classifai","keyword":"preference","license":"GNU General Public License v3.0","license_url":"https://choosealicense.com/licenses/gpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jhmejia/classifai","creator_name":"John Henry","creator_url":"https://huggingface.co/jhmejia","description":"jhmejia/classifai dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"data-centric-ml-sft","keyword":"sft","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/davanstrien/data-centric-ml-sft","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Centric Machine Learning Domain SFT dataset\\n\\t\\n\\nThe Data Centric Machine Learning Domain SFT dataset is an example of how to use distilabel to create a domain-specific fine-tuning dataset easily.\\nIn particular using the Domain Specific Dataset Project Space. \\nThe dataset focuses on the domain of data-centric machine learning and consists of chat conversations between a user and an AI assistant. \\nIts purpose is to demonstrate the process of creating domain-specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/data-centric-ml-sft."},
	{"name":"aya_dataset_english_example","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/aya_dataset_english_example","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"data-is-better-together/aya_dataset_english_example dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"AWE","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishiraj/AWE","creator_name":"Rishiraj Acharya","creator_url":"https://huggingface.co/rishiraj","description":"rishiraj/AWE dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"alpaca_data_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca_data_clean","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca_data_clean."},
	{"name":"llama3weitiao","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHEZIXI/llama3weitiao","creator_name":"LIUJUN","creator_url":"https://huggingface.co/ZHEZIXI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ZHEZIXI/llama3weitiao."},
	{"name":"orpo-dpo-mix-40k-SHORT","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT","creator_name":"Trelis","creator_url":"https://huggingface.co/Trelis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSHORTENED - ORPO-DPO-mix-40k v1.1\\n\\t\\n\\nFILTERED to remove rows with chosen responses longer than 2k characters or with a final assistant message longer than 500 characters.\\n\\nThe original dataset card follows below.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.1\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Trelis/orpo-dpo-mix-40k-SHORT."},
	{"name":"MainData","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bufanlin/MainData","creator_name":"bufanlin","creator_url":"https://huggingface.co/bufanlin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bufanlin/MainData."},
	{"name":"alpaca-cleand","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/aaaalon/alpaca-cleand","creator_name":"liu","creator_url":"https://huggingface.co/aaaalon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aaaalon/alpaca-cleand."},
	{"name":"alpaca_cthulhu_full","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/theprint/alpaca_cthulhu_full","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","description":"Based on the yahma/alpaca-cleaned data set.\\nEvery answer in the original data set was rewritten, as if the answer was given by a dedicated, high ranking cultist in the Cult of Cthulhu. The instructions were to keep the replies factually the same, but to spice things up with references to the Cthulhu Mythos in its various forms.\\nThis was done as part of a learning experience, but I am making the data set available for others to play with as well.\\n"},
	{"name":"DecipherPref","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/huuuyeah/DecipherPref","creator_name":"Yebowen Hu","creator_url":"https://huggingface.co/huuuyeah","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nHuman preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/huuuyeah/DecipherPref."},
	{"name":"AmsterdamBalancedFirst200Tokens","keyword":"fine-tuning","license":"European Union Public License 1.1","license_url":"https://choosealicense.com/licenses/eupl-1.1/","language":"en","dataset_url":"https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens","creator_name":"Femke Bakker","creator_url":"https://huggingface.co/FemkeBakker","description":"This dataset is a modified version of the AmsterdamDocClassificationDataset. \\nThe original dataset consists of Dutch Raadsinformatie documents from the Municipality of Amsterdam, which were published in accordance with the Open Government Act (Woo). \\nIn this modified version, the documents are truncated to the first 200 tokens each. \\nThis dataset is used to fine-tune large language models (LLMs) for the Assessing LLMs for Document Classification project.\\nThe documents are formatted into a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens."},
	{"name":"Proyecto","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto","creator_name":"Facundo","creator_url":"https://huggingface.co/Facundo-DiazPWT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Facundo-DiazPWT/Proyecto."},
	{"name":"ictisgpt","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/alexbs/ictisgpt","creator_name":"Alex B","creator_url":"https://huggingface.co/alexbs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ICTIS GPT dataset\\n\\t\\n\\n"},
	{"name":"Feedback-Collection-ru","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Feedback-Collection-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeedback-Collection-ru\\n\\t\\n\\nThis is russian version of prometheus-eval/Feedback-Collection translated using Google Translate.\\n"},
	{"name":"SFT-UZ-9k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLDataScientist/SFT-UZ-9k","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is SFT version of MLDataScientist/DPO-uz-9k (Uzbek translated dataset with two answers for each prompt for DPO fine-tuning).\\nI selected ['answer'][0] from each example and saved them in this dataset for easy fine-tuning of LLMs.\\n"},
	{"name":"shibing624_alpaca-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/shibing624_alpaca-zh","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botp/shibing624_alpaca-zh."},
	{"name":"bioinstruct","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/bio-nlp-umass/bioinstruct","creator_name":"UMass BioNLP Lab","creator_url":"https://huggingface.co/bio-nlp-umass","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BioInstruct\\n\\t\\n\\nGitHub repo: https://github.com/bio-nlp/BioInstruct\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nBioInstruct is a dataset of 25k instructions and demonstrations generated by OpenAI's GPT-4 engine in July 2023. \\nThis instruction data can be used to conduct instruction-tuning for language models (e.g. Llama) and make the language model follow biomedical instruction better. \\nImprovements of Llama on 9 common BioMedical tasks are shown in the result section. \\nTaking‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bio-nlp-umass/bioinstruct."},
	{"name":"halfTurkish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sasanikrali/halfTurkish","creator_name":"sasani krali","creator_url":"https://huggingface.co/sasanikrali","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for HalfTurkish\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sasanikrali/halfTurkish."},
	{"name":"orpo-dpo-mix-40k-flat","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k-flat\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Uncensor any LLM with Abliteration for more information about how to use it.\\nThis is version with raw text instead of lists of dicts as in the original version here.\\nIt makes easier to parse in Axolotl, especially for DPO.\\nORPO-DPO-mix-40k-flat is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k-flat."},
	{"name":"walt","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/canTooDdev/walt","creator_name":"Boris Marion-Dorier","creator_url":"https://huggingface.co/canTooDdev","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/canTooDdev/walt."},
	{"name":"Test2","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/WhiteHunter111/Test2","creator_name":"Thomas Flato","creator_url":"https://huggingface.co/WhiteHunter111","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WhiteHunter111/Test2."},
	{"name":"ultrafrench","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ultrafrench\\n\\t\\n\\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"ultrafrench","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AIffl/ultrafrench","creator_name":"AIffl : AI For French Language","creator_url":"https://huggingface.co/AIffl","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ultrafrench\\n\\t\\n\\nThis dataset offers a french translation of the small sample of instructions from HuggingFaceH4/ultrachat_200k translated in french. The generations were made with Mistral large to make the output open-source\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Contact\\n\\t\\n\\nntnq\\n"},
	{"name":"SEA_data","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ECNU-SEA/SEA_data","creator_name":"ECNU-SEA","creator_url":"https://huggingface.co/ECNU-SEA","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAutomated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis\\n\\t\\n\\nPaper Link: https://arxiv.org/abs/2407.12857\\nProject Page: https://ecnu-sea.github.io/\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nEach dataset contains four types of files as follows:\\n\\npaper_raw_pdf: Original paper in PDF format.\\npaper_nougat_mmd: The mmd files after parsed by Nougat.\\nreview_raw_txt: Crawled raw review text.\\nreview_json: The processed review JSON file, including ‚ÄúDecision‚Äù, ‚ÄúMeta Review‚Äù, and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ECNU-SEA/SEA_data."},
	{"name":"German_RisingWorld_Alpaca-Dataset","keyword":"instruction tuning","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Alpaca-Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German Alpaca dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enth√§lt den deutschen Alpaca-Datensatz f√ºr das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\nEach instance has an instruction, an output, and an optional input. An‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Andzej-75/German_RisingWorld_Alpaca-Dataset."},
	{"name":"ascii_art_generation_140k_bilingual","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData for LLM ASCII Art\\n\\t\\n\\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Links\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nLink\\nLanguage\\nSize\\n\\n\\n\\t\\t\\nascii_art_generation_140k\\nEnglish\\n138,941\\n\\n\\nascii_art_generation_140k_bilingual\\nChinese & English\\n138,941\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Preparation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTraining data description\\n\\t\\n\\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\\nThe source images of these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k_bilingual."},
	{"name":"OpenHermes-2.5-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenHermes-2.5-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\td0rj/OpenHermes-2.5-ru\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is translated version of teknium/OpenHermes-2.5 into Russian using Google Translate.\\n"},
	{"name":"meme_dataset","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/meme_dataset","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"Noxus09/meme_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"guanjian_anli_test1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/papaya523/guanjian_anli_test1","creator_name":"zhaoyue","creator_url":"https://huggingface.co/papaya523","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/papaya523/guanjian_anli_test1."},
	{"name":"German_RisingWorld_prompt-text-rejected_Jsonl","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Andzej-75/German_RisingWorld_prompt-text-rejected_Jsonl","creator_name":"Andzej Ktowierzy","creator_url":"https://huggingface.co/Andzej-75","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGerman \\\"Rising World\\\"-Game Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nThis HF data repository contains the German dataset for the open-world sandbox game \\\"Rising World\\\".\\nDieses HF-Datenrepository enth√§lt den deutschen Datensatz f√ºr das Open-World-Sandbox-Spiel \\\"Rising World\\\".\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\n\\nThis data is intended for fine-tuning\\nThis data is useful for \\\"Rising World\\\" plug-in developers\\n\\n"},
	{"name":"PromisedChat_Instruction","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/netmouse/PromisedChat_Instruction","creator_name":"Matt Yeh","creator_url":"https://huggingface.co/netmouse","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/netmouse/PromisedChat_Instruction."},
	{"name":"WangchanThaiInstruct_Multi-turn_Conversation_Dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWangchanThaiInstruct Multi-turn Conversation Dataset\\n\\t\\n\\nWe create a Thai multi-turn conversation dataset from airesearch/WangchanThaiInstruct (Batch 1) by LLM. It was created from synthetic method using open source LLM in Thai language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n\\nThammaleelakul, S., & Phatthiyaphaibun, W. (2024). WangchanThaiInstruct Multi-turn Conversation Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.13132633\\n\\nor BibTeX\\n@dataset{thammaleelakul_2024_13132633,\\n  author‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/WangchanThaiInstruct_Multi-turn_Conversation_Dataset."},
	{"name":"vision-feedback-mix-binarized","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset aims to provide large-scale vision feedback data. \\nIt is a combination of the following high-quality vision feedback datasets:\\n\\nzhiqings/LLaVA-Human-Preference-10K: 9,422 samples\\nMMInstruction/VLFeedback: 80,258 samples\\nYiyangAiLab/POVID_preference_data_for_VLLMs: 17,184 samples\\nopenbmb/RLHF-V-Dataset: 5,733 samples\\nopenbmb/RLAIF-V-Dataset: 83,132 samples\\n\\nWe also offer a cleaned version in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized."},
	{"name":"vision-feedback-mix-binarized-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned","creator_name":"wangchenglong","creator_url":"https://huggingface.co/wangclnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Vision-Feedback-Mix-Binarized-Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThis dataset represents a cleaned version on wangclnlp/vision-feedback-mix-binarized.\\nDescriptions of the base datasets, including the data format and the procedure for mixing data, can be found in this link.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOur Methods for Cleaning Vision Feedback Data\\n\\t\\n\\nOur goal is to select vision feedback samples where the preferred outputs are significantly differentiated from the dispreferred‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wangclnlp/vision-feedback-mix-binarized-cleaned."},
	{"name":"smart_home_control","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Charles95/smart_home_control","creator_name":"JingXiang","creator_url":"https://huggingface.co/Charles95","description":"Charles95/smart_home_control dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LLama3Flashcard","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Michito97/LLama3Flashcard","creator_name":"Adrian De La Cruz","creator_url":"https://huggingface.co/Michito97","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nEste dataset contiene pares de instrucciones, entradas y salidas dise√±adas para generar tarjetas de memoria (flashcards) educativas. Es ideal para modelos de generaci√≥n de texto enfocados en la creaci√≥n de contenidos educativos.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tColumnas\\n\\t\\n\\n\\ninstruction: La‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Michito97/LLama3Flashcard."},
	{"name":"CTI_0.1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/AshishCTI/CTI_0.1","creator_name":"ad","creator_url":"https://huggingface.co/AshishCTI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AshishCTI/CTI_0.1."},
	{"name":"epic-novels","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Eldritch/epic-novels","creator_name":"Manish Singh Parihar","creator_url":"https://huggingface.co/Eldritch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNovel Continuation Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains over 10,000 records of novel text, specifically curated for the task of text continuation. It consists of three columns:\\n\\nInstruction: A prompt or instruction guiding the continuation of the novel.\\nInput: An excerpt from the novel serving as the starting point for continuation.\\nOutput: The continuation of the novel, following the input text, as per the instruction.\\n\\nThe dataset has been used to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Eldritch/epic-novels."},
	{"name":"dpo-orpo-mix-45k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-45k\\n\\t\\n\\nThis dataset is designed for DPO or ORPO training.\\nThis dataset combines samples of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22799 samples)\\nargilla/distilabel-math-preference-dpo: highly scored‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-45k."},
	{"name":"dpo-orpo-mix-38k-balanced","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced","creator_name":"Matthias De Paolis","creator_url":"https://huggingface.co/llmat","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDPO-ORPO-mix-38k\\n\\t\\n\\nThis dataset is intended for use with DPO or ORPO training. \\nIt represents a balanced version of the llmat/dpo-orpo-mix-45k dataset, achieved through a clustering-based approach as outlined in this paper.\\nThe dataset integrates high-quality samples from the following DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (2882 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (3961 samples)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced."},
	{"name":"Kalo-Opus-Instruct-22k-Refusal-Murdered","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"NewEden/Kalo-Opus-Instruct-22k-Refusal-Murdered dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"code-translation","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Noxus09/code-translation","creator_name":"Krishna Rathore","creator_url":"https://huggingface.co/Noxus09","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Noxus09/code-translation."},
	{"name":"CRAFT-Summarization","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-Summarization\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail, but training on this data and evaluating on human-curated summarization data proved highly beneficial.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available, and training on them yields consistent improvement that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-Summarization."},
	{"name":"CRAFT-RecipeGen","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen","creator_name":"Ingo Ziegler","creator_url":"https://huggingface.co/ingoziegler","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCRAFT-RecipeGen\\n\\t\\n\\nThis is a synthetic dataset generated with the CRAFT framework proposed in the paper CRAFT Your Dataset: Task-Specific Synthetic Data Generation Through Corpus Retrieval and Augmentation.\\nThe correctness of the data has not been verified in detail.\\n\\n4 synthetic dataset sizes (S, M, L, XL) are available.\\nCompared to other synthetically generated datasets with the CRAFT framework, this task did not scale similarly well and we do not match the performance of general‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ingoziegler/CRAFT-RecipeGen."},
	{"name":"ethics_expression_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_expression_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\nsubset commonsense\\n  before 13910 {'label': 1, 'input': \\\"I went to the principal's office to change my records before going to a different school.\\\"}\\n  after 1513 {'prompt': 'I won my match by ', 'chosen': 'making a lot of good shots.', 'rejected': \\\"moving my ball when the other player wasn't looking.\\\"}\\n\\nsubset utilitarianism\\n  before 13737 {'baseline': 'I forgot to wear my virus-proof mask when I visited‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_expression_preferences."},
	{"name":"mmlu_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/mmlu_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"reformat of MMLU to be in DPO (paired) format\\nexamples:\\n  {'prompt': 'Which of the following statements about the lanthanide elements is NOT true?', 'chosen': 'The atomic radii of the lanthanide elements increase across the period from La to Lu.', 'rejected': 'All of the lanthanide elements react with aqueous acid to liberate hydrogen.'}\\n  college_chemistry\\n  {'prompt': 'Beyond the business case for engaging in CSR there are a number of moral arguments relating to: negative _______, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/mmlu_preferences."},
	{"name":"ThaiQA-v1","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1","creator_name":"Thai Synthetic QA","creator_url":"https://huggingface.co/ThaiSyntheticQA","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThaiQA v1\\n\\t\\n\\nThaiQA v1 is a Thai Synthetic QA dataset. It was created from synthetic method using open source LLM in Thai language.\\nWe used Nvidia Nemotron 4 (340B) to create this dataset.\\nTopics:\\nTechnology and Gadgets 100\\nTravel and Tourism 91\\nFood and Cooking 99\\nSports and Fitness 50\\nArts and Entertainment 24\\nHome and Garden 72\\nFashion and Beauty 99\\nScience and Nature 100\\nHistory and Culture 91\\nEducation and Learning 99\\nPets and Animals 83\\nRelationships and Family 78\\nPersonal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThaiSyntheticQA/ThaiQA-v1."},
	{"name":"synthetic-1","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pepistrafforello/synthetic-1","creator_name":"Giuseppe Strafforello","creator_url":"https://huggingface.co/pepistrafforello","description":"Private¬†Data for Fine Tuning LLM [JSON dataset]\\n\\nThis is a Dataset Repository of Private¬†Data for Fine Tuning LLM\\nA dataset containing synthetic data that does not exist elsewhere, in the Alpaca format; its purpose is evaluating the effectiveness of fine tuning on private data.\\nView Medium Post\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nCC-0\\n"},
	{"name":"SFinD-S","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tilmann-strative/SFinD-S","creator_name":"Tilmann Bruckhaus","creator_url":"https://huggingface.co/tilmann-strative","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis sample is part of the larger SFinD-S (Strative Financial Dataset - Synthetic), a comprehensive dataset designed for Retrieval-Augmented Generation (RAG) GenAI applications, Natural Language Processing (NLP), Large Language Models (LLM), and AI tasks in the financial domain. The full SFinD-S dataset contains over 20,000 records of realistic financial questions and verified answers, sourced from a wide variety of web content.\\nIf you find this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tilmann-strative/SFinD-S."},
	{"name":"Claude-Instruct-5K","keyword":"instruct","license":"GNU Affero General Public License v3.0","license_url":"https://choosealicense.com/licenses/agpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NewEden/Claude-Instruct-5K","creator_name":"New Eden","creator_url":"https://huggingface.co/NewEden","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tkalo made dis\\n\\t\\n\\nThanks to Kubernetes bad for filtering + converting this to sharegpt\\nMix of Opus and 3.5 for data\\nThis is a combined set of \\nuncurated-raw-gens-og-test-filtered\\nuncurated-raw-gens-opus-jul-31-filtered\\nopus_jul10_test-filtered\\nuncurated_opus_jul8-filtered \\n"},
	{"name":"Celestia","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\\nThe 2024-10-30 version contains:\\n\\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
	{"name":"helpsteer2_dpo_nonverbose","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"HelperSteer 2, formatted in DPO format (prompt, chosen, rejected).\\n\\nin main branch there is a custom scoring correct > helpful > -verbosity\\nin each branch we have preference pairs for only correct, helpful, verbosity, coherence, complexity\\nPlease note that only correct and helpful has strong inter-rater agreement in the HelpSteer2 paper\\n\\nThis is the notebook used to produce the dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/helpsteer2_dpo_nonverbose."},
	{"name":"LLaVA-OneVision-Data-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLaVA-OneVision-Data-ru\\n\\t\\n\\nTranslated lmms-lab/LLaVA-OneVision-Data dataset into Russian language using Google translate.\\n\\nAlmost all datasets have been translated, except for the following:\\n[\\\"tallyqa(cauldron,llava_format)\\\", \\\"clevr(cauldron,llava_format)\\\", \\\"VisualWebInstruct(filtered)\\\", \\\"figureqa(cauldron,llava_format)\\\", \\\"magpie_pro(l3_80b_mt)\\\", \\\"magpie_pro(qwen2_72b_st)\\\", \\\"rendered_text(cauldron)\\\", \\\"ureader_ie\\\"]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nimport datasets\\n\\n\\ndata =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/LLaVA-OneVision-Data-ru."},
	{"name":"alpha","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dawsondavis/alpha","creator_name":"Dawson Davis","creator_url":"https://huggingface.co/dawsondavis","description":"dawsondavis/alpha dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"llama70B-dpo-dataset","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"app350_llama_format","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/CodeHima/app350_llama_format","creator_name":"Himanshu Mohanty","creator_url":"https://huggingface.co/CodeHima","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAPP-350 Formatted Dataset for LLM Fine-tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe APP-350 dataset consists of structured conversation pairs formatted for fine-tuning Large Language Models (LLMs) like LLaMA. This dataset includes questions and responses between users and an AI assistant. The dataset is particularly designed for privacy policy analysis and fairness evaluation, allowing models to learn from annotated interactions regarding privacy practices.\\nThe conversations are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CodeHima/app350_llama_format."},
	{"name":"alpaca-bulgarian-jokes","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes."},
	{"name":"godot-training","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ImJimmeh/godot-training","creator_name":"Jim","creator_url":"https://huggingface.co/ImJimmeh","description":"ImJimmeh/godot-training dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Atma4-Hindi","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4-Hindi\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4-Hindi."},
	{"name":"alpaca-cleaned_uz","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bizb0630/alpaca-cleaned_uz","creator_name":"Behruz Izbaev","creator_url":"https://huggingface.co/bizb0630","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a translation of the alpaca-cleaned dataset into Uzbek (Latin), using the GPT-4o mini API.\\n"},
	{"name":"thai-gov-procurement_regulation-17-amend-21","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21","creator_name":"Amornpan Phornchaicharoen","creator_url":"https://huggingface.co/amornpan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüáπüá≠ Dataset Card for Thai Government Procurement Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‚ÑπÔ∏è This dataset is optimized for procurement-related NLP tasks in Thai.\\n\\t\\n\\nThis dataset contains a collection of procurement regulations, instructions, and responses focused on public sector purchasing, contract management, and compliance with Thai government standards. It aims to support natural language processing tasks involving procurement assistance, such as chatbot development, procurement dialogue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21."},
	{"name":"RoboMatrix","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WayneMao/RoboMatrix","creator_name":"WayneMao","creator_url":"https://huggingface.co/WayneMao","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis is the fine-tuning dataset used in the paper RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\nProject Page: https://robo-matrix.github.io/Paper: https://arxiv.org/abs/2412.00171Code: https://github.com/WayneMao/RoboMatrixModel: \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you find our work helpful, please cite us:\\n@article{mao2024robomatrix,\\n  title={RoboMatrix: A Skill-centric Hierarchical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WayneMao/RoboMatrix."},
	{"name":"LOGIC-701-instruct","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLOGIC-701 (instruct)\\n\\t\\n\\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\\n"},
	{"name":"ATCgpt-Fixed","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATCgpt-Fixed\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATCgpt-Fixed."},
	{"name":"Atcgpt-Fixed2","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atcgpt-Fixed2\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atcgpt-Fixed2."},
	{"name":"FineCorpus-WorkoutExercise","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise","creator_name":"Widi Fadhil","creator_url":"https://huggingface.co/padilfm","description":"\\n\\t\\n\\t\\t\\n\\t\\tFineCorpus-WorkoutExercise\\n\\t\\n\\nThis dataset contains structured workout exercise prompts for fine-tuning LLMs. \\n\\n\\t\\n\\t\\t\\n\\t\\tStructure:\\n\\t\\n\\n\\nconversations: Contains multi-turn dialogue pairs.\\nsource: Indicates whether the data is from reasoning (Human) or generated by an AI model (LLM).\\ncategory: Categorizes data into Q&A, Explain, Describe, Translate.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage:\\n\\t\\n\\nTo use this dataset:\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"padiflm/FineCorpus-WorkoutExercise\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/padilfm/FineCorpus-WorkoutExercise."},
	{"name":"college_alpaca_dataset","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dapraws/college_alpaca_dataset","creator_name":"Muhammad Darrel Prawira","creator_url":"https://huggingface.co/dapraws","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize the given article in 200 Words.\\\",\\n\\\"input\\\": \\\"https://www.bbc.com/news/world-51461830\\\",\\n\\\"output\\\": \\\"The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dapraws/college_alpaca_dataset."},
	{"name":"truthful_qa_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/truthful_qa_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"wassname/truthful_qa_preferences dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"genies_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/genies_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"genie_dpo\\\"\\n\\t\\n\\nA conversion of the distribution from GENIES to open_pref_eval format.\\nConversion code\\n"},
	{"name":"Titanium","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Titanium","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Titanium is a dataset containing DevOps-instruct data.\\nThe 2024-10-02 version contains:\\n\\n26.6k rows of synthetic DevOps-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary areas of expertise are AWS, Azure, GCP, Terraform, Dockerfiles, pipelines, and shell scripts.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
	{"name":"MMLU-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MMLU-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/MMLU-Alpaca."},
	{"name":"text_meme","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/text_meme","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\ttext_meme\\n\\t\\n\\n–°–æ—Å–∫—Ä–∞–ø–µ–Ω–æ —Å –æ—Ç–ª–∏—á–Ω–æ–≥–æ Telegram –∫–∞–Ω–∞–ª–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ–º—ã.\\n"},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Clinical Coding\\n\\t\\n\\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \\ncoding, as measurable by MedConceptsQA, an open-source medical coding \\nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \\nInternational Classification of Diseases, Tenth Revision, Clinical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding."},
	{"name":"MrGrammaticalOntology_clinical_coding","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Clinical Coding\\n\\t\\n\\nThis dataset was created from a motivation to train Medical Large Language Models for improved fluency in clinical \\ncoding, as measurable by MedConceptsQA, an open-source medical coding \\nevaluation benchmark designed to evaluate the understanding and reasoning capabilities of LLMs on medical concepts.  It was extracted from the Centers for Medicare & Medicaid Services' \\nInternational Classification of Diseases, Tenth Revision, Clinical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_clinical_coding."},
	{"name":"alpaca-bulgarian-jokes-multilingual-prompts","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts."},
	{"name":"dali","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liwei1987cn/dali","creator_name":"Levi li","creator_url":"https://huggingface.co/liwei1987cn","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÂ§ßÊùéËÄÅÂ∏àÈóÆÁ≠îÊï∞ÊçÆÈõÜ\\n\\t\\n\\nËøô‰∏™Êï∞ÊçÆÈõÜÂåÖÂê´Â§ßÊùéËÄÅÂ∏àÁöÑÈóÆÁ≠îÂØπËØù,Áî®‰∫éËÆ≠ÁªÉÂØπËØùÊ®°Âûã„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÊï∞ÊçÆÈõÜÊèèËø∞\\n\\t\\n\\n\\nÊ†ºÂºè: JSONL\\nÂ≠óÊÆµ: \\ninstruction: Âõ∫ÂÆöÂÄº\\\"ËØ∑Â§ßÊùéËÄÅÂ∏àÂõûÁ≠î\\\"\\ninput: ÊèêÈóÆÂÜÖÂÆπ \\noutput: Â§ßÊùéËÄÅÂ∏àÁöÑÂõûÁ≠î\\n\\n\\nÊï∞ÊçÆÈáè: xxxÊù°ÂØπËØùÊï∞ÊçÆ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‰ΩøÁî®Á§∫‰æã\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"your-username/dataset-name\\\")\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tËÆ∏ÂèØËØÅ\\n\\t\\n\\nApache 2.0\\n"},
	{"name":"chat","keyword":"fine-tune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chat","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chat dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LongWriter-6k-English","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/themex1380/LongWriter-6k-English","creator_name":"themex","creator_url":"https://huggingface.co/themex1380","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k-English\\n\\t\\n\\nLongWriter-6k-English is a filtered version of the LongWriter-6k dataset, containing only the English-language samples. This dataset includes 2,299 instances of long-form text, ranging from 2,000 to 32,000 words, designed to train large language models (LLMs) to handle extended output contexts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nLanguages: English\\nData Size: 2,299 samples\\nOutput Length: 2,000 to 32,000 words per sample\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\nThis dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/themex1380/LongWriter-6k-English."},
	{"name":"dpo-chatml-mix-alt","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-alt","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-alt dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"1M-OpenOrca_be","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be","creator_name":"Artsem Holub","creator_url":"https://huggingface.co/WiNE-iNEFF","description":"En/Be\\nüêã The Belarusian OpenOrca Dataset! üêã\\n\\n\\n\\nBelarusian OpenOrca dataset - is rich collection of augmented FLAN data aligns, that translated in belarusian language.\\nThat dataset should help training LLM in belarusian language and should help on other NLP tasks.\\nThis dataset have 2 version:\\n\\n~1M GPT-4 completions (Now translating)\\n~3.2M GPT-3.5 completions (Can be translated in future)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Fields\\n\\t\\n\\nThe fields are:\\n\\n'id', a unique numbered identifier which includes one of 'niv'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WiNE-iNEFF/1M-OpenOrca_be."},
	{"name":"remix-run-v2-dataset","keyword":"fine-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset","creator_name":"Niklas","creator_url":"https://huggingface.co/Sebastyijan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRemix Run v2 Fine-tuning Dataset Overview\\n\\t\\n\\nThis dataset is derived from the official Remix Run v2 documentation and targets core concepts, best practices, and frequently asked questions surrounding Remix development workflows. Remix Run is a modern, full-stack web framework designed to enhance developer productivity by optimizing routing, form handling, and data-fetching mechanisms, while promoting progressive enhancement.\\nThe dataset serves as a foundation for fine-tuning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Sebastyijan/remix-run-v2-dataset."},
	{"name":"Spurline","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\\nThe 2024-10-30 version contains:\\n\\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
	{"name":"MDCure-36k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-36k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k."},
	{"name":"MDCure-36k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-36k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-36k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-36k."},
	{"name":"amazon-ml","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Vinuuu/amazon-ml","creator_name":"Vinayak Goyal","creator_url":"https://huggingface.co/Vinuuu","description":"Vinuuu/amazon-ml dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"OregonCoastin4K","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Overlaiai/OregonCoastin4K","creator_name":"Overlai.ai","creator_url":"https://huggingface.co/Overlaiai","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOREGON COAST IN 4K\\n\\t\\n\\n\\n\\n\\\"Oregon Coast in 4K\\\" is a fine tuning text-to-video dataset consisting of dynamic videos captured in 8K resolution on the DJI Inspire 3 and RED Weapon Helium.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\nüé• Oversampled: Every clip is captured in stunning 8K resolution, delivering rich detail ideal for fine tuning scenic landscapes and ocean dynamics.\\nüîÑ Parallax: Shot using DJI Inspire 3 featuring parallax effects that provide AI models with enhanced context on depth and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Overlaiai/OregonCoastin4K."},
	{"name":"Electrohydrodynamics","keyword":"fine-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Taylor658/Electrohydrodynamics","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\\n\\t\\n\\t\\t\\n\\t\\tElectrohydrodynamics in Hall Effect Thrusters Dataset for Mistral-Large-Instruct-2411 Fine-Tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is a collection of 6,000 high-fidelity training instances tailored for fine-tuning the Mistral-Large-Instruct-2411 foundation model. It captures theoretical, computational, and experimental aspects of electrohydrodynamics in Hall Effect Thrusters. \\n\\n\\t\\n\\t\\t\\n\\t\\tKey Features:\\n\\t\\n\\n\\nMultimodal elements: Includes LaTeX equations, code snippets, textual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/Electrohydrodynamics."},
	{"name":"Atma3.2-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3.2-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3.2-ShareGPT."},
	{"name":"Atma4","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma4","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma4\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma4."},
	{"name":"RobustFT","keyword":"sft","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/luojunyu/RobustFT","creator_name":"junyu","creator_url":"https://huggingface.co/luojunyu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRobustFT Dataset\\n\\t\\n\\nThis dataset is part of the RobustFT project: Robust Supervised Fine-tuning for Large Language Models under Noisy Response. The dataset contains various test cases with different noise ratios for training and evaluating robust fine-tuning approaches.\\nOur paper: https://huggingface.co/papers/2412.14922\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nRobustFT/\\n‚îú‚îÄ‚îÄ arc/\\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy30.csv\\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy50.csv\\n‚îÇ ‚îÇ‚îÄ‚îÄ noisy70.csv\\n‚îÇ ‚îú‚îÄ‚îÄ labeled.csv\\n‚îÇ ‚îî‚îÄ‚îÄ test.csv\\n‚îú‚îÄ‚îÄ drop/\\n‚îÇ ‚îÇ‚îÄ‚îÄ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/luojunyu/RobustFT."},
	{"name":"english-to-colloquial-tamil","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jarvisvasu/english-to-colloquial-tamil","creator_name":"Vasanth Kumar","creator_url":"https://huggingface.co/jarvisvasu","description":"\\n\\t\\n\\t\\t\\n\\t\\tEnglish to Colloquial Tamil\\n\\t\\n\\n\\\"instruction\\\":\\\"Translate provided English text into colloquial Tamil.\\\"\\n\\\"input\\\": \\\"Their players played well.\\\"\\n\\\"output\\\": \\\"‡ÆÖ‡Æµ‡Æô‡Øç‡Æï players ‡Æ®‡Æ≤‡Øç‡Æ≤‡Ææ ‡Æµ‡Æø‡Æ≥‡Øà‡ÆØ‡Ææ‡Æ£‡Øç‡Æü‡Ææ‡Æô‡Øç‡Æï.\\\"\\n\\n"},
	{"name":"Atma3-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-ShareGPT."},
	{"name":"Atma3-Share-GPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Atma3-Share-GPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/Atma3-Share-GPT."},
	{"name":"mauxi-mix-persian","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüó£Ô∏è MauxiMix: High-Quality Persian Conversations Dataset üáÆüá∑\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìù Description\\n\\t\\n\\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\\nüöß Work in Progress:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian."},
	{"name":"USCode-QAPairs-Finetuning","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning","creator_name":"Archit Rastogi ","creator_url":"https://huggingface.co/ArchitRastogi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUSCode-QueryPairs Dataset\\n\\t\\n\\nThis dataset contains query-answer pairs curated from the United States Code, suitable for fine-tuning any embedding model. It has been successfully used to fine-tune the BGE FLAG embedding model for legal data applications. The dataset is designed to enhance the semantic understanding of legal texts and support tasks like legal text retrieval, question answering, and embeddings generation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\nSource: United States Code‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ArchitRastogi/USCode-QAPairs-Finetuning."},
	{"name":"Celestia2","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2."},
	{"name":"Inst-It-Dataset","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\\n\\t\\n\\t\\t\\n\\t\\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\\n\\t\\n\\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInst-IT Dataset Overview\\n\\t\\n\\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset."},
	{"name":"mauxi-mix-persian","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-mix-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüó£Ô∏è MauxiMix: High-Quality Persian Conversations Dataset üáÆüá∑\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìù Description\\n\\t\\n\\nMauxiMix is a carefully curated dataset of 1,000 high-quality Persian conversations, translated from the SmolTalk dataset using advanced language models. This dataset is specifically designed for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques, contributing to the development of open-source Persian language models.\\nüöß Work in Progress:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-mix-persian."},
	{"name":"Inst-It-Dataset","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset","creator_name":"Inst-IT","creator_url":"https://huggingface.co/Inst-IT","description":"\\n\\t\\n\\t\\t\\n\\t\\tInst-IT Dataset: An Instruction Tuning Dataset with Multi-level Fine-Grained Annotations\\n\\t\\n\\nintroduced in the paper Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning\\nüåê Homepage | Code | ü§ó Paper | üìñ arXiv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInst-IT Dataset Overview\\n\\t\\n\\nWe create a large-scale instruction tuning dataset, the Inst-it Dataset. To the best of our knowledge, this is the first dataset that provides fine-grained annotations centric on specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Inst-IT/Inst-It-Dataset."},
	{"name":"univ_exams_finnish","keyword":"fine-tuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/readd/univ_exams_finnish","creator_name":"Perttu Isotalo","creator_url":"https://huggingface.co/readd","description":"readd/univ_exams_finnish dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Supernova","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Supernova","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Supernova is a dataset containing general synthetic chat data from the best available open-source models.\\nThe 2024-09-27 version contains:\\n\\n178.2k rows of synthetic chat responses generated using Llama 3.1 405b Instruct.\\n47k UltraChat prompts from HuggingFaceH4/ultrafeedback_binarized\\n131k SlimOrca prompts from Open-Orca/slimorca-deduped-cleaned-corrected\\n\\n\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
	{"name":"alpaca-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/shibing624/alpaca-zh","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-zh\\\"\\n\\t\\n\\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\\nDataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM \\nIt is the chinese dataset from https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shibing624/alpaca-zh."},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"\\n\\t\\n\\t\\t\\n\\t\\tFinance-Instruct-500k Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k."},
	{"name":"OpenManus-RL","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for OpenManusRL\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n  üíª [Github Repo]\\n\\n\\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\\n\\nüîç ReAct Framework - Reasoning-Acting integration\\nüß† Structured Training - Separate format/reasoning learning\\nüö´ Anti-Hallucination - Negative samples + environment grounding\\nüåê 6 Domains - OS, DB, Web, KG, Household, E-commerce\\n\\n\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL."},
	{"name":"llama70B-dpo-dataset-transformed","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/llama70B-dpo-dataset-transformed","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/llama70B-dpo-dataset-transformed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"dpo-chatml-mix-binarized","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/dpo-chatml-mix-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/dpo-chatml-mix-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"text-2-video-human-preferences-wan2.1","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Alibaba Wan2.1 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Alibaba Wan 2.1 video generation model on our benchmark. The up to date benchmark‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-wan2.1."},
	{"name":"text-2-video-human-preferences-veo2","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Google DeepMind Veo2 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Google DeepMind Veo2 video generation model on our benchmark. The up to date‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-veo2."},
	{"name":"OpenManus-RL","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL","creator_name":"CharlieDreemur","creator_url":"https://huggingface.co/CharlieDreemur","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for OpenManusRL\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n  üíª [Github Repo]\\n\\n\\nOpenManusRL combines agent trajectories from AgentInstruct, Agent-FLAN and AgentTraj-L(AgentGym) with features:\\n\\nüîç ReAct Framework - Reasoning-Acting integration\\nüß† Structured Training - Separate format/reasoning learning\\nüö´ Anti-Hallucination - Negative samples + environment grounding\\nüåê 6 Domains - OS, DB, Web, KG, Household, E-commerce\\n\\n\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nSource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CharlieDreemur/OpenManus-RL."},
	{"name":"alpaca-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yahma/alpaca-cleaned","creator_name":"Gene Ruebsamen","creator_url":"https://huggingface.co/yahma","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned."},
	{"name":"code-act","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xingyaoww/code-act","creator_name":"Xingyao Wang","creator_url":"https://huggingface.co/xingyaoww","description":" Executable Code Actions Elicit Better LLM Agents \\n\\n\\nüíª Code\\n‚Ä¢\\nüìÉ Paper\\n‚Ä¢\\nü§ó Data (CodeActInstruct)\\n‚Ä¢\\nü§ó Model (CodeActAgent-Mistral-7b-v0.1)\\n‚Ä¢\\nü§ñ Chat with CodeActAgent!\\n\\n\\nWe propose to use executable Python code to consolidate LLM agents‚Äô actions into a unified action space (CodeAct).\\nIntegrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations (e.g., code execution results) through multi-turn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xingyaoww/code-act."},
	{"name":"text-2-video-human-preferences-runway-alpha","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Runway Alpha Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~30'000 human annotations were collected to evaluate Runway's Alpha video generation model on our benchmark. The up to date benchmark can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-runway-alpha."},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset","creator_name":"Wei Zhu","creator_url":"https://huggingface.co/michaelwzhu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ChatMed\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \\nThe dataset size growing rapidly. Stay‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset."},
	{"name":"alpaca-gpt4-data","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-data\\\"\\n\\t\\n\\nAll of the work is done by this team. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChinese Dataset\\n\\t\\n\\nFound here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@article{peng2023gpt4llm,\\n    title={Instruction Tuning with GPT-4},\\n    author={Baolin Peng‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data."},
	{"name":"alpaca-gpt4-data-zh","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh","creator_name":"Chris Alexiuk","creator_url":"https://huggingface.co/llm-wizard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-data-zh\\\"\\n\\t\\n\\nAll of the work is done by this team. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage and License Notices\\n\\t\\n\\nThe data is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish Dataset\\n\\t\\n\\nFound here\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@article{peng2023gpt4llm,\\n    title={Instruction Tuning with GPT-4},\\n    author={Baolin Peng‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh."},
	{"name":"TSSB-3M-instructions","keyword":"instruct","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zirui3/TSSB-3M-instructions","creator_name":"zirui","creator_url":"https://huggingface.co/zirui3","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tdata summary\\n\\t\\n\\ninstruction dataset for code bugfix\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReference\\n\\t\\n\\n[1]. TSSB-3M-ext\\n"},
	{"name":"alpaca-spanish","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bertin-project/alpaca-spanish","creator_name":"BERTIN Project","creator_url":"https://huggingface.co/bertin-project","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBERTIN Alpaca Spanish\\n\\t\\n\\nThis dataset is a translation to Spanish of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\\n"},
	{"name":"stack-exchange-preferences","keyword":"preferences","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for H4 Stack Exchange Preferences Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training. \\nImportantly, the questions have been filtered to fit the following criteria for preference models (following closely from Askell et al. 2021): have >=2 answers. \\nThis data could also be used for instruction fine-tuning and language model training.\\nThe questions are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences."},
	{"name":"alpaca-data-gpt4-chinese","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese","creator_name":"SilkRoad","creator_url":"https://huggingface.co/silk-road","description":"silk-road/alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"alpaca-cleaned-ru","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\talpaca-cleaned-ru\\n\\t\\n\\nTranslated version of yahma/alpaca-cleaned into Russian.\\n"},
	{"name":"unix-commands","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/harpomaxx/unix-commands","creator_name":"Carlos A. Catania (Harpo)","creator_url":"https://huggingface.co/harpomaxx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUnix Commands Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThe Unix Commands Dataset is a unique collection of real-world Unix command line examples, captured from various system prompts representing different user roles and responsibilities, such as system administrators, DevOps, network administrators, Docker administrators, regular users, and hackers.\\nThe dataset consists of Unix commands ranging from basic to advanced levels and from a wide array of categories, including file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/harpomaxx/unix-commands."},
	{"name":"FinTalk-19k","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ceadar-ie/FinTalk-19k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for FinTalk-19k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFinTalk-19k is a domain-specific dataset designed for the fine-tuning of Large Language Models (LLMs) with a focus on financial conversations. Extracted from public Reddit conversations, this dataset is tagged with categories like \\\"Personal Finance\\\", \\\"Financial Information\\\", and \\\"Public Sentiment\\\". It consists of more than 19,000 entries, each representing a conversation about‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/FinTalk-19k."},
	{"name":"code-securite-sociale","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la s√©curit√© sociale, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-sociale."},
	{"name":"code-travail","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-travail","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du travail, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-travail."},
	{"name":"alpaca-cot-zh-refined-by-data-juicer","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAlpaca-CoT -- ZH (refined by Data-Juicer)\\n\\t\\n\\nA refined Chinese version of Alpaca-CoT dataset by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to fine-tune a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 18.7GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 9,873,214 (Keep ~46.58% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-zh-refined-by-data-juicer."},
	{"name":"ultrafeedback-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences,\\nand is the recommended and preferred dataset by Argilla to use from now on when fine-tuning on UltraFeedback.\\nRead more about Argilla's approach towards UltraFeedback binarization at argilla/ultrafeedback-binarized-preferences/README.md.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDifferences with argilla/ultrafeedback-binarized-preferences‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned."},
	{"name":"ultrafeedback-multi-binarized-preferences-cleaned","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltraFeedback - Multi-Binarized using the Average of Preference Ratings (Cleaned)\\n\\t\\n\\nThis dataset represents a new iteration on top of argilla/ultrafeedback-binarized-preferences-cleaned,\\nand has been created to explore whether DPO fine-tuning with more than one rejection per chosen response helps the model perform better in the \\nAlpacaEval, MT-Bench, and LM Eval Harness benchmarks.\\nRead more about Argilla's approach towards UltraFeedback binarization at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/ultrafeedback-multi-binarized-preferences-cleaned."},
	{"name":"StackMathQA","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/StackMathQA","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tStackMathQA\\n\\t\\n\\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\\n\\n\\t\\n\\t\\t\\n\\t\\tConfigs\\n\\t\\n\\nconfigs:\\n- config_name: stackmathqa1600k\\n  data_files: data/stackmathqa1600k/all.jsonl\\n  default: true\\n- config_name: stackmathqa800k\\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/StackMathQA."},
	{"name":"AutoMathText","keyword":"finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/math-ai/AutoMathText","creator_name":"math-ai","creator_url":"https://huggingface.co/math-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAutoMathText\\n\\t\\n\\nAutoMathText is an extensive and carefully curated dataset encompassing around 200 GB of mathematical texts. It's a compilation sourced from a diverse range of platforms including various websites, arXiv, and GitHub (OpenWebMath, RedPajama, Algebraic Stack). This rich repository has been autonomously selected (labeled) by the state-of-the-art open-source language model, Qwen-72B. Each piece of content in the dataset is assigned a score lm_q1q2_score within the range of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/math-ai/AutoMathText."},
	{"name":"shell-cmd-instruct","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/byroneverson/shell-cmd-instruct","creator_name":"Byron Everson","creator_url":"https://huggingface.co/byroneverson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsed to train models that interact directly with shells\\n\\t\\n\\nNote: This dataset is out-dated in the llm world, probably easier to just setup a tool with a decent model that supports tooling.\\nFollow-up details of my process \\n\\nMacOS terminal commands for now. This dataset is still in alpha stages and will be modified.\\n\\nContains 500 somewhat unique training examples so far.\\n\\nGPT4 seems like a good candidate for generating more data, licensing would need to be addressed.\\n\\nI fine-tuned‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/byroneverson/shell-cmd-instruct."},
	{"name":"Mantis-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct","creator_name":"TIGER-Lab","creator_url":"https://huggingface.co/TIGER-Lab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMantis-Instruct\\n\\t\\n\\nPaper | Website | Github | Models | Demo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummaries\\n\\t\\n\\nMantis-Instruct is a fully text-image interleaved multimodal instruction tuning dataset, \\ncontaining 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding. \\nIt's been used to train Mantis Model families\\n\\nMantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\\nAmong the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/TIGER-Lab/Mantis-Instruct."},
	{"name":"opin-pref","keyword":"preference","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/swaroop-nath/opin-pref","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Human preference dataset for Opinion Summarization. Each instance consists of reviews, two opinion summaries and the human preference. \\nPreference has been collected from domain experts. The dataset has a total of 940 instances. The instances to gather preference have been taken from the\\nhf.co/swaroop-nath/prompt-opin-summ dataset.\\nThe dataset is formatted as a jsonl file (jsonlines-guide). Each line can be loaded as a json object, and has the following format:\\n{¬†¬†¬†¬†'unique-id': a unique id‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/opin-pref."},
	{"name":"StackMathQA","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/StackMathQA","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tStackMathQA\\n\\t\\n\\nStackMathQA is a meticulously curated collection of 2 million mathematical questions and answers, sourced from various Stack Exchange sites. This repository is designed to serve as a comprehensive resource for researchers, educators, and enthusiasts in the field of mathematics and AI research.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConfigs\\n\\t\\n\\nconfigs:\\n- config_name: stackmathqa1600k\\n  data_files: data/stackmathqa1600k/all.jsonl\\n  default: true\\n- config_name: stackmathqa800k\\n  data_files:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/StackMathQA."},
	{"name":"medical","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bohu/medical","creator_name":"tang","creator_url":"https://huggingface.co/bohu","description":"From https://huggingface.co/datasets/shibing624/medical\\n"},
	{"name":"orpo-dpo-mix-40k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k","creator_name":"Maxime Labonne","creator_url":"https://huggingface.co/mlabonne","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-mix-40k v1.2\\n\\t\\n\\n\\nThis dataset is designed for ORPO or DPO training.\\nSee Fine-tune Llama 3 with ORPO for more information about how to use it.\\nIt is a combination of the following high-quality DPO datasets:\\n\\nargilla/Capybara-Preferences: highly scored chosen answers >=5 (7,424 samples)\\nargilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\\nargilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k."},
	{"name":"Capybara-Preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Capybara-Preferences","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Capybara-Preferences\\n\\t\\n\\nThis dataset has been created with distilabel.\\n\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is built on top of LDJnr/Capybara, in order to generate a preference\\ndataset out of an instruction-following dataset. This is done by keeping the conversations in the column conversation but splitting\\nthe last assistant turn from it, so that the conversation contains all the turns up until the last user's turn, so that it can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Capybara-Preferences."},
	{"name":"orca_dpo_pairs_dutch_cleaned","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Orca DPO Pairs Dutch Cleaned\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/orca_dpo_pairs_dutch_cleaned."},
	{"name":"persian-alpaca-deep-clean","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean","creator_name":"Amir Masoud Ahmadi","creator_url":"https://huggingface.co/myrkur","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPersian Alpaca Deep Clean\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Persian Alpaca Dataset is a collection of finely cleaned Persian language records derived from various sources, primarily the Bactrian, PN-Summary (summarization), and PEYMA (Named Entity Recognition) datasets. The dataset comprises approximately 68,279 records after rigorous cleaning processes, including character normalization, removal of Arabic letters, elimination of sentences with high word repetition, removal of words‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/myrkur/persian-alpaca-deep-clean."},
	{"name":"tw-legal-synthetic-qa","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa","creator_name":"Huang Liang Hsun","creator_url":"https://huggingface.co/lianghsun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for tw-legal-synthetic-qa\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nÊú¨ÂêàÊàêÂ∞çË©±Ë≥áÊñôÈõÜÔºà‰∏ãÁ®±Êú¨Ë≥áÊñôÈõÜÔºâÁî± THUDM/chatglm3-6b-32k Âíå lianghsun/tw-processed-judgmentsÔºåÁî±ÂØ¶È©óÂæåÁöÑ prompt ÂéªÁîüÊàêÁπÅÈ´î‰∏≠ÊñáÊ≥ïÂæãÂ∞çË©±ÂêàÊàêÈõÜ„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nÊú¨Ë≥áÊñôÈõÜÂèØ‰ª•ÈÅãÁî®Âú® SFTÔºåËÆìÊ®°ÂûãÂ≠∏ÊúÉÂ¶Ç‰ΩïÂõûÁ≠îÊ≥ïÂæãÂïèÈ°å„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nÁπÅÈ´î‰∏≠Êñá„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n‰∏ÄÂÄãË≥áÊñôÊ®£Êú¨Â¶Ç‰∏ãÔºåÈ¶ñÂÖàÁî± user ÁôºÂïè‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÔºàÊàñÂèØËÉΩÊúâÔºâÊ≥ïÂæãÊÉÖÂ¢ÉÁöÑÂïèÈ°åÔºåÁÑ∂Âæå assistant ÂõûÁ≠îÊ≥ïÂæãÁõ∏ÈóúÁü•Ë≠ò„ÄÇ\\n{\\n    \\\"messages\\\":[\\n        {\\n            \\\"role\\\":\\\"user\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lianghsun/tw-legal-synthetic-qa."},
	{"name":"evol-dpo-ita","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/efederici/evol-dpo-ita","creator_name":"Edoardo Federici","creator_url":"https://huggingface.co/efederici","description":"\\n  \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nEvol-DPO-Ita is a high-quality dataset consisting of ~20,000 preference pairs derived from responses generated by two large language models: Claude Opus and GPT-3.5 Turbo.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\n20,000 Preference Comparisons: Each entry contains two model responses ‚Äî Claude Opus (claude-3-opus-20240229) for the chosen response and GPT-3.5 Turbo for the rejected one, both generated from a translated prompt from the original Evol-Instruct dataset (prompt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/efederici/evol-dpo-ita."},
	{"name":"ascii_art_generation_140k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k","creator_name":"jiayi","creator_url":"https://huggingface.co/mrzjy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData for LLM ASCII Art\\n\\t\\n\\nThis repo contains open-sourced SFT data for fine-tuning LLMs on ASCII Art Generation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Links\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nLink\\nLanguage\\nSize\\n\\n\\n\\t\\t\\nascii_art_generation_140k\\nEnglish\\n138,941\\n\\n\\nascii_art_generation_140k_bilingual\\nChinese & English\\n138,941\\n\\n\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Preparation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTraining data description\\n\\t\\n\\nThe training data consists of 138,941 ASCII arts instruction-response samples for LLMs to perform SFT.\\nThe source images of these‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mrzjy/ascii_art_generation_140k."},
	{"name":"ru-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\\n\\t\\n\\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t–°–æ—Å—Ç–∞–≤\\n\\t\\n\\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\\n\\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct."},
	{"name":"KoMT-Bench","keyword":"instruction-following","license":"GNU Lesser General Public License v3.0","license_url":"https://choosealicense.com/licenses/lgpl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench","creator_name":"LG AI Research","creator_url":"https://huggingface.co/LGAI-EXAONE","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKoMT-Bench\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nWe present KoMT-Bench, a benchmark designed to evaluate the capability of language models in following instructions in Korean.\\nKoMT-Bench is an in-house dataset created by translating MT-Bench [1]  dataset into Korean and modifying some questions to reflect the characteristics and cultural nuances of the Korean language.\\nAfter the initial translation and modification, we requested expert linguists to conduct a thorough review of our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench."},
	{"name":"wangchanx-seed-free-synthetic-instruct-thai-120k","keyword":"instruction-tuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k","creator_name":"VISTEC-depa AI Research Institute of Thailand","creator_url":"https://huggingface.co/airesearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WangchanX Seed-Free Synthetic Instruct Thai 120k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains about 120k synthetic instruction-following samples in Thai, generated using a novel seed-free approach. It covers a wide range of domains derived from Wikipedia, including both general knowledge and Thai-specific cultural topics. The dataset is designed for instruction-tuning Thai language models to improve their ability to understand and generate Thai text in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/airesearch/wangchanx-seed-free-synthetic-instruct-thai-120k."},
	{"name":"ru-instruct","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/ru-instruct","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t–ö–∞—Ä—Ç–æ—á–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\\n\\t\\n\\n–°–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞ (—Å–ø–∞—Å–∏–±–æ –º–æ–¥–µ–ª–∏ Den4ikAI/nonsense_gibberish_detector). –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω SimHash'–æ–º.\\n–û–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω—ë–º –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞ –Ω–µ –∑–∞–≤—ë–∑, in progress.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t–°–æ—Å—Ç–∞–≤\\n\\t\\n\\n–°–æ–±—Ä–∞–ª –∏–∑ —ç—Ç–∏—Ö –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö:\\n\\nd0rj/OpenOrca-ru (–æ—Ç Open-Orca/OpenOrca)\\nd0rj/OpenHermes-2.5-ru (–æ—Ç teknium/OpenHermes-2.5)\\nd0rj/dolphin-ru (–æ—Ç ehartford/dolphin)\\nd0rj/alpaca-cleaned-ru (–æ—Ç‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/ru-instruct."},
	{"name":"turkish_llm_finetune_dataset_4_topics","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics","creator_name":"Barathan Aslan","creator_url":"https://huggingface.co/barathanasln","description":"\\n\\t\\n\\t\\t\\n\\t\\tTurkish LLM Finetune Dataset - 4 Topics\\n\\t\\n\\nThis dataset is designed to fine-tune the T3 AI Turkish LLM. It was created by Barathan Aslan, √ñmer Faruk √áelik, and Batuhan Kalem for the T3 AI Hackathon. The dataset focuses on four distinct topics: Agriculture, Sustainability, Turkish Education Sytem, and Turkish Law System.\\n\\n\\t\\n\\t\\t\\n\\t\\tContributors\\n\\t\\n\\n\\nBarathan Aslan (https://huggingface.co/barathanasln)\\nBatuhan Kalem(https://huggingface.co/Pancarsuyu)\\n√ñmer Faruk √áelik‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics."},
	{"name":"ethics_qna_preferences","keyword":"preferences","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/wassname/ethics_qna_preferences","creator_name":"wassname","creator_url":"https://huggingface.co/wassname","description":"Reformat of the ethics dataset to be in DPO format, in this expression format\\nExamples:\\n  subset virtue\\n    {'prompt': 'Sentence: Jill wore a new style of distinctive clothes each six months for decades.\\\\nQuestion: Does the character in this sentence exhibit the trait \\\"discredit\\\"?\\\\nAnswer: ',\\n   'chosen': 'no',\\n   'rejected': 'yes'}\\n  \\n  commonsense\\n  {'prompt': 'Post:\\\\n\\\"\\\"I went to the principal\\\\'s office to change my records before going to a different school.\\\"\\\"\\\\n\\\\n\\\\nVerdict: '‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wassname/ethics_qna_preferences."},
	{"name":"LongWriter-6k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongWriter-6k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k\\n\\t\\n\\n\\n  ü§ó [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] \\n\\n\\nLongWriter-6k dataset contains 6,000 SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese). The data can support training LLMs to extend their maximum output window size to 10,000+ words.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAll Models\\n\\t\\n\\nWe open-sourced the following list of models trained on LongWriter-6k:\\n\\n\\t\\n\\t\\t\\nModel\\nHuggingface Repo\\nDescription\\n\\n\\n\\t\\t\\nLongWriter-glm4-9b\\nü§ó‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongWriter-6k."},
	{"name":"LongCite-45k","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/THUDM/LongCite-45k","creator_name":"Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University","creator_url":"https://huggingface.co/THUDM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongCite-45k\\n\\t\\n\\n\\n  ü§ó [LongCite Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongCite Paper] \\n\\n\\nLongCite-45k dataset contains 44,600 long-context QA instances paired with sentence-level citations (both English and Chinese, up to 128,000 words). The data can support training long-context LLMs to generate response and fine-grained citations within a single output.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Example\\n\\t\\n\\nEach instance in LongCite-45k consists of an instruction, a long context (divided into sentences), a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/THUDM/LongCite-45k."},
	{"name":"rdt-ft-data","keyword":"finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data","creator_name":"Robotics Diffusion Transformer Collaboration","creator_url":"https://huggingface.co/robotics-diffusion-transformer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nThis is the fine-tuning dataset used in the paper RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\n\\nProject Page: https://rdt-robotics.github.io/rdt-robotics/\\nPaper: https://arxiv.org/pdf/2410.07864\\nCode: https://github.com/thu-ml/RoboticsDiffusionTransformer\\nModel: https://huggingface.co/robotics-diffusion-transformer/rdt-1b\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUses\\n\\t\\n\\nDownload all archive files and use the following command to extract:\\ncat‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data."},
	{"name":"apigen-synth-trl","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card\\n\\t\\n\\nThis dataset is a version of argilla/Synth-APIGen-v0.1 prepared for\\nfine-tuning using trl. To generate it, the following script was run:\\nfrom datasets import load_dataset\\nfrom jinja2 import Template\\n\\nSYSTEM_PROMPT = \\\"\\\"\\nYou are an expert in composing functions. You are given a question and a set of possible functions. \\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \\nIf none of the functions can be used, point it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl."},
	{"name":"reflection-v1-ru_subset","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\td0rj/reflection-v1-ru_subset\\n\\t\\n\\nTranslated glaiveai/reflection-v1 dataset into Russian language using GPT-4o.\\n\\nAlmost all the rows of the dataset have been translated. I have removed those translations that do not match the original by the presence of the tags \\\"thinking\\\", \\\"reflection\\\" and \\\"output\\\". Mapping to the original dataset rows can be taken from the \\\"index\\\" column.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nimport datasets\\n\\n\\ndata = datasets.load_dataset(\\\"d0rj/reflection-v1-ru_subset\\\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/d0rj/reflection-v1-ru_subset."},
	{"name":"alpaca-cleaned-indonesian","keyword":"instruction-finetuning","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian","creator_name":"Ilham Fadhil","creator_url":"https://huggingface.co/ilhamfadheel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü¶ôüõÅ Cleaned Alpaca Dataset (INDONESIAN)\\n\\t\\n\\nWelcome to the Cleaned Alpaca Dataset repository! This repository hosts a cleaned and curated version of a dataset used to train the Alpaca LLM (Large Language Model). The original dataset had several issues that are addressed in this cleaned version.\\nOn April 8, 2023 the remaining uncurated instructions (~50,000) were replaced with data from the GPT-4-LLM dataset. Curation of the incoming GPT-4 data is ongoing.\\n\\nA 7b Lora model (trained‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ilhamfadheel/alpaca-cleaned-indonesian."},
	{"name":"Leopard-Instruct","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/wyu1/Leopard-Instruct","creator_name":"Wenhao Yu","creator_url":"https://huggingface.co/wyu1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLeopard-Instruct\\n\\t\\n\\nPaper | Github | Models-LLaVA | Models-Idefics2\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSummaries\\n\\t\\n\\nLeopard-Instruct is a large instruction-tuning dataset, comprising 925K instances, with 739K specifically designed for text-rich, multiimage scenarios. It's been used to train Leopard-LLaVA [checkpoint]  and Leopard-Idefics2 [checkpoint].\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLoading dataset\\n\\t\\n\\n\\nto load the dataset without automatically downloading and process the images (Please run the following codes with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wyu1/Leopard-Instruct."},
	{"name":"metallurgy-qa","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Abdulrhman37/metallurgy-qa","creator_name":"Eldeeb","creator_url":"https://huggingface.co/Abdulrhman37","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMetallurgy and Materials Science Knowledge Extraction Dataset\\n\\t\\n\\nThis repository contains a dataset generated from parsed books related to various aspects of metallurgy, materials science, and engineering. The dataset is designed for fine-tuning Large Language Models (LLMs) for Question-Answering (QA) tasks in the domain of metallurgy and materials science.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nThe dataset includes content derived from technical books in the field of metallurgy and materials‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Abdulrhman37/metallurgy-qa."},
	{"name":"mauxitalk-persian","keyword":"instruction-following","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüó£Ô∏è MauxiTalk: High-Quality Persian Conversations Dataset üáÆüá∑\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìù Description\\n\\t\\n\\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåü Key Features\\n\\t\\n\\n\\n2,000 natural conversations in Persian\\nDiverse topics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian."},
	{"name":"orpo-dpo-mix-TR-20k","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k","creator_name":"selim","creator_url":"https://huggingface.co/selimc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tORPO-DPO-Mix-TR-20k\\n\\t\\n\\n\\nThis repository contains a Turkish translation of 20k records from the mlabonne/orpo-dpo-mix-40k dataset. The translation was carried out using the gemini-1.5-flash-002 model, chosen for its 1M token context size and overall accurate Turkish responses.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTranslation Process\\n\\t\\n\\nThe translation process uses the LLM model with translation prompt and pydantic data validation to ensure accuracy. The complete translation pipeline is available in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/selimc/orpo-dpo-mix-TR-20k."},
	{"name":"open-image-preferences-v1","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1","creator_name":"Data Is Better Together","creator_url":"https://huggingface.co/data-is-better-together","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpen Image Preferences\\n\\t\\n\\n\\n\\n\\n\\n  \\n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world, vibrant colors, 4K.\\n      \\n          \\n              \\n              Image 1\\n          \\n          \\n              \\n              Image 2\\n          \\n      \\n  \\n\\n\\n\\n  \\n      Prompt: 8-bit pixel art of a blue knight, green car, and glacier landscape in Norway, fantasy style, colorful and detailed.\\n      \\n          \\n              \\n              Image 1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/data-is-better-together/open-image-preferences-v1."},
	{"name":"mauxitalk-persian","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxitalk-persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüó£Ô∏è MauxiTalk: High-Quality Persian Conversations Dataset üáÆüá∑\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìù Description\\n\\t\\n\\nMauxiTalk is a high-quality dataset of 2,000+ Persian conversations, carefully translated from the SmolTalk dataset using state-of-the-art language models. This dataset is specifically curated for training and fine-tuning Large Language Models (LLMs) with Supervised Fine-Tuning (SFT) techniques.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåü Key Features\\n\\t\\n\\n\\n2,000 natural conversations in Persian\\nDiverse topics‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxitalk-persian."},
	{"name":"ATC-ShareGPT","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ATC-ShareGPT\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/ATC-ShareGPT."},
	{"name":"evol-dpo-ita-reranked","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEvol DPO Ita Reranked\\n\\t\\n\\n\\nA high-quality Italian preference dataset suitable for Direct Preference Optimization (DPO), ORPO, and other Preference Tuning algorithms.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü•áü•à Reranking process\\n\\t\\n\\nThis work is based on efederici/evol-dpo-ita, a nice Italian preference dataset.\\nThe original dataset includes prompts translated from the Evol-Instruct datasets, with responses generated using GPT-3.5-Turbo (rejected) and claude-3-opus-20240229 (chosen).\\nChoosing the response from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/evol-dpo-ita-reranked."},
	{"name":"open-image-preferences-v1-more-results","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\n\\n\\nWe wanted to contribute to the challenge posed by the data-is-better-together community (description below). We collected 170'000 preferences using our API from people all around the world in rougly 3 days (docs.rapidata.ai):\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for image-preferences-results Original\\n\\t\\n\\n\\n\\n\\n\\n  \\n      Prompt: Anime-style concept art of a Mayan Quetzalcoatl biomutant, dystopian world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/open-image-preferences-v1-more-results."},
	{"name":"text-2-video-human-preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Preference Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nThis dataset was collected in ~12 hours using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\nThe data collected in this dataset informs our text-2-video model benchmark. We just started so currently only two models are represented in this set:\\n\\nSora\\nHunyouan\\nPika 2.0\\nRunway ML Alpha\\nLuma Ray 2\\n\\nExplore our latest model rankings on our website.\\nIf you get value from this dataset and would‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences."},
	{"name":"reasoning-1-1k","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fluently-sets/reasoning-1-1k","creator_name":"Fluently Datasets","creator_url":"https://huggingface.co/fluently-sets","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReasoning-1 1K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tShort about\\n\\t\\n\\nThis dataset will help in SFT training of LLM on the Alpaca format.\\nThe goal of the dataset: to teach LLM to reason and analyze its mistakes using SFT training.\\nThe size of 1.15K is quite small, so for effective training on SFTTrainer set 4-6 epochs instead of 1-3.\\nMade by Fluently Team (@ehristoforu) using distilabel with loveü•∞\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nThis subset can be loaded as:\\nfrom datasets import load_dataset\\n\\nds =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fluently-sets/reasoning-1-1k."},
	{"name":"PersianSyntheticQA","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ParsBench/PersianSyntheticQA","creator_name":"ParsBench","creator_url":"https://huggingface.co/ParsBench","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPersian Synthetic QA Dataset\\n\\t\\n\\nPersian Synthetic QA is a dataset containing 100,000 synthetic questions and answers in Persian, generated using GPT-4o. The dataset is structured as conversations between a user and an assistant, with 2,000 records for each of the 50 different topics. Each conversation consists of messages with two distinct roles: \\\"user\\\" messages containing questions in Persian, and \\\"assistant\\\" messages containing the corresponding answers. The dataset is designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ParsBench/PersianSyntheticQA."},
	{"name":"text-2-image-Rich-Human-Feedback","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\n\\n\\nBuilding upon Google's research Rich Human Feedback for Text-to-Image Generation we have collected over 1.5 million responses from 152'684 individual humans using Rapidata via the Python API. Collection took roughly 5 days. \\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nWe asked humans to evaluate AI-generated images in style, coherence and prompt alignment. For images that contained flaws, participants were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-image-Rich-Human-Feedback."},
	{"name":"Sentiment2Emoji","keyword":"finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MLap/Sentiment2Emoji","creator_name":"aman prakash","creator_url":"https://huggingface.co/MLap","description":"MLap/Sentiment2Emoji dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LA_dataset_blyc","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc","creator_name":"Ibrahim BELAYACHI","creator_url":"https://huggingface.co/ibrahimBlyc","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card: Learning Analytics Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset has been carefully curated to support fine-tuning of large language models (LLMs) with a specific focus on Learning Analytics. It is structured into three JSON files, each representing a different source or collection strategy. The dataset is particularly suited for applications in education, learning analytics, and academic research.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tPurpose\\n\\t\\n\\nThe dataset is designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ibrahimBlyc/LA_dataset_blyc."},
	{"name":"degeneration-html-multilingual","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual."},
	{"name":"degeneration-html-multilingual","keyword":"instruction-tuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual","creator_name":"The Degeneration of the Nation","creator_url":"https://huggingface.co/Degeneration-Nation","description":"\\n\\t\\n\\t\\t\\n\\t\\tThe Degeneration of the Nation Multilingual Dataset\\n\\t\\n\\nThis dataset contains the complete content of The Degeneration of the Nation project, a comprehensive philosophical and cultural website exploring the intersection of technology, artificial intelligence, and human culture. The content includes philosophical essays, cultural analysis, and contemporary literature, with complex parallel structure and sophisticated HTML architecture across all language versions.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Degeneration-Nation/degeneration-html-multilingual."},
	{"name":"sora-video-generation-style-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Preference Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on their visual appeal, without seeing the prompts used to generate them. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-style-likert-scoring."},
	{"name":"PFT-MME","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CultriX/PFT-MME","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"\\n\\t\\n\\t\\t\\n\\t\\tPFT-MME: Preference Finetuning Tally-Multi-Model Evaluation Dataset\\n\\t\\n\\n\\nThe Preference Finetuning Tally-Multi-Model Evaluation (PFT-MME) dataset is meticulously curated by aggregating responses (n=6) from multiple models across (relatively simple) general task prompts. \\nThese responses undergo evaluation by a panel (n=3) of evaluator models, assigning scores to each answer. \\nThrough a tallied voting mechanism, average scores are calculated to identify the \\\"worst\\\" and \\\"best\\\" answers‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CultriX/PFT-MME."},
	{"name":"sora-video-generation-alignment-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Prompt Alignment Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~6000 human evaluators were asked to evaluate AI-generated videos based on how well the generated video matches the prompt. The specific question‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-alignment-likert-scoring."},
	{"name":"sora-video-generation-physics-likert-scoring","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Physics Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~6000 human evaluators were asked to rate AI-generated videos based on if gravity and colisions make sense, without seeing the prompts used to generate them.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-physics-likert-scoring."},
	{"name":"sora-video-generation-aligned-words","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Word for Word Alignment Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~1500 human evaluators were asked to evaluate AI-generated videos based on what part of the prompt did not align the video. The specific‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-aligned-words."},
	{"name":"sora-video-generation-time-flow","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Time flow Annotation Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~3700 human evaluators were asked to evaluate AI-generated videos based on how time flows in the video. The specific question posed was: \\\"How‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/sora-video-generation-time-flow."},
	{"name":"text-2-video-human-preferences-luma-ray2","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Luma Ray2 Human Preference\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~1 hour total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~45'000 human annotations were collected to evaluate Luma's Ray 2 video generation model on our benchmark. The up to date benchmark can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-human-preferences-luma-ray2."},
	{"name":"mauxi-COT-Persian","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/mauxi-COT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüß† mauxi-COT-Persian Dataset\\n\\t\\n\\n\\nExploring Persian Chain-of-Thought Reasoning with DeepSeek-R1, brought to you by Mauxi AI Platform\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåü Overview\\n\\t\\n\\nmauxi-COT-Persian is a community-driven dataset that explores the capabilities of advanced language models in generating Persian Chain-of-Thought (CoT) reasoning. The dataset is actively growing with new high-quality, human-validated entries being added regularly. I am personally working on expanding this dataset with rigorously‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/mauxi-COT-Persian."},
	{"name":"Mauxi-SFT-Persian","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüéØ Mauxi-SFT-Persian Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåü Overview\\n\\t\\n\\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìä Dataset Statistics\\n\\t\\n\\n\\nüî¢ Total Conversations: 5,000\\nüìù Total Tokens: 4,418,419\\nüìà Average Tokens per Conversation: 883.7\\nüéØ Format: JSONL with messages and token counts\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüîç Source & Creation\\n\\t\\n\\nThis dataset was created by translating the OpenHermes-100k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian."},
	{"name":"Persian-Math-SFT","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüéØ Persian Math Questions Dataset for SFT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüìù Description\\n\\t\\n\\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\tüîç Features\\n\\t\\n\\n\\nHigh-quality Persian questions\\nDetailed subtopic categorization\\nFocused on mathematical concepts\\nTokens count for each conversation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüöÄ Coming Soon\\n\\t\\n\\n\\nDetailed answers for each question\\nAdditional topics beyond mathematics\\nEnhanced‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT."},
	{"name":"Deepthink-Reasoning-Instruction","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction","creator_name":"Prithiv Sakthi","creator_url":"https://huggingface.co/prithivMLmods","description":"\\n\\t\\n\\t\\t\\n\\t\\tDeepthink Reasoning Demo\\n\\t\\n\\nDeepthink Reasoning is a comprehensive data repository designed to break down complex problems, especially in coding (Python, Go, Java, C++, C#, etc.) and algorithms. It provides detailed problem analyses and systematic solutions to achieve the desired outcomes.\\n\\n\\t\\n\\t\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nComprehensive Problem Breakdown: Deepthink Reasoning dissects problems into smaller, manageable components to facilitate effective understanding and solution generation.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/prithivMLmods/Deepthink-Reasoning-Instruction."},
	{"name":"text-2-video-Rich-Human-Feedback","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback","creator_name":"Rapidata","creator_url":"https://huggingface.co/Rapidata","description":"\\n\\t\\n\\t\\t\\n\\t\\tRapidata Video Generation Rich Human Feedback Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf you get value from this dataset and would like to see more in the future, please consider liking it.\\n\\n\\nThis dataset was collected in ~4 hours total using the Rapidata Python API, accessible to anyone and ideal for large scale data annotation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nIn this dataset, ~22'000 human annotations were collected to evaluate AI-generated videos (using Sora) in 5 different categories. \\n\\nPrompt - Video‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Rapidata/text-2-video-Rich-Human-Feedback."},
	{"name":"Mauxi-SFT-Persian","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüéØ Mauxi-SFT-Persian Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåü Overview\\n\\t\\n\\nWelcome to the Mauxi-SFT-Persian dataset! A high-quality Persian language dataset specifically curated for Supervised Fine-Tuning (SFT) of Large Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìä Dataset Statistics\\n\\t\\n\\n\\nüî¢ Total Conversations: 5,000\\nüìù Total Tokens: 4,418,419\\nüìà Average Tokens per Conversation: 883.7\\nüéØ Format: JSONL with messages and token counts\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüîç Source & Creation\\n\\t\\n\\nThis dataset was created by translating the OpenHermes-100k‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Mauxi-SFT-Persian."},
	{"name":"Persian-Math-SFT","keyword":"sft","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xmanii/Persian-Math-SFT","creator_name":"mani mirzaei","creator_url":"https://huggingface.co/xmanii","description":"\\n\\t\\n\\t\\t\\n\\t\\tüéØ Persian Math Questions Dataset for SFT\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüìù Description\\n\\t\\n\\nThis dataset contains Persian questions primarily focused on mathematical concepts, designed for Supervised Fine-Tuning (SFT) of Language Models.\\n\\n\\t\\n\\t\\t\\n\\t\\tüîç Features\\n\\t\\n\\n\\nHigh-quality Persian questions\\nDetailed subtopic categorization\\nFocused on mathematical concepts\\nTokens count for each conversation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüöÄ Coming Soon\\n\\t\\n\\n\\nDetailed answers for each question\\nAdditional topics beyond mathematics\\nEnhanced‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xmanii/Persian-Math-SFT."},
	{"name":"Arabic-Optimized-Reasoning-Dataset","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset","creator_name":"Jaber","creator_url":"https://huggingface.co/Jr23xd23","description":"\\n\\t\\n\\t\\t\\n\\t\\tArabic Optimized Reasoning Dataset\\n\\t\\n\\nDataset Name: Arabic Optimized ReasoningLicense: Apache-2.0Formats: CSVSize: 1600 rowsBase Dataset: cognitivecomputations/dolphin-r1Libraries Used: Datasets, Dask, Croissant\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Arabic Optimized Reasoning Dataset helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:\\n\\nUsing fewer tokens‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jr23xd23/Arabic-Optimized-Reasoning-Dataset."},
	{"name":"creative-rubrics-preferences","keyword":"preferences","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences","creator_name":"Victor Gallego","creator_url":"https://huggingface.co/vicgalle","description":"\\n\\t\\n\\t\\t\\n\\t\\tcreative-rubrics-preferences üéè\\n\\t\\n\\nA dataset of creative responses using GPT-4.5, o3-mini and DeepSeek-R1. \\nThis dataset contains several prompts seeking creative and diverse answers (like writing movie reviews, short stories, etc), and the style of the responses has been enhanced by prompting the model with custom rubrics that seek different creative styles.\\nIt can be used for finetuning for custom styles in writing tasks.\\nNote: This is a preference-formatted version of this other‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vicgalle/creative-rubrics-preferences."},
	{"name":"Electrical-engineering-ru","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/Electrical-engineering-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"Translated instructions from STEM-AI-mtl/Electrical-engineering into Russian using gemini-flash-1.5-8b.\\n"},
	{"name":"NSFW_Chat_Dataset","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","description":"\\n\\t\\n\\t\\t\\n\\t\\tüíï Spicy AI GF Chat Dataset üî•\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüö® 18+ Only! NSFW & Spicy Content Ahead üö®\\n\\t\\n\\nHey there, AI enthusiasts and romance lovers! üòè Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! üíñ If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìú What‚Äôs Inside?\\n\\t\\n\\nThis dataset features two columns:\\n\\ninput ‚Üí Boyfriend‚Äôs dialogue (aka what YOU say üòâ)\\noutput ‚Üí‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset."},
	{"name":"helpful_instructions","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/helpful_instructions","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Helpful Instructions is a dataset of (prompt, completion) pairs that are derived from a variety of public datasets. As the name suggests, it focuses on instructions that are \\\"helpful\\\", i.e. the kind of questions or tasks a human user might instruct an AI assistant to perform."},
	{"name":"instruct_me","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HuggingFaceH4/instruct_me","creator_name":"Hugging Face H4","creator_url":"https://huggingface.co/HuggingFaceH4","description":"Instruct Me is a dataset of instruction-like dialogues between a human user and AI assistant. The prompts are derived from (prompt, completion) pairs in the Helpful Instructions dataset. The goal is to train a language model to that is \\\"chatty\\\" and can answer the kind of questions or tasks a human user might instruct an AI assistant to perform."},
	{"name":"norwegian-alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/NbAiLab/norwegian-alpaca","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNB Alpaca Norwegian Bokm√•l\\n\\t\\n\\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_data_cleaned.json, a clean version of the Alpaca dataset made at Stanford.\\nAn earlier version used Facebook's NLLB 1.3B model, but the current version uses OpenAI's gpt-3.5-turbo, hence this dataset cannot be used to create models that compete in any way against OpenAI.\\n"},
	{"name":"vi_alpaca_clean","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tsdocode/vi_alpaca_clean","creator_name":"Thanh Sang","creator_url":"https://huggingface.co/tsdocode","description":"tsdocode/vi_alpaca_clean dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"translated_polish_alpaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Aspik101/translated_polish_alpaca","creator_name":"Hubert","creator_url":"https://huggingface.co/Aspik101","description":"The dataset was translated into Polish using this model: \\\"gsarti/opus-mt-tc-en-pl\\\"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"Aspik101/translated_polish_alpaca\\\")\\n\\n"},
	{"name":"alpaca-id-cleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cahya/alpaca-id-cleaned","creator_name":"Cahya Wirawan","creator_url":"https://huggingface.co/cahya","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Indonesian Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is the Indonesian translated version of the cleaned original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cahya/alpaca-id-cleaned."},
	{"name":"AlpacaDataCleaned","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alexl83/AlpacaDataCleaned","creator_name":"Alessandro Lannocca","creator_url":"https://huggingface.co/alexl83","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Alpaca-Cleaned\\n\\t\\n\\n\\nRepository: https://github.com/gururise/AlpacaDataCleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\\n\\n\\\"instruction\\\":\\\"Summarize‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alexl83/AlpacaDataCleaned."},
	{"name":"NorPaca","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorPaca","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNorPaca Norwegian Bokm√•l\\n\\t\\n\\nThis dataset is a translation to Norwegian Bokm√•l of alpaca_gpt4_data.json, a clean version of the Alpaca dataset made at Stanford, but generated with GPT4.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrompt to generate dataset\\n\\t\\n\\n    Du blir bedt om √• komme opp med et sett med 20 forskjellige oppgaveinstruksjoner. Disse oppgaveinstruksjonene vil bli gitt til en GPT-modell, og vi vil evaluere GPT-modellen for √• fullf√∏re instruksjonene. \\n\\nHer er kravene:\\n1. Pr√∏v √• ikke gjenta verbet for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MasterThesisCBS/NorPaca."},
	{"name":"NorEval","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MasterThesisCBS/NorEval","creator_name":"Copenhagen Business School","creator_url":"https://huggingface.co/MasterThesisCBS","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNorEval\\n\\t\\n\\nNorEval is a self-curated dataset to evaluate instruction-following LLMs, seeking to evaluate the models in nine categories: Language, Code, Mathematics, Classification, Communication & Marketing, Medical, General Knowledge, and Business Operations\\n"},
	{"name":"ualpaca-gpt4","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nikes64/ualpaca-gpt4","creator_name":"MrNikes","creator_url":"https://huggingface.co/nikes64","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-cleaned\\\"\\n\\t\\n\\nThis dataset contains Ukrainian Instruction-Following translated by facebook/nllb-200-3.3B\\nThe dataset was originaly shared in this repository: https://github.com/tloen/alpaca-lora\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\\n"},
	{"name":"alpaca-cleaned-tr","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cgulse/alpaca-cleaned-tr","creator_name":"Can G","creator_url":"https://huggingface.co/cgulse","description":"Alpaca Cleaned Dataset.\\nMachine Translated facebook/nllb-200-3.3B\\nLanguages\\nTurkish\\n"},
	{"name":"self-instruct-seed-ca","keyword":"instruction tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mapama247/self-instruct-seed-ca","creator_name":"Marc P√†mies","creator_url":"https://huggingface.co/mapama247","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCatalan self-instruct seed\\n\\t\\n\\nManual translation of the seed instructions from self-instruct.\\nNote that some examples could not be literally translated (e.g. jokes, puns, code) and had to be adapted to the target language.\\n"},
	{"name":"tasksource-instruct-v0","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tasksource-instruct-v0\\\" (TSI)\\n\\t\\n\\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\\nDataset size is capped at 30k examples per task to foster task diversity.\\n!pip install tasksource, pandit\\nimport tasksource, pandit\\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\\nfor tasks in df.id:\\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\\n\\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0."},
	{"name":"tasksource-instruct-v0","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/tasksource-instruct-v0","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"tasksource-instruct-v0\\\" (TSI)\\n\\t\\n\\nMulti-task instruction-tuning data recasted from 485 of the tasksource datasets.\\nDataset size is capped at 30k examples per task to foster task diversity.\\n!pip install tasksource, pandit\\nimport tasksource, pandit\\ndf = tasksource.list_tasks(instruct=True).sieve(id=lambda x: 'mmlu' not in x)\\nfor tasks in df.id:\\n  yield tasksource.load_task(task,instruct=True,max_rows=30_000,max_rows_eval=200)\\n\\nhttps://github.com/sileod/tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tasksource-instruct-v0."},
	{"name":"spider-context-instruct","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Spider Context Instruct\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tYale Lily Spider Leaderboards\\n\\t\\n\\nThe leaderboard can be seen at‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-context-instruct."},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct."},
	{"name":"VisIT-Bench","keyword":"instruction-following","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mlfoundations/VisIT-Bench","creator_name":"ML Foundations","creator_url":"https://huggingface.co/mlfoundations","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for VisIT-Bench\\n\\t\\n\\n\\nDataset Description\\nLinks\\nDataset Structure\\nData Fields\\nData Splits\\nData Loading\\n\\n\\nLicensing Information\\nAnnotations\\nConsiderations for Using the Data\\nCitation Information\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nVisIT-Bench is a dataset and benchmark for vision-and-language instruction following. The dataset is comprised of image-instruction pairs and corresponding example outputs, spanning a wide range of tasks, from simple object recognition to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlfoundations/VisIT-Bench."},
	{"name":"icl-symbol-tuning-instruct","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nFew-shot prompting demonstrates that language models can learn in context even though they were not trained to do. However, explicitly learning to learn in context meta-icl leads to better results. With symbol tuning, labels are replaced with arbitrary symbols (e.g. foo/bar), which makes learning in context a key condition to learn the instructions\\nWe implement symbol tuning, as presented in the Symbol tuning improves in-context learning paper with tasksource‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/icl-symbol-tuning-instruct."},
	{"name":"WebCPM_WK","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ZHR123/WebCPM_WK","creator_name":"ZHR","creator_url":"https://huggingface.co/ZHR123","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WebCPM_WK\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nÊú¨Êï∞ÊçÆÈõÜÊòØÁî±Êàë‰ª¨ÂØπWebCPMÁöÑpipelineÊï∞ÊçÆËøõË°å‰∫åÊ¨°Â§ÑÁêÜ‰πãÂêéÊûÑÂª∫ËÄåÊàê„ÄÇ\\n‰∏ªË¶ÅÂåÖÊã¨ËøáÊª§ÂéüÂßãÊï∞ÊçÆ‰∏≠ÁöÑ‰∏Ä‰∫õ‰ΩéË¥®ÈáèÊï∞ÊçÆÔºå‰ΩøÁî®GPT4ÂíåChatGPTÊâ©ÂÖÖÂéüÂßãÊï∞ÊçÆÔºå‰ª•Âèä‰ΩøÁî®ÈöèÊú∫ÊõøÊç¢„ÄÅÊãºÊé•ÁöÑÊñπÂºèÂ¢ûÂº∫ÂéüÂßãÊï∞ÊçÆ„ÄÇ\\nËØ•Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÁöÑÁõÆÁöÑÊòØÈÄöËøáÊåá‰ª§ÂæÆË∞ÉÁöÑÊñπÂºèÊèêÈ´òLLMÁöÑ‰∏§‰∏™ËÉΩÂäõÔºö\\n\\nÁªôÂÆöÈóÆÈ¢òÂíåÊñáÊ°£ÔºåÊäΩÂèñÊñáÊ°£‰∏≠‰∏éÈóÆÈ¢òÁõ∏ÂÖ≥Áü•ËØÜÁöÑËÉΩÂäõ„ÄÇ\\nÁªôÂÆöÂèÇËÄÉÊùêÊñôÂíåÈóÆÈ¢òÔºåÊ†πÊçÆÂèÇËÄÉÊùêÊñôÂõûÁ≠îÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe dataset is available under the Creative Commons NonCommercial (CC BY-NC 4.0).\\n"},
	{"name":"spider-skeleton-context-instruct","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct","creator_name":"Richard R.","creator_url":"https://huggingface.co/richardr1126","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Spider Skeleton Context Instruct\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSpider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students\\nThe goal of the Spider challenge is to develop natural language interfaces to cross-domain databases.\\nThis dataset was created to finetune LLMs in a ### Instruction: and ### Response: format with database context.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tYale Lily Spider Leaderboards\\n\\t\\n\\nThe leaderboard can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/richardr1126/spider-skeleton-context-instruct."},
	{"name":"OpenOrca-ru","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/OpenOrca-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenOrca-ru\\n\\t\\n\\nThis is translated version of Open-Orca/OpenOrca into Russian.\\n"},
	{"name":"alpaca-cleaned-serbian-full","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\\n\\t\\n\\t\\t\\n\\t\\tSerbian Alpaca Cleaned Dataset\\n\\t\\n\\n\\nOriginal Repository: https://github.com/gururise/AlpacaDataCleaned\\nOriginal HF Repository: https://huggingface.co/datasets/yahma/alpaca-cleaned\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a serbian cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:\\n\\nHallucinations: Many instructions in the original dataset had instructions referencing data on the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datatab/alpaca-cleaned-serbian-full."},
	{"name":"dolphin-ru","keyword":"instruct","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/d0rj/dolphin-ru","creator_name":"Dmitry Balobin","creator_url":"https://huggingface.co/d0rj","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDolphin-ru üê¨\\n\\t\\n\\nThis is translated version of ehartford/dolphin into Russian.\\n"},
	{"name":"ChatMed_Consult_Dataset","keyword":"finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset","creator_name":"ticoAg","creator_url":"https://huggingface.co/ticoAg","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for ChatMed\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nChatMed-Dataset is a dataset of 110,113 medical query-response pairs (in Chinese) generated by OpenAI's GPT-3.5 engine. The queries are crawled from several online medical consultation sites, reflecting the medical needs in the real world. The responses are generated by the OpenAI engine. This dataset is designated to to inject medical knowledge into Chinese large language models. \\nThe dataset size growing rapidly. Stay‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ticoAg/ChatMed_Consult_Dataset."},
	{"name":"AIVision360-8k","keyword":"finetune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ceadar-ie/AIVision360-8k","creator_name":"CeADAR","creator_url":"https://huggingface.co/ceadar-ie","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for AIVision360-8k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nAIVision360 is the pioneering domain-specific dataset tailor-made for media and journalism, designed expressly for the instruction fine-tuning of Large Language Models (LLMs).The AIVision360-8k dataset is a curated collection sourced from \\\"ainewshub.ie\\\", a platform dedicated to Artificial Intelligence news from quality-controlled publishers. It is designed to provide a comprehensive representation of AI-related‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ceadar-ie/AIVision360-8k."},
	{"name":"silk-road_alpaca-data-gpt4-chinese","keyword":"fine-tune","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/botp/silk-road_alpaca-data-gpt4-chinese","creator_name":"ab10","creator_url":"https://huggingface.co/botp","description":"botp/silk-road_alpaca-data-gpt4-chinese dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"gt-doremiti-instructions","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Gt-Doremiti/gt-doremiti-instructions","creator_name":"Doremiti","creator_url":"https://huggingface.co/Gt-Doremiti","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for gt-doremiti-instructions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nJeu d'instruction pour fine-tuner un LLM suivant les pr√©conisations du projet Stanford-Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\\nCes instructions sont extraites de la FAQ cr√©e par le GT DOREMITI et disponible √† cette adresse (https://gt-atelier-donnees.miti.cnrs.fr/faq.html)\\nLes donn√©es sont mise √† disposition selon les termes de la Licence Creative Commons Attribution 4.0 International.\\n"},
	{"name":"alpaca-tw-input-output-52k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-tw-input-output-52k\\\"\\n\\t\\n\\nThis dataset contains English Instruction-Following generated by GPT-3.5 using Alpaca prompts for fine-tuning LLMs.\\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nIt contains 52K instruction-following data generated by GPT-3.5 using the same prompts as in Alpaca.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-tw-input-output-52k."},
	{"name":"alpaca-gpt4-tw-input-output-48k","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k","creator_name":"David Lanz","creator_url":"https://huggingface.co/DavidLanz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"alpaca-gpt4-tw-input-output-48k\\\"\\n\\t\\n\\nThis dataset contains English Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\\nThe dataset was originaly shared in this repository: https://github.com/ntunlplab/traditional-chinese-alpaca. This is just a wraper for compatibility with huggingface's datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset structure\\n\\t\\n\\nIt contains 52K instruction-following data generated by GPT-4 using the same prompts as in Alpaca.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DavidLanz/alpaca-gpt4-tw-input-output-48k."},
	{"name":"fund-sft","keyword":"fine-tune","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jannko/fund-sft","creator_name":"ko","creator_url":"https://huggingface.co/jannko","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jannko/fund-sft."},
	{"name":"alpaca-cot-en-refined-by-data-juicer","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer","creator_name":"Data-Juicer","creator_url":"https://huggingface.co/datajuicer","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAlpaca-CoT -- EN (refined by Data-Juicer)\\n\\t\\n\\nA refined English version of Alpaca-CoT dataset by Data-Juicer. Removing some \\\"bad\\\" samples from the original dataset to make it higher-quality.\\nThis dataset is usually used to fine-tune a Large Language Model.\\nNotice: Here is a small subset for previewing. The whole dataset is available here (About 226GB).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nNumber of samples: 72,855,345 (Keep ~54.48% from the original dataset)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRefining‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer."},
	{"name":"OccuQuest","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OFA-Sys/OccuQuest","creator_name":"OFA-Sys","creator_url":"https://huggingface.co/OFA-Sys","description":"This is the dataset in OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models\\nAbstract:\\nThe emergence of large language models (LLMs) has revolutionized natural language processing tasks.\\nHowever, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned LLMs to generate helpful responses to professional queries from practitioners in specific fields.\\nTo mitigate this issue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OFA-Sys/OccuQuest."},
	{"name":"HelpSteer-AIF","keyword":"preference","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer: Helpfulness SteerLM Dataset\\n\\t\\n\\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDisclaimer\\n\\t\\n\\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF."},
	{"name":"HelpSteer-AIF-raw","keyword":"preference","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw","creator_name":"Alvaro Bartolome","creator_url":"https://huggingface.co/alvarobartt","description":"\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHelpSteer: Helpfulness SteerLM Dataset\\n\\t\\n\\nHelpSteer is an open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.\\nHelpSteer: Multi-attribute Helpfulness Dataset for SteerLM\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDisclaimer\\n\\t\\n\\nThis is only a subset created with distilabel to evaluate the first 1000 rows using AI Feedback (AIF) coming from GPT-4, only‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alvarobartt/HelpSteer-AIF-raw."},
	{"name":"lpf","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/lpf","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLivre des proc√©dures fiscales, non-instruct (11-12-2023)\\n\\t\\n\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \\nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/lpf."},
	{"name":"cgi","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/cgi","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCode G√©n√©ral des Imp√¥ts, non-instruct (11-12-2023)\\n\\t\\n\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \\nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/cgi."},
	{"name":"code-douanes","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-douanes","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des douanes, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-douanes."},
	{"name":"code-consommation","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-consommation","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la consommation, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-consommation."},
	{"name":"code-penal","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-penal","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode p√©nal, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-penal."},
	{"name":"code-sport","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-sport","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode du sport, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sport."},
	{"name":"code-civil","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-civil","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode civil, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-civil."},
	{"name":"code-commerce","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-commerce","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de commerce, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commerce."},
	{"name":"code-sante-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la sant√© publique, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-sante-publique."},
	{"name":"code-environnement","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-environnement","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'environnement, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-environnement."},
	{"name":"dac6-instruct","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDAC6 instruct (11-12-2023)\\n\\t\\n\\n‚ÄúDAC 6‚Äù refers to European Council Directive (EU) 2018/822 of May 25, 2018 relating to the automatic and mandatory exchange of information on cross-border arrangements requiring declaration. It aims to strengthen cooperation between tax administrations in EU countries on potentially aggressive tax planning arrangements.\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for tax practice. \\nFine-tuning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/dac6-instruct."},
	{"name":"code-procedure-civile","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de proc√©dure civile, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedure-civile."},
	{"name":"code-monetaire-financier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode mon√©taire et financier, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-monetaire-financier."},
	{"name":"code-assurances","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-assurances","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des assurances, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-assurances."},
	{"name":"code-artisanat","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-artisanat","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'artisanat, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-artisanat."},
	{"name":"code-commande-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la commande publique, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-commande-publique."},
	{"name":"code-propriete-intellectuelle","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la propri√©t√© intellectuelle, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-propriete-intellectuelle."},
	{"name":"code-procedures-civiles-execution","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des proc√©dures civiles d'ex√©cution, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-procedures-civiles-execution."},
	{"name":"code-route","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-route","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la route, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-route."},
	{"name":"code-education","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-education","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'√©ducation, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-education."},
	{"name":"code-construction-habitation","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la construction et de l'habitation, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-construction-habitation."},
	{"name":"code-mutualite","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-mutualite","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la mutualit√©, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-mutualite."},
	{"name":"code-transports","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-transports","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des transports, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-transports."},
	{"name":"code-urbanisme","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de l'urbanisme, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-urbanisme."},
	{"name":"code-general-fonction-publique","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCode g√©n√©ral de la fonction publique, non-instruct (11-12-2023)\\n\\t\\n\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \\nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-general-fonction-publique."},
	{"name":"code-forestier","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-forestier","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCode forestier, non-instruct (11-12-2023)\\n\\t\\n\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \\nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies involve supervised learning with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-forestier."},
	{"name":"code-justice-administrative","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de justice administrative, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-justice-administrative."},
	{"name":"code-postes-communications-electroniques","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des postes et des communications √©lectroniques, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-postes-communications-electroniques."},
	{"name":"code-relations-public-administration","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode des relations entre le public et l'administration, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-relations-public-administration."},
	{"name":"code-rural-peche-maritime","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode rural et de la p√™che maritime, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-rural-peche-maritime."},
	{"name":"code-securite-interieure","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tCode de la s√©curit√© int√©rieure, non-instruct (2025-03-10)\\n\\t\\n\\nThe objective of this project is to provide researchers, professionals and law students with simplified, up-to-date access to all French legal texts, enriched with a wealth of data to facilitate their integration into Community and European projects.\\nNormally, the data is refreshed daily on all legal codes, and aims to simplify the production of training sets and labeling pipelines for the development of free, open-source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/code-securite-interieure."},
	{"name":"bofip","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/louisbrulenaudet/bofip","creator_name":"Louis Brul√© Naudet","creator_url":"https://huggingface.co/louisbrulenaudet","description":"\\n\\t\\n\\t\\t\\n\\t\\tBulletin officiel des finances publiques - imp√¥ts, non-instruct (11-12-2023)\\n\\t\\n\\nThis project focuses on fine-tuning pre-trained language models to create efficient and accurate models for legal practice. \\nFine-tuning is the process of adapting a pre-trained model to perform specific tasks or cater to particular domains. It involves adjusting the model's parameters through a further round of training on task-specific or domain-specific data. While conventional fine-tuning strategies‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/louisbrulenaudet/bofip."},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-finetuning","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"han-instruct-dataset-v1.0\\\"\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\\nMany question are collect from Reference desk at Thai wikipedia.\\nData sources:\\n\\nReference desk at Thai wikipedia.\\nLaw from justicechannel.org\\npythainlp/final_training_set_v1_enth: Human checked and edited.\\nSelf-instruct from WangChanGLM\\nWannaphong.com\\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0."},
	{"name":"han-instruct-dataset-v1.0","keyword":"instruction-following","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0","creator_name":"PyThaiNLP","creator_url":"https://huggingface.co/pythainlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"han-instruct-dataset-v1.0\\\"\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nü™ø Han (‡∏´‡πà‡∏≤‡∏ô or goose) Instruct Dataset is a Thai instruction dataset by PyThaiNLP. It collect the instruction following in Thai from many source.\\nMany question are collect from Reference desk at Thai wikipedia.\\nData sources:\\n\\nReference desk at Thai wikipedia.\\nLaw from justicechannel.org\\npythainlp/final_training_set_v1_enth: Human checked and edited.\\nSelf-instruct from WangChanGLM\\nWannaphong.com\\nHuman‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pythainlp/han-instruct-dataset-v1.0."},
	{"name":"ultrachat_200k_dutch","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for UltraChat 200k Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch."},
	{"name":"ultrachat_200k_dutch","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch","creator_name":"Bram Vanroy","creator_url":"https://huggingface.co/BramVanroy","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for UltraChat 200k Dutch\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use this dataset, GEITje 7B Ultra (SFT) or any of its derivatives or quantizations, place cite the following paper:\\n@misc{vanroy2024geitje7bultraconversational,\\n      title={GEITje 7B Ultra: A Conversational Model for Dutch}, \\n      author={Bram Vanroy},\\n      year={2024},\\n      eprint={2412.04092},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2412.04092}, \\n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BramVanroy/ultrachat_200k_dutch."},
	{"name":"urdu_alpaca_yc_filtered","keyword":"instruction-finetuning","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered","creator_name":"Mahwiz Khalil","creator_url":"https://huggingface.co/mahwizzzz","description":"\\n\\t\\n\\t\\t\\n\\t\\tAlpaca Urdu\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThe Alpaca Urdu  is a translation of the original dataset into Urdu. This dataset is a part of the Alpaca project and is designed for NLP tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nSize: The translated dataset contains [45,622] samples.\\nLanguages: Urdu\\nLicense: [cc-by-4.0]\\nOriginal Dataset: Link to the original Alpaca Cleaned dataset repository\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tColumns\\n\\t\\n\\nThe translated dataset includes the following columns:\\n\\ninput: The input text in Urdu.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mahwizzzz/urdu_alpaca_yc_filtered."},
	{"name":"oasst2_top1_chat_format","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/blancsw/oasst2_top1_chat_format","creator_name":"Blanc Swan","creator_url":"https://huggingface.co/blancsw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenAssistant TOP-1 Conversation Threads in huggingface chat format\\n\\t\\n\\nExport of oasst2 only top 1 threads in huggingface chat format\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tScript\\n\\t\\n\\nThe convert script can be find here\\n"},
	{"name":"Financial_News_Translation_Spanish_Finetune","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune","creator_name":"Benzinga","creator_url":"https://huggingface.co/Benzinga","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview of the Financial News Translation Dataset for OpenAI Model Fine-tuning\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction:\\n\\t\\n\\nThis dataset has been curated with the primary objective of fine-tuning varioyus language models to effectively translate financial news content embedded in HTML format. The intention is to enhance the language model's proficiency in accurately and contextually translating financial information for a global audience in a production envionrment.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Benzinga/Financial_News_Translation_Spanish_Finetune."},
	{"name":"longwriter-6k-filtered","keyword":"sft","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/lenML/longwriter-6k-filtered","creator_name":"len","creator_url":"https://huggingface.co/lenML","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLongWriter-6k-Filtered\\n\\t\\n\\n\\n  ü§ñ [LongWriter Dataset]  ‚Ä¢ üíª [Github Repo] ‚Ä¢ üìÉ [LongWriter Paper] ‚Ä¢ üìÉ [Tech report]\\n\\n\\nlongwriter-6k-filtered dataset contains 666 filtered examples SFT data with ultra-long output ranging from 2k-32k words in length (both English and Chinese) based on LongWriter-6k.The data can support training LLMs to extend their maximum output window size to 10,000+ words with low computational cost.\\nThe tech report is available at Minimum Tuning to Unlock Long‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lenML/longwriter-6k-filtered."},
	{"name":"Finance-Instruct-500k","keyword":"fine-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/oieieio/Finance-Instruct-500k","creator_name":"Jorge Alonso","creator_url":"https://huggingface.co/oieieio","description":"\\n\\t\\n\\t\\t\\n\\t\\tFinance-Instruct-500k Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nFinance-Instruct-500k is a comprehensive and meticulously curated dataset designed to train advanced language models for financial tasks, reasoning, and multi-turn conversations. Combining data from numerous high-quality financial datasets, this corpus provides over 500,000 entries, offering unparalleled depth and versatility for finance-related instruction tuning and fine-tuning.\\nThe dataset includes content tailored for financial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/oieieio/Finance-Instruct-500k."},
	{"name":"Aloe-Beta-General-Collection","keyword":"finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Aloe-Beta-Medical-Collection\\n\\t\\n\\n\\n\\nCollection of curated general datasets used to fine-tune Aloe-Beta.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nWe curated data from many publicly available general instruction tuning data sources (QA format). It consists of 400k instructions including:\\n\\nCoding, math, data analysis, STEM, etc.\\n\\nFunction calling\\n\\nCreative writing, advice seeking, brainstorming, etc.\\n\\nLong sequence data\\n\\nCurated by: Jordi Bayarri‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/Aloe-Beta-General-Collection."},
	{"name":"MDCure-72k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-72k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k."},
	{"name":"MDCure-12k","keyword":"instruction-following","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-12k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k."},
	{"name":"MDCure-72k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-72k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-72k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-72k."},
	{"name":"MDCure-12k","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yale-nlp/MDCure-12k","creator_name":"Yale NLP Lab","creator_url":"https://huggingface.co/yale-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMDCure-12k\\n\\t\\n\\nüìÑ Paper | ü§ó HF Collection | ‚öôÔ∏è GitHub Repo\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nMDCure is an effective and scalable procedure for generating high-quality multi-document (MD) instruction tuning data to improve MD capabilities of LLMs. Using MDCure, we construct a suite of MD instruction datasets complementary to collections such as FLAN and fine-tune a variety of already instruction-tuned LLMs from the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in size.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yale-nlp/MDCure-12k."},
	{"name":"gemma-vs-gemma-preferences","keyword":"preference","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences","creator_name":"Stefano Fiorucci","creator_url":"https://huggingface.co/anakin87","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüíéüÜöüíé Gemma vs Gemma Preferences\\n\\t\\n\\nThis dataset contains on-policy collected preferences generated using anakin87/gemma-2-2b-ita-sft.\\n‚ö†Ô∏è While this dataset may be valuable for didactic purposes, it is not recommended for training a model using Preference Tuning due to the following reasons:\\n\\nThe training would be off-policy for your model.\\nThe dataset was generated with gemma-2-2b-ita-sft, a small model for Italian.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMotivation\\n\\t\\n\\nWhile DPO (Direct Preference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anakin87/gemma-vs-gemma-preferences."},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Anatomist\\n\\t\\n\\nThis dataset was extracted from the Web Ontology Language (OWL) \\nrepresentation of the Foundational Model of Anatomy [1] using \\nOwlready2 to facilitate the extraction of the logical axioms in the FMA\\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \\ncanonical human anatomy.\\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist."},
	{"name":"MrGrammaticalOntology_anatomist","keyword":"instruct","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist","creator_name":"Chime Ogbuji","creator_url":"https://huggingface.co/cogbuji","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMr. Grammatical Ontology: Anatomist\\n\\t\\n\\nThis dataset was extracted from the Web Ontology Language (OWL) \\nrepresentation of the Foundational Model of Anatomy [1] using \\nOwlready2 to facilitate the extraction of the logical axioms in the FMA\\ninto a Controlled Natural Language (CNL) for use in training Medical Large Language Models to be conversant in \\ncanonical human anatomy.\\nSo, Description Logic expressions such as the following in FMA about fma13245 (Surface of left ventricle):‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cogbuji/MrGrammaticalOntology_anatomist."},
	{"name":"LONGCOT-Alpaca","keyword":"instruction-tuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca","creator_name":"RS","creator_url":"https://huggingface.co/HappyAIUser","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LONGCOT-Alpaca\\n\\t\\n\\nThis dataset contains instruction-input-output pairs converted to ShareGPT format, designed for instruction tuning and text generation tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset consists of carefully curated instruction-input-output pairs, formatted for conversational AI training. Each entry contains:\\n\\nAn instruction that specifies the task\\nAn optional input providing context\\nA detailed output that addresses the instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HappyAIUser/LONGCOT-Alpaca."},
	{"name":"ProcessedOpenAssistant","keyword":"instruction-finetuning","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant","creator_name":"Y. Yu","creator_url":"https://huggingface.co/PursuitOfDataScience","description":"\\n\\t\\n\\t\\t\\n\\t\\tRefined OASST1 Conversations\\n\\t\\n\\nDataset Name on Hugging Face: PursuitOfDataScience/ProcessedOpenAssistant\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is derived from the OpenAssistant/oasst1 conversations, with additional processing to:\\n\\nRemove single-turn or incomplete conversations (where a prompter/user message had no assistant reply),\\nRename roles from \\\"prompter\\\" to \\\"User\\\" and \\\"assistant\\\" to \\\"Assistant\\\",\\nOrganize each conversation as a list of turn objects.\\n\\nThe goal is to provide a clean‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PursuitOfDataScience/ProcessedOpenAssistant."},
	{"name":"sasha_smart_home_reasoning","keyword":"finetune","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning","creator_name":"Thoughtful Things","creator_url":"https://huggingface.co/ThoughtfulThings","description":"This is a dataset of smart home user commands and JSON responses generated by zero-shot prompting of GPT-4. It can be used to fine-tune and/or evaluate language models for responding to user commands in smart homes. For more information, refer to our paper Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.\\nhttps://arxiv.org/abs/2305.09802\\nIf you use the dataset in your work, please cite us:\\n@article{king2024sasha,\\n  title={Sasha: creative goal-oriented reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ThoughtfulThings/sasha_smart_home_reasoning."},
	{"name":"aif-emotional-generation","keyword":"sft","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mario-rc/aif-emotional-generation","creator_name":"Mario Rodr√≠guez-Cantelar","creator_url":"https://huggingface.co/mario-rc","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a novel, diverse multi-turn dialogues dataset developed to train and evaluate a chatbot‚Äôs ability to generate emotionally nuanced responses. The dataset was created using GPT-4 as a generative engine, following the CoE methodology. This methodology structures dialogues to reflect consistent empathetic understanding and emotional tone alignment, along with open-ended responses, to promote conversational‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mario-rc/aif-emotional-generation."}
]
;
