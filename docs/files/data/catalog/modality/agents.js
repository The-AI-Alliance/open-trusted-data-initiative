const data_for_modality_agents = 
[
	{"name":"hibo-function-calling-v1","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1","creator_name":"Thibaud Perrin","creator_url":"https://huggingface.co/thibaud-perrin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\thibo-function-calling-v1\\n\\t\\n\\n\\n    \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìñ Dataset Description\\n\\t\\n\\nThis dataset, named \\\"hibo-function-calling-v1\\\", is designed to facilitate the fine-tuning of Large Language Models (LLMs) for function calling tasks. It comprises a single 'train' split containing 323,271 data points across three columns: 'dataset_origin', 'system', and 'chat'. \\nThe dataset is a result of merging two distinct sources: gathnex/Gath_baize and glaiveai/glaive-function-calling-v2, with an aim to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"pandora-tool-calling","keyword":"function-calling","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPandora Tool Calling\\n\\t\\n\\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopyright and license\\n\\t\\n\\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\\nProject developed under a BSD-3-Clause license.\\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"glaive-v2-single-turn-func-call-chatml","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/recastai/glaive-v2-single-turn-func-call-chatml","creator_name":"Re:cast AI","creator_url":"https://huggingface.co/recastai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"glaive-v2-single-turn-func-call-chatml\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset has been created by Re:cast AI to transform the existing dataset glaiveai/glaive-function-calling-v2 into a chatml friendly format for use in SFT tasks with pretrained models.\\nThe original dataset was filtered and altered with the following:\\n\\nRemoved examples that do not produce a function completion response.\\nEach example is a single-turn between user and assistant along with a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/recastai/glaive-v2-single-turn-func-call-chatml.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Agent-FLAN","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Agent-FLAN","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAgent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\\n\\t\\n\\nThis page holds the dataset proposed in Agent-FLAN, which consists of AgentInstruct, Toolbench, and customized negative agent samples as its source datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‚ú® Introduction\\n\\t\\n\\n[ü§ó HuggingFace]\\n[üìÉ Paper]\\n[üåê Project Page]\\n\\nOpen-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Agent-FLAN.","first_N":5,"first_N_keywords":["apache-2.0","arxiv:2403.12881","üá∫üá∏ Region: US","agent"],"keywords_longer_than_N":false},
	{"name":"Agentic-DPO-V0.1","keyword":"agents","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1","creator_name":"Capx AI","creator_url":"https://huggingface.co/Capx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAgentic DPO V1.0\\n\\t\\n\\n\\n\\nThe Capx Agentic DPO (Direct Prompt Optimization) Dataset is a unique collection of prompts, chosen answers, and rejected answers designed to train and optimize AI models for agentic and intuitive processing. \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset covers a wide range of topics, including but not limited to problem-solving, creativity, analysis, and general knowledge. The prompts are specifically crafted to elicit agentic responses from the AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"BioKGBench-Dataset","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AutoLab-Westlake/BioKGBench-Dataset","creator_name":"AutoLab Westlake","creator_url":"https://huggingface.co/AutoLab-Westlake","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAgent4S-BioKG\\n\\t\\n\\nA Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science.\\n\\n\\n    \\n    \\n\\n     Github \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nPursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models(LLMs).However, to evaluate such systems, people either rely on direct Question-Answering(QA) to the LLM itself, or in a biomedical experimental‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AutoLab-Westlake/BioKGBench-Dataset.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","other","fact-checking","closed-domain-qa"],"keywords_longer_than_N":true},
	{"name":"multi-agent-scam-conversation","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"mmau","keyword":"function-calling","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/apple/mmau","creator_name":"Apple","creator_url":"https://huggingface.co/apple","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMAU Dataset: A Holistic Benchmark of Agent Capabilities Across Diverse Domains\\n\\t\\n\\n\\n\\n","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","1K<n<10K","arxiv:2407.18961"],"keywords_longer_than_N":true},
	{"name":"mmau","keyword":"agent","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/apple/mmau","creator_name":"Apple","creator_url":"https://huggingface.co/apple","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMAU Dataset: A Holistic Benchmark of Agent Capabilities Across Diverse Domains\\n\\t\\n\\n\\n\\n","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","1K<n<10K","arxiv:2407.18961"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling-llama3-chat","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling-llama3-chat","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFunction Calling dataset for Assist LLM for Home Assistant\\n\\t\\n\\nThis dataset is generated by using other conversation agent pipelines as teachers\\nfrom the deivce-actions-v2 dataset.\\nThis dataset is used to support fine tuning of llama based models.\\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFunction Calling dataset for Assist LLM for Home Assistant\\n\\t\\n\\nThis dataset is generated by using other conversation agent pipelines as teachers\\nfrom the deivce-actions-v2 dataset.\\nThis dataset is used to support fine tuning of llama based models.\\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling-messages","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling-messages","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFunction Calling dataset for Assist LLM for Home Assistant\\n\\t\\n\\nThis dataset is generated by using other conversation agent pipelines as teachers\\nfrom the deivce-actions-v2 dataset.\\nThis dataset is used to support fine tuning of llama based models.\\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"glaive-function-calling-v2-pl","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mpieck/glaive-function-calling-v2-pl","creator_name":"Maciej Piecko","creator_url":"https://huggingface.co/mpieck","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for glaive-function-calling-v2-pl Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is a fragment of glaiveai/glaive-function-calling-v2 dataset translated to polish. \\nIt contains first 3.3k (out of 5k total, this is work in progress) instructions of the original dataset. Only instructions having function definitions or function calls are included, instructions without functions (ordinary unstructured) from the original dataset are skipped.\\n Some repeating instructions were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mpieck/glaive-function-calling-v2-pl.","first_N":5,"first_N_keywords":["text-generation","question-answering","Polish","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWE-agent-trajectories","keyword":"agents","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nebius/SWE-agent-trajectories","creator_name":"Nebius","creator_url":"https://huggingface.co/nebius","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 80,036 trajectories generated by a software engineering agent based on the SWE-agent framework, using various models as action generators. In these trajectories, the agent attempts to solve GitHub issues from the nebius/SWE-bench-extra and the dev split of princeton-nlp/SWE-bench.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset was created as part of a research project focused on developing a software engineering agent using open-weight models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nebius/SWE-agent-trajectories.","first_N":5,"first_N_keywords":["cc-by-4.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"Seal-Tools","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/casey-martin/Seal-Tools","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","description":"\\n\\t\\n\\t\\t\\n\\t\\tSeal-Tools\\n\\t\\n\\n\\n\\nThis Huggingface repository contains the dataset generated in Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark.\\n\\n\\t\\n\\t\\t\\n\\t\\tAbstract\\n\\t\\n\\nSeal-Tools contains self-instruct API-like tools. Seal-Tools not only offers a large\\nnumber of tools, but also includes instances\\nwhich demonstrate the practical application\\nof tools. Seeking to generate data on a large\\nscale while ensuring reliability, we propose a\\nself-instruct method to generate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/Seal-Tools.","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"chatml-function-calling-v2","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ankush13r/chatml-function-calling-v2","creator_name":"Ankush Rana","creator_url":"https://huggingface.co/ankush13r","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Conversion\\n\\t\\n\\nThis dataset is a converted version of the Glaive Function Calling v2 dataset, originally hosted on Hugging Face.\\n\\n\\t\\n\\t\\t\\n\\t\\tChat Template for Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis chat template is designed to work with this dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\tTemplate\\n\\t\\n\\n\\nchat_template = \\\"\\\"{%- set tools = tools if tools is defined else None -%}\\n{%- set date_string = date_string if date_string is defined else \\\"1 Sep 2024\\\" -%}\\n\\n{%- set system_message = messages[0].content if‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ankush13r/chatml-function-calling-v2.","first_N":5,"first_N_keywords":["text-generation","question-answering","feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"crosswoz-sft","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BruceNju/crosswoz-sft","creator_name":"zhongyah","creator_url":"https://huggingface.co/BruceNju","description":"multilinguality:  \\n- monolingual  \\n\\ndescription: |  \\n                          \\n    ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éCrossWOZÊï∞ÊçÆÈõÜÂ§ÑÁêÜÁöÑÂØπËØùÊï∞ÊçÆÈõÜÔºå‰∏ìÈó®Áî®‰∫éÂ§ßÊ®°ÂûãÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâ‰ªªÂä°„ÄÇ  \\n    Êï∞ÊçÆÈõÜÂåÖÂê´Â§öËΩÆÂØπËØù„ÄÅÁî®Êà∑ÁõÆÊ†á„ÄÅÂØπËØùÁä∂ÊÄÅÁ≠â‰ø°ÊÅØÔºåÈÄÇÂêàËÆ≠ÁªÉ‰ªªÂä°ÂûãÂØπËØùÁ≥ªÁªü„ÄÇ  \\n\\n    ÂéüÂßãÊï∞ÊçÆÊù•Ê∫ê‰∫éCrossWOZÈ°πÁõÆÔºåÁªèËøá‰∏ìÈó®ÁöÑÈ¢ÑÂ§ÑÁêÜ‰ΩøÂÖ∂Êõ¥ÈÄÇÂêàÁé∞‰ª£Â§ßÊ®°ÂûãËÆ≠ÁªÉ„ÄÇ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tÊ†∏ÂøÉÁâπÂæÅÔºö\\n\\t\\n\\nËøôÊòØÈ¶ñ‰∏™Â§ßËßÑÊ®°ÁöÑ‰∏≠ÊñáË∑®Âüü‰ªªÂä°ÂûãÂØπËØùÊï∞ÊçÆÈõÜ\\nÂåÖÂê´6,012‰∏™ÂØπËØùÔºå102,000‰∏™ËØùËØ≠ÔºåË¶ÜÁõñ5‰∏™È¢ÜÂüü(ÈÖíÂ∫ó„ÄÅÈ§êÂéÖ„ÄÅÊôØÁÇπ„ÄÅÂú∞ÈìÅÂíåÂá∫ÁßüËΩ¶)\\nÁ∫¶60%ÁöÑÂØπËØùÂåÖÂê´Ë∑®ÂüüÁî®Êà∑ÁõÆÊ†á\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‰∏ªË¶ÅÂàõÊñ∞ÁÇπÔºö\\n\\t\\n\\nÊõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÂüüÈó¥‰æùËµñÂÖ≥Á≥ªÔºö\\n\\n‰∏Ä‰∏™È¢ÜÂüüÁöÑÈÄâÊã©‰ºöÂä®ÊÄÅÂΩ±ÂìçÂÖ∂‰ªñÁõ∏ÂÖ≥È¢ÜÂüüÁöÑÈÄâÊã©\\n‰æãÂ¶ÇÁî®Êà∑ÈÄâÊã©ÁöÑÊôØÁÇπ‰ºöÂΩ±ÂìçÂêéÁª≠ÈÖíÂ∫óÁöÑÊé®ËçêËåÉÂõ¥(ÈúÄË¶ÅÂú®ÊôØÁÇπÈôÑËøë)\\n\\nÂÆåÊï¥ÁöÑÊ†áÊ≥®Ôºö\\n\\nÂêåÊó∂Êèê‰æõÁî®Êà∑Á´ØÂíåÁ≥ªÁªüÁ´ØÁöÑÂØπËØùÁä∂ÊÄÅÊ†áÊ≥®\\nÂåÖÂê´ÂØπËØùË°å‰∏∫(dialogue acts)ÁöÑÊ†áÊ≥®\\nÁî®Êà∑Áä∂ÊÄÅÊ†áÊ≥®ÊúâÂä©‰∫éËøΩË∏™ÂØπËØùÊµÅÁ®ãÂíåÂª∫Ê®°Áî®Êà∑Ë°å‰∏∫‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BruceNju/crosswoz-sft.","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"func_calls_ds","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/retrain-pipelines/func_calls_ds","creator_name":"retrain-pipelines","creator_url":"https://huggingface.co/retrain-pipelines","description":"\\n\\t\\n\\t\\t\\n\\t\\tretrain-pipelines Function Calling\\n\\t\\n\\nversion 0.10  -  2025-03-16 13:07:17 UTC\\nSource datasets :\\n\\nmain¬†:\\nXlam Function Calling 60k\\nSalesforce/xlam-function-calling-60k\\n(26d14eb -\\n  2025-01-24 19:25:58 UTC)\\nlicense¬†:\\ncc-by-4.0\\narxiv¬†:\\n- 2406.18518\\n\\n\\ndata-enrichment¬†:\\nNatural Questions Clean\\nlighteval/natural_questions_clean\\n(a72f7fa -\\n  2023-10-17 20:29:08 UTC)\\nlicense¬†:\\nunknown\\nThe herein dataset has 2 configs : continued_pre_training and supervised_finetuning.\\nThe former serves for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/retrain-pipelines/func_calls_ds.","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","Salesforce/xlam-function-calling-60k","lighteval/natural_questions_clean"],"keywords_longer_than_N":true},
	{"name":"web_agents_google_flight_trajectories","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anonx3247/web_agents_google_flight_trajectories","creator_name":"Anas lecaillon","creator_url":"https://huggingface.co/anonx3247","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWeb Agent Google Flight Trajectories\\n\\t\\n\\nThis dataset was originally created on Nov 23 2024 during EF's Ai On Edge Hackathon.\\nThe purpose of this dataset is to give both positive and negative image web-agent trajectories to finetune small edge-models on web agentic tasks.\\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"turkish-function-calling-2k","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/turkish-function-calling-2k","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"Used argilla-warehouse/python-seed-tools to sample tools.\\n","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"function-calling-sharegpt","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hypervariance/function-calling-sharegpt","creator_name":"hypervariance","creator_url":"https://huggingface.co/hypervariance","description":"This is a dataset for finetuning models on function calling based on glaiveai/glaive-function-calling-v2.\\nThe dataset includes 86,864 examples of chats that include function calling as part of the conversation. The system prompt includes either 0, 1, or 2 functions that the assistant can use, and instructions on how the agent can use it.\\nChanges include:\\n\\nUsing ShareGPT format for chats\\nAdding \\\"function_response\\\" as a role\\nRemoving code examples\\nRemoving examples with invalid JSON as function‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hypervariance/function-calling-sharegpt.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"Agent-FLAN","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/internlm/Agent-FLAN","creator_name":"InternLM","creator_url":"https://huggingface.co/internlm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAgent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\\n\\t\\n\\nThis page holds the dataset proposed in Agent-FLAN, which consists of AgentInstruct, Toolbench, and customized negative agent samples as its source datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‚ú® Introduction\\n\\t\\n\\n[ü§ó HuggingFace]\\n[üìÉ Paper]\\n[üåê Project Page]\\n\\nOpen-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/internlm/Agent-FLAN.","first_N":5,"first_N_keywords":["apache-2.0","arxiv:2403.12881","üá∫üá∏ Region: US","agent"],"keywords_longer_than_N":false},
	{"name":"SWE-bench-extra","keyword":"agents","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nebius/SWE-bench-extra","creator_name":"Nebius","creator_url":"https://huggingface.co/nebius","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nSWE-bench Extra is a dataset that can be used to train or evaluate agentic systems specializing in resolving GitHub issues. It is based on the methodology used to build SWE-bench benchmark and includes 6,415 Issue-Pull Request pairs sourced from 1,988 Python repositories.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe SWE-bench Extra dataset supports the development of software engineering agents capable of autonomously solving GitHub issues. The data collection process, based‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nebius/SWE-bench-extra.","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"agent-leaderboard","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galileo-ai/agent-leaderboard","creator_name":"Galileo","creator_url":"https://huggingface.co/galileo-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAgent Leaderboard\\n\\t\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Agent Leaderboard evaluates language models' ability to effectively utilize tools in complex scenarios. With major tech CEOs predicting 2025 as a pivotal year for AI agents, we built this leaderboard to answer: \\\"How do AI agents perform in real-world business scenarios?\\\"\\nGet latest update of the leaderboard on Hugging Face Spaces. For more info, checkout the blog post for a detailed overview of our evaluation methodology.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/galileo-ai/agent-leaderboard.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"agent-leaderboard","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galileo-ai/agent-leaderboard","creator_name":"Galileo","creator_url":"https://huggingface.co/galileo-ai","description":"\\n\\t\\n\\t\\t\\n\\t\\tAgent Leaderboard\\n\\t\\n\\n\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Agent Leaderboard evaluates language models' ability to effectively utilize tools in complex scenarios. With major tech CEOs predicting 2025 as a pivotal year for AI agents, we built this leaderboard to answer: \\\"How do AI agents perform in real-world business scenarios?\\\"\\nGet latest update of the leaderboard on Hugging Face Spaces. For more info, checkout the blog post for a detailed overview of our evaluation methodology.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/galileo-ai/agent-leaderboard.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Taskbench","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/microsoft/Taskbench","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","description":"\\n\\n\\n\\n\\n\\n  \\nTaskBench: Benchmarking Large Language Models for Task Automation\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nTaskBench is a benchmark for evaluating large language models (LLMs) on task automation. Task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To address this challenge, we propose a comprehensive evaluation‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/Taskbench.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Synth-APIGen-v0.1","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card for Synth-APIGen-v0.1\\n\\t\\n\\nThis dataset has been created with distilabel.\\nPipeline script: pipeline_apigen_train.py.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset creation\\n\\t\\n\\nIt has been created with distilabel==1.4.0 version.\\nThis dataset is an implementation of APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets in distilabel,\\ngenerated from synthetic functions. The process can be summarized as follows:\\n\\nGenerate (or in this case modify)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"apigen-synth-trl","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card\\n\\t\\n\\nThis dataset is a version of argilla/Synth-APIGen-v0.1 prepared for\\nfine-tuning using trl. To generate it, the following script was run:\\nfrom datasets import load_dataset\\nfrom jinja2 import Template\\n\\nSYSTEM_PROMPT = \\\"\\\"\\nYou are an expert in composing functions. You are given a question and a set of possible functions. \\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \\nIf none of the functions can be used, point it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"apigen-function-calling","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/apigen-function-calling","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card for argilla/apigen-function-calling\\n\\t\\n\\nThis dataset is a merge of argilla/Synth-APIGen-v0.1\\nand Salesforce/xlam-function-calling-60k, making\\nover 100K function calling examples following the APIGen recipe.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPrepare for training\\n\\t\\n\\nThis version is not ready to do fine tuning, but you can run a script like prepare_for_sft.py\\nto prepare it, and run the same recipe that can be found in\\nargilla/Llama-3.2-1B-Instruct-APIGen-FC-v0.1#training-procedure.\\nModify the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/apigen-function-calling.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"WorFBench_test","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/WorFBench_test","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":" WorFBench \\n Benchmarking Agentic Workflow Generation \\n\\n\\n  üìÑarXiv ‚Ä¢\\n  ü§óHFPaper ‚Ä¢\\n  üåêWeb ‚Ä¢\\n  üñ•Ô∏èCode ‚Ä¢\\n  üìäDataset\\n\\n\\n\\nüåªAcknowledgement\\nüåüOverview\\nüîßInstallation\\n‚úèÔ∏èModel-Inference\\nüìùWorkflow-Generation\\nü§îWorkflow-Evaluation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåªAcknowledgement\\n\\t\\n\\nOur code of training module is referenced and adapted from LLaMA-Factory. And the Dataset is collected from ToolBench, ToolAlpaca, Lumos, WikiHow, Seal-Tools, Alfworld, Webshop, IntercodeSql. Our end-to-end evaluation module is based on IPR‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/WorFBench_test.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"WorFBench_train","keyword":"agents","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/WorFBench_train","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"This repository contains the data presented in Benchmarking Agentic Workflow Generation.\\nCode: https://github.com/zjunlp/WorfBench\\n","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"apigen-smollm-trl-FC","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-smollm-trl-FC","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset card for argilla-warehouse/apigen-smollm-trl-FC\\n\\t\\n\\nThis dataset is a merge of argilla/Synth-APIGen-v0.1\\nand Salesforce/xlam-function-calling-60k, and was prepared for training using the script\\nprepare_for_sft.py that can be found in the repository files.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReferences\\n\\t\\n\\n@article{liu2024apigen,\\n  title={APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets},\\n  author={Liu, Zuxin and Hoang, Thai and Zhang, Jianguo and Zhu, Ming‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-smollm-trl-FC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Solana","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ordlibrary/Solana","creator_name":"8 bit","creator_url":"https://huggingface.co/ordlibrary","description":"ordlibrary/Solana dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","üá∫üá∏ Region: US","solana","crypto","blockchain"],"keywords_longer_than_N":true},
	{"name":"DroidCall","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mllmTeam/DroidCall","creator_name":"mllm","creator_url":"https://huggingface.co/mllmTeam","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDroidCall: A Dataset for LLM-powered Android Intent Invocation\\n\\t\\n\\npaper|github\\nDroidCall is the first open-sourced, high-quality dataset designed for fine-tuning LLMs for accurate intent invocation on Android devices.\\nThis repo contains data generated by DroidCall. The process of data generation is shown in the figure below\\n\\nDetails can be found in our paper and github repository.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhat is Android Intent Invocation?\\n\\t\\n\\nAndroid Intent is a key machanism in Android that‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mllmTeam/DroidCall.","first_N":5,"first_N_keywords":["text-generation","question-answering","task-planning","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"nestful","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ibm-research/nestful","creator_name":"IBM Research","creator_url":"https://huggingface.co/ibm-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tNESTFUL: Nested Function-Calling Dataset\\n\\t\\n\\n\\n\\n\\n\\n\\nNESTFUL is a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to\\na subsequent call.\\nThe NESTFUL dataset includes over 1800 nested sequences from two main areas: mathematical reasoning and coding tools. The mathematical reasoning portion is generated from \\nthe MathQA dataset, while the coding portion is generated from the\\nStarCoder2-Instruct dataset.\\nAll‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/nestful.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü™Ñ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\\n\\t\\n\\n\\n  üåê[Website] ¬†\\n  üìù[Paper] ¬†\\n  ü§ó[Data] ¬†\\n  ü§ó[Model] ¬†\\n  ü§ó[Demo] ¬†\\n\\n\\nWe introduce ü™ÑLumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \\nLumos has following features:\\n\\nüß© Modular Architecture:\\nüß© Lumos consists of planning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"Coding-Agent-Github-2025-Feb","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepNLP/Coding-Agent-Github-2025-Feb","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tCoding Agent AI Agent Directory to Host All Coding Agent related AI Agents Web Traffic Data, Search Ranking, Community, Reviews and More.\\n\\t\\n\\nThis is the Coding Agent Dataset from pypi package \\\"coding_agent\\\" https://pypi.org/project/coding_agent. You can use this package to download and get statistics (forks/stars/website traffic) of AI agents on website from AI Agent Marketplace AI Agent Directory  (http://www.deepnlp.org/store/ai-agent) and AI Agent Search Portal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/Coding-Agent-Github-2025-Feb.","first_N":5,"first_N_keywords":["mit","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true}
]
;
