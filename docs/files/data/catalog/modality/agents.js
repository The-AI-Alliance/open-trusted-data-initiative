const data_for_modality_agents = 
[
	{"name":"turkish-function-calling-2k","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/turkish-function-calling-2k","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"Used argilla-warehouse/python-seed-tools to sample tools.\n","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"When2Call","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nvidia/When2Call","creator_name":"NVIDIA","creator_url":"https://huggingface.co/nvidia","description":"\n\t\n\t\t\n\t\tWhen2Call\n\t\n\n\n ðŸ’¾ GithubÂ Â  | Â Â  ðŸ“„ Paper\n\n\n\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nWhen2Call is a benchmark designed to evaluate tool-calling decision-making for large language models (LLMs), including when to generate a tool call, when to ask follow-up questions, when to admit the question can't be answered with the tools provided, and what to do if the question seems to require tool use but a tool call can't be made. \nWe find that state-of-the-art tool-calling LMs show significant room forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/When2Call.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"SWE-PolyBench","keyword":"agents","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AmazonScience/SWE-PolyBench","creator_name":"Amazon Science","creator_url":"https://huggingface.co/AmazonScience","description":"\n\t\n\t\t\n\t\tSWE-PolyBench\n\t\n\nSWE-PolyBench is a multi language repo level software engineering benchmark. Currently it includes 4 languages: Python, Java, Javascript, and Typescript. The number of instances in each language is:\nJavascript: 1017\nTypescript: 729\nPython: 199\nJava: 165\n\n\t\n\t\t\n\t\tDatasets\n\t\n\nThere are total two datasets available under SWE-PolyBench. AmazonScience/SWE-PolyBench is the full dataset and AmazonScience/SWE-PolyBench_500 is the stratified sampled dataset with 500 instances.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AmazonScience/SWE-PolyBench.","first_N":5,"first_N_keywords":["mit","1K - 10K","csv","Tabular","Text"],"keywords_longer_than_N":true},
	{"name":"SWE-PolyBench_500","keyword":"agents","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AmazonScience/SWE-PolyBench_500","creator_name":"Amazon Science","creator_url":"https://huggingface.co/AmazonScience","description":"\n\t\n\t\t\n\t\tSWE-PolyBench\n\t\n\nSWE-PolyBench is a multi language repo level software engineering benchmark. Currently it includes 4 languages: Python, Java, Javascript, and Typescript. The number of instances in each language is:\nJavascript: 1017\nTypescript: 729\nPython: 199\nJava: 165\n\n\t\n\t\t\n\t\tDatasets\n\t\n\nThere are total two datasets available under SWE-PolyBench. AmazonScience/SWE-PolyBench is the full dataset and AmazonScience/SWE-PolyBench_500 is the stratified sampled dataset with 500 instances.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/AmazonScience/SWE-PolyBench_500.","first_N":5,"first_N_keywords":["mit","< 1K","csv","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"crypto-agent-safe-function-calling","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/SentientAGI/crypto-agent-safe-function-calling","creator_name":"Sentient Foundation","creator_url":"https://huggingface.co/SentientAGI","description":"\n\t\n\t\t\n\t\tCrAI-SafeFuncCall Dataset\n\t\n\nðŸ“„ Paper: Real AI Agents with Fake Memories: Fatal Context Manipulation\nAttacks on Web3 Agents\nðŸ¤— Dataset: CrAI-SafeFuncCall\nðŸ“Š Benchmark: CrAI-Bench\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe CrAI-SafeFuncCall dataset is designed to enhance the security of AI agents when performing function calls in the high-stakes domain of cryptocurrency and financial applications. It focuses on the critical challenge of detecting and mitigating memory injection attacks. Derived from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/SentientAGI/crypto-agent-safe-function-calling.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWE-smith","keyword":"agents","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SWE-bench/SWE-smith","creator_name":"SWE-bench","creator_url":"https://huggingface.co/SWE-bench","description":"\n  \n    \n    SWE-smith Dataset\n  \n\n\nCode\nâ€¢\nPaper\nâ€¢\nSite\n\n\nThe SWE-smith Dataset is a training dataset of 50137 task instances from 128 GitHub repositories, collected using the SWE-smith toolkit.\nIt is the largest dataset to date for training software engineering agents.\nAll SWE-smith task instances come with an executable environment.\nTo learn more about how to use this dataset to train Language Models for Software Engineering, please refer to the documentation.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SWE-smith-trajectories","keyword":"agents","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories","creator_name":"SWE-bench","creator_url":"https://huggingface.co/SWE-bench","description":"\n  \n    \n    SWE-smith Trajectories\n  \n\n\nCode\nâ€¢\nPaper\nâ€¢\nSite\n\n\nThis dataset contains the 5017 trajectories we fine-tuned Qwen 2.5 Coder Instruct on, leading to\nSWE-agent-LM-32B, a coding LM agent that\nachieve 40.2% on SWE-bench Verified (no verifiers or multiple rollouts, just 1 attempt per instance).\nTrajectories were generated by running SWE-agent + Claude 3.7 Sonnet on task instances from\nthe SWE-smith dataset.\n","first_N":5,"first_N_keywords":["text-generation","English","mit","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"function-calling-sharegpt","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hypervariance/function-calling-sharegpt","creator_name":"hypervariance","creator_url":"https://huggingface.co/hypervariance","description":"This is a dataset for finetuning models on function calling based on glaiveai/glaive-function-calling-v2.\nThe dataset includes 86,864 examples of chats that include function calling as part of the conversation. The system prompt includes either 0, 1, or 2 functions that the assistant can use, and instructions on how the agent can use it.\nChanges include:\n\nUsing ShareGPT format for chats\nAdding \"function_response\" as a role\nRemoving code examples\nRemoving examples with invalid JSON as functionâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/hypervariance/function-calling-sharegpt.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_complex_qa_plan_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_complex_qa_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_unified_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_unified_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_ground_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_ground_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_maths_plan_onetime","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_maths_plan_onetime.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"lumos_web_agent_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_web_agent_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"apigen-function-calling","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/apigen-function-calling","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n\t\n\t\t\n\t\tDataset card for argilla/apigen-function-calling\n\t\n\nThis dataset is a merge of argilla/Synth-APIGen-v0.1\nand Salesforce/xlam-function-calling-60k, making\nover 100K function calling examples following the APIGen recipe.\n\n\t\n\t\t\n\t\tPrepare for training\n\t\n\nThis version is not ready to do fine tuning, but you can run a script like prepare_for_sft.py\nto prepare it, and run the same recipe that can be found in\nargilla/Llama-3.2-1B-Instruct-APIGen-FC-v0.1#training-procedure.\nModify the promptâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/apigen-function-calling.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Agent-FLAN","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/internlm/Agent-FLAN","creator_name":"InternLM","creator_url":"https://huggingface.co/internlm","description":"\n\t\n\t\t\n\t\n\t\n\t\tAgent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\n\t\n\nThis page holds the dataset proposed in Agent-FLAN, which consists of AgentInstruct, Toolbench, and customized negative agent samples as its source datasets.\n\n\t\n\t\t\n\t\n\t\n\t\tâœ¨ Introduction\n\t\n\n[ðŸ¤— HuggingFace]\n[ðŸ“ƒ Paper]\n[ðŸŒ Project Page]\n\nOpen-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models whenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/internlm/Agent-FLAN.","first_N":5,"first_N_keywords":["apache-2.0","arxiv:2403.12881","ðŸ‡ºðŸ‡¸ Region: US","agent"],"keywords_longer_than_N":false},
	{"name":"mmau","keyword":"function-calling","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/apple/mmau","creator_name":"Apple","creator_url":"https://huggingface.co/apple","description":"\n\t\n\t\t\n\t\n\t\n\t\tMMAU Dataset: A Holistic Benchmark of Agent Capabilities Across Diverse Domains\n\t\n\n\n\n","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","1K<n<10K","arxiv:2407.18961"],"keywords_longer_than_N":true},
	{"name":"mmau","keyword":"agent","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/apple/mmau","creator_name":"Apple","creator_url":"https://huggingface.co/apple","description":"\n\t\n\t\t\n\t\n\t\n\t\tMMAU Dataset: A Holistic Benchmark of Agent Capabilities Across Diverse Domains\n\t\n\n\n\n","first_N":5,"first_N_keywords":["text-generation","English","cc-by-sa-4.0","1K<n<10K","arxiv:2407.18961"],"keywords_longer_than_N":true},
	{"name":"agent-leaderboard","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galileo-ai/agent-leaderboard","creator_name":"Galileo","creator_url":"https://huggingface.co/galileo-ai","description":"\n\t\n\t\t\n\t\tAgent Leaderboard\n\t\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe Agent Leaderboard evaluates language models' ability to effectively utilize tools in complex scenarios. With major tech CEOs predicting 2025 as a pivotal year for AI agents, we built this leaderboard to answer: \"How do AI agents perform in real-world business scenarios?\"\nGet latest update of the leaderboard on Hugging Face Spaces. For more info, checkout the blog post for a detailed overview of our evaluation methodology.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/galileo-ai/agent-leaderboard.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"agent-leaderboard","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/galileo-ai/agent-leaderboard","creator_name":"Galileo","creator_url":"https://huggingface.co/galileo-ai","description":"\n\t\n\t\t\n\t\tAgent Leaderboard\n\t\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nThe Agent Leaderboard evaluates language models' ability to effectively utilize tools in complex scenarios. With major tech CEOs predicting 2025 as a pivotal year for AI agents, we built this leaderboard to answer: \"How do AI agents perform in real-world business scenarios?\"\nGet latest update of the leaderboard on Hugging Face Spaces. For more info, checkout the blog post for a detailed overview of our evaluation methodology.â€¦ See the full description on the dataset page: https://huggingface.co/datasets/galileo-ai/agent-leaderboard.","first_N":5,"first_N_keywords":["English","apache-2.0","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"tw-function-call-reasoning-10k","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/twinkle-ai/tw-function-call-reasoning-10k","creator_name":"Twinkle AI","creator_url":"https://huggingface.co/twinkle-ai","description":"\n\t\n\t\t\n\t\tDataset Card for tw-function-call-reasoning-10k\n\t\n\n\n\n\næœ¬è³‡æ–™é›†ç‚ºç¹é«”ä¸­æ–‡ç‰ˆæœ¬çš„å‡½å¼å‘¼å«ï¼ˆFunction Callingï¼‰è³‡æ–™é›†ï¼Œç¿»è­¯è‡ª AymanTarig/function-calling-v0.2-with-r1-cotï¼Œè€Œè©²è³‡æ–™é›†æœ¬èº«æ˜¯ Salesforce/xlam-function-calling-60k çš„ä¿®æ­£ç‰ˆã€‚æˆ‘å€‘åˆ©ç”¨èªžè¨€æ¨¡åž‹ç¿»è­¯å¾Œï¼Œç¶“äººå·¥ä¿®æ”¹ï¼Œæ—¨åœ¨æ‰“é€ é«˜å“è³ªçš„ç¹é«”ä¸­æ–‡å·¥å…·ä½¿ç”¨èªžæ–™ã€‚\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\ntw-function-call-reasoning-10k æ˜¯ä¸€å€‹å°ˆç‚ºèªžè¨€æ¨¡åž‹ã€Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ˆFunction Callingï¼‰ã€è¨“ç·´æ‰€è¨­è¨ˆçš„ç¹é«”ä¸­æ–‡è³‡æ–™é›†ã€‚å…¶å…§å®¹æºè‡ª AymanTarig/function-calling-v0.2-with-r1-cotï¼Œè©²è³‡æ–™é›†åˆç‚º Salesforce/xlam-function-calling-60kâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/twinkle-ai/tw-function-call-reasoning-10k.","first_N":5,"first_N_keywords":["text-generation","Chinese","English","cc-by-4.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"fiftyone-function-calling-14k","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Voxel51/fiftyone-function-calling-14k","creator_name":"Voxel51","creator_url":"https://huggingface.co/Voxel51","description":"\n\t\n\t\t\n\t\tFiftyOne Function Calling 14k Dataset\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset is derived from the FiftyOne documentation and is designed to train AI assistants to understand and answer questions about FiftyOne's functionality. The dataset follows the format specified in the APIGen paper, structuring the data to map natural language queries to appropriate API tools and their usage.\n\n\t\n\t\t\n\t\tPurpose\n\t\n\n\nTrain AI models to understand FiftyOne-related queries\nProvide structured examples ofâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Voxel51/fiftyone-function-calling-14k.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","question-answering","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"MAD","keyword":"agents","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mcemri/MAD","creator_name":"Mert Cemri","creator_url":"https://huggingface.co/mcemri","description":"MAD: Multi-Agent System Traces Dataset\nA dataset of Multi-Agent System (MAS) execution traces annotated with the Multi-Agent Systems Failure Taxonomy (MAST). Each record provides details about the MAS, the Language Model (LLM) used, the benchmark task, a link to the raw trace file, and structured MAST failure annotations.\nCheckout https://github.com/multi-agent-systems-failure-taxonomy/MAST for the code!\n","first_N":5,"first_N_keywords":["cc-by-4.0","ðŸ‡ºðŸ‡¸ Region: US","code","agents","multi-agent-systems"],"keywords_longer_than_N":true},
	{"name":"Synth-APIGen-v0.1","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\n  \n    \n  \n\n\n\n\t\n\t\t\n\t\tDataset card for Synth-APIGen-v0.1\n\t\n\nThis dataset has been created with distilabel.\nPipeline script: pipeline_apigen_train.py.\n\n\t\n\t\t\n\t\tDataset creation\n\t\n\nIt has been created with distilabel==1.4.0 version.\nThis dataset is an implementation of APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets in distilabel,\ngenerated from synthetic functions. The process can be summarized as follows:\n\nGenerate (or in this case modify) pythonâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"web_agents_google_flight_trajectories","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/anonx3247/web_agents_google_flight_trajectories","creator_name":"Anas lecaillon","creator_url":"https://huggingface.co/anonx3247","description":"\n\t\n\t\t\n\t\tWeb Agent Google Flight Trajectories\n\t\n\nThis dataset was originally created on Nov 23 2024 during EF's Ai On Edge Hackathon.\nThe purpose of this dataset is to give both positive and negative image web-agent trajectories to finetune small edge-models on web agentic tasks.\n","first_N":5,"first_N_keywords":["English","mit","1K - 10K","imagefolder","Image"],"keywords_longer_than_N":true},
	{"name":"WorFBench_test","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/WorFBench_test","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":" WorFBench \n Benchmarking Agentic Workflow Generation \n\n\n  ðŸ“„arXiv â€¢\n  ðŸ¤—HFPaper â€¢\n  ðŸŒWeb â€¢\n  ðŸ–¥ï¸Code â€¢\n  ðŸ“ŠDataset\n\n\n\nðŸŒ»Acknowledgement\nðŸŒŸOverview\nðŸ”§Installation\nâœï¸Model-Inference\nðŸ“Workflow-Generation\nðŸ¤”Workflow-Evaluation\n\n\n\t\n\t\t\n\t\tðŸŒ»Acknowledgement\n\t\n\nOur code of training module is referenced and adapted from LLaMA-Factory. And the Dataset is collected from ToolBench, ToolAlpaca, Lumos, WikiHow, Seal-Tools, Alfworld, Webshop, IntercodeSql. Our end-to-end evaluation module is based on IPRâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/WorFBench_test.","first_N":5,"first_N_keywords":["text-generation","apache-2.0","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Celestia2","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\nThis dataset focuses on challenging multi-turn conversations and contains:\n\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\n\nThis datasetâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2.","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","csv","Text"],"keywords_longer_than_N":true},
	{"name":"nestful","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ibm-research/nestful","creator_name":"IBM Research","creator_url":"https://huggingface.co/ibm-research","description":"\n\t\n\t\t\n\t\tNESTFUL: Nested Function-Calling Dataset\n\t\n\n\n\n\n\n\nNESTFUL is a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to\na subsequent call.\nThe NESTFUL dataset includes over 1800 nested sequences from two main areas: mathematical reasoning and coding tools. The mathematical reasoning portion is generated from \nthe MathQA dataset, while the coding portion is generated from the\nStarCoder2-Instruct dataset.\nAllâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/nestful.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"Seal-Tools","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/casey-martin/Seal-Tools","creator_name":"Casey","creator_url":"https://huggingface.co/casey-martin","description":"\n\t\n\t\t\n\t\tSeal-Tools\n\t\n\n\n\nThis Huggingface repository contains the dataset generated in Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark.\n\n\t\n\t\t\n\t\tAbstract\n\t\n\nSeal-Tools contains self-instruct API-like tools. Seal-Tools not only offers a large\nnumber of tools, but also includes instances\nwhich demonstrate the practical application\nof tools. Seeking to generate data on a large\nscale while ensuring reliability, we propose a\nself-instruct method to generateâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/casey-martin/Seal-Tools.","first_N":5,"first_N_keywords":["text-generation","mit","10K - 100K","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"Agentic-DPO-V0.1","keyword":"agents","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1","creator_name":"Capx AI","creator_url":"https://huggingface.co/Capx","description":"\n\t\n\t\t\n\t\tAgentic DPO V1.0\n\t\n\n\n\nThe Capx Agentic DPO (Direct Prompt Optimization) Dataset is a unique collection of prompts, chosen answers, and rejected answers designed to train and optimize AI models for agentic and intuitive processing. \n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThe dataset covers a wide range of topics, including but not limited to problem-solving, creativity, analysis, and general knowledge. The prompts are specifically crafted to elicit agentic responses from the AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Capx/Agentic-DPO-V0.1.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"crosswoz-sft","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/BruceNju/crosswoz-sft","creator_name":"zhongyah","creator_url":"https://huggingface.co/BruceNju","description":"multilinguality:  \n- monolingual  \n\ndescription: |  \n                          \n    è¿™æ˜¯ä¸€ä¸ªåŸºäºŽCrossWOZæ•°æ®é›†å¤„ç†çš„å¯¹è¯æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºŽå¤§æ¨¡åž‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»»åŠ¡ã€‚  \n    æ•°æ®é›†åŒ…å«å¤šè½®å¯¹è¯ã€ç”¨æˆ·ç›®æ ‡ã€å¯¹è¯çŠ¶æ€ç­‰ä¿¡æ¯ï¼Œé€‚åˆè®­ç»ƒä»»åŠ¡åž‹å¯¹è¯ç³»ç»Ÿã€‚  \n\n    åŽŸå§‹æ•°æ®æ¥æºäºŽCrossWOZé¡¹ç›®ï¼Œç»è¿‡ä¸“é—¨çš„é¢„å¤„ç†ä½¿å…¶æ›´é€‚åˆçŽ°ä»£å¤§æ¨¡åž‹è®­ç»ƒã€‚\n\n\n\t\n\t\t\n\t\n\t\n\t\tæ ¸å¿ƒç‰¹å¾ï¼š\n\t\n\nè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡è·¨åŸŸä»»åŠ¡åž‹å¯¹è¯æ•°æ®é›†\nåŒ…å«6,012ä¸ªå¯¹è¯ï¼Œ102,000ä¸ªè¯è¯­ï¼Œè¦†ç›–5ä¸ªé¢†åŸŸ(é…’åº—ã€é¤åŽ…ã€æ™¯ç‚¹ã€åœ°é“å’Œå‡ºç§Ÿè½¦)\nçº¦60%çš„å¯¹è¯åŒ…å«è·¨åŸŸç”¨æˆ·ç›®æ ‡\n\n\n\t\n\t\t\n\t\n\t\n\t\tä¸»è¦åˆ›æ–°ç‚¹ï¼š\n\t\n\næ›´å…·æŒ‘æˆ˜æ€§çš„åŸŸé—´ä¾èµ–å…³ç³»ï¼š\n\nä¸€ä¸ªé¢†åŸŸçš„é€‰æ‹©ä¼šåŠ¨æ€å½±å“å…¶ä»–ç›¸å…³é¢†åŸŸçš„é€‰æ‹©\nä¾‹å¦‚ç”¨æˆ·é€‰æ‹©çš„æ™¯ç‚¹ä¼šå½±å“åŽç»­é…’åº—çš„æŽ¨èèŒƒå›´(éœ€è¦åœ¨æ™¯ç‚¹é™„è¿‘)\n\nå®Œæ•´çš„æ ‡æ³¨ï¼š\n\nåŒæ—¶æä¾›ç”¨æˆ·ç«¯å’Œç³»ç»Ÿç«¯çš„å¯¹è¯çŠ¶æ€æ ‡æ³¨\nåŒ…å«å¯¹è¯è¡Œä¸º(dialogue acts)çš„æ ‡æ³¨\nç”¨æˆ·çŠ¶æ€æ ‡æ³¨æœ‰åŠ©äºŽè¿½è¸ªå¯¹è¯æµç¨‹å’Œå»ºæ¨¡ç”¨æˆ·è¡Œä¸ºâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BruceNju/crosswoz-sft.","first_N":5,"first_N_keywords":["question-answering","Chinese","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"chatml-function-calling-v2","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ankush13r/chatml-function-calling-v2","creator_name":"Ankush Rana","creator_url":"https://huggingface.co/ankush13r","description":"\n\t\n\t\t\n\t\tDataset Conversion\n\t\n\nThis dataset is a converted version of the Glaive Function Calling v2 dataset, originally hosted on Hugging Face.\n\n\t\n\t\t\n\t\tChat Template for Dataset\n\t\n\n\n\t\n\t\t\n\t\tDescription\n\t\n\nThis chat template is designed to work with this dataset.\n\n\t\n\t\t\n\t\tTemplate\n\t\n\n\nchat_template = \"\"\"{%- set tools = tools if tools is defined else None -%}\n{%- set date_string = date_string if date_string is defined else \"1 Sep 2024\" -%}\n\n{%- set system_message = messages[0].content ifâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ankush13r/chatml-function-calling-v2.","first_N":5,"first_N_keywords":["text-generation","question-answering","feature-extraction","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling-llama3-chat","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling-llama3-chat","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\n\t\n\t\t\n\t\tFunction Calling dataset for Assist LLM for Home Assistant\n\t\n\nThis dataset is generated by using other conversation agent pipelines as teachers\nfrom the deivce-actions-v2 dataset.\nThis dataset is used to support fine tuning of llama based models.\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling-messages","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling-messages","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\n\t\n\t\t\n\t\tFunction Calling dataset for Assist LLM for Home Assistant\n\t\n\nThis dataset is generated by using other conversation agent pipelines as teachers\nfrom the deivce-actions-v2 dataset.\nThis dataset is used to support fine tuning of llama based models.\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWE-agent-trajectories","keyword":"agents","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nebius/SWE-agent-trajectories","creator_name":"Nebius","creator_url":"https://huggingface.co/nebius","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset contains 80,036 trajectories generated by a software engineering agent based on the SWE-agent framework, using various models as action generators. In these trajectories, the agent attempts to solve GitHub issues from the nebius/SWE-bench-extra and the dev split of princeton-nlp/SWE-bench.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nThis dataset was created as part of a research project focused on developing a software engineering agent using open-weight modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nebius/SWE-agent-trajectories.","first_N":5,"first_N_keywords":["cc-by-4.0","10K - 100K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"SWE-bench-extra","keyword":"agents","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nebius/SWE-bench-extra","creator_name":"Nebius","creator_url":"https://huggingface.co/nebius","description":"Note: This dataset has an improved and significantly larger successor: SWE-rebench.\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSWE-bench Extra is a dataset that can be used to train or evaluate agentic systems specializing in resolving GitHub issues. It is based on the methodology used to build SWE-bench benchmark and includes 6,415 Issue-Pull Request pairs sourced from 1,988 Python repositories.\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe SWE-bench Extra dataset supports the development of software engineering agentsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nebius/SWE-bench-extra.","first_N":5,"first_N_keywords":["cc-by-4.0","1K - 10K","parquet","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"DroidCall","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mllmTeam/DroidCall","creator_name":"mllm","creator_url":"https://huggingface.co/mllmTeam","description":"\n\t\n\t\t\n\t\tDroidCall: A Dataset for LLM-powered Android Intent Invocation\n\t\n\npaper|github\nDroidCall is the first open-sourced, high-quality dataset designed for fine-tuning LLMs for accurate intent invocation on Android devices.\nThis repo contains data generated by DroidCall. The process of data generation is shown in the figure below\n\nDetails can be found in our paper and github repository.\n\n\t\n\t\t\n\t\tWhat is Android Intent Invocation?\n\t\n\nAndroid Intent is a key machanism in Android that allowsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mllmTeam/DroidCall.","first_N":5,"first_N_keywords":["text-generation","question-answering","task-planning","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"assist-llm-function-calling","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/allenporter/assist-llm-function-calling","creator_name":"Allen Porter","creator_url":"https://huggingface.co/allenporter","description":"\n\t\n\t\t\n\t\tFunction Calling dataset for Assist LLM for Home Assistant\n\t\n\nThis dataset is generated by using other conversation agent pipelines as teachers\nfrom the deivce-actions-v2 dataset.\nThis dataset is used to support fine tuning of llama based models.\nSee Device Actions for a notebook for construction of this dataset and the device-actions dataset.\n","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"WorFBench_train","keyword":"agents","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/WorFBench_train","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"This repository contains the data presented in Benchmarking Agentic Workflow Generation.\nCode: https://github.com/zjunlp/WorfBench\n","first_N":5,"first_N_keywords":["apache-2.0","10K - 100K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"apigen-smollm-trl-FC","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-smollm-trl-FC","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset card for argilla-warehouse/apigen-smollm-trl-FC\n\t\n\nThis dataset is a merge of argilla/Synth-APIGen-v0.1\nand Salesforce/xlam-function-calling-60k, and was prepared for training using the script\nprepare_for_sft.py that can be found in the repository files.\n\n\t\n\t\t\n\t\n\t\n\t\tReferences\n\t\n\n@article{liu2024apigen,\n  title={APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets},\n  author={Liu, Zuxin and Hoang, Thai and Zhang, Jianguo and Zhu, Mingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-smollm-trl-FC.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"Solana","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ordlibrary/Solana","creator_name":"8 bit","creator_url":"https://huggingface.co/ordlibrary","description":"ordlibrary/Solana dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["mit","ðŸ‡ºðŸ‡¸ Region: US","solana","crypto","blockchain"],"keywords_longer_than_N":true},
	{"name":"apigen-synth-trl","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl","creator_name":"Argilla Warehouse","creator_url":"https://huggingface.co/argilla-warehouse","description":"\n\t\n\t\t\n\t\tDataset card\n\t\n\nThis dataset is a version of argilla/Synth-APIGen-v0.1 prepared for\nfine-tuning using trl. To generate it, the following script was run:\nfrom datasets import load_dataset\nfrom jinja2 import Template\n\nSYSTEM_PROMPT = \"\"\"\nYou are an expert in composing functions. You are given a question and a set of possible functions. \nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \nIf none of the functions can be used, point it outâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/argilla-warehouse/apigen-synth-trl.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"glaive-function-calling-v2-pl","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mpieck/glaive-function-calling-v2-pl","creator_name":"Maciej Piecko","creator_url":"https://huggingface.co/mpieck","description":"\n\t\n\t\t\n\t\tDataset Card for glaive-function-calling-v2-pl Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis is a fragment of glaiveai/glaive-function-calling-v2 dataset translated to polish. \nIt contains first 4.3k (out of 5k total, this is work in progress) instructions of the original dataset. Only instructions having function definitions or function calls are included, instructions without functions (ordinary unstructured) from the original dataset are skipped.\n Some repeating instructions wereâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mpieck/glaive-function-calling-v2-pl.","first_N":5,"first_N_keywords":["text-generation","question-answering","Polish","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"func_calls","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/retrain-pipelines/func_calls","creator_name":"retrain-pipelines","creator_url":"https://huggingface.co/retrain-pipelines","description":"\n  \n    retrain-pipelines Function Calling\n\n  version 0.10  -  2025-04-17 16:55:04 UTC\n\n\nSource datasets :\n\nmainÂ :\nXlam Function Calling 60kÂ Â \nSalesforce/xlam-function-calling-60k\n(26d14eb -\n  2025-01-24 19:25:58 UTC)\n\nlicenseÂ :\ncc-by-4.0\narxivÂ :\n- 2406.18518\n\n\ndata-enrichmentÂ :\nNatural Questions CleanÂ Â \nlighteval/natural_questions_clean\n(a72f7fa -\n  2023-10-17 20:29:08 UTC)\n\nlicenseÂ :\nunknown\n\n\n\n\nThe herein dataset has 2 configs : continued_pre_training and supervised_finetuning.\nThe formerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/retrain-pipelines/func_calls.","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","Salesforce/xlam-function-calling-60k","lighteval/natural_questions_clean"],"keywords_longer_than_N":true},
	{"name":"ai-search-agent","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepNLP/ai-search-agent","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","description":"\n\t\n\t\t\n\t\tAI Search Agent Agent Meta and Traffic Dataset in AI Agent Marketplace | AI Agent Directory | AI Agent Index from DeepNLP\n\t\n\nThis dataset is collected from AI Agent Marketplace Index and Directory at http://www.deepnlp.org, which contains AI Agents's meta information such as agent's name, website, description, as well as the monthly updated Web performance metrics, including Google,Bing average search ranking positions, Github Stars, Arxiv References, etc.\nThe dataset is helpful for AIâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/ai-search-agent.","first_N":5,"first_N_keywords":["text-generation","mit","< 1K","json","Text"],"keywords_longer_than_N":true},
	{"name":"Taskbench","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/microsoft/Taskbench","creator_name":"Microsoft","creator_url":"https://huggingface.co/microsoft","description":"\n\n\n\n\n\n  \nTaskBench: Benchmarking Large Language Models for Task Automation\n\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nTaskBench is a benchmark for evaluating large language models (LLMs) on task automation. Task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To address this challenge, we propose a comprehensive evaluation frameworkâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/microsoft/Taskbench.","first_N":5,"first_N_keywords":["English","mit","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"hibo-function-calling-v1","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1","creator_name":"Thibaud Perrin","creator_url":"https://huggingface.co/thibaud-perrin","description":"\n\t\n\t\t\n\t\thibo-function-calling-v1\n\t\n\n\n    \n\n\n\n\n\t\n\t\t\n\t\tðŸ“– Dataset Description\n\t\n\nThis dataset, named \"hibo-function-calling-v1\", is designed to facilitate the fine-tuning of Large Language Models (LLMs) for function calling tasks. It comprises a single 'train' split containing 323,271 data points across three columns: 'dataset_origin', 'system', and 'chat'. \nThe dataset is a result of merging two distinct sources: gathnex/Gath_baize and glaiveai/glaive-function-calling-v2, with an aim to provideâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/thibaud-perrin/hibo-function-calling-v1.","first_N":5,"first_N_keywords":["text-generation","English","mit","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"pandora-tool-calling","keyword":"function-calling","license":"BSD 3-Clause \"New\" or \"Revised\" License","license_url":"https://choosealicense.com/licenses/bsd-3-clause/","language":"en","dataset_url":"https://huggingface.co/datasets/danilopeixoto/pandora-tool-calling","creator_name":"Danilo Peixoto","creator_url":"https://huggingface.co/danilopeixoto","description":"\n\t\n\t\t\n\t\tPandora Tool Calling\n\t\n\nA tool-calling dataset for Supervised fine-tuning of the Pandora Large Language Model (LLM).\nThe dataset is based on the glaiveai/glaive-function-calling-v2 dataset.\n\n\t\n\t\t\n\t\tCopyright and license\n\t\n\nCopyright (c) 2024, Danilo Peixoto Ferreira. All rights reserved.\nProject developed under a BSD-3-Clause license.\n","first_N":5,"first_N_keywords":["text-generation","bsd-3-clause","100K - 1M","parquet","Text"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_ground_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_ground_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"lumos_multimodal_plan_iterative","keyword":"language-agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative","creator_name":"Lumos  Agents (AI2)","creator_url":"https://huggingface.co/ai2lumos","description":"\n\t\n\t\t\n\t\tðŸª„ Agent Lumos: Unified and Modular Training for Open-Source Language Agents\n\t\n\n\n  ðŸŒ[Website] Â \n  ðŸ“[Paper] Â \n  ðŸ¤—[Data] Â \n  ðŸ¤—[Model] Â \n  ðŸ¤—[Demo] Â \n\n\nWe introduce ðŸª„Lumos, Language Agents with Unified Formats, Modular Design, and Open-Source LLMs. Lumos unifies a suite of complex interactive tasks and achieves competitive performance with GPT-4/3.5-based and larger open-source agents. \nLumos has following features:\n\nðŸ§© Modular Architecture:\nðŸ§© Lumos consists of planning, groundingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ai2lumos/lumos_multimodal_plan_iterative.","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"glaive-v2-single-turn-func-call-chatml","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/recastai/glaive-v2-single-turn-func-call-chatml","creator_name":"Re:cast AI","creator_url":"https://huggingface.co/recastai","description":"\n\t\n\t\t\n\t\tDataset Card for \"glaive-v2-single-turn-func-call-chatml\"\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis dataset has been created by Re:cast AI to transform the existing dataset glaiveai/glaive-function-calling-v2 into a chatml friendly format for use in SFT tasks with pretrained models.\nThe original dataset was filtered and altered with the following:\n\nRemoved examples that do not produce a function completion response.\nEach example is a single-turn between user and assistant along with a customâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/recastai/glaive-v2-single-turn-func-call-chatml.","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Agent-FLAN","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/agicorp/Agent-FLAN","creator_name":"agicorp","creator_url":"https://huggingface.co/agicorp","description":"\n\t\n\t\t\n\t\n\t\n\t\tAgent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\n\t\n\nThis page holds the dataset proposed in Agent-FLAN, which consists of AgentInstruct, Toolbench, and customized negative agent samples as its source datasets.\n\n\t\n\t\t\n\t\n\t\n\t\tâœ¨ Introduction\n\t\n\n[ðŸ¤— HuggingFace]\n[ðŸ“ƒ Paper]\n[ðŸŒ Project Page]\n\nOpen-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models whenâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/agicorp/Agent-FLAN.","first_N":5,"first_N_keywords":["apache-2.0","arxiv:2403.12881","ðŸ‡ºðŸ‡¸ Region: US","agent"],"keywords_longer_than_N":false},
	{"name":"multi-agent-scam-conversation","keyword":"agent","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation","creator_name":"Pitipat Gumphusiri","creator_url":"https://huggingface.co/BothBosu","description":"\n\t\n\t\t\n\t\tSynthetic Multi-Turn Scam and Non-Scam Phone Conversation Dataset with Agentic Personalities\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe Synthetic Multi-Turn Scam and Non-Scam Phone Dialogue Dataset with Agentic Personalities is an enhanced collection of simulated phone conversations between two AI agents, one acting as a scammer or non-scammer and the other as an innocent receiver. Each dialogue is labeled as either a scam or non-scam interaction. This dataset is designed to help developâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/BothBosu/multi-agent-scam-conversation.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","csv"],"keywords_longer_than_N":true},
	{"name":"BioKGBench-Dataset","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/AutoLab-Westlake/BioKGBench-Dataset","creator_name":"AutoLab Westlake","creator_url":"https://huggingface.co/AutoLab-Westlake","description":"\n\t\n\t\t\n\t\tAgent4S-BioKG\n\t\n\nA Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science.\n\n\n    \n    \n\n     Github \n\n\n\n\t\n\t\t\n\t\tIntroduction\n\t\n\nPursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models(LLMs).However, to evaluate such systems, people either rely on direct Question-Answering(QA) to the LLM itself, or in a biomedical experimental manner. Howâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/AutoLab-Westlake/BioKGBench-Dataset.","first_N":5,"first_N_keywords":["question-answering","text-retrieval","other","fact-checking","closed-domain-qa"],"keywords_longer_than_N":true},
	{"name":"DeepShop","keyword":"agent","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepShop/DeepShop","creator_name":"DeepShop","creator_url":"https://huggingface.co/DeepShop","description":"DeepShop/DeepShop dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"crypto-agent-safe-function-calling","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/peiyao-sentient/crypto-agent-safe-function-calling","creator_name":"Peiyao Sheng","creator_url":"https://huggingface.co/peiyao-sentient","description":"\n\t\n\t\t\n\t\tCrAI-SafeFuncCall Dataset\n\t\n\nðŸ“„ Paper: Real AI Agents with Fake Memories: Fatal Context Manipulation\nAttacks on Web3 Agents\nðŸ¤— Dataset: CrAI-SafeFuncCall\nðŸ“Š Benchmark: CrAI-Bench\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThe CrAI-SafeFuncCall dataset is designed to enhance the security of AI agents when performing function calls in the high-stakes domain of cryptocurrency and financial applications. It focuses on the critical challenge of detecting and mitigating memory injection attacks. Derived from theâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/peiyao-sentient/crypto-agent-safe-function-calling.","first_N":5,"first_N_keywords":["question-answering","text-generation","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Open_CaptchaWorld","keyword":"agents","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/OpenCaptchaWorld/Open_CaptchaWorld","creator_name":"Open_CaptchaWorld","creator_url":"https://huggingface.co/OpenCaptchaWorld","description":"OpenCaptchaWorld/Open_CaptchaWorld dataset hosted on Hugging Face and contributed by the HF Datasets community","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","imagefolder"],"keywords_longer_than_N":true},
	{"name":"deepresearch_trace","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Looogic/deepresearch_trace","creator_name":"LogicðŸ¤—","creator_url":"https://huggingface.co/Looogic","description":"\n\t\n\t\t\n\t\tðŸ”¬ DeepResearch Tool Use Conversations\n\t\n\nA high-quality dataset of multi-turn conversations between humans and AI agents, featuring sophisticated tool use for research and report generation tasks.\n\n\t\n\t\t\n\t\tðŸŒŸ Key Features\n\t\n\n\nMulti-turn conversations with complex reasoning chains\nTool use integration including search, web scraping, and note-taking\nComprehensive metadata with execution metrics and performance tracking\nResearch-focused tasks requiring information synthesis and analysisâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Looogic/deepresearch_trace.","first_N":5,"first_N_keywords":["text-generation","text2text-generation","English","mit","n<1K"],"keywords_longer_than_N":true},
	{"name":"toolcall_bench","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Compumacy/toolcall_bench","creator_name":"Compumacy","creator_url":"https://huggingface.co/Compumacy","description":"\n\t\n\t\t\n\t\tWhen2Call\n\t\n\n\n ðŸ’¾ GithubÂ Â  | Â Â  ðŸ“„ Paper\n\n\n\n\t\n\t\t\n\t\tDataset Description:\n\t\n\nWhen2Call is a benchmark designed to evaluate tool-calling decision-making for large language models (LLMs), including when to generate a tool call, when to ask follow-up questions, when to admit the question can't be answered with the tools provided, and what to do if the question seems to require tool use but a tool call can't be made. \nWe find that state-of-the-art tool-calling LMs show significant room forâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/Compumacy/toolcall_bench.","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"fc-reward-bench","keyword":"function-calling","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ibm-research/fc-reward-bench","creator_name":"IBM Research","creator_url":"https://huggingface.co/ibm-research","description":"\n\t\n\t\t\n\t\tfc-reward-bench\n\t\n\nfc-reward-bench is a benchmark designed to evaluate reward model performance in function-calling tasks. It features 1,500 unique user inputs derived from the single-turn splits of the BFCL-v3 dataset. Each input is paired with both correct and incorrect function calls. Correct calls are sourced directly from BFCL, while incorrect calls are generated by 25 permissively licensed models.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Structure\n\t\n\nEach entry in the dataset includes the followingâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/ibm-research/fc-reward-bench.","first_N":5,"first_N_keywords":["text-classification","English","apache-2.0","1K - 10K","arrow"],"keywords_longer_than_N":true},
	{"name":"func_calls_ds","keyword":"function-calling","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/retrain-pipelines/func_calls_ds","creator_name":"retrain-pipelines","creator_url":"https://huggingface.co/retrain-pipelines","description":"\n  \n    retrain-pipelines Function Calling\n\n  version 0.29  -  2025-04-17 18:12:01 UTC\n\n\nSource datasets :\n\nmainÂ :\nXlam Function Calling 60kÂ Â \nSalesforce/xlam-function-calling-60k\n(26d14eb -\n  2025-01-24 19:25:58 UTC)\n\nlicenseÂ :\ncc-by-4.0\narxivÂ :\n- 2406.18518\n\n\ndata-enrichmentÂ :\nNatural Questions CleanÂ Â \nlighteval/natural_questions_clean\n(a72f7fa -\n  2023-10-17 20:29:08 UTC)\n\nlicenseÂ :\nunknown\n\n\n\n\nThe herein dataset has 2 configs : continued_pre_training and supervised_finetuning.\nThe formerâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/retrain-pipelines/func_calls_ds.","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","Salesforce/xlam-function-calling-60k","lighteval/natural_questions_clean"],"keywords_longer_than_N":true},
	{"name":"Coding-Agent-Github-2025-Feb","keyword":"agent","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/DeepNLP/Coding-Agent-Github-2025-Feb","creator_name":"DeepNLP","creator_url":"https://huggingface.co/DeepNLP","description":"\n\t\n\t\t\n\t\tCoding Agent AI Agent Directory to Host All Coding Agent related AI Agents Web Traffic Data, Search Ranking, Community, Reviews and More.\n\t\n\nThis is the Coding Agent Dataset from pypi package \"coding_agent\" https://pypi.org/project/coding_agent. You can use this package to download and get statistics (forks/stars/website traffic) of AI agents on website from AI Agent Marketplace AI Agent Directory  (http://www.deepnlp.org/store/ai-agent) and AI Agent Search Portalâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/DeepNLP/Coding-Agent-Github-2025-Feb.","first_N":5,"first_N_keywords":["mit","< 1K","json","Text","Datasets"],"keywords_longer_than_N":true},
	{"name":"turkish-function-calling-20k","keyword":"function-calling","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/atasoglu/turkish-function-calling-20k","creator_name":"Ahmet","creator_url":"https://huggingface.co/atasoglu","description":"Used argilla-warehouse/python-seed-tools to sample tools.\n\n\t\n\t\t\n\t\tPreprocessing\n\t\n\nSince some answers might not contain a valid JSON schema, ensure that you preprocess and validate the answer to check if it satisfies the query using the given tools. You can use the preprocessing code below:\nimport json\nfrom datasets import Dataset, load_dataset\n\n\ndef validate_answers(sample):\n    if sample[\"answers\"] is None:\n        return True\n    try:\n        tools = json.loads(sample[\"tools\"])â€¦ See the full description on the dataset page: https://huggingface.co/datasets/atasoglu/turkish-function-calling-20k.","first_N":5,"first_N_keywords":["text-generation","Turkish","mit","10K - 100K","parquet"],"keywords_longer_than_N":true}
]
;
