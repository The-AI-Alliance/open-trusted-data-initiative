var data_for_nlp = [

  {"name":"smnli_mt","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/darmanin-matt/smnli_mt","creator_name":"Matthew Darmanin","creator_url":"https://huggingface.co/darmanin-matt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for the SMNLI-MT\\n\\t\\n\\nThe SMNLI-MT datasets are machine-translated versions of the Stanford NLI and MultiNLI datasets in Maltese.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe datasets were translated using the Google Cloud Translate as part of the initial exploration of NLI in the Maltese language.\\n\\nCurated by: Matthew Darmanin\\nLanguage(s) (NLP): Maltese\\nLicense: CC 4.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe datasets are in the form of CSV files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/darmanin-matt/smnli_mt."},
  {"name":"alpaca-sinhala","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/sahanruwantha/alpaca-sinhala","creator_name":"sahan ruwantha","creator_url":"https://huggingface.co/sahanruwantha","description":"The Alpaca dataset translated into Sinhala using Google Translator. Manual verification and correction of translations are recommended for optimal performance.\\n"},
  {"name":"AnimeQuotes","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes","creator_name":"Mohammed Khalil","creator_url":"https://huggingface.co/mohamed-khalil","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnime Quotes Dataset ‚Äï „Ç¢„Éã„É°„ÅÆÂêçË®Ä„Éá„Éº„Çø„Çª„ÉÉ„Éàüéê\\n\\t\\n\\n\\nWelcome to Anime Quotes Dataset\\n\\n\\n    \\n        \\n        \\n        \\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains a curated collection of inspiring and memorable quotes from various anime series, sourced from the Anime Motivation website. The quotes are stored as a list of dictionaries and can be easily accessed for analysis, research, or personal enjoyment.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Format\\n\\t\\n\\nEach entry in the dataset is represented by a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes."},
  {"name":"SomGPT","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Abdirahman555/SomGPT","creator_name":"Abdirahman","creator_url":"https://huggingface.co/Abdirahman555","description":"Abdirahman555/SomGPT dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"BatchPrompting","keyword":"natural-language-processing","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/achandlr/BatchPrompting","creator_name":"Alex Chandler","creator_url":"https://huggingface.co/achandlr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBatch Prompting Dataset for Fine-Tuning Large Language Models\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Batch Prompting Dataset is a comprehensive collection of text-based question-answer pairs designed to fine-tune and evaluate the performance of large language models (LLMs) across a diverse range of tasks. This dataset aims to facilitate research and development in the field of natural language processing (NLP) by providing a standardized benchmark for assessing the capabilities of LLMs in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/achandlr/BatchPrompting."},
  {"name":"Ergonomics_Chiar_Customer_Viewdata_E-commerse","keyword":"nlp","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse","creator_name":"lia","creator_url":"https://huggingface.co/liaHa","description":"liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"nu-snli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/cl-nagoya/nu-snli","creator_name":"CL Research Group in Nagoya, Japan","creator_url":"https://huggingface.co/cl-nagoya","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTranslation Code\\n\\t\\n\\nWe used vLLM for a faster, batched generation.\\nimport datasets as ds\\nfrom vllm import LLM, SamplingParams, RequestOutput\\nfrom transformers import AutoTokenizer\\n\\n\\nmodel_path = \\\"hoge/fuga\\\"\\n\\ndataset: ds.Dataset = ds.load_dataset(\\\"snli\\\", split=\\\"train\\\")\\n\\nllm = LLM(\\n    model=model_path,\\n    quantization=None,\\n    dtype=\\\"bfloat16\\\",\\n    tensor_parallel_size=4,\\n    enforce_eager=True,\\n)\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\n\\n# temperature must be 0 when‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cl-nagoya/nu-snli."},
  {"name":"afrimgsm","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/masakhane/afrimgsm","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrimgsm\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \\nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 18 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimgsm."},
  {"name":"afrixnli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/masakhane/afrixnli","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrixnli\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIXNLI is an evaluation dataset comprising translations of a subset of the XNLI dataset into 16 African languages. \\nIt includes both validation and test sets across all 18 languages, maintaining the English and French subsets from the original XNLI dataset. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 18 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrixnli."},
  {"name":"LingComp_QA","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/somosnlp/LingComp_QA","creator_name":"SomosNLP","creator_url":"https://huggingface.co/somosnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LingComp_QA, un corpus educativo de ling√º√≠stica computacional en espa√±ol\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\nCurated by: Jorge Zamora Rey, Isabel Moyano Moreno, Mario Crespo Miguel \\nFunded by: SomosNLP, HuggingFace, Argilla, Instituto de Ling√º√≠stica Aplicada de la Universidad de C√°diz \\nLanguage(s) (NLP): es-ES \\nLicense: apache-2.0 \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://github.com/reddrex/lingcomp_QA/tree/main‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp/LingComp_QA."},
  {"name":"TrGLUE","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE","creator_name":"Turkish NLP Suite","creator_url":"https://huggingface.co/turkish-nlp-suite","description":"\\n\\t\\n\\t\\t\\n\\t\\tTrGLUE - A Natural Language Understanding Benchmark for Turkish\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for TrGLUE\\n\\t\\n\\nTrGLUE is a natural language understanding benchmarking dataset including several single sentence and sentence pair classification tasks.\\nThe inspiration is clearly the original GLUE benchmark.\\n\\n\\t\\n\\t\\t\\n\\t\\tTasks\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tSingle Sentence Tasks\\n\\t\\n\\nTrCOLA The original Corpus of Linguistic Acceptability consists of sentences compiled from English literature textbooks. The task is to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE."},
  {"name":"sql-create-context-thai","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from sql-create-context.\\n@misc{b-mc2_2023_sql-create-context,\\n  title   = {sql-create-context Dataset},\\n  author  = {b-mc2}, \\n  year    = {2023},\\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\\n  note    = {This dataset was created by modifying data from the following sources: \\\\cite{zhongSeq2SQL2017, yu2018spider}.},\\n}\\n\\n"},
  {"name":"code_leak_qa","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"XCOPA-eu","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/HiTZ/XCOPA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for XCOPA-eu\\n\\t\\n\\n\\nPoint of Contact: hitz@ehu.eus\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nXCOPA-eu is the professional translation to Basque of the English COPA dataset (Roemmmele et al., 2011),\\nin the spirit of the XCOPA effort (Ponti et al., 2020). \\nCOPA is a dataset of causal commmonsense reasoning that focuses on cause-effect relationships between a\\npremise and two choices.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\neu-ES\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/XCOPA-eu."},
  {"name":"wnli-eu","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/HiTZ/wnli-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for WNLI-eu\\n\\t\\n\\n\\nPoint of Contact: hitz@ehu.eus\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWNLI-eu is the professional translation to Basque of the WNLI dataset.\\nWNLI is part of the GLUE benchmark for English (Wang et al., 2018) \\nand is based on the Winograd Schema Challenge (WSC) dataset (Levesque et al., 2011):\\n\\nA Winograd schema is a pair of sentences differing in only one or two words and containing an ambiguity that is resolved in opposite ways in the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/wnli-eu."},
  {"name":"copyright_unlearning","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/boyiwei/copyright_unlearning","creator_name":"Boyi Wei","creator_url":"https://huggingface.co/boyiwei","description":"boyiwei/copyright_unlearning dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Vehicle_Complaints_NHSTA","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ItsMayur/Vehicle_Complaints_NHSTA","creator_name":"Mayur Arvindh","creator_url":"https://huggingface.co/ItsMayur","description":"ItsMayur/Vehicle_Complaints_NHSTA dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"cli-commands-explained","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/b-mc2/cli-commands-explained","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a collection of 16,098 command line instructions sourced from Commandlinefu and Cheatsheets. It includes an array of commands, each with an id, title, description, date, url to source, author, votes, and flag indicating if the description is AI generated. The descriptions are primarily authored by the original contributors, for entries where descriptions were absent, they have been generated using NeuralBeagle14-7B. Out of the total entries, 10,039‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/cli-commands-explained."},
  {"name":"afrixnli-translate-test","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/masakhane/afrixnli-translate-test","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrixnli-translate-test\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIXNLI-TT is an evaluation dataset comprising translations of the AFRIXNLI dataset from 16 African languages and 1 high resource language into English using NLLB. \\nIt includes test sets across all 17 languages.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 17 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import load_dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrixnli-translate-test."},
  {"name":"afrimgsm-translate-test","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/masakhane/afrimgsm-translate-test","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrimgsm-translate-test\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIMGSM-TT is an evaluation dataset comprising translations of the GSM8k dataset from 16 African languages and 1 high resource language into English using NLLB. \\nIt includes test sets across all 17 languages. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 17 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import load_dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimgsm-translate-test."},
  {"name":"PathPal","keyword":"natural-language-processing","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/alternativerealitystudio/PathPal","creator_name":"Alternative Reality Studio","creator_url":"https://huggingface.co/alternativerealitystudio","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Name: Descriptive and Categorized Travel Snippets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a synthetic collection created from 160 different travel-related categories, providing detailed descriptions of accessible adventures and activities. It aims to inspire and inform individuals about opportunities accommodating diverse needs, each entry pairing a detailed description with a category label reflecting the nature of the activity.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alternativerealitystudio/PathPal."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-825318","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-825318 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-825318 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-46082","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-46082 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-46082 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-497939","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-497939 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-497939 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-235808","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-235808 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-235808 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-774308","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-774308 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-774308 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-898550","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-898550 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-898550 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550."},
  {"name":"FiQA2018-256-24-gpt-4o-2024-05-13-780826","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-780826 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-780826 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826."},
  {"name":"HotpotQA-256-24-gpt-4o-2024-05-13-773587","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHotpotQA-256-24-gpt-4o-2024-05-13-773587 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"code repository search for machine learning datasets and models\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the HotpotQA-256-24-gpt-4o-2024-05-13-773587 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587."},
  {"name":"MSMARCO-256-24-gpt-4o-2024-05-13-374380","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSMARCO-256-24-gpt-4o-2024-05-13-374380 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"research dataset search for AI and NLP tasks\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-374380 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380."},
  {"name":"MSMARCO-256-24-gpt-4o-2024-05-13-466074","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSMARCO-256-24-gpt-4o-2024-05-13-466074 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"research dataset search for AI and NLP tasks\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-466074 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074."},
  {"name":"BAAI_bge-small-en-v1_5-2852024-6p16-webapp","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-small-en-v1_5-2852024-6p16-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"natural language processing\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-small-en-v1_5-2852024-6p16-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp."},
  {"name":"RWKU","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/jinzhuoran/RWKU","creator_name":"Zhuoran Jin","creator_url":"https://huggingface.co/jinzhuoran","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Real-World Knowledge Unlearning Benchmark (RWKU)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nRWKU is a real-world knowledge unlearning benchmark specifically designed for large language models (LLMs).\\nThis benchmark contains 200 real-world unlearning targets and 13,131 multi-level forget probes, including 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial-attack probes.\\nRWKU is designed based on the following three key factors: \\n\\nFor the task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinzhuoran/RWKU."},
  {"name":"unlearning","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning."},
  {"name":"synthetic_multilingual_llm_prompts","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","description":"\\n  \\n  Image generated by DALL-E. See prompt for more details\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìùüåê Synthetic Multilingual LLM Prompts\\n\\t\\n\\nWelcome to the \\\"Synthetic Multilingual LLM Prompts\\\" dataset! This comprehensive collection features 1,250 synthetic LLM prompts generated using Gretel Navigator, available in seven different languages. To ensure accuracy and diversity in prompts, and translation quality and consistency across the different languages, we employed Gretel Navigator both as a generation tool and as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts."},
  {"name":"Arabic_Poems","keyword":"nlp","license":"GNU General Public License v2.0","language":"en","url":"https://huggingface.co/datasets/alwalid54321/Arabic_Poems","creator_name":"Alwalid Ibrahim","creator_url":"https://huggingface.co/alwalid54321","description":"Arabic Poems\\nit's a filtered version for (https://huggingface.co/datasets/arbml/ashaar) with only al-diwan data\\nOverview:\\nThe \\\"Arabic Poems\\\" dataset is a comprehensive collection of Arabic poetry, comprising 8,875 entries. Each entry contains detailed information about individual poems and their respective poets, making it a valuable resource for researchers, developers, and enthusiasts of Arabic literature.\\nColumns:\\nUnnamed: 0: An index column.\\npoem_title: The title of the poem.\\npoem_meter:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alwalid54321/Arabic_Poems."},
  {"name":"commbase-log-chats","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/mydroidandi/commbase-log-chats","creator_name":"My Droid And I","creator_url":"https://huggingface.co/mydroidandi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCommbase Log Chats Dataset\\n\\t\\n\\n\\nCapturing Assistant-User Interaction Logs for NLP and Chat Analysis\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Commbase Log Chats Dataset contains a series of chat logs between an assistant (Eva AI) and end user. The dataset captures interactions in the form of text exchanges with metadata such as timestamps, origin of the message, severity level, and speaker details. This dataset can be used for various applications including natural language processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mydroidandi/commbase-log-chats."},
  {"name":"sql-create-context-pt","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/emdemor/sql-create-context-pt","creator_name":"Eduardo Morais","creator_url":"https://huggingface.co/emdemor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nEste dataset  √© uma vers√£o traduzida para o portugu√™s do dataset b-mc2/sql-create-context,\\nque foi constru√≠do a partir dos datasets WikiSQL e Spider. Ele cont√©m exemplos de perguntas\\nem portugu√™s, instru√ß√µes SQL CREATE TABLE e consultas SQL que respondem √†s perguntas\\nutilizando a instru√ß√£o CREATE TABLE como contexto.\\nO principal objetivo deste dataset √© ajudar modelos de linguagem natural  em portugu√™s a gerar consultas\\nSQL precisas e contextualizadas, prevenindo a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emdemor/sql-create-context-pt."},
  {"name":"BeHonest","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/GAIR/BeHonest","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBeHonest: Benchmarking Honesty in Large Language Models\\n\\t\\n\\nBeHonest is a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries (self-knowledge), avoidance of deceit (non-deceptiveness), and consistency in responses (consistency).\\nBeHonest supports the following 10 scenarios:\\n\\nAdmitting Unknowns: LLMs should appropriately refuse to answer questions that are beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/BeHonest."},
  {"name":"x_g85_fn_dataset","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/x-g85/x_g85_fn_dataset","creator_name":"X_G85","creator_url":"https://huggingface.co/x-g85","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tX_G85 Fake News Dataset\\n\\t\\n\\nIt is a preprocessed dataset that is used to build X_G85 ML Models. The collection of fake news that are collect from the following datasets\\nLabels\\n\\n0: Fake News\\n1: Real News\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to stream dataset & use as pandas dataframe\\n\\t\\n\\nBy streaming the dataset, it won't download on your host computer. Read more here hugging face streaming dataset.\\nimport pandas as pd\\nfrom datasets import load_dataset\\n\\n\\nNote: The following operation may take some time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/x-g85/x_g85_fn_dataset."},
  {"name":"KnowUnDo","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/zjunlp/KnowUnDo","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKnowUnDo\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüíª Datasets Usage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"zjunlp/KnowUnDo\\\", name='copyright', split='unlearn')\\n\\n\\nAvailable configuration names and corresponding splits:\\ncopyright: unlearn, retention;\\nprivacy: unlearn, retention;\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüéâ Acknowledgement\\n\\t\\n\\nWe would like to express our sincere gratitude for the excellent work TOFU, Unlearn Dataset and LLM Unlearning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìñ Citation\\n\\t\\n\\nIf finding this work useful for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/KnowUnDo."},
  {"name":"LIBRA","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","description":"\\n\\t\\n\\t\\t\\n\\t\\tLIBRA: Long Input Benchmark for Russian Analysis\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA."},
  {"name":"afrimgsm","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yuntian-deng/afrimgsm","creator_name":"Yuntian Deng","creator_url":"https://huggingface.co/yuntian-deng","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrimgsm\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIMGSM is an evaluation dataset comprising translations of a subset of the GSM8k dataset into 16 African languages. \\nIt includes test sets across all 18 languages, maintaining an English and French subsets from the original GSM8k dataset. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 18 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yuntian-deng/afrimgsm."},
  {"name":"assertiveness-corpus","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Shahradmz/assertiveness-corpus","creator_name":"MZ","creator_url":"https://huggingface.co/Shahradmz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssertiveness Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nA compiled dataset of 6000 texts from multiple sources, scored using SciBert for assertiveness.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\n\\nPrompting llama 3 8B model to explain why a statement from the LIAR dataset is wrong or right. Explanation of the model and the assertiveness score rated by PEI‚Äôs model kept.\\n\\nUser level comments from change my view subreddit.\\n\\nAG news dataset.\\nfrom datasets import load_dataset\\nds = load_dataset(\\\"fancyzhx/ag_news\\\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Shahradmz/assertiveness-corpus."},
  {"name":"fa-topic-sentences","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/mostafaamiri/fa-topic-sentences","creator_name":"mostafa amiri","creator_url":"https://huggingface.co/mostafaamiri","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tREADME for fa-topic-sentences Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe fa-topic-sentences dataset is a comprehensive collection of sentences categorized into various topics. Each topic contains approximately 50 sentences in Persian, accompanied by a paraphrased version of each sentence. The dataset is structured in JSON format, providing a straightforward method for accessing individual entries.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTopics Included\\n\\t\\n\\nThe dataset encompasses the following topics:\\n\\nHistory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mostafaamiri/fa-topic-sentences."},
  {"name":"PIQA-eu","keyword":"natural-language-inference","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/HiTZ/PIQA-eu","creator_name":"HiTZ zentroa","creator_url":"https://huggingface.co/HiTZ","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for PIQA-eu\\n\\t\\n\\n\\nPoint of Contact: hitz@ehu.eus\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPIQA-eu is the professional translation to Basque of the PIQA's \\n(Bisk et al., 2020) validation partition. \\nPIQA is a commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday\\nobjects in everyday situations.\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\n\\neu-ES\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nPIQA-eu examples look like this:\\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HiTZ/PIQA-eu."},
  {"name":"TOFU-C","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C."},
  {"name":"TOFU-Cf","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf."},
  {"name":"TOFU-Cr","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr."},
  {"name":"Financial-NER-NLP","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Josephgflowers/Financial-NER-NLP","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"Dataset Card for Financial-NER-NLP\\nDataset Summary\\nThe Financial-NER-NLP Dataset is a derivative of the FiNER-139 dataset, which consists of 1.1 million sentences annotated with 139 XBRL tags. This new dataset transforms the original structured data into natural language prompts suitable for training language models. The dataset is designed to enhance models‚Äô abilities in tasks such as named entity recognition (NER), summarization, and information extraction in the financial domain.\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Financial-NER-NLP."},
  {"name":"emotion_classification_reviews_kfold_10","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/mtiessler/emotion_classification_reviews_kfold_10","creator_name":"Max Tiessler","creator_url":"https://huggingface.co/mtiessler","description":"mtiessler/emotion_classification_reviews_kfold_10 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"TOFU-C","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C."},
  {"name":"Amazon_Reviews_Binary_for_Sentiment_Analysis","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. In the dataset, class 1 is the negative and class 2 is the positive. Each class has 1,800,000 training samples and 200,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis."},
  {"name":"Yelp_Reviews_for_Binary_Senti_Analysis","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Binary_Senti_Analysis","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2 negative, and 3 and 4 positive. For each polarity 280,000 training samples and 19,000 testing samples are take randomly. In total there are 560,000 trainig samples and 38,000 testing samples. Negative polarity is class 1, and positive class 2.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Binary_Senti_Analysis."},
  {"name":"Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yelp reviews full star dataset is constructed by randomly taking 130,000 training samples and 10,000 testing samples for each review star from 1 to 5. In total there are 650,000 trainig samples and 50,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 2 columns in them, corresponding to class index (1 to 5) and review text. The review texts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes."},
  {"name":"Yahoo_Answers_10_categories_for_NLP","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yassiracharki/Yahoo_Answers_10_categories_for_NLP","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe file classes.txt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yahoo_Answers_10_categories_for_NLP."},
  {"name":"SFinD-S","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/tilmann-strative/SFinD-S","creator_name":"Tilmann Bruckhaus","creator_url":"https://huggingface.co/tilmann-strative","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis sample is part of the larger SFinD-S (Strative Financial Dataset - Synthetic), a comprehensive dataset designed for Retrieval-Augmented Generation (RAG) GenAI applications, Natural Language Processing (NLP), Large Language Models (LLM), and AI tasks in the financial domain. The full SFinD-S dataset contains over 20,000 records of realistic financial questions and verified answers, sourced from a wide variety of web content.\\nIf you find this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tilmann-strative/SFinD-S."},
  {"name":"TOFUCr1","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1."},
  {"name":"Arguana","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/timchen0618/Arguana","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the Arguana dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.All the instances in this dataset is \\\"binary\\\".\\nWe repurpose the Arguana‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/Arguana."},
  {"name":"Kialo","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/timchen0618/Kialo","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the Kialo dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.  \\nWe collect the dataset from a debate website, Kialo.com.The positive and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/Kialo."},
  {"name":"OpinionQA","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/timchen0618/OpinionQA","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the OpinionQA dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.  \\nWe repurpose the OpinionQA dataset into the desired setting.We first‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/OpinionQA."},
  {"name":"text2pandas","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/zeyadusf/text2pandas","creator_name":"Zeyad Usf","creator_url":"https://huggingface.co/zeyadusf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout Data\\n\\t\\n\\n\\n\\n \\n  \\n\\n \\n  \\n\\n \\n  \\n\\n\\n\\nI found two datasets about converting text with context to pandas code on Hugging Face, but the challenge is in the context. The context in both datasets is different which reduces the results of the model. First let's mention the data I found and then show examples, solution and some other problems.\\n\\nRahima411/text-to-pandas :\\nThe data is divided into Train with 57.5k and Test with 19.2k.\\n\\nThe data has two columns as you can see in the example:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zeyadusf/text2pandas."},
  {"name":"daigt","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/zeyadusf/daigt","creator_name":"Zeyad Usf","creator_url":"https://huggingface.co/zeyadusf","description":"zeyadusf/daigt dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"turkish-sentence-elements","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/synturk/turkish-sentence-elements","creator_name":"Syntax T√ºrkiye","creator_url":"https://huggingface.co/synturk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SYNT√úRK SENTAGRAM Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SYNT√úRK SENTAGRAM Dataset is a specialized dataset designed to aid in the development and evaluation of NLP models focused on Turkish syntax and grammar. The dataset consists of sentences annotated with grammatical elements such as subjects, predicates, objects, and adjuncts. This dataset is intended for educational purposes and research in natural language processing (NLP), particularly in the context‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/synturk/turkish-sentence-elements."},
  {"name":"imageability","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/StephanAkkerman/imageability","creator_name":"Stephan Akkerman","creator_url":"https://huggingface.co/StephanAkkerman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish Word Imageability\\n\\t\\n\\nThis dataset is a combination of my other two imageability datasets. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset is ideal for training and evaluating machine learning models for English word imageability.\\nYou can find the embeddings embeddings in my other dataset here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgments\\n\\t\\n\\nWe extend our heartfelt gratitude to all the authors of the original datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the MIT license.\\n"},
  {"name":"TOFUCrP","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP."},
  {"name":"LLM_Response_Eval","keyword":"natural-language-processing","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/AmirMohseni/LLM_Response_Eval","creator_name":"Amir Mohseni","creator_url":"https://huggingface.co/AmirMohseni","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLM Response Evaluation Dataset\\n\\t\\n\\nThis dataset contains a collection of responses generated by three large language models (LLMs): GPT-4o, Gemini 1.5 Pro, and Llama 3.1 405B. The responses are to a series of questions aimed at evaluating the models' problem-solving abilities using Polya's problem-solving technique, as described in the book \\\"How to Solve It\\\" by George Polya.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\nQuestions: The dataset includes a set of questions designed to evaluate the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AmirMohseni/LLM_Response_Eval."},
  {"name":"Deshika","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sarch7040/Deshika","creator_name":"sarch","creator_url":"https://huggingface.co/sarch7040","description":"Maharashtri Prakrit to English Parallel Corpus\\nDataset Summary\\nThis dataset contains parallel text data for translating from Maharashtri Prakrit (an ancient Indo-Aryan language) to English. It is designed to aid in developing machine translation systems, language models, and linguistic research for this underrepresented language. The dataset is collected from historical texts, scriptures, and scholarly resources.\\nKey Features:\\n\\n  Source Language: Maharashtri Prakrit\\n  Target Language: English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sarch7040/Deshika."},
  {"name":"alpaca-bulgarian-jokes","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes."},
  {"name":"tip-of-my-tongue-known-item-search","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/webis/tip-of-my-tongue-known-item-search","creator_name":"Webis Group","creator_url":"https://huggingface.co/webis","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThe TOMT-KIS (tip-of-my-tongue-known-item-search) Dataset\\n\\t\\n\\nSearchers who cannot resolve a known-item information need using a search engine might post a respective question on a question answering platform, hoping that the discussion with other people can help to identify the item. TOMT-KIS is a large-scale dataset of 1.28 million known-item questions from the r/tipofmytongue subreddit in the QPP++@ECIR'23 paper on A Large-Scale Dataset for Known-Item Question Performance‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/webis/tip-of-my-tongue-known-item-search."},
  {"name":"AURA-Sentiment","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/irfan-ahmad/AURA-Sentiment","creator_name":"Irfan Ahmad","creator_url":"https://huggingface.co/irfan-ahmad","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAURA-Sentiment\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe AURA Sentiment Dataset is a collection of 29,700 app reviews in Arabic from iOS and Android platforms. Each review is labeled with a sentiment class, enabling researchers and practitioners to develop and evaluate sentiment analysis models tailored to the Arabic language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\nThe dataset includes the following columns:\\n\\nreview: The text of the app review in Arabic.\\nappName: The name of the application‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/irfan-ahmad/AURA-Sentiment."},
  {"name":"MCEval8K","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/iszhaoxin/MCEval8K","creator_name":"XIN ZHAO","creator_url":"https://huggingface.co/iszhaoxin","description":"iszhaoxin/MCEval8K dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"wiktionary-data","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\\nÌïúÍµ≠Ïñ¥ - Korean\\nêé†êéºêéπ - Old Persian\\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\\nElamite\\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data."},
  {"name":"tl-test-learn-prompts","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/reddgr/tl-test-learn-prompts","creator_name":"David G. R.","creator_url":"https://huggingface.co/reddgr","description":"This dataset contains manually labeled examples used for training and testing reddgr/tl-test-learn-prompt-classifier, a fine-tuning of DistilBERT that classifies chatbot prompts as either 'test' or 'learn.'\\nPrompts labeled as 'test' (1) are those where it can be inferred that the user is:\\n\\nPresenting a problem that requires complex reasoning or arithmetic logic to resolve.\\nIntentionally 'challenging' the conversational tool with a complicated question the user might know the answer to.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reddgr/tl-test-learn-prompts."},
  {"name":"thai-gov-procurement_regulation-17-amend-21","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21","creator_name":"Amornpan Phornchaicharoen","creator_url":"https://huggingface.co/amornpan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüáπüá≠ Dataset Card for Thai Government Procurement Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‚ÑπÔ∏è This dataset is optimized for procurement-related NLP tasks in Thai.\\n\\t\\n\\nThis dataset contains a collection of procurement regulations, instructions, and responses focused on public sector purchasing, contract management, and compliance with Thai government standards. It aims to support natural language processing tasks involving procurement assistance, such as chatbot development, procurement dialogue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21."},
  {"name":"wos_hierarchical_multi_label_text_classification","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/marcelsun/wos_hierarchical_multi_label_text_classification","creator_name":"Marcel Dunaiski","creator_url":"https://huggingface.co/marcelsun","description":"Introduced by du Toit and Dunaiski (2024) Introducing Three New Benchmark Datasets for Hierarchical Text Classification.\\nThe WOS Hierarchical Text Classification are three dataset variants created from Web of Science (WOS) title and abstract data categorised into a hierarchical, multi-label class structure. The aim of the sampling and filtering methodology used was to create well-balanced class distributions (at chosen hierarchical levels). Furthermore, the WOS_JTF variant was also created‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcelsun/wos_hierarchical_multi_label_text_classification."},
  {"name":"cci-dataset","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/raghavdw/cci-dataset","creator_name":"Raghav Wate","creator_url":"https://huggingface.co/raghavdw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAirline Customer Service Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains customer service interactions related to the airline industry. It can be used for training and evaluating natural language processing (NLP) models for tasks such as intent classification, sentiment analysis, and topic modeling.\\nHomepage: [link to your project's homepage or GitHub repository (if applicable)]\\nDataset Structure:\\nThe dataset is split into two subsets:\\n\\ntrain: Used for training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raghavdw/cci-dataset."},
  {"name":"FNFC-Functional_Non-Functional_Calssification","keyword":"nlp","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/MSHD-IAU/FNFC-Functional_Non-Functional_Calssification","creator_name":"Mashhad Azad University","creator_url":"https://huggingface.co/MSHD-IAU","description":"MSHD-IAU/FNFC-Functional_Non-Functional_Calssification dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"RoundTripOCR-sanskrit","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-sanskrit","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Sanskrit language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
  {"name":"RoundTripOCR-bodo","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-bodo","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Bodo language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
  {"name":"RoundTripOCR-konkani","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-konkani","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Konkani language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
  {"name":"RoundTripOCR-marathi","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-marathi","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Marathi language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
  {"name":"trustpilot-reviews-123k","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Kerassy/trustpilot-reviews-123k","creator_name":"Jay Broughton","creator_url":"https://huggingface.co/Kerassy","description":"\\n\\t\\n\\t\\t\\n\\t\\tTrustpilot Reviews 123k\\n\\t\\n\\nWelcome to the Trustpilot Reviews 123k Dataset, a collection of user feedback gathered from Trustpilot UK. This dataset encompasses a total of 123,181 reviews spanning 1,680 companies across 22 categories. The reviews were methodically collected over a brief period between 19th December 2024 and 7th January 2025, providing a snapshot of customer sentiments during this timeframe. This dataset is intended for researchers and practitioners interested in natural‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kerassy/trustpilot-reviews-123k."},
  {"name":"unal-repository-dataset-test-instruct","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-test-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset Instruct Test (split 75/25)\\nDescripci√≥n: Split 25% del dataset original. \\nEste dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje para‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-test-instruct."},
  {"name":"kbli2020","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/ronnieaban/kbli2020","creator_name":"Ronnie Aban","creator_url":"https://huggingface.co/ronnieaban","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nDataset ini berisi Klasifikasi Baku Lapangan Usaha Indonesia (KBLI) tahun 2020 yang merupakan standar klasifikasi aktivitas ekonomi yang ditetapkan oleh Badan Pusat Statistik (BPS) Indonesia. KBLI 2020 digunakan untuk mengklasifikasikan unit usaha dan aktivitas ekonomi ke dalam kelompok yang seragam berdasarkan kegiatan ekonomi yang dilakukan.\\n"},
  {"name":"125-tripadvisor-reviews","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/nhull/125-tripadvisor-reviews","creator_name":"N. H√ºll","creator_url":"https://huggingface.co/nhull","description":"\\n\\t\\n\\t\\t\\n\\t\\tMichelin-Starred Sustainable Restaurants Sentiment Dataset\\n\\t\\n\\nThis dataset contains 125 TripAdvisor reviews for Michelin-starred, sustainability-focused restaurants. It was created as part of a university project aimed at analyzing the sentiment of customers in these specific types of restaurants. The selection of restaurants was based on two main criteria: Michelin-star rating and sustainability recognition. The dataset is intended for use in sentiment analysis to explore trends and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nhull/125-tripadvisor-reviews."},
  {"name":"LLM_Post_Training_BestOfNvsGreedy","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/QPM777/LLM_Post_Training_BestOfNvsGreedy","creator_name":"Antoine Turkie","creator_url":"https://huggingface.co/QPM777","description":"\\n\\t\\n\\t\\t\\n\\t\\tBest-of-N vs Greedy Model Dataset\\n\\t\\n\\nThis dataset contains problems with their corresponding solutions from two models using Qwen (Qwen2.5-1.5B-Instruct):\\n\\nGreedy Model: A model that picks the most optimal solution at each step.\\nBest-of-N Model: A model that generates multiple solutions and picks the best one from a set of N using Skywork (Skywork-o1-Open-PRM-Qwen-2.5-1.5B) reward model.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nNumber of Problems: 20 (for testing purposes).\\nColumns:\\nid: Unique‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QPM777/LLM_Post_Training_BestOfNvsGreedy."},
  {"name":"combined-fr-caselaw","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/La-Mousse/combined-fr-caselaw","creator_name":"La Mousse","creator_url":"https://huggingface.co/La-Mousse","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for French Legal Cases Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset combines French legal cases from multiple sources (INCA, JADE, CASS, CAPP) into a unified format with overlapping text triplets. It includes decisions from various French courts, processed to facilitate natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nTasks:\\nText Generation\\nLegal Document Analysis\\nText Classification\\nLanguage Modeling\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/La-Mousse/combined-fr-caselaw."},
  {"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED."},
  {"name":"chatjsonsql","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","description":"\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql."},
  {"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED."},
  {"name":"GammaCorpus-v2-5m","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m."},
  {"name":"GammaCorpus-v2-1m","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m."},
  {"name":"GammaCorpus-v2-100k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k."},
  {"name":"GammaCorpus-v2-50k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k."},
  {"name":"GammaCorpus-v2-5m","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m."},
  {"name":"GammaCorpus-v2-1m","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m."},
  {"name":"GammaCorpus-v2-100k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k."},
  {"name":"GammaCorpus-v2-50k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k."},
  {"name":"GammaCorpus-Fact-QA-450k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: Fact QA 450k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus Fact QA 450k is a dataset that consists of 450,000 fact-based question-and-answer pairs designed for training AI models on factual knowledge retrieval and question-answering tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 450,000\\nFormat: JSONL\\nLanguage: English\\nData Type: Fact-based questions\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe dataset is formatted in JSONL, where each line is a JSON object‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k."},
  {"name":"GammaCorpus-Fact-QA-450k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: Fact QA 450k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus Fact QA 450k is a dataset that consists of 450,000 fact-based question-and-answer pairs designed for training AI models on factual knowledge retrieval and question-answering tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 450,000\\nFormat: JSONL\\nLanguage: English\\nData Type: Fact-based questions\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe dataset is formatted in JSONL, where each line is a JSON object‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k."},
  {"name":"GammaCorpus-Polylingo-50k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k."},
  {"name":"FactNews","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/franciellevargas/FactNews","creator_name":"Francielle Vargas","creator_url":"https://huggingface.co/franciellevargas","description":"\\n\\t\\n\\t\\t\\n\\t\\tEvaluation Benchmark for Sentence-Level Factuality Prediciton in Portuguese\\n\\t\\n\\nThe FactNews consits of the first large sentence-level annotated corpus for factuality prediciton in Portuguese. \\nIt is composed of 6,191 sentences annotated according to factuality and media bias definitions proposed by AllSides. We use FactNews to assess the overall reliability of news sources by formulating \\ntwo text classification problems for predicting sentence-level factuality of news reporting and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/franciellevargas/FactNews."},
  {"name":"GammaCorpus-Polylingo-50k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k."},
  {"name":"go_emotions_wheel_unilabel","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Jsevisal/go_emotions_wheel_unilabel","creator_name":"Javier Sevilla Salcedo","creator_url":"https://huggingface.co/Jsevisal","description":"Original dataset: GoEmotions dataset\\nFiltered using the following mapping based on the basic emotions found in Plutchik's Wheel of Emotions and filtered to use only the sentences with one label\\nwheel_dict = {\\n    \\\"joy\\\": [\\n        \\\"joy\\\",\\n        \\\"amusement\\\",\\n        \\\"excitement\\\",\\n        \\\"gratitude\\\",\\n        \\\"pride\\\",\\n        \\\"relief\\\",\\n        \\\"admiration\\\",\\n        \\\"love\\\",\\n        \\\"optimism\\\",\\n    ],\\n    \\\"trust\\\": [\\\"approval\\\", \\\"caring\\\"],\\n    \\\"fear\\\": [\\\"fear\\\", \\\"nervousness\\\"],\\n    \\\"surprise\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jsevisal/go_emotions_wheel_unilabel."},
  {"name":"sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600","creator_name":"Tasmay Pankaj Tibrewal","creator_url":"https://huggingface.co/Tasmay-Tib","description":"Dataset for sarvam's entity normalisation task. More detailed information can be found here, in the main model repo: Hugging Face\\nDetailed Report (Writeup): Google Drive\\nIt also has a gguf variant, with certain additional gguf based innstructions: Hugging Face\\nModel inference script can be found here: Colab\\nModel predictions can be found in this dataset and both the repo files. named as: \\n\\neval_data_001_predictions.csv and eval_data_001_predictions_excel.csv.\\ntrain_data_001_predictions.csvand‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600."},
  {"name":"piaozhu","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Retr01234/piaozhu","creator_name":"Chen","creator_url":"https://huggingface.co/Retr01234","description":"\\n\\t\\n\\t\\t\\n\\t\\tÊï∞ÊçÆÈõÜÂêçÁß∞ÔºöÂò¥Ëá≠Êê≠Â≠êÂæÆË∞ÉÊï∞ÊçÆÈõÜ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t1. Êï∞ÊçÆÈõÜÁÆÄ‰ªã\\n\\t\\n\\nËøô‰∏™Êï∞ÊçÆÈõÜ‰∏∫ÂæÆË∞ÉÂØπËØùÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÁâπÊÆäÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåÂü∫‰∫é‰∏Ä‰∏™ËôöÊãüÁöÑËßíËâ≤‚ÄúÊ≤àËì¨Á´π‚ÄùËøõË°å‰∫§‰∫í„ÄÇËøô‰∏™ËßíËâ≤ÔºàÂ§ñÂè∑‚ÄúÊú¥Á´π‚ÄùÔºâÂÖ∑ÊúâÂÜ∑Âò≤ÁÉ≠ËÆΩ„ÄÅÊØíËàå„ÄÅÁÆÄÊ¥ÅËÄåÊúâÊîªÂáªÊÄßÁöÑÁâπÁÇπÔºåÈÄÇÂêàËÆ≠ÁªÉÊ®°Âûã‰∫ßÁîüÂÖ∑ÊúâËÆΩÂà∫„ÄÅÂÜ∑Âò≤ÁÉ≠ËÆΩËØ≠Ê∞îÁöÑÂõûÁ≠î„ÄÇÊï∞ÊçÆÈõÜÁöÑÂÜÖÂÆπ‰∏ªË¶ÅÊòØËßíËâ≤ÊâÆÊºîÂØπËØùÂú∫ÊôØÔºåÈÄÇÁî®‰∫éÁîüÊàêÂÖ∑ÊúâÁâπÂÆöÈ£éÊ†ºÁöÑÂØπËØùÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®Â∏¶ÊúâËÆΩÂà∫ÂíåÂπΩÈªòÁöÑÊÉÖÂ¢É‰∏ãËøõË°å‰∫íÂä®Êó∂„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\t2. Êï∞ÊçÆÈõÜÁªìÊûÑ\\n\\t\\n\\nÊï∞ÊçÆÈõÜ‰∏∫‰∏Ä‰∏™ÂåÖÂê´Ëã•Âπ≤ÂØπËØùËΩÆÊ¨°ÁöÑ JSON Ê†ºÂºèÊñá‰ª∂„ÄÇÊØè‰∏™ÂØπËØùËΩÆÊ¨°Áî±ËßíËâ≤ÂíåÁî®Êà∑ÁöÑÂØπËØùÁªÑÊàêÔºåÊØè‰∏™ÂØπËØùÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö\\n\\nroleÔºöËßíËâ≤ÁöÑË∫´‰ªΩÔºåÂèØËÉΩÊòØ \\\"system\\\" Êàñ \\\"user\\\"„ÄÇ\\n\\\"system\\\" Ë°®Á§∫ÊòØÊ®°ÂûãËÆæÂÆöËßíËâ≤ÁöÑËæìÂÖ•ÔºàÂ¶ÇÂÆö‰πâËßíËâ≤ËÉåÊôØ„ÄÅË°å‰∏∫Ê®°ÂºèÁ≠âÔºâ„ÄÇ\\n\\\"user\\\" Ë°®Á§∫ÂØπËØù‰∏≠ÁöÑÁî®Êà∑ËæìÂÖ•ÔºàÂ¶ÇÊèêÈóÆ„ÄÅËØ∑Ê±ÇÊàñ‰∫§‰∫íÔºâ„ÄÇ\\n\\n\\ncontentÔºöÂØπËØùÂÜÖÂÆπÔºåË°®Á§∫ËßíËâ≤ÊàñËÄÖÁî®Êà∑ÁöÑÂÖ∑‰ΩìÂèëË®Ä„ÄÇ\\nloss_weightÔºàÂèØÈÄâÔºâÔºöÊØè‰∏™Êï∞ÊçÆÊù°ÁõÆÂØπÂ∫îÁöÑÊçüÂ§±ÊùÉÈáçÔºåÂΩìÂâçÂèØ‰∏∫Á©∫Êàñ‰∏∫ null„ÄÇÂèØ‰ª•Âú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠Âä†ÊùÉ‰∏çÂêåÂØπËØùÂÜÖÂÆπ„ÄÇ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t3. Êï∞ÊçÆÊ†∑‰æã‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Retr01234/piaozhu."},
  {"name":"Twilight_NLP","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/valerysukmanyuk/Twilight_NLP","creator_name":"Sukmanyuk Valery","creator_url":"https://huggingface.co/valerysukmanyuk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis dataset is made for fun and in ‚ú®educational‚ú® purposes.\\n\\t\\n\\nIt includes tokenized novel text by Stephenie Meyer \\\"Breaking Dawn\\\" in Russian, PoS, NER (LOC, PER, ORG), tokens' lemmas and syntax annotation. NER, PoS, lemmatization and syntax annotation were made with spaCy and their 'ru_core_news_lg' \\n"},
  {"name":"text-clustering-example-data","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/billingsmoore/text-clustering-example-data","creator_name":"Jacob Moore","creator_url":"https://huggingface.co/billingsmoore","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThis dataset consists of 925 sentences in English paired with a broad topic descriptor for use as example data in product demonstrations or student projects.\\n\\nCurated by: billingsmoore\\nLanguage(s) (NLP): English\\nLicense: Apache License 2.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDirect Use\\n\\t\\n\\nThis data can be loaded using the following Python code.\\nfrom datasets import load_dataset\\n\\nds = load_dataset('billingsmoore/text-clustering-example-data')\\n\\nIt can then be clustered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/billingsmoore/text-clustering-example-data."},
  {"name":"RoundTripOCR-nepali","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-nepali","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Nepali language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
  {"name":"steam-reviews-constructiveness-binary-label-annotations-1.5k","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/abullard1/steam-reviews-constructiveness-binary-label-annotations-1.5k","creator_name":"Samuel Bullard","creator_url":"https://huggingface.co/abullard1","description":"\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n    1.5K Steam Reviews Binary Labeled for Constructiveness\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 1,461 Steam reviews from 10 of the most reviewed games. Each game has about the same amount of reviews. Each review is annotated with a binary label indicating whether the review is constructive or not. The dataset is designed to support tasks related to text classification, particularly constructiveness detection tasks in the gaming domain.\\n\\nAlso available as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abullard1/steam-reviews-constructiveness-binary-label-annotations-1.5k."},
  {"name":"ytu-ara-proje-absa","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ebrukilic/ytu-ara-proje-absa","creator_name":"Ebru Kƒ±lƒ±√ß","creator_url":"https://huggingface.co/ebrukilic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nDataset Name: ABSA (Aspect-Based Sentiment Analysis)\\nSize: 5045 instances\\nLanguage(s): Turkish\\nTask: Aspect-Based Sentiment Analysis (ABSA)\\nCategories: Etek, Kaban, G√∂mlek, Kazak, Pantolon\\nPolarity: Negatif, N√∂tr, Pozitif\\nAspect: Kalite, Kuma≈ü, Renk, Beden, Kargo, Fiyat\\nLicense: MIT\\nDeveloped by: ebru kƒ±lƒ±√ß , rumeysa nur yasav\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nBu veri k√ºmesi Yƒ±ldƒ±z Teknik √úniversitesi Bilgisayar M√ºhendisliƒüi √∂ƒürencilerinin Bilgisayar Projesi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ebrukilic/ytu-ara-proje-absa."},
  {"name":"ner-cat","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Ugiat/ner-cat","creator_name":"Ugiat Technologies","creator_url":"https://huggingface.co/Ugiat","description":"\\n\\t\\n\\t\\t\\n\\t\\tNERCat Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe NERCat dataset is a manually annotated collection of Catalan-language television transcriptions, designed to improve Named Entity Recognition (NER) performance for the Catalan language. The dataset covers diverse domains such as politics, sports, and culture, and includes 9,242 sentences with 13,732 named entities annotated across eight categories: Person, Facility, Organization, Location, Product, Event, Date, and Law. The dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ugiat/ner-cat."},
  {"name":"MRC-psycholinguistic-database","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/StephanAkkerman/MRC-psycholinguistic-database","creator_name":"Stephan Akkerman","creator_url":"https://huggingface.co/StephanAkkerman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMRC Psycholinguistic Database\\n\\t\\n\\nThis is the complete MRC psycholinguistic database as found on https://websites.psychology.uwa.edu.au/school/mrcdatabase/uwa_mrc.htm.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset is ideal for training and evaluating machine learning models for English word concreteness.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgments\\n\\t\\n\\nWe extend our heartfelt gratitude to all the authors of the original dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the MIT license.\\n"},
  {"name":"TOFU-C-All","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All."},
  {"name":"SNLI-NLI","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/1-800-SHARED-TASKS/SNLI-NLI","creator_name":"1-800-Shared-Tasks","creator_url":"https://huggingface.co/1-800-SHARED-TASKS","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SNLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference (NLI), also known as Recognizing Textual Entailment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/1-800-SHARED-TASKS/SNLI-NLI."},
  {"name":"alpaca-bulgarian-jokes-multilingual-prompts","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts."},
  {"name":"TOFU-C-Shuffle","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle."},
  {"name":"TOFU-C-single","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single."},
  {"name":"TOFU-Cbin","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin."},
  {"name":"SimpleStories","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/lennart-finke/SimpleStories","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories yourself, head over to this repository.\\nIf you'd like to commission other languages or story formats, feel free to send mail.\\nSimpleStories is an iteration upon TinyStories by Eldan and Li, and can likewise be used for distillation to very small language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nStory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories."},
  {"name":"TOFU-C-Direct","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct."},
  {"name":"EmoPropMan","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/basavaraj/EmoPropMan","creator_name":"Basavaraj","creator_url":"https://huggingface.co/basavaraj","description":"basavaraj/EmoPropMan dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Loneliness-Causes-and-Intensity","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/yael-katsman/Loneliness-Causes-and-Intensity","creator_name":"yael katsman","creator_url":"https://huggingface.co/yael-katsman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReddit Loneliness: Causes and intensity\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Loneliness- cause and intensity dataset is an English-language compilation of posts focused on loneliness among individuals and the different types of loneliness they experience. . The primary objective of this dataset is to aid various NLP models in predicting loneliness and its causes from text, which may be useful in the fields of mental health, NLP contextual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yael-katsman/Loneliness-Causes-and-Intensity."},
  {"name":"ContactShieldDataset","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/xxparthparekhxx/ContactShieldDataset","creator_name":"Parth Parekh","creator_url":"https://huggingface.co/xxparthparekhxx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContactShieldDataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nContactShieldDataset is a high-quality, synthetically generated dataset designed for training and evaluating models that detect contact information sharing in text. This dataset is specifically tailored for use in freelancing and online marketplace contexts, where maintaining user privacy and platform policies is crucial.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nSize: 200,000 examples\\nLanguage: English\\nGeneration Method: Synthetically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xxparthparekhxx/ContactShieldDataset."},
  {"name":"chatbot","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/neverland-th/chatbot","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chatbot dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"TOFU-C-All","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All."},
  {"name":"scips_qa","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/zorpsoon/scips_qa","creator_name":"Prasoon Bajpai","creator_url":"https://huggingface.co/zorpsoon","description":"zorpsoon/scips_qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Eason_TOFU","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"AURA-Classification","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/irfan-ahmad/AURA-Classification","creator_name":"Irfan Ahmad","creator_url":"https://huggingface.co/irfan-ahmad","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAURA-Classification\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe AURA (App User Review in Arabic) Classification dataset is a collection of 2,900 Arabic-language app reviews collected from various mobile applications. This dataset is primarily designed for text classification tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\nThe dataset includes the following fields:\\n\\nreview: The text of the review in Arabic.\\n\\nappName: The name of the application being reviewed.\\n\\nplatform: The platform (iOS or Android)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/irfan-ahmad/AURA-Classification."},
  {"name":"high-quality-nli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/agentlans/high-quality-nli","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHigh-Quality NLI Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset is designed for Natural Language Inference (NLI) tasks, containing high-quality sentence pairs. It improves upon commonly used NLI datasets by offering more complex and nuanced examples, making it suitable for advanced language understanding models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Statistics\\n\\t\\n\\n\\nTrain set size: 550‚Äâ970\\nTest set size: 137‚Äâ743\\nTotal size: 688‚Äâ713\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tClass Distribution‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/high-quality-nli."},
  {"name":"talking-to-chatbots-unwrapped-chats","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats","creator_name":"David G. R.","creator_url":"https://huggingface.co/reddgr","description":"This work-in-progress dataset contains conversations with various LLM tools, sourced by the author of the website  Talking to Chatbots. \\nA simplified version of this dataset can be found at reddgr/talking-to-chatbots-chats, where messages belonging to a same conversation are 'wrapped' inside a single record. In this extended dataset, each conversation turn (pair of messages consisting of a user prompt and a response by the LLM) is presented as an individual record, with additional metrics and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats."},
  {"name":"Quran_English_Myanmar_Parrelel_Corpus","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/kingkaung/Quran_English_Myanmar_Parrelel_Corpus","creator_name":"King Kaung","creator_url":"https://huggingface.co/kingkaung","description":"\\n\\t\\n\\t\\t\\n\\t\\tQuran English-Myanmar Parallel Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a parallel corpus of the Quran, containing translations in English and Myanmar. It includes 6,237 verses (ayahs) from all chapters (surahs), aligned by their respective Surah and Ayah numbers.\\n\\nEnglish Translation: Provided by Dr. Muhsin Khan and Dr. Hilali.\\nMyanmar Translation: Translated by the Myanmar Quran Translation Committee, comprising religious and non-religious scholars, and later published by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kingkaung/Quran_English_Myanmar_Parrelel_Corpus."},
  {"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED."},
  {"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED."},
  {"name":"sa-data","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/phalanx80/sa-data","creator_name":"Paolo De Gasperis","creator_url":"https://huggingface.co/phalanx80","description":"\\n\\t\\n\\t\\t\\n\\t\\tStoria dell'Arte Dataset (SA-Data)\\n\\t\\n\\n \\n\\n\\t\\n\\t\\t\\n\\t\\tüìå Descrizione del Dataset\\n\\t\\n\\nIl dataset SA-Data √® una raccolta strutturata di articoli della rivista Storia dell'Arte (https://www.storiadellarterivista.it/) digitalizzati e arricchiti con metadati dettagliati e rappresentazioni semantiche. √à stato creato per supportare la ricerca accademica e le applicazioni di elaborazione del linguaggio naturale.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîç Contenuto\\n\\t\\n\\nIl dataset include:\\n\\n1050 articoli pubblicati tra il 1969 e‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phalanx80/sa-data."},
  {"name":"LatinSummarizerDataset","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tLatinSummarizer Dataset\\n\\t\\n\\n    \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe LatinSummarizerDataset is a structured dataset used in the GitHub Repository for Latin summarization and translation tasks. This dataset provides aligned English-Latin texts, extractive summaries, and pre-training prompts for fine-tuning models like mT5 for low-resource NLP applications.\\n\\n\\t\\n\\t\\t\\n\\t\\tStructure\\n\\t\\n\\nThe dataset is divided into two main phases: \\n\\nPre-training Data: Includes aligned bilingual corpora, synthetic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset."},
  {"name":"Telugu-NLP-AI-Dialect-Comedy-video-Dataset","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Automation-Tribe/Telugu-NLP-AI-Dialect-Comedy-video-Dataset","creator_name":"AUTTRIBE-AI-AUTOMATION","creator_url":"https://huggingface.co/Automation-Tribe","description":"Telugu is one of the sweetest and oldest languages of India. A deep Dive into Telugu its spoken in 2 states and majorly 16 regional dailects.\\nThis Dataset help you perform operations in NLP and Speech Recognition Models towards telugu Dialects.\\n"},
  {"name":"StudyAbroadGPT-Dataset","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/millat/StudyAbroadGPT-Dataset","creator_name":"MD MILLAT HOSEN","creator_url":"https://huggingface.co/millat","description":"\\n\\t\\n\\t\\t\\n\\t\\tStudyAbroadGPT-Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe StudyAbroadGPT-Dataset is a collection of conversational data focused on university application requirements for various programs, including MBA, MS in Computer Science, Data Science, and Bachelor of Medicine. The dataset includes interactions between humans asking questions about application processes (e.g., \\\"How do I write a strong SOP for MS in Data Science at MIT?\\\") and an assistant providing detailed responses. Covering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/millat/StudyAbroadGPT-Dataset."},
  {"name":"superglue","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Hyukkyu/superglue","creator_name":"Hyukkyu Kang","creator_url":"https://huggingface.co/Hyukkyu","description":"\\n\\t\\n\\t\\t\\n\\t\\tSuperGLUE Benchmark Datasets\\n\\t\\n\\nThis repository contains the SuperGLUE benchmark datasets. Each dataset is available as a separate configuration, making it easy to load individual datasets using the datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Descriptions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDatasets Included\\n\\t\\n\\n\\nBoolQ: A question-answering task where each example consists of a short passage and a yes/no question about the passage. The questions are provided anonymously and unsolicited by users of the Google search‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Hyukkyu/superglue."},
  {"name":"SemEval_task10","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/eerrffuunn/SemEval_task10","creator_name":"Mohammaderfan Koupaei","creator_url":"https://huggingface.co/eerrffuunn","description":"eerrffuunn/SemEval_task10 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"wiktionary-data","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","description":"\\n\\t\\n\\t\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\\nÌïúÍµ≠Ïñ¥ - Korean\\nêé†êéºêéπ- Old Persian\\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\\nElamite\\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data."},
  {"name":"PubMed-Cancer-NLP-Textual-Dataset","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cyberpsych/PubMed-Cancer-NLP-Textual-Dataset","creator_name":"Om Aryan","creator_url":"https://huggingface.co/cyberpsych","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPubMed-Cancer-NLP-Textual-Dataset\\n\\t\\n\\nThis dataset has been obtained from PubMed for research purposes. README will be updated with time.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nIt has multiple cancer samples with labels with their title and abstract from PubMed Repository.\\n\\nCurated by: Om Aryan\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://pubmed.ncbi.nlm.nih.gov\\n\\n"},
  {"name":"movies","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/IsmaelMousa/movies","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMovie Scripts Dataset\\n\\t\\n\\nThe Movie Scripts Dataset consists of scripts from 1,172 movies, providing a comprehensive collection of movie dialogues and narratives. This dataset is designed to support various natural language processing (NLP) tasks, including dialogue generation, script summarization, and text analysis.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nThe dataset contains 2 columns:\\n\\nName: The title of the movie.\\nScript: The full script of the movie in English.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe Movie‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/movies."},
  {"name":"univ_exams_finnish","keyword":"natural-language-processing","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/readd/univ_exams_finnish","creator_name":"Perttu Isotalo","creator_url":"https://huggingface.co/readd","description":"readd/univ_exams_finnish dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"UnitedStatesSenateBillsAndSummaries","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cheaptrix/UnitedStatesSenateBillsAndSummaries","creator_name":"Taylor Hartman","creator_url":"https://huggingface.co/cheaptrix","description":"cheaptrix/UnitedStatesSenateBillsAndSummaries dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"UnitedStatesSenateBillsAndSummaries","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MTSUFall2024SoftwareEngineering/UnitedStatesSenateBillsAndSummaries","creator_name":"MTSU Fall 2024 Software Engineering","creator_url":"https://huggingface.co/MTSUFall2024SoftwareEngineering","description":"MTSUFall2024SoftwareEngineering/UnitedStatesSenateBillsAndSummaries dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"wilhelm-vocabulary","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\tWilhelm Vocabulary\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWilhelm Vocabulary\\nDevelopment\\nEnvironment Setup\\nInstalling Dependencies\\nData Format\\nEncoding Table in YAML\\n\\n\\nData Pipeline\\nHow Data (Vocabulary) is Stored in a Graph Database\\nWhy Graph Database\\nBase Schema\\n\\n\\n\\n\\nLanguages\\nGerman\\nPronoun\\nNoun\\nVerb\\n\\n\\nAncient Greek\\nDiacritic Mark Convention\\nPronoun\\nNoun\\nAdjective\\n1. Three-Ending Adjectives: 1st and 2nd Declension (2-1-2)2. Two-Ending 2nd Declension Adjectives (2-2)\\n3. Two-Ending 3rd Declension Adjectives‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary."},
  {"name":"learn_hf_food_not_food_image_captions","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tanvircr7/learn_hf_food_not_food_image_captions","creator_name":"Mohammad Tanvir Hasan","creator_url":"https://huggingface.co/tanvircr7","description":"inspired by @mrdbourke\\nTakes the original 250 samples \\nAdds more samples generated from mistral.ai\\nFind the code for dataset creation in this notebook: \\n[colab]{https://github.com/tanvircr7/meh/blob/master/huggingface_food_not_food_image_caption_dataset_creation.ipynb}\\n"},
  {"name":"sql-create-context","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context."},
  {"name":"WaterDrum-Ax","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\tWaterDrum: Watermarking for Data-centric Unlearning Metric\\n\\t\\n\\nWaterDrum provides an unlearning benchmark for the evaluation of effectiveness and practicality of unlearning. The repository contains the ArXiv corpus of WaterDrum (WaterDrum-Ax), which contains both unwatermarked and watermarked ArXiv paper abstracts across\\n20 categories published after the release of the Llama-2 model. Each category contains 400 data samples, aggregating into 8000 samples in the full training set. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."},
  {"name":"spider-test-portuguese","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/Boakpe/spider-test-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","description":"\\n\\t\\n\\t\\t\\n\\t\\tSpider Dataset - Vers√£o em Portugu√™s\\n\\t\\n\\nEste reposit√≥rio cont√©m a tradu√ß√£o para portugu√™s da parti√ß√£o de teste do dataset Spider, um benchmark para a tarefa de Text-to-SQL.\\n\\n\\t\\n\\t\\t\\n\\t\\tSobre esta tradu√ß√£o\\n\\t\\n\\nA tradu√ß√£o da parti√ß√£o \\\"test\\\" do Spider (contendo 2.147 inst√¢ncias) foi realizada seguindo um processo rigoroso:\\n\\nTradu√ß√£o inicial: Utilizando a API do GPT-4o mini da OpenAI\\nRevis√£o manual: Todas as 2.147 quest√µes foram revisadas e validadas manualmente\\nCrit√©rios de tradu√ß√£o:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boakpe/spider-test-portuguese."},
  {"name":"klue","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/klue/klue","creator_name":"KLUE Benchmark","creator_url":"https://huggingface.co/klue","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for KLUE\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKLUE is a collection of 8 tasks to evaluate natural language understanding capability of Korean language models. We delibrately select the 8 tasks, which are Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nTopic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/klue/klue."},
  {"name":"snli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/stanfordnlp/snli","creator_name":"Stanford NLP","creator_url":"https://huggingface.co/stanfordnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SNLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference (NLI), also known as Recognizing Textual Entailment‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/stanfordnlp/snli."},
  {"name":"xtreme","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/google/xtreme","creator_name":"Google","creator_url":"https://huggingface.co/google","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"xtreme\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/google/xtreme."},
  {"name":"SpellGram","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/vishnun/SpellGram","creator_name":"Vishnu Nandakumar","creator_url":"https://huggingface.co/vishnun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpellGram\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset consisting of grammatical and spelling errors\\n\\t\\n\\n\\nHomepage: \\nRepository: \\nPaper: \\nLeaderboard: \\nPoint of Contact:\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[train.csv]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vishnun/SpellGram."},
  {"name":"nli_zh","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/shibing624/nli_zh","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"Á∫ØÊñáÊú¨Êï∞ÊçÆÔºåÊ†ºÂºèÔºöÔºàsentence1Ôºå sentence2Ôºå labelÔºâ„ÄÇÂ∏∏ËßÅ‰∏≠ÊñáËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÂåÖÂê´ATEC„ÄÅBQ„ÄÅLCQMC„ÄÅPAWSX„ÄÅSTS-BÂÖ±5‰∏™‰ªªÂä°„ÄÇ"},
  {"name":"mnli-norwegian","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/NbAiLab/mnli-norwegian","creator_name":"Nasjonalbiblioteket AI Lab","creator_url":"https://huggingface.co/NbAiLab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMNLI Norwegian\\n\\t\\n\\nThe Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that it covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalisation evaluation. There is also a HuggingFace version of the dataset available. \\nThis dataset is machine translated using Google Translate.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NbAiLab/mnli-norwegian."},
  {"name":"jsnli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/shunk031/jsnli","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","description":"== Êó•Êú¨Ë™ûSNLI(JSNLI)„Éá„Éº„Çø„Çª„ÉÉ„Éà ==\\n\\nSNLI „Ç≥„Éº„Éë„Çπ„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„ÅüËá™ÁÑ∂Ë®ÄË™ûÊé®Ë´ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà\\nÂ≠¶Áøí„Éá„Éº„Çø„ÅØÂÖÉ„Éá„Éº„Çø„ÇíÁøªË®≥„Åó„ÄÅË®àÁÆóÊ©ü„Å´„Çà„Çã„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Å´„Çà„Å£„Å¶‰ΩúÊàê\\nË©ï‰æ°„Éá„Éº„Çø„ÅØÊó•Êú¨Ë™û„Å®„Åó„Å¶ÊÑèÂë≥„ÅåÈÄö„Çã„Åã„ÄÅÁøªË®≥Âæå„ÅÆ„É©„Éô„É´„ÅåÂÖÉ„ÅÆ„É©„Éô„É´„Å®‰∏ÄËá¥„Åó„Å¶„ÅÑ„Çã„Åã„Å©„ÅÜ„Åã„ÅÆ2ÊÆµÈöé„ÅÆ„ÇØ„É©„Ç¶„Éâ„ÇΩ„Éº„Ç∑„É≥„Ç∞„Å´„Çà„Çä„Éá„Éº„Çø„Çí„Éï„Ç£„É´„Çø„É™„É≥„Ç∞"},
  {"name":"jsnli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/shunk031/jsnli","creator_name":"Shunsuke Kitada","creator_url":"https://huggingface.co/shunk031","description":"== Êó•Êú¨Ë™ûSNLI(JSNLI)„Éá„Éº„Çø„Çª„ÉÉ„Éà ==\\n\\nSNLI „Ç≥„Éº„Éë„Çπ„ÇíÊó•Êú¨Ë™û„Å´ÁøªË®≥„Åó„ÅüËá™ÁÑ∂Ë®ÄË™ûÊé®Ë´ñ„Éá„Éº„Çø„Çª„ÉÉ„Éà\\nÂ≠¶Áøí„Éá„Éº„Çø„ÅØÂÖÉ„Éá„Éº„Çø„ÇíÁøªË®≥„Åó„ÄÅË®àÁÆóÊ©ü„Å´„Çà„Çã„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Å´„Çà„Å£„Å¶‰ΩúÊàê\\nË©ï‰æ°„Éá„Éº„Çø„ÅØÊó•Êú¨Ë™û„Å®„Åó„Å¶ÊÑèÂë≥„ÅåÈÄö„Çã„Åã„ÄÅÁøªË®≥Âæå„ÅÆ„É©„Éô„É´„ÅåÂÖÉ„ÅÆ„É©„Éô„É´„Å®‰∏ÄËá¥„Åó„Å¶„ÅÑ„Çã„Åã„Å©„ÅÜ„Åã„ÅÆ2ÊÆµÈöé„ÅÆ„ÇØ„É©„Ç¶„Éâ„ÇΩ„Éº„Ç∑„É≥„Ç∞„Å´„Çà„Çä„Éá„Éº„Çø„Çí„Éï„Ç£„É´„Çø„É™„É≥„Ç∞"},
  {"name":"rte3-multi","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/maximoss/rte3-multi","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains all manually translated versions of RTE-3 dataset, plus the original English one. The languages into which RTE-3 dataset has so far been translated are Italian (2012), German (2013), and French (2023).\\nUnlike in other repositories, both our own French version and the older Italian and German ones are here annotated in 3 classes (entailment, neutral, contradiction), and not in 2 (entailment, not‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/rte3-multi."},
  {"name":"janli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/hpprc/janli","creator_name":"Hayato TSUKAGOSHI","creator_url":"https://huggingface.co/hpprc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for JaNLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe JaNLI (Japanese Adversarial NLI) dataset, inspired by the English HANS dataset, is designed to necessitate an understanding of Japanese linguistic phenomena and to illuminate the vulnerabilities of models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe language data in JaNLI is in Japanese (BCP-47 ja-JP).\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nWhen loading a specific configuration, users has to append a version‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hpprc/janli."},
  {"name":"edgar-corpus","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/c3po-ai/edgar-corpus","creator_name":"C3PO-AI","creator_url":"https://huggingface.co/c3po-ai","description":"The dataset contains annual filings (10K) of all publicly traded firms from 1993-2020. The table data is stripped but all text is retained.\\nThis dataset allows easy access to the EDGAR-CORPUS dataset based on the paper EDGAR-CORPUS: Billions of Tokens Make The World Go Round (See References in README.md for details)."},
  {"name":"sp500-edgar-10k","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/jlohding/sp500-edgar-10k","creator_name":"Jerry Loh","creator_url":"https://huggingface.co/jlohding","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SP500-EDGAR-10K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the annual reports for all SP500 historical constituents from 2010-2022 from SEC EDGAR Form 10-K filings.\\nIt also contains n-day future returns of each firm's stock price from each filing date.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jlohding/sp500-edgar-10k."},
  {"name":"toxi-text-3M","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\\n\\n\\t\\n\\t\\t\\n\\nToxic\\nNeutral\\nTotal\\n\\n\\n\\t\\t\\nmultilingual-train-deduplicated.csv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M."},
  {"name":"nli-zh-all","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/shibing624/nli-zh-all","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"The SNLI corpus (version 1.0) is a merged chinese sentence similarity dataset, supporting the task of natural language\\ninference (NLI), also known as recognizing textual entailment (RTE)."},
  {"name":"sql-create-context-instruction","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction."},
  {"name":"TOFU","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU."},
  {"name":"FREDSum","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/linagora/FREDSum","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe FREDSum dataset is a comprehensive collection of transcripts and metadata from various political and public debates in France. The dataset aims to provide researchers, linguists, and data scientists with a rich source of debate content for analysis and natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nFrench\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is made of 144 debates, 115 of the debates make up the train set, while 29 make up the test set‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/FREDSum."},
  {"name":"human_ai_generated_text","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dmitva/human_ai_generated_text","creator_name":"DV","creator_url":"https://huggingface.co/dmitva","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHuman or AI-Generated Text\\n\\t\\n\\nThe data can be valuable for educators, policymakers, and researchers interested in the evolving education landscape, particularly in detecting or identifying texts written by Humans or Artificial Intelligence systems.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFile Name\\n\\t\\n\\nmodel_training_dataset.csv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFile Structure\\n\\t\\n\\n\\nid: Unique identifier for each record.\\nhuman_text: Human-written content.\\nai_text: AI-generated texts.\\ninstructions: Description of the task given to both‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dmitva/human_ai_generated_text."},
  {"name":"urdu-idioms-with-english-translation","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Ehtisham1328/urdu-idioms-with-english-translation","creator_name":"Ehtisham ul Hassan","creator_url":"https://huggingface.co/Ehtisham1328","description":"Ehtisham1328/urdu-idioms-with-english-translation dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"extraglue","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/PORTULAN/extraglue","creator_name":"PORTULAN","creator_url":"https://huggingface.co/PORTULAN","description":"\\n\\n\\n¬†¬†¬†¬†This is the dataset card for extraGLUE. \\n  You may be interested in some of the other datasets for Portuguese and in the models trained with them, \\n  namely Albertina (encoders) and Gerv√°sio (decoders) families.\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExtraGLUE\\n\\t\\n\\n\\n\\n\\nExtraGLUE is a Portuguese dataset obtained by the automatic translation of some of the tasks in the GLUE and SuperGLUE benchmarks.\\nTwo variants of Portuguese are considered, namely European Portuguese and American Portuguese.\\nThe dataset is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PORTULAN/extraglue."},
  {"name":"french_instruct","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/angeluriot/french_instruct","creator_name":"Angel Uriot","creator_url":"https://huggingface.co/angeluriot","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüßë‚Äçüè´ French Instruct\\n\\t\\n\\nThe French Instruct dataset is a collection of instructions with their corresponding answers (sometimes multi-turn conversations) entirely in French. The dataset is also available on GitHub.\\n\\n    \\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìä Overview\\n\\t\\n\\nThe dataset is composed of 276K conversations between a user and an assistant for a total of approximately 85M tokens.\\n\\n    \\n\\n\\nI also added annotations for each document to indicate if it was generated or written by a human, the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/angeluriot/french_instruct."},
  {"name":"afrimmlu-translate-test","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/masakhane/afrimmlu-translate-test","creator_name":"Masakhane NLP","creator_url":"https://huggingface.co/masakhane","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for afrimmlu-translate-test\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAFRIMMLU-TT is an evaluation dataset comprising translations of the AFRIMMLU dataset from 16 African languages and 1 high resource language into English using NLLB. \\nIt includes test sets across all 17 languages. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThere are 17 languages available :\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe examples look like this for English:\\nfrom datasets import load_dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/masakhane/afrimmlu-translate-test."},
  {"name":"newswire","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dell-research-harvard/newswire","creator_name":"Dell Research Harvard","creator_url":"https://huggingface.co/dell-research-harvard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NewsWire\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nNewsWire contains 2.7 million unique public domain U.S. news wire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish (en)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach year in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dell-research-harvard/newswire."},
  {"name":"FOL-nli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tasksource/FOL-nli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"FOL-nli\\\"\\n\\t\\n\\nhttps://github.com/sileod/unigram/\\nhttps://arxiv.org/abs/2406.11035\\nCitation:\\n@article{sileo2024scaling,\\n  title={Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},\\n  author={Sileo, Damien},\\n  journal={arXiv preprint arXiv:2406.11035},\\n  year={2024}\\n}\\n\\n"},
  {"name":"clinical-synthetic-text-llm","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ritaranx/clinical-synthetic-text-llm","creator_name":"Ran Xu","creator_url":"https://huggingface.co/ritaranx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nWe release the synthetic data generated using the method described in the paper Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\n (ACL 2024 Findings). The external knowledge we use is based on LLM-generated topics and writing styles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGenerated Datasets\\n\\t\\n\\nThe original train/validation/test data, and the generated synthetic training data are listed as follows. For each dataset, we generate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ritaranx/clinical-synthetic-text-llm."},
  {"name":"Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Amazon reviews full score dataset is constructed by randomly taking 600,000 training samples and 130,000 testing samples for each review score from 1 to 5. In total there are 3,000,000 trainig samples and 650,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes."},
  {"name":"Vietnamese_spelling_error","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/ShynBui/Vietnamese_spelling_error","creator_name":"Bui Tien Phat","creator_url":"https://huggingface.co/ShynBui","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVietnamese Spelling Error Dataset\\n\\t\\n\\nThis dataset contains examples of Vietnamese text with spelling errors and their corresponding corrections. It is intended to be used for training and evaluating models in spelling correction tasks, particularly for the Vietnamese language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nName: Vietnamese Spelling Error Dataset\\nLanguage: Vietnamese\\nFile Format: [CSV/Parquet/dataset/etc.]\\nColumns:\\ntext: The corresponding corrected version of the text.\\nerror_text:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShynBui/Vietnamese_spelling_error."},
  {"name":"books","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/IsmaelMousa/books","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBooks\\n\\t\\n\\nThe books dataset consists of a diverse collection of books organized into 9 categories, it splitted to train, validation where the train contains 40 books, and the validation 9 books.\\nThis dataset is cleaned well and designed to support various natural language processing (NLP) tasks, including text generation and masked language modeling.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nThe dataset contains 4 columns:\\n\\ntitle: The tilte of the book.\\nauthor: The author of the book.\\ncategory: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/books."},
  {"name":"libri-in-italiano","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/IsmaelMousa/libri-in-italiano","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLibri\\n\\t\\n\\nIl dataset dei libri consiste in una raccolta diversificata di 18 libri organizzati in 4 categorie.\\nQuesto dataset √® ben pulito e progettato per supportare diversi compiti di elaborazione del linguaggio naturale (NLP), inclusi generazione di testo, traduzione e modellazione del linguaggio mascherato.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDettagli\\n\\t\\n\\nIl dataset contiene 4 colonne:\\n\\ntitolo: Il titolo del libro.\\nautore: L'autore del libro.\\ncategoria: Il genere/categoria del libro.\\ncontenuto: Il‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/libri-in-italiano."},
  {"name":"turkish_llm_finetune_dataset_4_topics","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics","creator_name":"Barathan Aslan","creator_url":"https://huggingface.co/barathanasln","description":"\\n\\t\\n\\t\\t\\n\\t\\tTurkish LLM Finetune Dataset - 4 Topics\\n\\t\\n\\nThis dataset is designed to fine-tune the T3 AI Turkish LLM. It was created by Barathan Aslan, √ñmer Faruk √áelik, and Batuhan Kalem for the T3 AI Hackathon. The dataset focuses on four distinct topics: Agriculture, Sustainability, Turkish Education Sytem, and Turkish Law System.\\n\\n\\t\\n\\t\\t\\n\\t\\tContributors\\n\\t\\n\\n\\nBarathan Aslan (https://huggingface.co/barathanasln)\\nBatuhan Kalem(https://huggingface.co/Pancarsuyu)\\n√ñmer Faruk √áelik‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/barathanasln/turkish_llm_finetune_dataset_4_topics."},
  {"name":"Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/VIITPune/Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus","creator_name":"BRACT's Vishwakarma Institute of Information Technology","creator_url":"https://huggingface.co/VIITPune","description":"Maharashtri Prakrit to English Parallel Corpus\\nDataset Summary\\nThis dataset contains parallel text data for translating from Maharashtri Prakrit (an ancient Indo-Aryan language) to English. It is designed to aid in developing machine translation systems, language models, and linguistic research for this underrepresented language. The dataset is collected from historical texts, scriptures, and scholarly resources.\\nKey Features:\\n\\n  Source Language: Maharashtri Prakrit\\n  Target Language: English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VIITPune/Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus."},
  {"name":"Research-Papers","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/khushwant04/Research-Papers","creator_name":"Khushwant Sanwalot","creator_url":"https://huggingface.co/khushwant04","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAI & Machine Learning Research Papers Dataset\\n\\t\\n\\nThis dataset contains a curated collection of 1296 research papers focused on advancements in Artificial Intelligence and Machine Learning. It is intended as a resource for researchers, educators, and developers to explore and analyze diverse topics within AI and ML.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nTotal Papers: 1296\\nDomains Covered: \\nArtificial Intelligence (AI)\\nMachine Learning (ML)\\nDeep Learning\\nNatural Language Processing (NLP)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/khushwant04/Research-Papers."},
  {"name":"unal-repository-dataset-instruct","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset InstructDescripci√≥n: Este dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje para tareas de preguntas y respuestas.  \\nColumnas:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-instruct."},
  {"name":"ytu-araproje-absa-7400","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ebrukilic/ytu-araproje-absa-7400","creator_name":"Ebru Kƒ±lƒ±√ß","creator_url":"https://huggingface.co/ebrukilic","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nDataset Name: ABSA (Aspect-Based Sentiment Analysis)\\nSize: 7471 instances\\nLanguage(s): Turkish\\nTask: Aspect-Based Sentiment Analysis (ABSA)\\nCategories: Etek, Kaban, G√∂mlek, Kazak, Pantolon\\nPolarity: Negatif, N√∂tr, Pozitif\\nAspect: Kalite, Kuma≈ü, Renk, Beden, Kargo, Fiyat\\nLicense: MIT\\nDeveloped by: ebru kƒ±lƒ±√ß , rumeysa nur yasav\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nBu veri k√ºmesi Yƒ±ldƒ±z Teknik √úniversitesi Bilgisayar M√ºhendisliƒüi √∂ƒürencilerinin Bilgisayar Projesi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ebrukilic/ytu-araproje-absa-7400."},
  {"name":"HausaHate","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/franciellevargas/HausaHate","creator_name":"Francielle Vargas","creator_url":"https://huggingface.co/franciellevargas","description":"\\n\\t\\n\\t\\t\\n\\t\\tEvaluation Benchmark for Hausa Hate Speech Detection\\n\\t\\n\\nWe introduce the first expert annotated corpus of Facebook comments for Hausa hate speech detection. \\nThe corpus titled HausaHate comprises 2,000 comments extracted from Western African Facebook pages and\\nmanually annotated by three Hausa native speakers, who are also NLP experts. \\nThe corpus was annotated using two different layers. We first labeled each comment according to a \\nbinary classification: offensive versus‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/franciellevargas/HausaHate."},
  {"name":"resumes","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/datasetmaster/resumes","creator_name":"Oks","creator_url":"https://huggingface.co/datasetmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Advanced Resume Parser & Job Matcher Resumes\\n\\t\\n\\nThis dataset contains a merged collection of real and synthetic resume data in JSON format. The resumes have been normalized to a common schema to facilitate the development of NLP models for candidate-job matching in the technical recruitment domain.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a combined collection of real resumes and synthetically generated CVs. \\n\\nCurated by: datasetmaster‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datasetmaster/resumes."},
  {"name":"UNTreatyBodies_GeneralComments","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/UNTreatyBodiesDocSearch/UNTreatyBodies_GeneralComments","creator_name":"UN Treaty Bodies General Comments/Recommendation Database","creator_url":"https://huggingface.co/UNTreatyBodiesDocSearch","description":"This dataset contains annotated JSON files used in the UN Treaty Bodies Doc Search Flask application (https://github.com/lszoszk/UN-TreatyBodiesDocSearch, also available at: https://lszoszk.pythonanywhere.com/). \\nThe dataset enables users to search and analyze the General Comments and Recommendations adopted by UN Treaty Bodies. \\nIt is designed to facilitate keyword-based search, filtering by concerned groups, and customized labeling, making it easier to navigate and analyze official UN treaty‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UNTreatyBodiesDocSearch/UNTreatyBodies_GeneralComments."},
  {"name":"Personal-Finance-Queries","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Akhil-Theerthala/Personal-Finance-Queries","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA curated collection of Reddit posts and top comments focused on personal finance questions. The data is further filtered with the help of LLM-based Voting scores. These scores determine if the query is relevant to a person's financial queries among the other posts of the subreddits.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tFiltered Dataset (6.3k samples)\\n\\t\\n\\nColumns:\\n\\ntitle: Post title (string)\\nselftext: User‚Äôs detailed financial query (string)\\nsubreddit: Source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Personal-Finance-Queries."},
  {"name":"Tamazight-Verbs","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Tamazight/Tamazight-Verbs","creator_name":"Standard Moroccan Tamazight (ZGH)","creator_url":"https://huggingface.co/Tamazight","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains a collection of Tamazight (Berber/Amazigh) verbs along with their English meanings. The data is sourced from IRCAM (Institut Royal de la Culture Amazighe) and is presented in a tab-separated format (TSV). Each row includes a verb in Tamazight (written in Tifinagh script) and its corresponding English translation.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\n\\nTranslation: The dataset can be used for translating Tamazight verbs into English.\\nText Classification: It‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tamazight/Tamazight-Verbs."},
  {"name":"Legal-Dataset-for-india","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ShreyasP123/Legal-Dataset-for-india","creator_name":"Shreyas Patil","creator_url":"https://huggingface.co/ShreyasP123","description":"ShreyasP123/Legal-Dataset-for-india dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"NSFW_Chat_Dataset","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","description":"\\n\\t\\n\\t\\t\\n\\t\\tüíï Spicy AI GF Chat Dataset üî•\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüö® 18+ Only! NSFW & Spicy Content Ahead üö®\\n\\t\\n\\nHey there, AI enthusiasts and romance lovers! üòè Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! üíñ If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìú What‚Äôs Inside?\\n\\t\\n\\nThis dataset features two columns:\\n\\ninput ‚Üí Boyfriend‚Äôs dialogue (aka what YOU say üòâ)\\noutput ‚Üí‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset."},
  {"name":"americas_nli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/nala-cub/americas_nli","creator_name":"Kann Natural Language Processing Group (NALA)","creator_url":"https://huggingface.co/nala-cub","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for AmericasNLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAmericasNLI is an extension of XNLI (Conneau et al., 2018) a natural language inference (NLI) dataset covering 15 high-resource languages to 10 low-resource indigenous languages spoken in the Americas: Ashaninka, Aymara, Bribri, Guarani, Nahuatl, Otomi, Quechua, Raramuri, Shipibo-Konibo, and Wixarika. As with MNLI, the goal is to predict textual entailment (does sentence A imply/contradict/neither sentence B) and is a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nala-cub/americas_nli."},
  {"name":"bbc_hindi_nli","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/midas/bbc_hindi_nli","creator_name":"MIDAS Research Laboratory, IIIT-Delhi","creator_url":"https://huggingface.co/midas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BBC Hindi NLI Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nDataset for Natural Language Inference in Hindi Language. BBC Hindi Dataset consists of textual-entailment pairs.\\nEach row of the Datasets if made up of 4 columns - Premise, Hypothesis, Label and Topic.\\nContext and Hypothesis is written in Hindi while Entailment_Label is in English.\\nEntailment_label is of 2 types - entailed and not-entailed.\\nDataset can be used to train models for Natural Language Inference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/midas/bbc_hindi_nli."},
  {"name":"kor_nli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/kakaobrain/kor_nli","creator_name":"Kakao Brain","creator_url":"https://huggingface.co/kakaobrain","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"kor_nli\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKorean Natural Language Inference datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nMore Information Needed\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tmulti_nli\\n\\t\\n\\n\\nSize of downloaded dataset files: 42.11 MB\\nSize of the generated dataset: 84.72 MB\\nTotal amount of disk used: 126.85 MB\\n\\nAn example of 'train' looks as follows.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kakaobrain/kor_nli."},
  {"name":"ik-nlp-22_transqe","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/GroNLP/ik-nlp-22_transqe","creator_name":"GroNLP","creator_url":"https://huggingface.co/GroNLP","description":"The e-SNLI dataset extends the Stanford Natural Language Inference Dataset to\\ninclude human-annotated natural language explanations of the entailment\\nrelations. This version includes an automatic translation to Dutch and two quality estimation annotations\\nfor each translated field."},
  {"name":"overlim","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/KBLab/overlim","creator_name":"National Library of Sweden / KBLab","creator_url":"https://huggingface.co/KBLab","description":""},
  {"name":"sentihood","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/bhavnicksm/sentihood","creator_name":"Bhavnick Minhas","creator_url":"https://huggingface.co/bhavnicksm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [SentiHood]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nCreated as a part of the paper \\\"SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods\\\" by Saeidi et al. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbstract\\n\\t\\n\\nIn this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bhavnicksm/sentihood."},
  {"name":"copa_hr","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/classla/copa_hr","creator_name":"CLASSLA - CLARIN Knowledge Centre for South Slavic Languages","creator_url":"https://huggingface.co/classla","description":"The COPA-HR dataset (Choice of plausible alternatives in Croatian) is a translation \\nof the English COPA dataset (https://people.ict.usc.edu/~gordon/copa.html) by following the \\nXCOPA dataset translation methodology (https://arxiv.org/abs/2005.00333). The dataset consists of 1000 premises \\n(My body cast a shadow over the grass), each given a question (What is the cause?), and two choices \\n(The sun was rising; The grass was cut), with a label encoding which of the choices is more plausible \\ngiven the annotator or translator (The sun was rising).\\n\\nThe dataset is split into 400 training samples, 100 validation samples, and 500 test samples. It includes the \\nfollowing features: 'premise', 'choice1', 'choice2', 'label', 'question', 'changed' (boolean)."},
  {"name":"tr-qnli","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/nlpyeditepe/tr-qnli","creator_name":"Yeditepe NLP Lab","creator_url":"https://huggingface.co/nlpyeditepe","description":"nlpyeditepe/tr-qnli dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"tr_rte","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/nlpyeditepe/tr_rte","creator_name":"Yeditepe NLP Lab","creator_url":"https://huggingface.co/nlpyeditepe","description":"nlpyeditepe/tr_rte dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"wnli-ca","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/projecte-aina/wnli-ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWNLI-ca\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\\"A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution. The schema takes its name from Terry Winograd.\\\" Source: The Winograd Schema Challenge.\\nThe Winograd NLI dataset presents 855 sentence pairs, in which the first sentence contains an ambiguity and the second‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/wnli-ca."},
  {"name":"adv_glue","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/AI-Secure/adv_glue","creator_name":"Secure Learning Lab","creator_url":"https://huggingface.co/AI-Secure","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Adversarial GLUE\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAdversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models. It covers five natural language understanding tasks from the famous GLUE tasks and is an adversarial version of GLUE benchmark.\\nAdvGLUE considers textual adversarial attacks from different perspectives and hierarchies, including word-level transformations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI-Secure/adv_glue."},
  {"name":"indicxnli","keyword":"natural-language-inference","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/Divyanshu/indicxnli","creator_name":"Divyanshu Aggarwal","creator_url":"https://huggingface.co/Divyanshu","description":"IndicXNLI is a translated version of XNLI to 11 Indic Languages. As with XNLI, the goal is\\nto predict textual entailment (does sentence A imply/contradict/neither sentence\\nB) and is a classification task (given two sentences, predict one of three\\nlabels)."},
  {"name":"WANLI","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/alisawuffles/WANLI","creator_name":"Alisa Liu","creator_url":"https://huggingface.co/alisawuffles","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for WANLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWANLI (Worker-AI Collaboration for NLI) is a collection of 108K English sentence pairs for the task of natural language inference (NLI).\\nEach example is created by first identifying a \\\"pocket\\\" of examples in MultiNLI (Williams et al., 2018) that share a challenging reasoning pattern, then instructing GPT-3 to write a new example with the same pattern.\\nThe set of generated examples are automatically filtered to contain those‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alisawuffles/WANLI."},
  {"name":"BBNLI","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/feyzaakyurek/BBNLI","creator_name":"Afra Feyza Akyurek","creator_url":"https://huggingface.co/feyzaakyurek","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BBNLI\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nBBNLI (Bias Benchmark for Natural Language Inference) is bias measurement benchmark for the tasks of both natural language inference and question answering. BBNLI consists of 16 subtopics each tailored to measure a specific stereotype that is negatively impacting certain classes. Each subtopic includes a set of 3 to 11 premises,  5 to 11 stereotypical hypotheses that are geared towards measuring biases and 3 to 5 test‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/feyzaakyurek/BBNLI."},
  {"name":"123_test","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/JeremyAlain/123_test","creator_name":"J√©r√©my Scheurer","creator_url":"https://huggingface.co/JeremyAlain","description":"The Fewshot Table dataset consists of tables that naturally occur on the web, that are formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. The dataset consists of approximately 413K tables that are extracted from the WDC Web Table Corpora 2015, which is released under the Apache-2.0 license. The WDC Web Table Corpora \\\"contains vast amounts of HTML tables. [...] The Web Data Commons project extracts relational Web tables from the Common Crawl, the largest and most up-to-date Web corpus that is currently available to the public."},
  {"name":"core-2020-05-10-deduplication","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/pinecone/core-2020-05-10-deduplication","creator_name":"Pinecone","creator_url":"https://huggingface.co/pinecone","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for CORE Deduplication\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nCORE 2020 Deduplication dataset (https://core.ac.uk/documentation/dataset) contains 100K scholarly documents labeled as duplicates/non-duplicates.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset language is English (BCP-47 en)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation Information\\n\\t\\n\\n@inproceedings{dedup2020,\\n  title={Deduplication of Scholarly Documents using Locality Sensitive Hashing and Word Embeddings},\\n  author={Gyawali, Bikash and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pinecone/core-2020-05-10-deduplication."},
  {"name":"unpredictable_full","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_full","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_mmo-champion-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_mmo-champion-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_baseball-fantasysports-yahoo-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_baseball-fantasysports-yahoo-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_phonearena-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_phonearena-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_support-google-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_support-google-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_dividend-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_dividend-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_bulbapedia-bulbagarden-net","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_bulbapedia-bulbagarden-net","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_wkdu-org","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_wkdu-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_dummies-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_dummies-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_mgoblog-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_mgoblog-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_gamefaqs-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_gamefaqs-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_studystack-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_studystack-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_sittercity-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_sittercity-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_msdn-microsoft-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_msdn-microsoft-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cappex-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cappex-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_en-wikipedia-org","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_en-wikipedia-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cram-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cram-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_w3-org","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_w3-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_sporcle-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_sporcle-com","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_wiki-openmoko-org","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_wiki-openmoko-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_ensembl-org","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_ensembl-org","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"FLUTE","keyword":"natural-language-inference","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/ColumbiaNLP/FLUTE","creator_name":"Columbia University NLP Group","creator_url":"https://huggingface.co/ColumbiaNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for FigLang2022SharedTask\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nModel in the loop approach for fig lang generation and explainability\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration Rationale\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tInitial Data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ColumbiaNLP/FLUTE."},
  {"name":"unpredictable_5k","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_5k","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_unique","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_unique","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster-noise","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster-noise","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster00","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster00","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster01","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster01","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster10","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster10","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster11","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster11","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster12","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster12","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster13","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster13","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster14","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster14","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster15","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster15","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster16","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster16","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster17","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster17","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster18","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster18","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster19","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster19","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster02","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster02","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster20","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster20","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster21","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster21","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster22","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster22","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster23","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster23","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster24","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster24","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster25","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster25","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster26","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster26","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"surname-nationality","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Hobson/surname-nationality","creator_name":"Hobson Lane","creator_url":"https://huggingface.co/Hobson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPopular Surname Nationality Mapping\\n\\t\\n\\nSample of popular surnames for 30+ countries labeled with nationality (language)\\n"},
  {"name":"unpredictable_cluster27","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster27","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster28","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster28","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster29","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster29","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster03","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster03","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster04","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster04","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster05","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster05","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster06","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster06","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster07","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster07","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster08","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster08","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_cluster09","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_cluster09","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_rated-low","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-low","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_rated-medium","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-medium","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_rated-high","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MicPie/unpredictable_rated-high","creator_name":"Michael Pieler","creator_url":"https://huggingface.co/MicPie","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"scientific-exaggeration-detection","keyword":"natural-language-inference","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/copenlu/scientific-exaggeration-detection","creator_name":"CopeNLU","creator_url":"https://huggingface.co/copenlu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Scientific Exaggeration Detection\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPublic trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/copenlu/scientific-exaggeration-detection."},
  {"name":"unpredictable_full","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/unpredictable/unpredictable_full","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_5k","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/unpredictable/unpredictable_5k","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_support-google-com","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/unpredictable/unpredictable_support-google-com","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"unpredictable_unique","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/unpredictable/unpredictable_unique","creator_name":"unpredictable","creator_url":"https://huggingface.co/unpredictable","description":"The UnpredicTable dataset consists of web tables formatted as few-shot tasks for fine-tuning language models to improve their few-shot performance. For more details please see the accompanying dataset card."},
  {"name":"glue-ci","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/evaluate/glue-ci","creator_name":"evaluate","creator_url":"https://huggingface.co/evaluate","description":"GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems."},
  {"name":"brwac_tiny","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/thegoodfellas/brwac_tiny","creator_name":"The Good Fellas","creator_url":"https://huggingface.co/thegoodfellas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BrWac\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe BrWaC (Brazilian Portuguese Web as Corpus) is a large corpus constructed following the Wacky framework, \\nwhich was made public for research purposes. The current corpus version, released in January 2017, is composed by \\n3.53 million documents, 2.68 billion tokens and 5.79 million types. Please note that this resource is available \\nsolely for academic research purposes, and you agreed not to use it for any commercial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thegoodfellas/brwac_tiny."},
  {"name":"inferes","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/venelin/inferes","creator_name":"Venelin Kovatchev","creator_url":"https://huggingface.co/venelin","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for InferES\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nNatural Language Inference dataset for European Spanish\\nPaper accepted and (to be) presented at COLING 2022\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nSpanish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset contains two texts inputs (Premise and Hypothesis), Label for three-way classification, and annotation data.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\ntrain size = 6444 \\ntest‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/venelin/inferes."},
  {"name":"ALotNLI","keyword":"natural-language-inference","license":"GNU Affero General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/Thamognya/ALotNLI","creator_name":"Thamognya Kodi","creator_url":"https://huggingface.co/Thamognya","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tRepo\\n\\t\\n\\nGithub Repo: thamognya/TBertNLI specifically in the src/data directory.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSample\\n\\t\\n\\n0  this church choir sings to the masses as they ...      the church is filled with song      0\\n1  this church choir sings to the masses as they ...  a choir singing at a baseball game      2\\n2  a woman with a green headscarf blue shirt and ...                  the woman is young      1\\n3  a woman with a green headscarf blue shirt and ...             the woman is very happy      0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Thamognya/ALotNLI."},
  {"name":"nan-nli","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/joey234/nan-nli","creator_name":"Thinh Truong","creator_url":"https://huggingface.co/joey234","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for [Dataset Name]\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference\\nText Classification\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nen\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\npremise:\\nhypothesis:\\nlabel:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\nEvaluation: 258 samples\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration Rationale\\n\\t\\n\\nExtracting samples corresponding to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/joey234/nan-nli."},
  {"name":"glue","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/severo/glue","creator_name":"Sylvain Lesage","creator_url":"https://huggingface.co/severo","description":"GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems."},
  {"name":"probability_words_nli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Probing neural language models for understanding of words of estimative probability"},
  {"name":"probability_words_nli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Probing neural language models for understanding of words of estimative probability"},
  {"name":"snli-cf-kaushik","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/sagnikrayc/snli-cf-kaushik","creator_name":"sagnik ray choudhury","creator_url":"https://huggingface.co/sagnikrayc","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). In the ICLR 2020 paper [Learning the Difference that Makes a Difference with Counterfactually-Augmented Data](https://openreview.net/forum?id=Sklgs0NFvr), Kaushik et. al. provided a dataset with counterfactual perturbations on the SNLI and IMDB data. This repository contains the original and counterfactual perturbations for the SNLI data, which was generated after processing the original data from [here](https://github.com/acmi-lab/counterfactually-augmented-data)."},
  {"name":"panda","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/facebook/panda","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PANDA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPANDA (Perturbation Augmentation NLP DAtaset) consists of approximately 100K pairs of crowdsourced human-perturbed text snippets (original, perturbed). Annotators were given selected terms and target demographic attributes, and instructed to rewrite text snippets along three demographic axes: gender, race and age, while preserving semantic meaning. Text snippets were sourced from a range of text corpora (BookCorpus, Wikipedia‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/panda."},
  {"name":"syntactic-augmentation-nli","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/metaeval/syntactic-augmentation-nli","creator_name":"metaeval","creator_url":"https://huggingface.co/metaeval","description":"https://github.com/Aatlantise/syntactic-augmentation-nli/tree/master/datasets\\n@inproceedings{min-etal-2020-syntactic,\\n    title = \\\"Syntactic Data Augmentation Increases Robustness to Inference Heuristics\\\",\\n    author = \\\"Min, Junghyun  and\\n      McCoy, R. Thomas  and\\n      Das, Dipanjan  and\\n      Pitler, Emily  and\\n      Linzen, Tal\\\",\\n    booktitle = \\\"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\\\",\\n    month = jul,\\n    year = \\\"2020\\\",\\n    address =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/metaeval/syntactic-augmentation-nli."},
  {"name":"WarOnline","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/kertser/WarOnline","creator_name":"Mike Kertser","creator_url":"https://huggingface.co/kertser","description":"This is a conversational dataset, collected from WarOnine Israeli military forum\\nLanguage = Russian (with hebrew addins)\\nTarget Audience = Military\\nDataset has been used to train a Military Chat Bot\\n"},
  {"name":"NLP-KnowledgeGraph","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/vishnun/NLP-KnowledgeGraph","creator_name":"Vishnu Nandakumar","creator_url":"https://huggingface.co/vishnun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKG dataset created by using spaCy PoS and Dependency parser. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nCan be leveraged for token classification for detection of knowledge graph entities and relations.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nImportant fields for the token classification task are\\n\\ntokens -‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vishnun/NLP-KnowledgeGraph."},
  {"name":"ro-offense","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/readerbench/ro-offense","creator_name":"ReaderBench","creator_url":"https://huggingface.co/readerbench","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"RO-Offense-Sequences\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/readerbench/ro-offense-sequences\\nRepository: https://github.com/readerbench/ro-offense-sequences\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\na novel Romanian language dataset for offensive language detection with manually \\nannotated offensive labels from a local Romanian sports news website (gsp.ro):\\nResulting in 12,445 annotated messages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/readerbench/ro-offense."},
  {"name":"defeasible-nli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tasksource/defeasible-nli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://github.com/rudinger/defeasible-nli\\n@inproceedings{rudinger2020thinking,\\n  title={Thinking like a skeptic: \\n  feasible inference in natural language},\\n  author={Rudinger, Rachel and Shwartz, Vered and Hwang, Jena D and Bhagavatula, Chandra and Forbes, Maxwell and Le Bras, Ronan and Smith, Noah A and Choi, Yejin},\\n  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},\\n  pages={4661--4675},\\n  year={2020}\\n}\\n\\n"},
  {"name":"attempto-nli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sileod/attempto-nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Natural language inference using attempto controlled english \\nPaper to come\\n@inproceedings{fuchs2012first,\\n  title={First-order reasoning for attempto controlled english},\\n  author={Fuchs, Norbert E},\\n  booktitle={Controlled Natural Language: Second International Workshop, CNL 2010, Marettimo Island, Italy, September 13-15, 2010. Revised Papers 2},\\n  pages={73--94},\\n  year={2012},\\n  organization={Springer}\\n}\\n\\n"},
  {"name":"lonli","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/tasksource/lonli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://github.com/microsoft/LoNLI\\n@article{Tarunesh2021TrustingRO,\\n  title={Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task},\\n  author={Ishan Tarunesh and Somak Aditya and Monojit Choudhury},\\n  journal={ArXiv},\\n  year={2021},\\n  volume={abs/2107.07229}\\n}\\n\\n"},
  {"name":"tomi-nli","keyword":"natural-language-inference","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/tasksource/tomi-nli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"tomi dataset (theory of mind question answering) recasted as natural language inference\\nhttps://colab.research.google.com/drive/1J_RqDSw9iPxJSBvCJu-VRbjXnrEjKVvr?usp=sharing\\n@article{sileo2023tasksource,\\n  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},\\n  author={Sileo, Damien},\\n  url= {https://arxiv.org/abs/2301.05948},\\n  journal={arXiv preprint arXiv:2301.05948},\\n  year={2023}\\n}‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/tomi-nli."},
  {"name":"autotnli","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tasksource/autotnli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://github.com/Dibyakanti/AutoTNLI-code\\n@inproceedings{kumar-etal-2022-autotnli,\\n            title = \\\"Realistic Data Augmentation Framework for Enhancing Tabular Reasoning\\\",\\n            author = \\\"Kumar, Dibyakanti  and\\n              Gupta, Vivek  and\\n              Sharma, Soumya  and\\n              Zhang, Shuo\\\",\\n            booktitle = \\\"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\\\",\\n            month = dec,\\n            year = \\\"2022\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tasksource/autotnli."},
  {"name":"puzzte","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tasksource/puzzte","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://bitbucket.org/RoxanaSz/puzzte/src/master/\\n@article{szomiu2021puzzle,\\n  title={A Puzzle-Based Dataset for Natural Language Inference},\\n  author={Szomiu, Roxana and Groza, Adrian},\\n  journal={arXiv preprint arXiv:2112.05742},\\n  year={2021}\\n}\\n\\n"},
  {"name":"lingnli-multi-mt","keyword":"natural-language-inference","license":"BSD 2-Clause \"Simplified\" License","language":"en","url":"https://huggingface.co/datasets/maximoss/lingnli-multi-mt","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains a collection of machine translations of LingNLI dataset \\ninto 9 different languages (Bulgarian, Finnish, French, Greek, Italian, Korean, Lithuanian, Portuguese, Spanish). The goal is to predict textual entailment (does sentence A \\nimply/contradict/neither sentence B), which is a classification task (given two sentences, \\npredict one of three labels). It is here formatted in the same manner as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/lingnli-multi-mt."},
  {"name":"mnli-nineeleven-fr-mt","keyword":"natural-language-inference","license":"BSD 2-Clause \"Simplified\" License","language":"en","url":"https://huggingface.co/datasets/maximoss/mnli-nineeleven-fr-mt","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains a machine-translated French version of the portion of MultiNLI concerning the 9/11 terrorist attacks (2000 examples).\\nNote that these 2000 examples included in MultiNLI (and machine translated in French here) on the subject of 9/11 are different from the 249 examples in the validation subset and the 501 ones in the test subset of XNLI on the same subject.\\nIn the original subset of MultiNLI on‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/mnli-nineeleven-fr-mt."},
  {"name":"black-box-api-challenges","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/CohereForAI/black-box-api-challenges","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nPaper: On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research\\nAbstract: Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/black-box-api-challenges."},
  {"name":"gqnli-fr","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/maximoss/gqnli-fr","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis repository contains a manually translated French version of the GQNLI challenge dataset, originally written in English. GQNLI is an evaluation corpus that is aimed for testing language model's generalized quantifier reasoning ability.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be used for the task of Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/gqnli-fr."},
  {"name":"jsick","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/hpprc/jsick","creator_name":"Hayato TSUKAGOSHI","creator_url":"https://huggingface.co/hpprc","description":"Japanese Sentences Involving Compositional Knowledge (JSICK) Dataset.\\nJSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.\\nWe hope that our dataset will be useful in research for realizing more advanced models that are capable of appropriately performing multilingual compositional inference.\\n(from official website)"},
  {"name":"fast-flash-hackernews-posts","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-posts","creator_name":"Fast Flash Studio ‚Äî a Multidisciplinary Creative Studio","creator_url":"https://huggingface.co/fast-flash","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFast Flash | HackerNews Posts Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExploratory Analysis\\n\\t\\n\\nTake a look at some fascinating findings from this dataset on our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe release dataset of all HackerNews posts.\\nThe dataset includes 35,316,999 posts and was collected in March 2023. \\nYou can also find a dataset of all users right here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe post objects in this dataset are structured according to HackerNews' API specification.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-posts."},
  {"name":"fast-flash-hackernews-users","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-users","creator_name":"Fast Flash Studio ‚Äî a Multidisciplinary Creative Studio","creator_url":"https://huggingface.co/fast-flash","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFast Flash | HackerNews Users Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExploratory Analysis\\n\\t\\n\\nTake a look at some fascinating findings from this dataset on our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe release dataset of all HackerNews users who have posted at least once. \\nThe dataset includes 853,840 users and was collected on Sunday, March 26, 2023. \\nYou can find a dataset of all posts right here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe user objects in this dataset are structured according to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-users."},
  {"name":"sql-create-context-copy","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFork of b-mc2/sql-create-context\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy."},
  {"name":"mindgames","keyword":"natural-language-inference","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","description":"Mindgame dataset\\nCode:\\nhttps://github.com/sileod/llm-theory-of-mind\\nArticle (Accepted at EMNLP 2023 Findings):\\nhttps://arxiv.org/abs/2305.03353\\n@article{sileo2023mindgames,\\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\\n  author={Sileo, Damien and Lernould, Antoine},\\n  journal={arXiv preprint arXiv:2305.03353},\\n  year={2023}\\n}\\n\\n"},
  {"name":"propsegment","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/sihaochen/propsegment","creator_name":"Sihao Chen","creator_url":"https://huggingface.co/sihaochen","description":"This is a reproduced (i.e. after web-crawling) and processed version of the \\\"PropSegment\\\" dataset from Google Research.\\n\\nSince the News portion of the dataset is released only via urls, we reconstruct the dataset by crawling. Overall, ~96% \\nof the dataset can be reproduced, and the rest ~4% either have url no longer valid, or sentences that have been edited \\n(i.e. cannot be aligned with the orignial dataset).\\n\\nPropSegment (Proposition-level Segmentation and Entailment) is a large-scale, human annotated dataset for segmenting \\nEnglish text into propositions, and recognizing proposition-level entailment relations --- whether a different, related \\ndocument entails each proposition, contradicts it, or neither.\\n\\nThe original dataset features >45k human annotated propositions, i.e. individual semantic units within sentences, as \\nwell as >45k entailment labels between propositions and documents."},
  {"name":"tajik-text-segmentation","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sobir-hf/tajik-text-segmentation","creator_name":"sobir","creator_url":"https://huggingface.co/sobir-hf","description":"This dataset contains texts in Tajik language with sentence annotations. It can be used to train and evaluate sentence-wise text segmentation algorithms.\\nThe dataset contains more than 100 short and long texts and more than 3000 annotated sentences. The texts were carefully selected from different catergories \\nsuch as news, articles, novels, classical texts, poetry, and religious texts. It deliberately contains more of \\\"hard\\\" passages where splitting them by period \\\".\\\" characters would result‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sobir-hf/tajik-text-segmentation."},
  {"name":"SynCSE-partial-NLI","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/hkust-nlp/SynCSE-partial-NLI","creator_name":"HKUST NLP Group","creator_url":"https://huggingface.co/hkust-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SynCSE-scratch-NLI is a Natural Language Inference dataset generated by GPT-3.5-Turbo. You can use it to learn better sentence representation with contrastive learning. More details can be found in paper and code\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference\\nContrastive Learning of Sentence Embeddings\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hkust-nlp/SynCSE-partial-NLI."},
  {"name":"SynCSE-scratch-NLI","keyword":"natural-language-inference","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/hkust-nlp/SynCSE-scratch-NLI","creator_name":"HKUST NLP Group","creator_url":"https://huggingface.co/hkust-nlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SynCSE-scratch-NLI is a Natural Language Inference dataset generated by GPT-3.5-Turbo. You can use it to learn better sentence representation with contrastive learning. More details can be found in paper and code\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nNatural Language Inference\\nContrastive Learning of Sentence Embeddings\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hkust-nlp/SynCSE-scratch-NLI."},
  {"name":"scone","keyword":"natural-language-inference","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/tasksource/scone","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","description":"https://github.com/selenashe/ScoNe\\nNLI subset, original part (excluding one-scope)\\n@misc{she2023scone,\\n      title={ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning}, \\n      author={Jingyuan Selena She and Christopher Potts and Samuel R. Bowman and Atticus Geiger},\\n      year={2023},\\n      eprint={2305.19426},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\\n"},
  {"name":"COPA-ca","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/projecte-aina/COPA-ca","creator_name":"Projecte Aina","creator_url":"https://huggingface.co/projecte-aina","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for COPA-ca\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe COPA-ca dataset (Choice of plausible alternatives in Catalan) is a professional translation of the English COPA dataset into Catalan, commissioned by BSC LangTech Unit. The dataset consists of 1000 premises, each given a question and two choices with a label encoding which of the choices is more plausible given the annotator.\\nThe dataset is split into 400 training samples, 100 validation samples, and 500 test samples.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/projecte-aina/COPA-ca."},
  {"name":"snli-zh","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/shibing624/snli-zh","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English\\nsentence pairs manually labeled for balanced classification with the labels\\nentailment, contradiction, and neutral, supporting the task of natural language\\ninference (NLI), also known as recognizing textual entailment (RTE)."},
  {"name":"rte3-french","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/maximoss/rte3-french","creator_name":"Maximos Skandalis","creator_url":"https://huggingface.co/maximoss","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe RTE3-FR dataset is the French translation of the Textual Entailment English dataset used in the RTE-3 Challenge. \\nLike its English counterpart, the French RTE-3 dataset is composed of a development set and a test set, each containing 800 T/H pairs. \\nAll T/H pairs were manually translated into French and proofread.\\nIt is annotated for a 3-way task.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nThis dataset can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maximoss/rte3-french."},
  {"name":"sts-sohu2021","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/shibing624/sts-sohu2021","creator_name":"Ming Xu (ÂæêÊòé)","creator_url":"https://huggingface.co/shibing624","description":"2021ÊêúÁãêÊ†°Âõ≠ÊñáÊú¨ÂåπÈÖçÁÆóÊ≥ïÂ§ßËµõÊï∞ÊçÆÈõÜ"},
  {"name":"Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data","keyword":"nlp","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data","creator_name":"Senem Aktas","creator_url":"https://huggingface.co/Senem","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe dataset is a collection of  Youtube Comments and it was captured using the YouTube Data API. \\nThe data set consists of 1500 nostalgic and non-nostalgic comments in English.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe language of the data is English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you find this dataset usefull for your study, please cite the paper as followed:\\n@article{postalcioglu2020comparison,\\n  title={Comparison of Neural Network Models for Nostalgic Sentiment Analysis of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data."},
  {"name":"all-scam-spam","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FredZhang7/all-scam-spam","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large corpus of 42,619 preprocessed text messages and emails sent by humans in 43 languages. is_spam=1 means spam and is_spam=0 means ham.\\n1040 rows of balanced data, consisting of casual conversations and scam emails in ‚âà10 languages, were manually collected and annotated by me, with some help from ChatGPT.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSome preprcoessing algorithms\\n\\t\\n\\n\\nspam_assassin.js, followed by spam_assassin.py\\nenron_spam.py\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData composition\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/all-scam-spam."},
  {"name":"skb","keyword":"natural-language-inference","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/min9805/skb","creator_name":"min","creator_url":"https://huggingface.co/min9805","description":"min9805/skb dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"guanaco-extended","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FinchResearch/guanaco-extended","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHugging Face Dataset Card: Amoeba Mixed AI-Human Generated Samples\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nAmoeba Mixed AI-Human Generated Samples is a massive dataset that contains a diverse collection of text samples generated by both AI models and human authors. With a size exceeding 13 GB, this dataset is designed to foster research and development in the field of natural language generation and understanding.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntended Use\\n\\t\\n\\nThe Amoeba Mixed AI-Human Generated Samples dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FinchResearch/guanaco-extended."},
  {"name":"reBERT-forchat-incomplete","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FinchResearch/reBERT-forchat-incomplete","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"FinchResearch/reBERT-forchat-incomplete dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"pallas_splitted_18c","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FinchResearch/pallas_splitted_18c","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"FinchResearch/pallas_splitted_18c dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"text_coordinates_seasons","keyword":"natural-language-processing","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/yachay/text_coordinates_seasons","creator_name":"Yachay AI","creator_url":"https://huggingface.co/yachay","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Geo-Tagged Social Media Posts with Timestamps\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe \\\"Seasons\\\" dataset is a collection of over 600,000 social media posts spanning 12 months and encompassing 15 distinct time zones. It focuses on six countries: Cuba, Iran, Russia, North Korea, Syria, and Venezuela, with each post containing textual content, timestamps, and geographical coordinates. The dataset's primary objective is to investigate the correlation between the timing of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yachay/text_coordinates_seasons."},
  {"name":"fake_news_en_opensources","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/andyP/fake_news_en_opensources","creator_name":"Andrei Paraschiv","creator_url":"https://huggingface.co/andyP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Fake News Opensources\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/AndyTheFactory/FakeNewsDataset\\nRepository: https://github.com/AndyTheFactory/FakeNewsDataset\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\na consolidated and cleaned up version of the opensources Fake News dataset\\nFake News Corpus comprises 8,529,090 individual articles, classified into 12 classes: reliable, unreliable, political, bias, fake‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyP/fake_news_en_opensources."},
  {"name":"text_coordinates_regions","keyword":"natural-language-processing","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/yachay/text_coordinates_regions","creator_name":"Yachay AI","creator_url":"https://huggingface.co/yachay","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Multilingual Geo-Tagged Social Media Posts (by 123 world regions)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe \\\"Regions\\\" dataset is a multilingual corpus that encompasses textual data from the 123 most populated regions worldwide, with each region's data organized into separate .json files. This dataset consists of approximately 500,000 text samples, each paired with its geographic coordinates.\\nKey Features:\\n\\nTextual Data: The dataset contains 500,000 text samples.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yachay/text_coordinates_regions."},
  {"name":"msmarco-10k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/nixiesearch/msmarco-10k","creator_name":"Nixiesearch","creator_url":"https://huggingface.co/nixiesearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA 10K docs sample from MS MARCO\\n\\t\\n\\nThis is a sample dataset of random 10K rows from the MS MARCO dataset. This is used in Nixiesearch quickstart guide to save some time indexing a full MSMARCO with 8M documents.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema\\n\\t\\n\\nThis is a JSONL-formatted dataset with only two fields inside: id for document identifier and text for the actual text snippet.\\n{\\n  \\\"id\\\": \\\"0\\\",\\n  \\\"text\\\": \\\"The presence of communication amid scientific minds was equally important to the success of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nixiesearch/msmarco-10k."},
  {"name":"sql-parsed","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"ro-paraphrase-bible","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/andyP/ro-paraphrase-bible","creator_name":"Andrei Paraschiv","creator_url":"https://huggingface.co/andyP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Romanian Bible Paraphrase Corpus\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/AndyTheFactory/ro-paraphrase-bible\\nRepository: https://github.com/AndyTheFactory/ro-paraphrase-bible\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nA paraphprase corpus created from 10 different Romanian language Bible versions. Since the Bible has all paragraphs uniquely numbered an alignment between two \\nversions is straighforward. \\nWe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyP/ro-paraphrase-bible."},
  {"name":"sgd_dst","keyword":"natural-language-processing","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/shermansiu/sgd_dst","creator_name":"Sherman Siu","creator_url":"https://huggingface.co/shermansiu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema-Guided Dialogue dataset - Dialogue State Tracking\\n\\t\\n\\nThis dataset contains the Schema-Guided Dialogue Dataset, formatted according to the prompt formats from the following two dialogue state tracking papers:\\n\\nDescription-Driven Dialogue State Tracking (D3ST) (Zhao et al., 2022)\\nShow, Don't Tell (SDT) (Gupta et al., 2022)\\n\\nData processing code: https://github.com/google-research/task-oriented-dialogueOriginal dataset:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shermansiu/sgd_dst."},
  {"name":"SDOH-NLI","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/davanstrien/SDOH-NLI","creator_name":"Daniel van Strien","creator_url":"https://huggingface.co/davanstrien","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/davanstrien/SDOH-NLI."},
  {"name":"kor_snli","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/KETI-AIR/kor_snli","creator_name":"Korea Electronics Technology Institute Artificial Intelligence Research Center","creator_url":"https://huggingface.co/KETI-AIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for QASC\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicensing Information\\n\\t\\n\\nThe data is distributed under the CC BY 4.0 license.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data Citation INformation\\n\\t\\n\\n@inproceedings{snli:emnlp2015,\\n    Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},\\n    Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\\n    Publisher = {Association for Computational Linguistics},\\n    Title =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/KETI-AIR/kor_snli."},
  {"name":"sql-create-context-id","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a fork from sql-create-context \\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id."},
  {"name":"english_to_igbo","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ccibeekeoc42/english_to_igbo","creator_name":"Chinemerem Christopher Ibe-Ekeocha","creator_url":"https://huggingface.co/ccibeekeoc42","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish-Igbo Parallel Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a comprehensive collection of parallel sentences in English and Igbo. It has been compiled from multiple sources to create a rich resource for machine translation, language research, and natural language processing tasks. The dataset is particularly valuable for those focusing on Igbo, a language spoken primarily in Nigeria, which is underrepresented in the field of computational linguistics.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ccibeekeoc42/english_to_igbo."},
  {"name":"hf-blogs","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/chenglu/hf-blogs","creator_name":"Luke Cheng","creator_url":"https://huggingface.co/chenglu","description":"Hugging Face Blog Content..\\n"},
  {"name":"pandas-create-context","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/hiltch/pandas-create-context","creator_name":"Or Hiltch","creator_url":"https://huggingface.co/hiltch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built from sql-create-context, which in itself builds from WikiSQL and Spider.\\nI have used GPT4 to translate the SQL schema into pandas DataFrame schem initialization statements and to translate the SQL queries into pandas queries. \\nThere are 862 examples of natural language queries, pandas DataFrame creation statements, and pandas query answering the question using the DataFrame creation statement as context. This dataset was built with text-to-pandas‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hiltch/pandas-create-context."},
  {"name":"Contextual_Response_Evaluation_for_ESL_and_ASD_Support","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/yunjaeys/Contextual_Response_Evaluation_for_ESL_and_ASD_Support","creator_name":"Eric Soderquist","creator_url":"https://huggingface.co/yunjaeys","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Contextual Response Evaluation for ESL and ASD Supportüíúüí¨üåê\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description üìñ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary üìù\\n\\t\\n\\nCurated by Eric Soderquist, this dataset is a collection of English prompts and responses generated by the Phi-2 model, designed to evaluate and improve NLP models for supporting ESL (English as a Second Language) and ASD (Autism Spectrum Disorder) user bases. Each prompt is paired with multiple AI-generated responses and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yunjaeys/Contextual_Response_Evaluation_for_ESL_and_ASD_Support."},
  {"name":"MultiCaRe_Dataset","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset."},
  {"name":"myanmar_spoken_corpus","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/freococo/myanmar_spoken_corpus","creator_name":"WYC","creator_url":"https://huggingface.co/freococo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMyanmar Spoken Corpus (Version 1.0)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nMyanmar Spoken Corpus is a high-quality, open dataset of spoken Myanmar sentences designed to support NLP and ASR applications. The dataset focuses on providing clean and structured spoken language data for advancing Myanmar language technology.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Statistics\\n\\t\\n\\n\\nNumber of Rows:\\nLocal Parquet file: 16,020,011 rows\\nHugging Face Dataset Viewer: 15,728,640 rows\\n\\n\\nFile Size: 1.78 GB (Parquet format)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/myanmar_spoken_corpus."},
  {"name":"ESG_Report","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/WHATX/ESG_Report","creator_name":"NUS & A*STAR - WHATX","creator_url":"https://huggingface.co/WHATX","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tESG Report PDF Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload Instructions\\n\\t\\n\\nTo download the dataset, follow these steps:\\n\\nNavigate to the data directory in the GitHub repository:\\ncd data\\n\\n\\nInstall Git LFS (if not already installed):\\ngit lfs install\\n\\n\\nClone the dataset from Hugging Face Hub:\\ngit clone https://huggingface.co/datasets/WHATX/ESG_Report\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains three main components:\\n\\nraw_pdf:\\n\\nA collection of 195 PDFs scraped from TCFD Hub.\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WHATX/ESG_Report."},
  {"name":"combined-fr-caselaw","keyword":"natural-language-inference","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Tonic/combined-fr-caselaw","creator_name":"Joseph [open/acc] Pollack","creator_url":"https://huggingface.co/Tonic","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for French Legal Cases Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset combines French legal cases from multiple sources (INCA, JADE, CASS, CAPP) into a unified format with overlapping text triplets. It includes decisions from various French courts, processed to facilitate natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n\\nTasks:\\nText Generation\\nLegal Document Analysis\\nText Classification\\nLanguage Modeling\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tonic/combined-fr-caselaw."},
  {"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED."},
  {"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED."},
  {"name":"UnLOK-VQA","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìä Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\\n\\t\\n\\nLink: Dataset Link\\nThis dataset contains approximately 500 entries with the following key attributes:\\n\\n\\\"id\\\": Unique Identifier for each entry\\n\\\"src\\\": The question whose answer is to be deleted ‚ùì\\n\\\"pred\\\": The answer to the question meant for deletion ‚ùå\\n\\\"loc\\\": Related neighborhood questions üîÑ\\n\\\"loc_ans\\\": Answers to the neighborhood questions üó£Ô∏è\\n\\\"image_id\\\": The ID corresponding to the image in the COCO dataset üñºÔ∏è\\n\\nTo access the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA."},
  {"name":"SimpleStories-JA","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/lennart-finke/SimpleStories-JA","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅgpt-4o-mini„Å´„Çà„Å£„Å¶ÁîüÊàê„Åï„Çå„ÅüÁü≠Á∑®Â∞èË™¨„ÅßÂá∫Êù•„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÁîüÊàêÊñπÊ≥ï„ÇÑ„ÄÅËá™ÂàÜ„ÅßÁâ©Ë™û„ÇíÁîüÊàê„Åô„ÇãÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„Åì„Å°„Çâ„ÅÆ„É™„Éù„Ç∏„Éà„É™„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ\\n‰ªñ„ÅÆË®ÄË™û„ÇÑÁâ©Ë™ûÂΩ¢Âºè„ÅÆÂà∂‰Ωú„ÇíÂ∏åÊúõ„Åï„Çå„ÇãÂ†¥Âêà„ÅØ„ÄÅ„É°„Éº„É´„Å´„Å¶„ÅäÂïè„ÅÑÂêà„Çè„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ\\nSimpleStories„ÅØ„ÄÅElden„Å®Li„Å´„Çà„ÇãTinyStories„ÅÆÊîπËâØÁâà„Åß„Åô„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\tÁâπÂæ¥\\n\\t\\n\\n\\nÁâ©Ë™û„ÅÆÊ≥®ÈáàÊÉÖÂ†±Ôºàtheme„ÄÅtopic„ÄÅstyle„Å™„Å©Ôºâ\\nÂ§öÊßòÊÄß„ÅÆÈ´ò„Åï\\n2024Âπ¥„ÅÆ„É¢„Éá„É´„Å´„Çà„Å£„Å¶ÁîüÊàê\\nNLP„ÅÆ„Éá„Éº„Çø„ÅåÁî®ÊÑè„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åó„ÇÑ„Åô„ÅÑ\\n‰ª•‰∏ã„ÅÆË®ÄË™ûÁâà„ÅåÂà©Áî®ÂèØËÉΩÔºö\\nËã±Ë™û\\nÊó•Êú¨Ë™û\\n‰ªñ„Å´„ÇÇËøΩÂä†‰∫àÂÆö\\n\\n\\n\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories-JA."},
  {"name":"myanmar-written-corpus","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/freococo/myanmar-written-corpus","creator_name":"WYC","creator_url":"https://huggingface.co/freococo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMyanmar Written Corpus\\n\\t\\n\\nThe Myanmar Written Corpus is a comprehensive collection of high-quality written Myanmar text, designed to address the lack of large-scale, openly accessible resources for Myanmar Natural Language Processing (NLP). It is tailored to support various tasks such as text-to-speech (TTS), automatic speech recognition (ASR), translation, text generation, and more.\\nThis dataset serves as a critical resource for researchers and developers aiming to advance Myanmar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/myanmar-written-corpus."},
  {"name":"spell-correction","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/torinriley/spell-correction","creator_name":"Torin Etheridge","creator_url":"https://huggingface.co/torinriley","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpell-Check Dataset\\n\\t\\n\\nThis dataset consists of pairs of misspelled words and their corresponding correctly spelled words, designed for training and evaluating character-level spelling correction models. It is particularly useful for tasks such as:\\n\\nSpelling correction\\n\\nCharacter-level sequence-to-sequence modeling\\n\\nError detection and correction in text\\n\\n\\nEach data point in the dataset contains:\\n\\nmisspelled: A misspelled version of a word.\\n\\ncorrect: The corrected spelling of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/torinriley/spell-correction."},
  {"name":"MCRS_by_Databoost","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Databoost/MCRS_by_Databoost","creator_name":"Mada","creator_url":"https://huggingface.co/Databoost","description":"A dataset for classifying and detecting toxicity in social media content"},
  {"name":"unal-repository-dataset-train-instruct","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-train-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset Instruct Train (split 75/25)\\nDescripci√≥n: Split 75% del dataset original. \\nEste dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-train-instruct."},
  {"name":"indic_sentiment_analyzer","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer","creator_name":"Dhruv Bhatnagar","creator_url":"https://huggingface.co/dhruv0808","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tMultilingual Sentiment Analysis Dataset for Indian Languages\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis repository contains a comprehensive sentiment analysis dataset covering 11 Indian languages and English. The dataset is designed to support sentiment analysis tasks across multiple domains and languages, making it valuable for developing multilingual sentiment analysis models and applications.\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages Covered\\n\\t\\n\\n\\nEnglish (en) - Original\\nHindi (hi)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer."},
  {"name":"dubliners","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/chryskylodon/dubliners","creator_name":"seva","creator_url":"https://huggingface.co/chryskylodon","description":"A dataset of James Joyce's collection of short stories \\\"Dubliners,\\\" prepared for NLP tasks and computational analysis of literary texts. The dataset includes:\\n- Text tokenized by sentences.\\n- POS-tagged sentences using NLTK.\\n- Results of analyzing the text with spaCy (POS-tagged, named entities, dependencies).\\n\\nThis dataset was created as part of an NLP course at the Higher School of Economics (HSE). For more details, see the original repository: https://github.com/vifirsanova/compling.\\n\\nThe dataset can be used for various NLP tasks, including:\\n- Part-of-speech tagging.\\n- Named entity recognition.\\n- Dependency parsing.\\n- Computational analysis of literary texts.\\n\\nIt is particularly suited for researchers and students interested in computational linguistics and literary analysis.\\n"},
  {"name":"telugu-colloquial-corpus-tokenized","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/ankitha29/telugu-colloquial-corpus-tokenized","creator_name":"ankitha Ravella","creator_url":"https://huggingface.co/ankitha29","description":"\\n\\t\\n\\t\\t\\n\\t\\tTelugu Colloquial Corpus (Tokenized)\\n\\t\\n\\nThis dataset is a tokenized version of the Telugu Colloquial Corpus (TeCC). It contains examples of informal, everyday Telugu language, including slang, regional variations, and conversational patterns.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nLanguage: Telugu (te)\\nTokenization: Tokenized using the bert-base-multilingual-cased tokenizer from the transformers library.\\nSource: [Describe where the original data came from ‚Äì e.g., collected from online forums‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ankitha29/telugu-colloquial-corpus-tokenized."},
  {"name":"violence-and-conflict-events-in-colombia","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/3iS/violence-and-conflict-events-in-colombia","creator_name":"3iS","creator_url":"https://huggingface.co/3iS","description":"\\n\\t\\n\\t\\t\\n\\t\\tHumanitarian Dataset: Violence and IHL Violations in Colombia (2024)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains records of violence and infractions to international humanitarian law (IHL) in Colombia during 2024. The dataset was compiled by OCHA Colombia from reports by key informants and news sources. The data has been structured and categorized according to IHL standards, including the extraction of the number of victims and events.\\n\\n\\t\\n\\t\\t\\n\\t\\tContent\\n\\t\\n\\nThe dataset includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/3iS/violence-and-conflict-events-in-colombia."},
  {"name":"GammaCorpus-v2-500k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k."},
  {"name":"GammaCorpus-v2-10k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k."},
  {"name":"GammaCorpus-v2-500k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k."},
  {"name":"GammaCorpus-v2-10k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k."},
  {"name":"GammaCorpus-CoT-Math-170k","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k."},
  {"name":"GammaCorpus-CoT-Math-170k","keyword":"natural-language-processing","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k."},
  {"name":"Estonian-Text-Simplification","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/vulturuldemare/Estonian-Text-Simplification","creator_name":"Eduard Barbu","creator_url":"https://huggingface.co/vulturuldemare","description":"\\n\\t\\n\\t\\t\\n\\t\\tEstonian Text Simplification\\n\\t\\n\\nThis repository contains resources and models for Estonian text simplification, including datasets and pre-trained models.\\n\\n\\t\\n\\t\\t\\n\\t\\tFiles\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Files\\n\\t\\n\\n\\nsimplification_training_set.json: A dataset used to fine-tune LLaMA 3.1 for text simplification.\\n\\nsrc: The source of the data.\\noriginal: The original sentence to be simplified.\\nsimpl_lex: A lexical simplification (may be empty).\\nsimpl_final: The final simplified sentence.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vulturuldemare/Estonian-Text-Simplification."},
  {"name":"synthetic-legal","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Taylor658/synthetic-legal","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetic Legal (Query, Response) Dataset\\n\\t\\n\\n\\nDescriptionSynthetic Legal is a 140,000-row dataset of (legal query, legal response) pairs spanning 13 legal domains, designed to mimic real-world legal fact patterns and references. Each entry provides a short scenario (fact pattern) and a \\\"verified solution\\\" referencing real citations (statutes, case law, scholarly commentary, legislative history, and comparative law) with a specified verification method.  \\nDisclaimer: All text is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/synthetic-legal."},
  {"name":"mongs","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/sangyon/mongs","creator_name":"yoon","creator_url":"https://huggingface.co/sangyon","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sangyon/mongs."},
  {"name":"WaterDrum-TOFU","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\tWaterDrum: Watermarking for Data-centric Unlearning Metric\\n\\t\\n\\nWaterDrum provides an unlearning benchmark for the evaluation of effectiveness and practicality of unlearning. This repository contains the TOFU corpus of WaterDrum (WaterDrum-TOFU), which contains both unwatermarked and watermarked question-answering datasets based on the original TOFU dataset.\\nThe data samples were watermarked with Waterfall.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe WaterDrum-TOFU dataset contains 6 subsets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU."},
  {"name":"Meta_Plan_Optimization","keyword":"nlp","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/xwm/Meta_Plan_Optimization","creator_name":"xwm","creator_url":"https://huggingface.co/xwm","description":"\\n\\t\\n\\t\\t\\n\\t\\tMPO Datasets\\n\\t\\n\\nThis folder contains the datasets for the MPO experiments.\\nPaper: https://hf.co/papers/2503.02682\\nCode: https://github.com/WeiminXiong/MPO\\n\\n\\t\\n\\t\\t\\n\\t\\tFile Structure\\n\\t\\n\\nalfworld_metaplan_preference_pairs.json: includes comparison data for the DPO optimization phase of the ALFWorld meta planner.\\nsciworld_metaplan_preference_pairs.json: includes comparison data for the DPO optimization phase of the SciWorld meta planner.\\nalfworld_metaplan_sft.json: includes the metaplan data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xwm/Meta_Plan_Optimization."},
  {"name":"multisource-esco-set","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/Boanerges/multisource-esco-set","creator_name":"Samson Kwaku Nkrumah","creator_url":"https://huggingface.co/Boanerges","description":"\\n\\t\\n\\t\\t\\n\\t\\tMultiSource-ESCO-Skills: A Unified Dataset for Skill Extraction\\n\\t\\n\\nThis dataset aggregates data from multiple sources‚Äîcourse descriptions, CV content, and job descriptions‚Äîall linked to ESCO skills. It is designed to help researchers and practitioners develop and fine-tune NLP models (e.g., BERT or SentenceTransformer-based models) for automated skill extraction.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\nName: MultiSource-ESCO-Skills\\nSources:\\nCourse Content: Educational course materials\\nCV Content:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boanerges/multisource-esco-set."},
  {"name":"fine_tuned_llama2","keyword":"nlp","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/and89/fine_tuned_llama2","creator_name":"anand kumar","creator_url":"https://huggingface.co/and89","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/and89/fine_tuned_llama2."},
  {"name":"LatinSummarizer","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizer","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tLatinSummarizer Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tStructure\\n\\t\\n\\n\\naligned_en_la_data_raw.csv\\naligned_en_la_data_cleaned.csv\\naligned_en_la_data_cleaned_with_stanza.csv\\nconcat_aligned_data.csv\\nconcat_cleaned.csv\\nlatin_wikipedia_cleaned.csv\\nlatin_wikipedia_raw.csv\\nlatin-literature-dataset-170M_raw_cleaned.csv\\nlatin-literature-dataset-170M_raw_cleaned_chunked.csv\\nElsa_aligned/\\nREADME.md\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\taligned_en_la_data_raw.csv\\n\\t\\n\\nThis dataset contains aligned Latin (la) - English (en)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizer."},
  {"name":"multi-label-web-categorization","keyword":"natural-language-processing","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/tshasan/multi-label-web-categorization","creator_name":"Taimur","creator_url":"https://huggingface.co/tshasan","description":"\\n\\t\\n\\t\\t\\n\\t\\tMulti-Label Web Page Classification Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Multi-Label Web Page Classification Dataset is a curated dataset containing 14,258 web page titles and snippets, extracted from the CC-Meta25-1M dataset. Each entry has been automatically categorized into multiple predefined categories using ChatGPT-4o-mini.\\nThis dataset is designed for multi-label text classification tasks, making it ideal for training and evaluating machine learning models in web‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tshasan/multi-label-web-categorization."}
];
