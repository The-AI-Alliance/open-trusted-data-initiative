var data_for_modality_nlp = 
[
	{"name":"smnli_mt","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/darmanin-matt/smnli_mt","creator_name":"Matthew Darmanin","creator_url":"https://huggingface.co/darmanin-matt","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for the SMNLI-MT\\n\\t\\n\\nThe SMNLI-MT datasets are machine-translated versions of the Stanford NLI and MultiNLI datasets in Maltese.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe datasets were translated using the Google Cloud Translate as part of the initial exploration of NLI in the Maltese language.\\n\\nCurated by: Matthew Darmanin\\nLanguage(s) (NLP): Maltese\\nLicense: CC 4.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe datasets are in the form of CSV files‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/darmanin-matt/smnli_mt."},
	{"name":"alpaca-sinhala","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sahanruwantha/alpaca-sinhala","creator_name":"sahan ruwantha","creator_url":"https://huggingface.co/sahanruwantha","description":"The Alpaca dataset translated into Sinhala using Google Translator. Manual verification and correction of translations are recommended for optimal performance.\\n"},
	{"name":"AnimeQuotes","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes","creator_name":"Mohammed Khalil","creator_url":"https://huggingface.co/mohamed-khalil","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAnime Quotes Dataset ‚Äï „Ç¢„Éã„É°„ÅÆÂêçË®Ä„Éá„Éº„Çø„Çª„ÉÉ„Éàüéê\\n\\t\\n\\n\\nWelcome to Anime Quotes Dataset\\n\\n\\n    \\n        \\n        \\n        \\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains a curated collection of inspiring and memorable quotes from various anime series, sourced from the Anime Motivation website. The quotes are stored as a list of dictionaries and can be easily accessed for analysis, research, or personal enjoyment.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Format\\n\\t\\n\\nEach entry in the dataset is represented by a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mohamed-khalil/AnimeQuotes."},
	{"name":"SomGPT","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Abdirahman555/SomGPT","creator_name":"Abdirahman","creator_url":"https://huggingface.co/Abdirahman555","description":"Abdirahman555/SomGPT dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Ergonomics_Chiar_Customer_Viewdata_E-commerse","keyword":"nlp","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse","creator_name":"lia","creator_url":"https://huggingface.co/liaHa","description":"liaHa/Ergonomics_Chiar_Customer_Viewdata_E-commerse dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"LingComp_QA","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/somosnlp/LingComp_QA","creator_name":"SomosNLP","creator_url":"https://huggingface.co/somosnlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for LingComp_QA, un corpus educativo de ling√º√≠stica computacional en espa√±ol\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\nCurated by: Jorge Zamora Rey, Isabel Moyano Moreno, Mario Crespo Miguel \\nFunded by: SomosNLP, HuggingFace, Argilla, Instituto de Ling√º√≠stica Aplicada de la Universidad de C√°diz \\nLanguage(s) (NLP): es-ES \\nLicense: apache-2.0 \\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://github.com/reddrex/lingcomp_QA/tree/main‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/somosnlp/LingComp_QA."},
	{"name":"sql-create-context-thai","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/saksornr/sql-create-context-thai","creator_name":"Saksorn Ruangtanusak","creator_url":"https://huggingface.co/saksornr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from sql-create-context.\\n@misc{b-mc2_2023_sql-create-context,\\n  title   = {sql-create-context Dataset},\\n  author  = {b-mc2}, \\n  year    = {2023},\\n  url     = {https://huggingface.co/datasets/b-mc2/sql-create-context},\\n  note    = {This dataset was created by modifying data from the following sources: \\\\cite{zhongSeq2SQL2017, yu2018spider}.},\\n}\\n\\n"},
	{"name":"code_leak_qa","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/fyt7943/code_leak_qa","creator_name":"fyt","creator_url":"https://huggingface.co/fyt7943","description":"fyt7943/code_leak_qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"copyright_unlearning","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/boyiwei/copyright_unlearning","creator_name":"Boyi Wei","creator_url":"https://huggingface.co/boyiwei","description":"boyiwei/copyright_unlearning dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Vehicle_Complaints_NHSTA","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ItsMayur/Vehicle_Complaints_NHSTA","creator_name":"Mayur Arvindh","creator_url":"https://huggingface.co/ItsMayur","description":"ItsMayur/Vehicle_Complaints_NHSTA dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"cli-commands-explained","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/b-mc2/cli-commands-explained","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a collection of 16,098 command line instructions sourced from Commandlinefu and Cheatsheets. It includes an array of commands, each with an id, title, description, date, url to source, author, votes, and flag indicating if the description is AI generated. The descriptions are primarily authored by the original contributors, for entries where descriptions were absent, they have been generated using NeuralBeagle14-7B. Out of the total entries, 10,039‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/cli-commands-explained."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-825318","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-825318 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-825318 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-825318."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-46082","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-46082 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-46082 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-46082."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-497939","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-497939 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-497939 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-497939."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-235808","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-235808 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-235808 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-235808."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-774308","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-774308 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment analysis and opinion-based QA\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-774308 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-774308."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-898550","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-898550 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-898550 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-898550."},
	{"name":"FiQA2018-256-24-gpt-4o-2024-05-13-780826","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFiQA2018-256-24-gpt-4o-2024-05-13-780826 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"financial sentiment and QA analysis\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the FiQA2018-256-24-gpt-4o-2024-05-13-780826 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/FiQA2018-256-24-gpt-4o-2024-05-13-780826."},
	{"name":"HotpotQA-256-24-gpt-4o-2024-05-13-773587","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHotpotQA-256-24-gpt-4o-2024-05-13-773587 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"code repository search for machine learning datasets and models\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the HotpotQA-256-24-gpt-4o-2024-05-13-773587 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/HotpotQA-256-24-gpt-4o-2024-05-13-773587."},
	{"name":"MSMARCO-256-24-gpt-4o-2024-05-13-374380","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSMARCO-256-24-gpt-4o-2024-05-13-374380 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"research dataset search for AI and NLP tasks\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-374380 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-374380."},
	{"name":"MSMARCO-256-24-gpt-4o-2024-05-13-466074","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMSMARCO-256-24-gpt-4o-2024-05-13-466074 Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"research dataset search for AI and NLP tasks\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the MSMARCO-256-24-gpt-4o-2024-05-13-466074 model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/MSMARCO-256-24-gpt-4o-2024-05-13-466074."},
	{"name":"BAAI_bge-small-en-v1_5-2852024-6p16-webapp","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBAAI_bge-small-en-v1_5-2852024-6p16-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"natural language processing\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the BAAI_bge-small-en-v1_5-2852024-6p16-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-small-en-v1_5-2852024-6p16-webapp."},
	{"name":"RWKU","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/jinzhuoran/RWKU","creator_name":"Zhuoran Jin","creator_url":"https://huggingface.co/jinzhuoran","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Real-World Knowledge Unlearning Benchmark (RWKU)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nRWKU is a real-world knowledge unlearning benchmark specifically designed for large language models (LLMs).\\nThis benchmark contains 200 real-world unlearning targets and 13,131 multi-level forget probes, including 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial-attack probes.\\nRWKU is designed based on the following three key factors: \\n\\nFor the task‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jinzhuoran/RWKU."},
	{"name":"unlearning","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/LZ12DH/unlearning","creator_name":"Li Zhaodonghui","creator_url":"https://huggingface.co/LZ12DH","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LZ12DH/unlearning."},
	{"name":"synthetic_multilingual_llm_prompts","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts","creator_name":"Gretel.ai","creator_url":"https://huggingface.co/gretelai","description":"\\n  \\n  Image generated by DALL-E. See prompt for more details\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìùüåê Synthetic Multilingual LLM Prompts\\n\\t\\n\\nWelcome to the \\\"Synthetic Multilingual LLM Prompts\\\" dataset! This comprehensive collection features 1,250 synthetic LLM prompts generated using Gretel Navigator, available in seven different languages. To ensure accuracy and diversity in prompts, and translation quality and consistency across the different languages, we employed Gretel Navigator both as a generation tool and as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_multilingual_llm_prompts."},
	{"name":"Arabic_Poems","keyword":"nlp","license":"GNU General Public License v2.0","license_url":"https://choosealicense.com/licenses/gpl-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/alwalid54321/Arabic_Poems","creator_name":"Alwalid Ibrahim","creator_url":"https://huggingface.co/alwalid54321","description":"Arabic Poems\\nit's a filtered version for (https://huggingface.co/datasets/arbml/ashaar) with only al-diwan data\\nOverview:\\nThe \\\"Arabic Poems\\\" dataset is a comprehensive collection of Arabic poetry, comprising 8,875 entries. Each entry contains detailed information about individual poems and their respective poets, making it a valuable resource for researchers, developers, and enthusiasts of Arabic literature.\\nColumns:\\nUnnamed: 0: An index column.\\npoem_title: The title of the poem.\\npoem_meter:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alwalid54321/Arabic_Poems."},
	{"name":"commbase-log-chats","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mydroidandi/commbase-log-chats","creator_name":"My Droid And I","creator_url":"https://huggingface.co/mydroidandi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCommbase Log Chats Dataset\\n\\t\\n\\n\\nCapturing Assistant-User Interaction Logs for NLP and Chat Analysis\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe Commbase Log Chats Dataset contains a series of chat logs between an assistant (Eva AI) and end user. The dataset captures interactions in the form of text exchanges with metadata such as timestamps, origin of the message, severity level, and speaker details. This dataset can be used for various applications including natural language processing‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mydroidandi/commbase-log-chats."},
	{"name":"sql-create-context-pt","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/emdemor/sql-create-context-pt","creator_name":"Eduardo Morais","creator_url":"https://huggingface.co/emdemor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nEste dataset  √© uma vers√£o traduzida para o portugu√™s do dataset b-mc2/sql-create-context,\\nque foi constru√≠do a partir dos datasets WikiSQL e Spider. Ele cont√©m exemplos de perguntas\\nem portugu√™s, instru√ß√µes SQL CREATE TABLE e consultas SQL que respondem √†s perguntas\\nutilizando a instru√ß√£o CREATE TABLE como contexto.\\nO principal objetivo deste dataset √© ajudar modelos de linguagem natural  em portugu√™s a gerar consultas\\nSQL precisas e contextualizadas, prevenindo a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/emdemor/sql-create-context-pt."},
	{"name":"BeHonest","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/GAIR/BeHonest","creator_name":"SII - GAIR","creator_url":"https://huggingface.co/GAIR","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBeHonest: Benchmarking Honesty in Large Language Models\\n\\t\\n\\nBeHonest is a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively. BeHonest evaluates three essential aspects of honesty: awareness of knowledge boundaries (self-knowledge), avoidance of deceit (non-deceptiveness), and consistency in responses (consistency).\\nBeHonest supports the following 10 scenarios:\\n\\nAdmitting Unknowns: LLMs should appropriately refuse to answer questions that are beyond‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/GAIR/BeHonest."},
	{"name":"x_g85_fn_dataset","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/x-g85/x_g85_fn_dataset","creator_name":"X_G85","creator_url":"https://huggingface.co/x-g85","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tX_G85 Fake News Dataset\\n\\t\\n\\nIt is a preprocessed dataset that is used to build X_G85 ML Models. The collection of fake news that are collect from the following datasets\\nLabels\\n\\n0: Fake News\\n1: Real News\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to stream dataset & use as pandas dataframe\\n\\t\\n\\nBy streaming the dataset, it won't download on your host computer. Read more here hugging face streaming dataset.\\nimport pandas as pd\\nfrom datasets import load_dataset\\n\\n\\nNote: The following operation may take some time‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/x-g85/x_g85_fn_dataset."},
	{"name":"KnowUnDo","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/zjunlp/KnowUnDo","creator_name":"ZJUNLP","creator_url":"https://huggingface.co/zjunlp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKnowUnDo\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüíª Datasets Usage\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\\\"zjunlp/KnowUnDo\\\", name='copyright', split='unlearn')\\n\\n\\nAvailable configuration names and corresponding splits:\\ncopyright: unlearn, retention;\\nprivacy: unlearn, retention;\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüéâ Acknowledgement\\n\\t\\n\\nWe would like to express our sincere gratitude for the excellent work TOFU, Unlearn Dataset and LLM Unlearning.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìñ Citation\\n\\t\\n\\nIf finding this work useful for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zjunlp/KnowUnDo."},
	{"name":"LIBRA","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ai-forever/LIBRA","creator_name":"ai-forever","creator_url":"https://huggingface.co/ai-forever","description":"\\n\\t\\n\\t\\t\\n\\t\\tLIBRA: Long Input Benchmark for Russian Analysis\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLIBRA (Long Input Benchmark for Russian Analysis) is designed to evaluate the capabilities of large language models (LLMs) in understanding and processing long texts in Russian. This benchmark includes 21 datasets adapted for different tasks and complexities. The tasks are divided into four complexity groups and allow evaluation across various context lengths ranging from 4k up to 128k tokens.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ai-forever/LIBRA."},
	{"name":"assertiveness-corpus","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Shahradmz/assertiveness-corpus","creator_name":"MZ","creator_url":"https://huggingface.co/Shahradmz","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssertiveness Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nA compiled dataset of 6000 texts from multiple sources, scored using SciBert for assertiveness.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource\\n\\t\\n\\n\\nPrompting llama 3 8B model to explain why a statement from the LIAR dataset is wrong or right. Explanation of the model and the assertiveness score rated by PEI‚Äôs model kept.\\n\\nUser level comments from change my view subreddit.\\n\\nAG news dataset.\\nfrom datasets import load_dataset\\nds = load_dataset(\\\"fancyzhx/ag_news\\\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Shahradmz/assertiveness-corpus."},
	{"name":"fa-topic-sentences","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mostafaamiri/fa-topic-sentences","creator_name":"mostafa amiri","creator_url":"https://huggingface.co/mostafaamiri","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tREADME for fa-topic-sentences Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe fa-topic-sentences dataset is a comprehensive collection of sentences categorized into various topics. Each topic contains approximately 50 sentences in Persian, accompanied by a paraphrased version of each sentence. The dataset is structured in JSON format, providing a straightforward method for accessing individual entries.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTopics Included\\n\\t\\n\\nThe dataset encompasses the following topics:\\n\\nHistory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mostafaamiri/fa-topic-sentences."},
	{"name":"TOFU-C","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C."},
	{"name":"TOFU-Cf","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cf","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cf."},
	{"name":"TOFU-Cr","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cr","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cr."},
	{"name":"Financial-NER-NLP","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Josephgflowers/Financial-NER-NLP","creator_name":"Joseph G Flowers","creator_url":"https://huggingface.co/Josephgflowers","description":"Dataset Card for Financial-NER-NLP\\nDataset Summary\\nThe Financial-NER-NLP Dataset is a derivative of the FiNER-139 dataset, which consists of 1.1 million sentences annotated with 139 XBRL tags. This new dataset transforms the original structured data into natural language prompts suitable for training language models. The dataset is designed to enhance models‚Äô abilities in tasks such as named entity recognition (NER), summarization, and information extraction in the financial domain.\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Josephgflowers/Financial-NER-NLP."},
	{"name":"emotion_classification_reviews_kfold_10","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/mtiessler/emotion_classification_reviews_kfold_10","creator_name":"Max Tiessler","creator_url":"https://huggingface.co/mtiessler","description":"mtiessler/emotion_classification_reviews_kfold_10 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"TOFU-C","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C."},
	{"name":"Amazon_Reviews_Binary_for_Sentiment_Analysis","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. In the dataset, class 1 is the negative and class 2 is the positive. Each class has 1,800,000 training samples and 200,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_Binary_for_Sentiment_Analysis."},
	{"name":"Yelp_Reviews_for_Binary_Senti_Analysis","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Binary_Senti_Analysis","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2 negative, and 3 and 4 positive. For each polarity 280,000 training samples and 19,000 testing samples are take randomly. In total there are 560,000 trainig samples and 38,000 testing samples. Negative polarity is class 1, and positive class 2.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Binary_Senti_Analysis."},
	{"name":"Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yelp reviews full star dataset is constructed by randomly taking 130,000 training samples and 10,000 testing samples for each review star from 1 to 5. In total there are 650,000 trainig samples and 50,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 2 columns in them, corresponding to class index (1 to 5) and review text. The review texts‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes."},
	{"name":"Yahoo_Answers_10_categories_for_NLP","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yassiracharki/Yahoo_Answers_10_categories_for_NLP","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe file classes.txt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Yahoo_Answers_10_categories_for_NLP."},
	{"name":"SFinD-S","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tilmann-strative/SFinD-S","creator_name":"Tilmann Bruckhaus","creator_url":"https://huggingface.co/tilmann-strative","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis sample is part of the larger SFinD-S (Strative Financial Dataset - Synthetic), a comprehensive dataset designed for Retrieval-Augmented Generation (RAG) GenAI applications, Natural Language Processing (NLP), Large Language Models (LLM), and AI tasks in the financial domain. The full SFinD-S dataset contains over 20,000 records of realistic financial questions and verified answers, sourced from a wide variety of web content.\\nIf you find this dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tilmann-strative/SFinD-S."},
	{"name":"TOFUCr1","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCr1","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCr1."},
	{"name":"Arguana","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timchen0618/Arguana","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the Arguana dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.All the instances in this dataset is \\\"binary\\\".\\nWe repurpose the Arguana‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/Arguana."},
	{"name":"Kialo","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timchen0618/Kialo","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the Kialo dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.  \\nWe collect the dataset from a debate website, Kialo.com.The positive and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/Kialo."},
	{"name":"OpinionQA","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/timchen0618/OpinionQA","creator_name":"Hung-Ting Chen","creator_url":"https://huggingface.co/timchen0618","description":"This is the OpinionQA dataset from the BERDS benchmark. (Paper link: )The purpose of this dataset is evaluating diversity of retrieval systems given subjective questions.  \\nEach instance consists of a question and a list of valid perspectives (opinions) for the question.\\\"Type\\\" indicates the number of perspectives a question has. \\\"Binary\\\" questions come with two perspectives, while \\\"Multi\\\" questions come with more than two.  \\nWe repurpose the OpinionQA dataset into the desired setting.We first‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/timchen0618/OpinionQA."},
	{"name":"text2pandas","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zeyadusf/text2pandas","creator_name":"Zeyad Usf","creator_url":"https://huggingface.co/zeyadusf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout Data\\n\\t\\n\\n\\n\\n \\n  \\n\\n \\n  \\n\\n \\n  \\n\\n\\n\\nI found two datasets about converting text with context to pandas code on Hugging Face, but the challenge is in the context. The context in both datasets is different which reduces the results of the model. First let's mention the data I found and then show examples, solution and some other problems.\\n\\nRahima411/text-to-pandas :\\nThe data is divided into Train with 57.5k and Test with 19.2k.\\n\\nThe data has two columns as you can see in the example:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zeyadusf/text2pandas."},
	{"name":"daigt","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zeyadusf/daigt","creator_name":"Zeyad Usf","creator_url":"https://huggingface.co/zeyadusf","description":"zeyadusf/daigt dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"turkish-sentence-elements","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/synturk/turkish-sentence-elements","creator_name":"Syntax T√ºrkiye","creator_url":"https://huggingface.co/synturk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SYNT√úRK SENTAGRAM Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe SYNT√úRK SENTAGRAM Dataset is a specialized dataset designed to aid in the development and evaluation of NLP models focused on Turkish syntax and grammar. The dataset consists of sentences annotated with grammatical elements such as subjects, predicates, objects, and adjuncts. This dataset is intended for educational purposes and research in natural language processing (NLP), particularly in the context‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/synturk/turkish-sentence-elements."},
	{"name":"imageability","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/StephanAkkerman/imageability","creator_name":"Stephan Akkerman","creator_url":"https://huggingface.co/StephanAkkerman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish Word Imageability\\n\\t\\n\\nThis dataset is a combination of my other two imageability datasets. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset is ideal for training and evaluating machine learning models for English word imageability.\\nYou can find the embeddings embeddings in my other dataset here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgments\\n\\t\\n\\nWe extend our heartfelt gratitude to all the authors of the original datasets.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the MIT license.\\n"},
	{"name":"TOFUCrP","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFUCrP","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFUCrP."},
	{"name":"Deshika","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sarch7040/Deshika","creator_name":"sarch","creator_url":"https://huggingface.co/sarch7040","description":"Maharashtri Prakrit to English Parallel Corpus\\nDataset Summary\\nThis dataset contains parallel text data for translating from Maharashtri Prakrit (an ancient Indo-Aryan language) to English. It is designed to aid in developing machine translation systems, language models, and linguistic research for this underrepresented language. The dataset is collected from historical texts, scriptures, and scholarly resources.\\nKey Features:\\n\\n  Source Language: Maharashtri Prakrit\\n  Target Language: English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sarch7040/Deshika."},
	{"name":"alpaca-bulgarian-jokes","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes."},
	{"name":"AURA-Sentiment","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/irfan-ahmad/AURA-Sentiment","creator_name":"Irfan Ahmad","creator_url":"https://huggingface.co/irfan-ahmad","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAURA-Sentiment\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe AURA Sentiment Dataset is a collection of 29,700 app reviews in Arabic from iOS and Android platforms. Each review is labeled with a sentiment class, enabling researchers and practitioners to develop and evaluate sentiment analysis models tailored to the Arabic language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\nThe dataset includes the following columns:\\n\\nreview: The text of the app review in Arabic.\\nappName: The name of the application‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/irfan-ahmad/AURA-Sentiment."},
	{"name":"wiktionary-data","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wiktionary-data","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\\nÌïúÍµ≠Ïñ¥ - Korean\\nêé†êéºêéπ - Old Persian\\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\\nElamite\\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wiktionary-data."},
	{"name":"tl-test-learn-prompts","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/reddgr/tl-test-learn-prompts","creator_name":"David G. R.","creator_url":"https://huggingface.co/reddgr","description":"This dataset contains manually labeled examples used for training and testing reddgr/tl-test-learn-prompt-classifier, a fine-tuning of DistilBERT that classifies chatbot prompts as either 'test' or 'learn.'\\nPrompts labeled as 'test' (1) are those where it can be inferred that the user is:\\n\\nPresenting a problem that requires complex reasoning or arithmetic logic to resolve.\\nIntentionally 'challenging' the conversational tool with a complicated question the user might know the answer to.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reddgr/tl-test-learn-prompts."},
	{"name":"thai-gov-procurement_regulation-17-amend-21","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21","creator_name":"Amornpan Phornchaicharoen","creator_url":"https://huggingface.co/amornpan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüáπüá≠ Dataset Card for Thai Government Procurement Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t‚ÑπÔ∏è This dataset is optimized for procurement-related NLP tasks in Thai.\\n\\t\\n\\nThis dataset contains a collection of procurement regulations, instructions, and responses focused on public sector purchasing, contract management, and compliance with Thai government standards. It aims to support natural language processing tasks involving procurement assistance, such as chatbot development, procurement dialogue‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/amornpan/thai-gov-procurement_regulation-17-amend-21."},
	{"name":"wos_hierarchical_multi_label_text_classification","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/marcelsun/wos_hierarchical_multi_label_text_classification","creator_name":"Marcel Dunaiski","creator_url":"https://huggingface.co/marcelsun","description":"Introduced by du Toit and Dunaiski (2024) Introducing Three New Benchmark Datasets for Hierarchical Text Classification.\\nThe WOS Hierarchical Text Classification are three dataset variants created from Web of Science (WOS) title and abstract data categorised into a hierarchical, multi-label class structure. The aim of the sampling and filtering methodology used was to create well-balanced class distributions (at chosen hierarchical levels). Furthermore, the WOS_JTF variant was also created‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/marcelsun/wos_hierarchical_multi_label_text_classification."},
	{"name":"cci-dataset","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/raghavdw/cci-dataset","creator_name":"Raghav Wate","creator_url":"https://huggingface.co/raghavdw","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAirline Customer Service Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains customer service interactions related to the airline industry. It can be used for training and evaluating natural language processing (NLP) models for tasks such as intent classification, sentiment analysis, and topic modeling.\\nHomepage: [link to your project's homepage or GitHub repository (if applicable)]\\nDataset Structure:\\nThe dataset is split into two subsets:\\n\\ntrain: Used for training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/raghavdw/cci-dataset."},
	{"name":"FNFC-Functional_Non-Functional_Calssification","keyword":"nlp","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MSHD-IAU/FNFC-Functional_Non-Functional_Calssification","creator_name":"Mashhad Azad University","creator_url":"https://huggingface.co/MSHD-IAU","description":"MSHD-IAU/FNFC-Functional_Non-Functional_Calssification dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"RoundTripOCR-sanskrit","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-sanskrit","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Sanskrit language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
	{"name":"RoundTripOCR-bodo","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-bodo","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Bodo language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
	{"name":"RoundTripOCR-konkani","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-konkani","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Konkani language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
	{"name":"RoundTripOCR-marathi","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-marathi","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Marathi language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
	{"name":"trustpilot-reviews-123k","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Kerassy/trustpilot-reviews-123k","creator_name":"Jay Broughton","creator_url":"https://huggingface.co/Kerassy","description":"\\n\\t\\n\\t\\t\\n\\t\\tTrustpilot Reviews 123k\\n\\t\\n\\nWelcome to the Trustpilot Reviews 123k Dataset, a collection of user feedback gathered from Trustpilot UK. This dataset encompasses a total of 123,181 reviews spanning 1,680 companies across 22 categories. The reviews were methodically collected over a brief period between 19th December 2024 and 7th January 2025, providing a snapshot of customer sentiments during this timeframe. This dataset is intended for researchers and practitioners interested in natural‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Kerassy/trustpilot-reviews-123k."},
	{"name":"unal-repository-dataset-test-instruct","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-test-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset Instruct Test (split 75/25)\\nDescripci√≥n: Split 25% del dataset original. \\nEste dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje para‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-test-instruct."},
	{"name":"kbli2020","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ronnieaban/kbli2020","creator_name":"Ronnie Aban","creator_url":"https://huggingface.co/ronnieaban","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nDataset ini berisi Klasifikasi Baku Lapangan Usaha Indonesia (KBLI) tahun 2020 yang merupakan standar klasifikasi aktivitas ekonomi yang ditetapkan oleh Badan Pusat Statistik (BPS) Indonesia. KBLI 2020 digunakan untuk mengklasifikasikan unit usaha dan aktivitas ekonomi ke dalam kelompok yang seragam berdasarkan kegiatan ekonomi yang dilakukan.\\n"},
	{"name":"125-tripadvisor-reviews","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nhull/125-tripadvisor-reviews","creator_name":"N. H√ºll","creator_url":"https://huggingface.co/nhull","description":"\\n\\t\\n\\t\\t\\n\\t\\tMichelin-Starred Sustainable Restaurants Sentiment Dataset\\n\\t\\n\\nThis dataset contains 125 TripAdvisor reviews for Michelin-starred, sustainability-focused restaurants. It was created as part of a university project aimed at analyzing the sentiment of customers in these specific types of restaurants. The selection of restaurants was based on two main criteria: Michelin-star rating and sustainability recognition. The dataset is intended for use in sentiment analysis to explore trends and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nhull/125-tripadvisor-reviews."},
	{"name":"LLM_Post_Training_BestOfNvsGreedy","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QPM777/LLM_Post_Training_BestOfNvsGreedy","creator_name":"Antoine Turkie","creator_url":"https://huggingface.co/QPM777","description":"\\n\\t\\n\\t\\t\\n\\t\\tBest-of-N vs Greedy Model Dataset\\n\\t\\n\\nThis dataset contains problems with their corresponding solutions from two models using Qwen (Qwen2.5-1.5B-Instruct):\\n\\nGreedy Model: A model that picks the most optimal solution at each step.\\nBest-of-N Model: A model that generates multiple solutions and picks the best one from a set of N using Skywork (Skywork-o1-Open-PRM-Qwen-2.5-1.5B) reward model.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nNumber of Problems: 20 (for testing purposes).\\nColumns:\\nid: Unique‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QPM777/LLM_Post_Training_BestOfNvsGreedy."},
	{"name":"chatjsonsql","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rishi2903/chatjsonsql","creator_name":"Rishabh Mekala","creator_url":"https://huggingface.co/rishi2903","description":"\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rishi2903/chatjsonsql."},
	{"name":"GammaCorpus-v1-10k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 10k - UNFILTERED\\n\\t\\n\\n\\n5 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 10k Unfiltered dataset consists of 10,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 5 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI models.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-10k-UNFILTERED."},
	{"name":"GammaCorpus-v2-5m","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 5 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 5m dataset consists of 5 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-5m."},
	{"name":"GammaCorpus-v2-1m","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 1 Million Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 1m dataset consists of 1 million structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-1m."},
	{"name":"GammaCorpus-v2-100k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 100k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 100k dataset consists of 100 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-100k."},
	{"name":"GammaCorpus-v2-50k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 50k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 50k dataset consists of 50 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-50k."},
	{"name":"GammaCorpus-Fact-QA-450k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: Fact QA 450k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus Fact QA 450k is a dataset that consists of 450,000 fact-based question-and-answer pairs designed for training AI models on factual knowledge retrieval and question-answering tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 450,000\\nFormat: JSONL\\nLanguage: English\\nData Type: Fact-based questions\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nThe dataset is formatted in JSONL, where each line is a JSON object‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k."},
	{"name":"FactNews","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/franciellevargas/FactNews","creator_name":"Francielle Vargas","creator_url":"https://huggingface.co/franciellevargas","description":"\\n\\t\\n\\t\\t\\n\\t\\tEvaluation Benchmark for Sentence-Level Factuality Prediciton in Portuguese\\n\\t\\n\\nThe FactNews consits of the first large sentence-level annotated corpus for factuality prediciton in Portuguese. \\nIt is composed of 6,191 sentences annotated according to factuality and media bias definitions proposed by AllSides. We use FactNews to assess the overall reliability of news sources by formulating \\ntwo text classification problems for predicting sentence-level factuality of news reporting and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/franciellevargas/FactNews."},
	{"name":"GammaCorpus-Polylingo-50k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus Polylingo 50k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus Polylingo 50k dataset consists of 50,000 structured, unfiltered, single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\nLanguage: The language used in the interaction.\\n\\nThis dataset is designed to help in the training and evaluation of conversational AI models for linguistic purposes. This dataset can be especially helpful if you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-Polylingo-50k."},
	{"name":"go_emotions_wheel_unilabel","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Jsevisal/go_emotions_wheel_unilabel","creator_name":"Javier Sevilla Salcedo","creator_url":"https://huggingface.co/Jsevisal","description":"Original dataset: GoEmotions dataset\\nFiltered using the following mapping based on the basic emotions found in Plutchik's Wheel of Emotions and filtered to use only the sentences with one label\\nwheel_dict = {\\n    \\\"joy\\\": [\\n        \\\"joy\\\",\\n        \\\"amusement\\\",\\n        \\\"excitement\\\",\\n        \\\"gratitude\\\",\\n        \\\"pride\\\",\\n        \\\"relief\\\",\\n        \\\"admiration\\\",\\n        \\\"love\\\",\\n        \\\"optimism\\\",\\n    ],\\n    \\\"trust\\\": [\\\"approval\\\", \\\"caring\\\"],\\n    \\\"fear\\\": [\\\"fear\\\", \\\"nervousness\\\"],\\n    \\\"surprise\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Jsevisal/go_emotions_wheel_unilabel."},
	{"name":"sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600","creator_name":"Tasmay Pankaj Tibrewal","creator_url":"https://huggingface.co/Tasmay-Tib","description":"Dataset for sarvam's entity normalisation task. More detailed information can be found here, in the main model repo: Hugging Face\\nDetailed Report (Writeup): Google Drive\\nIt also has a gguf variant, with certain additional gguf based innstructions: Hugging Face\\nModel inference script can be found here: Colab\\nModel predictions can be found in this dataset and both the repo files. named as: \\n\\neval_data_001_predictions.csv and eval_data_001_predictions_excel.csv.\\ntrain_data_001_predictions.csvand‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tasmay-Tib/sarvam-entity-recognition-gemini-2.0-flash-thinking-01-21-distill-1600."},
	{"name":"piaozhu","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Retr01234/piaozhu","creator_name":"Chen","creator_url":"https://huggingface.co/Retr01234","description":"\\n\\t\\n\\t\\t\\n\\t\\tÊï∞ÊçÆÈõÜÂêçÁß∞ÔºöÂò¥Ëá≠Êê≠Â≠êÂæÆË∞ÉÊï∞ÊçÆÈõÜ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t1. Êï∞ÊçÆÈõÜÁÆÄ‰ªã\\n\\t\\n\\nËøô‰∏™Êï∞ÊçÆÈõÜ‰∏∫ÂæÆË∞ÉÂØπËØùÁîüÊàêÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÁâπÊÆäÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåÂü∫‰∫é‰∏Ä‰∏™ËôöÊãüÁöÑËßíËâ≤‚ÄúÊ≤àËì¨Á´π‚ÄùËøõË°å‰∫§‰∫í„ÄÇËøô‰∏™ËßíËâ≤ÔºàÂ§ñÂè∑‚ÄúÊú¥Á´π‚ÄùÔºâÂÖ∑ÊúâÂÜ∑Âò≤ÁÉ≠ËÆΩ„ÄÅÊØíËàå„ÄÅÁÆÄÊ¥ÅËÄåÊúâÊîªÂáªÊÄßÁöÑÁâπÁÇπÔºåÈÄÇÂêàËÆ≠ÁªÉÊ®°Âûã‰∫ßÁîüÂÖ∑ÊúâËÆΩÂà∫„ÄÅÂÜ∑Âò≤ÁÉ≠ËÆΩËØ≠Ê∞îÁöÑÂõûÁ≠î„ÄÇÊï∞ÊçÆÈõÜÁöÑÂÜÖÂÆπ‰∏ªË¶ÅÊòØËßíËâ≤ÊâÆÊºîÂØπËØùÂú∫ÊôØÔºåÈÄÇÁî®‰∫éÁîüÊàêÂÖ∑ÊúâÁâπÂÆöÈ£éÊ†ºÁöÑÂØπËØùÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®Â∏¶ÊúâËÆΩÂà∫ÂíåÂπΩÈªòÁöÑÊÉÖÂ¢É‰∏ãËøõË°å‰∫íÂä®Êó∂„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\t2. Êï∞ÊçÆÈõÜÁªìÊûÑ\\n\\t\\n\\nÊï∞ÊçÆÈõÜ‰∏∫‰∏Ä‰∏™ÂåÖÂê´Ëã•Âπ≤ÂØπËØùËΩÆÊ¨°ÁöÑ JSON Ê†ºÂºèÊñá‰ª∂„ÄÇÊØè‰∏™ÂØπËØùËΩÆÊ¨°Áî±ËßíËâ≤ÂíåÁî®Êà∑ÁöÑÂØπËØùÁªÑÊàêÔºåÊØè‰∏™ÂØπËØùÂåÖÂê´‰ª•‰∏ãÂ≠óÊÆµÔºö\\n\\nroleÔºöËßíËâ≤ÁöÑË∫´‰ªΩÔºåÂèØËÉΩÊòØ \\\"system\\\" Êàñ \\\"user\\\"„ÄÇ\\n\\\"system\\\" Ë°®Á§∫ÊòØÊ®°ÂûãËÆæÂÆöËßíËâ≤ÁöÑËæìÂÖ•ÔºàÂ¶ÇÂÆö‰πâËßíËâ≤ËÉåÊôØ„ÄÅË°å‰∏∫Ê®°ÂºèÁ≠âÔºâ„ÄÇ\\n\\\"user\\\" Ë°®Á§∫ÂØπËØù‰∏≠ÁöÑÁî®Êà∑ËæìÂÖ•ÔºàÂ¶ÇÊèêÈóÆ„ÄÅËØ∑Ê±ÇÊàñ‰∫§‰∫íÔºâ„ÄÇ\\n\\n\\ncontentÔºöÂØπËØùÂÜÖÂÆπÔºåË°®Á§∫ËßíËâ≤ÊàñËÄÖÁî®Êà∑ÁöÑÂÖ∑‰ΩìÂèëË®Ä„ÄÇ\\nloss_weightÔºàÂèØÈÄâÔºâÔºöÊØè‰∏™Êï∞ÊçÆÊù°ÁõÆÂØπÂ∫îÁöÑÊçüÂ§±ÊùÉÈáçÔºåÂΩìÂâçÂèØ‰∏∫Á©∫Êàñ‰∏∫ null„ÄÇÂèØ‰ª•Âú®Ê®°ÂûãËÆ≠ÁªÉ‰∏≠Âä†ÊùÉ‰∏çÂêåÂØπËØùÂÜÖÂÆπ„ÄÇ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t3. Êï∞ÊçÆÊ†∑‰æã‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Retr01234/piaozhu."},
	{"name":"Twilight_NLP","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/valerysukmanyuk/Twilight_NLP","creator_name":"Sukmanyuk Valery","creator_url":"https://huggingface.co/valerysukmanyuk","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis dataset is made for fun and in ‚ú®educational‚ú® purposes.\\n\\t\\n\\nIt includes tokenized novel text by Stephenie Meyer \\\"Breaking Dawn\\\" in Russian, PoS, NER (LOC, PER, ORG), tokens' lemmas and syntax annotation. NER, PoS, lemmatization and syntax annotation were made with spaCy and their 'ru_core_news_lg' \\n"},
	{"name":"text-clustering-example-data","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/billingsmoore/text-clustering-example-data","creator_name":"Jacob Moore","creator_url":"https://huggingface.co/billingsmoore","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThis dataset consists of 925 sentences in English paired with a broad topic descriptor for use as example data in product demonstrations or student projects.\\n\\nCurated by: billingsmoore\\nLanguage(s) (NLP): English\\nLicense: Apache License 2.0\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDirect Use\\n\\t\\n\\nThis data can be loaded using the following Python code.\\nfrom datasets import load_dataset\\n\\nds = load_dataset('billingsmoore/text-clustering-example-data')\\n\\nIt can then be clustered‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/billingsmoore/text-clustering-example-data."},
	{"name":"RoundTripOCR-nepali","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cfilt/RoundTripOCR-nepali","creator_name":"Computation for Indian Language Technology","creator_url":"https://huggingface.co/cfilt","description":"Post-OCR error correction dataset (train, test and validation set) for Nepali language generated using RoundTripOCR technique.\\nCode: https://github.com/harshvivek14/RoundTripOCR\\n"},
	{"name":"steam-reviews-constructiveness-binary-label-annotations-1.5k","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/abullard1/steam-reviews-constructiveness-binary-label-annotations-1.5k","creator_name":"Samuel Bullard","creator_url":"https://huggingface.co/abullard1","description":"\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n    1.5K Steam Reviews Binary Labeled for Constructiveness\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 1,461 Steam reviews from 10 of the most reviewed games. Each game has about the same amount of reviews. Each review is annotated with a binary label indicating whether the review is constructive or not. The dataset is designed to support tasks related to text classification, particularly constructiveness detection tasks in the gaming domain.\\n\\nAlso available as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/abullard1/steam-reviews-constructiveness-binary-label-annotations-1.5k."},
	{"name":"ytu-ara-proje-absa","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ebrukilic/ytu-ara-proje-absa","creator_name":"Ebru Kƒ±lƒ±√ß","creator_url":"https://huggingface.co/ebrukilic","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nDataset Name: ABSA (Aspect-Based Sentiment Analysis)\\nSize: 5045 instances\\nLanguage(s): Turkish\\nTask: Aspect-Based Sentiment Analysis (ABSA)\\nCategories: Etek, Kaban, G√∂mlek, Kazak, Pantolon\\nPolarity: Negatif, N√∂tr, Pozitif\\nAspect: Kalite, Kuma≈ü, Renk, Beden, Kargo, Fiyat\\nLicense: MIT\\nDeveloped by: ebru kƒ±lƒ±√ß , rumeysa nur yasav\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nBu veri k√ºmesi Yƒ±ldƒ±z Teknik √úniversitesi Bilgisayar M√ºhendisliƒüi √∂ƒürencilerinin Bilgisayar Projesi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ebrukilic/ytu-ara-proje-absa."},
	{"name":"ner-cat","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ugiat/ner-cat","creator_name":"Ugiat Technologies","creator_url":"https://huggingface.co/Ugiat","description":"\\n\\t\\n\\t\\t\\n\\t\\tNERCat Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe NERCat dataset is a manually annotated collection of Catalan-language television transcriptions, designed to improve Named Entity Recognition (NER) performance for the Catalan language. The dataset covers diverse domains such as politics, sports, and culture, and includes 9,242 sentences with 13,732 named entities annotated across eight categories: Person, Facility, Organization, Location, Product, Event, Date, and Law. The dataset was‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ugiat/ner-cat."},
	{"name":"MRC-psycholinguistic-database","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/StephanAkkerman/MRC-psycholinguistic-database","creator_name":"Stephan Akkerman","creator_url":"https://huggingface.co/StephanAkkerman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMRC Psycholinguistic Database\\n\\t\\n\\nThis is the complete MRC psycholinguistic database as found on https://websites.psychology.uwa.edu.au/school/mrcdatabase/uwa_mrc.htm.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThis dataset is ideal for training and evaluating machine learning models for English word concreteness.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAcknowledgments\\n\\t\\n\\nWe extend our heartfelt gratitude to all the authors of the original dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThis dataset is made available under the MIT license.\\n"},
	{"name":"TOFU-C-All","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-All","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-All."},
	{"name":"alpaca-bulgarian-jokes-multilingual-prompts","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts","creator_name":"Nikola","creator_url":"https://huggingface.co/vislupus","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBulgarian Jokes Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Bulgarian Jokes Dataset is a collection of Bulgarian-language jokes gathered and prepared for use in training and fine-tuning natural language processing (NLP) models. This dataset is designed to help researchers and developers build models capable of understanding and generating humorous content in Bulgarian.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is structured in a format suitable for NLP training and fine-tuning tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vislupus/alpaca-bulgarian-jokes-multilingual-prompts."},
	{"name":"TOFU-C-Shuffle","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Shuffle."},
	{"name":"TOFU-C-single","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Gyikoo/TOFU-C-single","creator_name":"Xinyi Gao","creator_url":"https://huggingface.co/Gyikoo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gyikoo/TOFU-C-single."},
	{"name":"TOFU-Cbin","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-Cbin","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-Cbin."},
	{"name":"SimpleStories","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lennart-finke/SimpleStories","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories yourself, head over to this repository.\\nIf you'd like to commission other languages or story formats, feel free to send mail.\\nSimpleStories is an iteration upon TinyStories by Eldan and Li, and can likewise be used for distillation to very small language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\n\\nStory‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories."},
	{"name":"TOFU-C-Direct","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/kimperyang/TOFU-C-Direct","creator_name":"Jingbo Yang","creator_url":"https://huggingface.co/kimperyang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kimperyang/TOFU-C-Direct."},
	{"name":"EmoPropMan","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/basavaraj/EmoPropMan","creator_name":"Basavaraj","creator_url":"https://huggingface.co/basavaraj","description":"basavaraj/EmoPropMan dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Loneliness-Causes-and-Intensity","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yael-katsman/Loneliness-Causes-and-Intensity","creator_name":"yael katsman","creator_url":"https://huggingface.co/yael-katsman","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tReddit Loneliness: Causes and intensity\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Loneliness- cause and intensity dataset is an English-language compilation of posts focused on loneliness among individuals and the different types of loneliness they experience. . The primary objective of this dataset is to aid various NLP models in predicting loneliness and its causes from text, which may be useful in the fields of mental health, NLP contextual‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yael-katsman/Loneliness-Causes-and-Intensity."},
	{"name":"ContactShieldDataset","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/xxparthparekhxx/ContactShieldDataset","creator_name":"Parth Parekh","creator_url":"https://huggingface.co/xxparthparekhxx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContactShieldDataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nContactShieldDataset is a high-quality, synthetically generated dataset designed for training and evaluating models that detect contact information sharing in text. This dataset is specifically tailored for use in freelancing and online marketplace contexts, where maintaining user privacy and platform policies is crucial.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nSize: 200,000 examples\\nLanguage: English\\nGeneration Method: Synthetically‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xxparthparekhxx/ContactShieldDataset."},
	{"name":"chatbot","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/neverland-th/chatbot","creator_name":"Neverlandweedshop","creator_url":"https://huggingface.co/neverland-th","description":"neverland-th/chatbot dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"TOFU-C-All","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/annnli/TOFU-C-All","creator_name":"Li An","creator_url":"https://huggingface.co/annnli","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/annnli/TOFU-C-All."},
	{"name":"scips_qa","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/zorpsoon/scips_qa","creator_name":"Prasoon Bajpai","creator_url":"https://huggingface.co/zorpsoon","description":"zorpsoon/scips_qa dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"Eason_TOFU","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/EasonZhong/Eason_TOFU","creator_name":"Yisheng Zhong","creator_url":"https://huggingface.co/EasonZhong","description":"EasonZhong/Eason_TOFU dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"AURA-Classification","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/irfan-ahmad/AURA-Classification","creator_name":"Irfan Ahmad","creator_url":"https://huggingface.co/irfan-ahmad","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAURA-Classification\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe AURA (App User Review in Arabic) Classification dataset is a collection of 2,900 Arabic-language app reviews collected from various mobile applications. This dataset is primarily designed for text classification tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFeatures\\n\\t\\n\\nThe dataset includes the following fields:\\n\\nreview: The text of the review in Arabic.\\n\\nappName: The name of the application being reviewed.\\n\\nplatform: The platform (iOS or Android)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/irfan-ahmad/AURA-Classification."},
	{"name":"talking-to-chatbots-unwrapped-chats","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats","creator_name":"David G. R.","creator_url":"https://huggingface.co/reddgr","description":"This work-in-progress dataset contains conversations with various LLM tools, sourced by the author of the website  Talking to Chatbots. \\nA simplified version of this dataset can be found at reddgr/talking-to-chatbots-chats, where messages belonging to a same conversation are 'wrapped' inside a single record. In this extended dataset, each conversation turn (pair of messages consisting of a user prompt and a response by the LLM) is presented as an individual record, with additional metrics and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reddgr/talking-to-chatbots-unwrapped-chats."},
	{"name":"Quran_English_Myanmar_Parrelel_Corpus","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kingkaung/Quran_English_Myanmar_Parrelel_Corpus","creator_name":"King Kaung","creator_url":"https://huggingface.co/kingkaung","description":"\\n\\t\\n\\t\\t\\n\\t\\tQuran English-Myanmar Parallel Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a parallel corpus of the Quran, containing translations in English and Myanmar. It includes 6,237 verses (ayahs) from all chapters (surahs), aligned by their respective Surah and Ayah numbers.\\n\\nEnglish Translation: Provided by Dr. Muhsin Khan and Dr. Hilali.\\nMyanmar Translation: Translated by the Myanmar Quran Translation Committee, comprising religious and non-religious scholars, and later published by‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kingkaung/Quran_English_Myanmar_Parrelel_Corpus."},
	{"name":"GammaCorpus-v1-70k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 70k - UNFILTERED\\n\\t\\n\\n\\n36 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 70k Unfiltered dataset consists of 70,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 35 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-70k-UNFILTERED."},
	{"name":"sa-data","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/phalanx80/sa-data","creator_name":"Paolo De Gasperis","creator_url":"https://huggingface.co/phalanx80","description":"\\n\\t\\n\\t\\t\\n\\t\\tStoria dell'Arte Dataset (SA-Data)\\n\\t\\n\\n \\n\\n\\t\\n\\t\\t\\n\\t\\tüìå Descrizione del Dataset\\n\\t\\n\\nIl dataset SA-Data √® una raccolta strutturata di articoli della rivista Storia dell'Arte (https://www.storiadellarterivista.it/) digitalizzati e arricchiti con metadati dettagliati e rappresentazioni semantiche. √à stato creato per supportare la ricerca accademica e le applicazioni di elaborazione del linguaggio naturale.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîç Contenuto\\n\\t\\n\\nIl dataset include:\\n\\n1050 articoli pubblicati tra il 1969 e‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/phalanx80/sa-data."},
	{"name":"LatinSummarizerDataset","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tLatinSummarizer Dataset\\n\\t\\n\\n    \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe LatinSummarizerDataset is a structured dataset used in the GitHub Repository for Latin summarization and translation tasks. This dataset provides aligned English-Latin texts, extractive summaries, and pre-training prompts for fine-tuning models like mT5 for low-resource NLP applications.\\n\\n\\t\\n\\t\\t\\n\\t\\tStructure\\n\\t\\n\\nThe dataset is divided into two main phases: \\n\\nPre-training Data: Includes aligned bilingual corpora, synthetic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizerDataset."},
	{"name":"Telugu-NLP-AI-Dialect-Comedy-video-Dataset","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Automation-Tribe/Telugu-NLP-AI-Dialect-Comedy-video-Dataset","creator_name":"AUTTRIBE-AI-AUTOMATION","creator_url":"https://huggingface.co/Automation-Tribe","description":"Telugu is one of the sweetest and oldest languages of India. A deep Dive into Telugu its spoken in 2 states and majorly 16 regional dailects.\\nThis Dataset help you perform operations in NLP and Speech Recognition Models towards telugu Dialects.\\n"},
	{"name":"StudyAbroadGPT-Dataset","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/millat/StudyAbroadGPT-Dataset","creator_name":"MD MILLAT HOSEN","creator_url":"https://huggingface.co/millat","description":"\\n\\t\\n\\t\\t\\n\\t\\tStudyAbroadGPT-Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe StudyAbroadGPT-Dataset is a collection of conversational data focused on university application requirements for various programs, including MBA, MS in Computer Science, Data Science, and Bachelor of Medicine. The dataset includes interactions between humans asking questions about application processes (e.g., \\\"How do I write a strong SOP for MS in Data Science at MIT?\\\") and an assistant providing detailed responses. Covering‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/millat/StudyAbroadGPT-Dataset."},
	{"name":"superglue","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hyukkyu/superglue","creator_name":"Hyukkyu Kang","creator_url":"https://huggingface.co/Hyukkyu","description":"\\n\\t\\n\\t\\t\\n\\t\\tSuperGLUE Benchmark Datasets\\n\\t\\n\\nThis repository contains the SuperGLUE benchmark datasets. Each dataset is available as a separate configuration, making it easy to load individual datasets using the datasets library.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Descriptions\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDatasets Included\\n\\t\\n\\n\\nBoolQ: A question-answering task where each example consists of a short passage and a yes/no question about the passage. The questions are provided anonymously and unsolicited by users of the Google search‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Hyukkyu/superglue."},
	{"name":"SemEval_task10","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/eerrffuunn/SemEval_task10","creator_name":"Mohammaderfan Koupaei","creator_url":"https://huggingface.co/eerrffuunn","description":"eerrffuunn/SemEval_task10 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"wiktionary-data","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/paion-data/wiktionary-data","creator_name":"Paion Data","creator_url":"https://huggingface.co/paion-data","description":"\\n\\t\\n\\t\\t\\n\\t\\tWiktionary Data on Hugging Face Datasets\\n\\t\\n\\n\\n\\n\\n\\nwiktionary-data is a sub-data extraction of the English Wiktionary that currently\\nsupports the following languages:\\n\\nDeutsch - German\\nLatinum - Latin\\n·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ - Ancient Greek\\nÌïúÍµ≠Ïñ¥ - Korean\\nêé†êéºêéπ- Old Persian\\níÄùíÖóíÅ∫íåë(íåù) - Akkadian\\nElamite\\n‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç - Sanskrit, or Classical Sanskrit\\n\\nwiktionary-data was originally a sub-module of wilhelm-graphdb. While\\nthe dataset it's getting bigger, I noticed a wave of more exciting potentials this‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/paion-data/wiktionary-data."},
	{"name":"PubMed-Cancer-NLP-Textual-Dataset","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cyberpsych/PubMed-Cancer-NLP-Textual-Dataset","creator_name":"Om Aryan","creator_url":"https://huggingface.co/cyberpsych","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPubMed-Cancer-NLP-Textual-Dataset\\n\\t\\n\\nThis dataset has been obtained from PubMed for research purposes. README will be updated with time.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nIt has multiple cancer samples with labels with their title and abstract from PubMed Repository.\\n\\nCurated by: Om Aryan\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\nRepository: https://pubmed.ncbi.nlm.nih.gov\\n\\n"},
	{"name":"movies","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IsmaelMousa/movies","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMovie Scripts Dataset\\n\\t\\n\\nThe Movie Scripts Dataset consists of scripts from 1,172 movies, providing a comprehensive collection of movie dialogues and narratives. This dataset is designed to support various natural language processing (NLP) tasks, including dialogue generation, script summarization, and text analysis.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nThe dataset contains 2 columns:\\n\\nName: The title of the movie.\\nScript: The full script of the movie in English.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUsage\\n\\t\\n\\nThe Movie‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/movies."},
	{"name":"UnitedStatesSenateBillsAndSummaries","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/cheaptrix/UnitedStatesSenateBillsAndSummaries","creator_name":"Taylor Hartman","creator_url":"https://huggingface.co/cheaptrix","description":"cheaptrix/UnitedStatesSenateBillsAndSummaries dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"UnitedStatesSenateBillsAndSummaries","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/MTSUFall2024SoftwareEngineering/UnitedStatesSenateBillsAndSummaries","creator_name":"MTSU Fall 2024 Software Engineering","creator_url":"https://huggingface.co/MTSUFall2024SoftwareEngineering","description":"MTSUFall2024SoftwareEngineering/UnitedStatesSenateBillsAndSummaries dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"wilhelm-vocabulary","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary","creator_name":"Jiaqi","creator_url":"https://huggingface.co/QubitPi","description":"\\n\\t\\n\\t\\t\\n\\t\\tWilhelm Vocabulary\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWilhelm Vocabulary\\nDevelopment\\nEnvironment Setup\\nInstalling Dependencies\\nData Format\\nEncoding Table in YAML\\n\\n\\nData Pipeline\\nHow Data (Vocabulary) is Stored in a Graph Database\\nWhy Graph Database\\nBase Schema\\n\\n\\n\\n\\nLanguages\\nGerman\\nPronoun\\nNoun\\nVerb\\n\\n\\nAncient Greek\\nDiacritic Mark Convention\\nPronoun\\nNoun\\nAdjective\\n1. Three-Ending Adjectives: 1st and 2nd Declension (2-1-2)2. Two-Ending 2nd Declension Adjectives (2-2)\\n3. Two-Ending 3rd Declension Adjectives‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QubitPi/wilhelm-vocabulary."},
	{"name":"learn_hf_food_not_food_image_captions","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/tanvircr7/learn_hf_food_not_food_image_captions","creator_name":"Mohammad Tanvir Hasan","creator_url":"https://huggingface.co/tanvircr7","description":"inspired by @mrdbourke\\nTakes the original 250 samples \\nAdds more samples generated from mistral.ai\\nFind the code for dataset creation in this notebook: \\n[colab]{https://github.com/tanvircr7/meh/blob/master/huggingface_food_not_food_image_caption_dataset_creation.ipynb}\\n"},
	{"name":"sql-create-context","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/b-mc2/sql-create-context","creator_name":"brianm","creator_url":"https://huggingface.co/b-mc2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context."},
	{"name":"WaterDrum-Ax","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\tWaterDrum: Watermarking for Data-centric Unlearning Metric\\n\\t\\n\\nWaterDrum provides an unlearning benchmark for the evaluation of effectiveness and practicality of unlearning. The repository contains the ArXiv corpus of WaterDrum (WaterDrum-Ax), which contains both unwatermarked and watermarked ArXiv paper abstracts across\\n20 categories published after the release of the Llama-2 model. Each category contains 400 data samples, aggregating into 8000 samples in the full training set. The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."},
	{"name":"spider-test-portuguese","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boakpe/spider-test-portuguese","creator_name":"Breno","creator_url":"https://huggingface.co/Boakpe","description":"\\n\\t\\n\\t\\t\\n\\t\\tSpider Dataset - Vers√£o em Portugu√™s\\n\\t\\n\\nEste reposit√≥rio cont√©m a tradu√ß√£o para portugu√™s da parti√ß√£o de teste do dataset Spider, um benchmark para a tarefa de Text-to-SQL.\\n\\n\\t\\n\\t\\t\\n\\t\\tSobre esta tradu√ß√£o\\n\\t\\n\\nA tradu√ß√£o da parti√ß√£o \\\"test\\\" do Spider (contendo 2.147 inst√¢ncias) foi realizada seguindo um processo rigoroso:\\n\\nTradu√ß√£o inicial: Utilizando a API do GPT-4o mini da OpenAI\\nRevis√£o manual: Todas as 2.147 quest√µes foram revisadas e validadas manualmente\\nCrit√©rios de tradu√ß√£o:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boakpe/spider-test-portuguese."},
	{"name":"SpellGram","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vishnun/SpellGram","creator_name":"Vishnu Nandakumar","creator_url":"https://huggingface.co/vishnun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpellGram\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset consisting of grammatical and spelling errors\\n\\t\\n\\n\\nHomepage: \\nRepository: \\nPaper: \\nLeaderboard: \\nPoint of Contact:\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[train.csv]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vishnun/SpellGram."},
	{"name":"edgar-corpus","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/c3po-ai/edgar-corpus","creator_name":"C3PO-AI","creator_url":"https://huggingface.co/c3po-ai","description":"The dataset contains annual filings (10K) of all publicly traded firms from 1993-2020. The table data is stripped but all text is retained.\\nThis dataset allows easy access to the EDGAR-CORPUS dataset based on the paper EDGAR-CORPUS: Billions of Tokens Make The World Go Round (See References in README.md for details)."},
	{"name":"sp500-edgar-10k","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/jlohding/sp500-edgar-10k","creator_name":"Jerry Loh","creator_url":"https://huggingface.co/jlohding","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for SP500-EDGAR-10K\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains the annual reports for all SP500 historical constituents from 2010-2022 from SEC EDGAR Form 10-K filings.\\nIt also contains n-day future returns of each firm's stock price from each filing date.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Splits\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Creation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSource Data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jlohding/sp500-edgar-10k."},
	{"name":"toxi-text-3M","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/toxi-text-3M","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large multilingual toxicity dataset with 3M rows of text data from 55 natural languages, all of which are written/sent by humans, not machine translation models.\\nThe preprocessed training data alone consists of 2,880,667 rows of comments, tweets, and messages. Among these rows, 416,529 are classified as toxic, while the remaining 2,463,773 are considered neutral. Below is a table to illustrate the data composition:\\n\\n\\t\\n\\t\\t\\n\\nToxic\\nNeutral\\nTotal\\n\\n\\n\\t\\t\\nmultilingual-train-deduplicated.csv‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/toxi-text-3M."},
	{"name":"sql-create-context-instruction","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction","creator_name":"Spartak Bughdaryan","creator_url":"https://huggingface.co/bugdaryan","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built upon SQL Create Context, which in turn was constructed using data from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-SQL LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-SQL datasets. The CREATE TABLE statement can often‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction."},
	{"name":"TOFU","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/locuslab/TOFU","creator_name":"Locus Lab","creator_url":"https://huggingface.co/locuslab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTOFU: Task of Fictitious Unlearning üç¢\\n\\t\\n\\nThe TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tQuick Links\\n\\t\\n\\n\\nWebsite: The landing page‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/locuslab/TOFU."},
	{"name":"FREDSum","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/linagora/FREDSum","creator_name":"LINAGORA Labs","creator_url":"https://huggingface.co/linagora","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe FREDSum dataset is a comprehensive collection of transcripts and metadata from various political and public debates in France. The dataset aims to provide researchers, linguists, and data scientists with a rich source of debate content for analysis and natural language processing tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nFrench\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is made of 144 debates, 115 of the debates make up the train set, while 29 make up the test set‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/linagora/FREDSum."},
	{"name":"human_ai_generated_text","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dmitva/human_ai_generated_text","creator_name":"DV","creator_url":"https://huggingface.co/dmitva","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHuman or AI-Generated Text\\n\\t\\n\\nThe data can be valuable for educators, policymakers, and researchers interested in the evolving education landscape, particularly in detecting or identifying texts written by Humans or Artificial Intelligence systems.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFile Name\\n\\t\\n\\nmodel_training_dataset.csv\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFile Structure\\n\\t\\n\\n\\nid: Unique identifier for each record.\\nhuman_text: Human-written content.\\nai_text: AI-generated texts.\\ninstructions: Description of the task given to both‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dmitva/human_ai_generated_text."},
	{"name":"urdu-idioms-with-english-translation","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Ehtisham1328/urdu-idioms-with-english-translation","creator_name":"Ehtisham ul Hassan","creator_url":"https://huggingface.co/Ehtisham1328","description":"Ehtisham1328/urdu-idioms-with-english-translation dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"newswire","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dell-research-harvard/newswire","creator_name":"Dell Research Harvard","creator_url":"https://huggingface.co/dell-research-harvard","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for NewsWire\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nNewsWire contains 2.7 million unique public domain U.S. news wire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish (en)\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nEach year in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dell-research-harvard/newswire."},
	{"name":"clinical-synthetic-text-llm","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ritaranx/clinical-synthetic-text-llm","creator_name":"Ran Xu","creator_url":"https://huggingface.co/ritaranx","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Description\\n\\t\\n\\nWe release the synthetic data generated using the method described in the paper Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\n (ACL 2024 Findings). The external knowledge we use is based on LLM-generated topics and writing styles.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGenerated Datasets\\n\\t\\n\\nThe original train/validation/test data, and the generated synthetic training data are listed as follows. For each dataset, we generate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ritaranx/clinical-synthetic-text-llm."},
	{"name":"Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes","creator_name":"yassir acharki","creator_url":"https://huggingface.co/yassiracharki","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\nThe Amazon reviews full score dataset is constructed by randomly taking 600,000 training samples and 130,000 testing samples for each review score from 1 to 5. In total there are 3,000,000 trainig samples and 650,000 testing samples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes."},
	{"name":"Vietnamese_spelling_error","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ShynBui/Vietnamese_spelling_error","creator_name":"Bui Tien Phat","creator_url":"https://huggingface.co/ShynBui","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVietnamese Spelling Error Dataset\\n\\t\\n\\nThis dataset contains examples of Vietnamese text with spelling errors and their corresponding corrections. It is intended to be used for training and evaluating models in spelling correction tasks, particularly for the Vietnamese language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nName: Vietnamese Spelling Error Dataset\\nLanguage: Vietnamese\\nFile Format: [CSV/Parquet/dataset/etc.]\\nColumns:\\ntext: The corresponding corrected version of the text.\\nerror_text:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ShynBui/Vietnamese_spelling_error."},
	{"name":"books","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IsmaelMousa/books","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBooks\\n\\t\\n\\nThe books dataset consists of a diverse collection of books organized into 9 categories, it splitted to train, validation where the train contains 40 books, and the validation 9 books.\\nThis dataset is cleaned well and designed to support various natural language processing (NLP) tasks, including text generation and masked language modeling.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\nThe dataset contains 4 columns:\\n\\ntitle: The tilte of the book.\\nauthor: The author of the book.\\ncategory: The‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/books."},
	{"name":"libri-in-italiano","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/IsmaelMousa/libri-in-italiano","creator_name":"Ismael","creator_url":"https://huggingface.co/IsmaelMousa","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLibri\\n\\t\\n\\nIl dataset dei libri consiste in una raccolta diversificata di 18 libri organizzati in 4 categorie.\\nQuesto dataset √® ben pulito e progettato per supportare diversi compiti di elaborazione del linguaggio naturale (NLP), inclusi generazione di testo, traduzione e modellazione del linguaggio mascherato.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDettagli\\n\\t\\n\\nIl dataset contiene 4 colonne:\\n\\ntitolo: Il titolo del libro.\\nautore: L'autore del libro.\\ncategoria: Il genere/categoria del libro.\\ncontenuto: Il‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/IsmaelMousa/libri-in-italiano."},
	{"name":"Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VIITPune/Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus","creator_name":"BRACT's Vishwakarma Institute of Information Technology","creator_url":"https://huggingface.co/VIITPune","description":"Maharashtri Prakrit to English Parallel Corpus\\nDataset Summary\\nThis dataset contains parallel text data for translating from Maharashtri Prakrit (an ancient Indo-Aryan language) to English. It is designed to aid in developing machine translation systems, language models, and linguistic research for this underrepresented language. The dataset is collected from historical texts, scriptures, and scholarly resources.\\nKey Features:\\n\\n  Source Language: Maharashtri Prakrit\\n  Target Language: English‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/VIITPune/Deshika-Maharashtri_Prakrit_to_English_Parallel_Corpus."},
	{"name":"Research-Papers","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/khushwant04/Research-Papers","creator_name":"Khushwant Sanwalot","creator_url":"https://huggingface.co/khushwant04","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAI & Machine Learning Research Papers Dataset\\n\\t\\n\\nThis dataset contains a curated collection of 1296 research papers focused on advancements in Artificial Intelligence and Machine Learning. It is intended as a resource for researchers, educators, and developers to explore and analyze diverse topics within AI and ML.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nTotal Papers: 1296\\nDomains Covered: \\nArtificial Intelligence (AI)\\nMachine Learning (ML)\\nDeep Learning\\nNatural Language Processing (NLP)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/khushwant04/Research-Papers."},
	{"name":"unal-repository-dataset-instruct","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset InstructDescripci√≥n: Este dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje para tareas de preguntas y respuestas.  \\nColumnas:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-instruct."},
	{"name":"ytu-araproje-absa-7400","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ebrukilic/ytu-araproje-absa-7400","creator_name":"Ebru Kƒ±lƒ±√ß","creator_url":"https://huggingface.co/ebrukilic","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nDataset Name: ABSA (Aspect-Based Sentiment Analysis)\\nSize: 7471 instances\\nLanguage(s): Turkish\\nTask: Aspect-Based Sentiment Analysis (ABSA)\\nCategories: Etek, Kaban, G√∂mlek, Kazak, Pantolon\\nPolarity: Negatif, N√∂tr, Pozitif\\nAspect: Kalite, Kuma≈ü, Renk, Beden, Kargo, Fiyat\\nLicense: MIT\\nDeveloped by: ebru kƒ±lƒ±√ß , rumeysa nur yasav\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\nBu veri k√ºmesi Yƒ±ldƒ±z Teknik √úniversitesi Bilgisayar M√ºhendisliƒüi √∂ƒürencilerinin Bilgisayar Projesi‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ebrukilic/ytu-araproje-absa-7400."},
	{"name":"HausaHate","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/franciellevargas/HausaHate","creator_name":"Francielle Vargas","creator_url":"https://huggingface.co/franciellevargas","description":"\\n\\t\\n\\t\\t\\n\\t\\tEvaluation Benchmark for Hausa Hate Speech Detection\\n\\t\\n\\nWe introduce the first expert annotated corpus of Facebook comments for Hausa hate speech detection. \\nThe corpus titled HausaHate comprises 2,000 comments extracted from Western African Facebook pages and\\nmanually annotated by three Hausa native speakers, who are also NLP experts. \\nThe corpus was annotated using two different layers. We first labeled each comment according to a \\nbinary classification: offensive versus‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/franciellevargas/HausaHate."},
	{"name":"resumes","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/datasetmaster/resumes","creator_name":"Oks","creator_url":"https://huggingface.co/datasetmaster","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Advanced Resume Parser & Job Matcher Resumes\\n\\t\\n\\nThis dataset contains a merged collection of real and synthetic resume data in JSON format. The resumes have been normalized to a common schema to facilitate the development of NLP models for candidate-job matching in the technical recruitment domain.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset is a combined collection of real resumes and synthetically generated CVs. \\n\\nCurated by: datasetmaster‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datasetmaster/resumes."},
	{"name":"UNTreatyBodies_GeneralComments","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/UNTreatyBodiesDocSearch/UNTreatyBodies_GeneralComments","creator_name":"UN Treaty Bodies General Comments/Recommendation Database","creator_url":"https://huggingface.co/UNTreatyBodiesDocSearch","description":"This dataset contains annotated JSON files used in the UN Treaty Bodies Doc Search Flask application (https://github.com/lszoszk/UN-TreatyBodiesDocSearch, also available at: https://lszoszk.pythonanywhere.com/). \\nThe dataset enables users to search and analyze the General Comments and Recommendations adopted by UN Treaty Bodies. \\nIt is designed to facilitate keyword-based search, filtering by concerned groups, and customized labeling, making it easier to navigate and analyze official UN treaty‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/UNTreatyBodiesDocSearch/UNTreatyBodies_GeneralComments."},
	{"name":"Personal-Finance-Queries","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Akhil-Theerthala/Personal-Finance-Queries","creator_name":"Akhil Theerthala","creator_url":"https://huggingface.co/Akhil-Theerthala","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA curated collection of Reddit posts and top comments focused on personal finance questions. The data is further filtered with the help of LLM-based Voting scores. These scores determine if the query is relevant to a person's financial queries among the other posts of the subreddits.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tFiltered Dataset (6.3k samples)\\n\\t\\n\\nColumns:\\n\\ntitle: Post title (string)\\nselftext: User‚Äôs detailed financial query (string)\\nsubreddit: Source‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Akhil-Theerthala/Personal-Finance-Queries."},
	{"name":"Legal-Dataset-for-india","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ShreyasP123/Legal-Dataset-for-india","creator_name":"Shreyas Patil","creator_url":"https://huggingface.co/ShreyasP123","description":"ShreyasP123/Legal-Dataset-for-india dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"NSFW_Chat_Dataset","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset","creator_name":"Utsav Maji","creator_url":"https://huggingface.co/utsavm","description":"\\n\\t\\n\\t\\t\\n\\t\\tüíï Spicy AI GF Chat Dataset üî•\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüö® 18+ Only! NSFW & Spicy Content Ahead üö®\\n\\t\\n\\nHey there, AI enthusiasts and romance lovers! üòè Welcome to the Spicy AI GF Chat Dataset, the ultimate dataset designed to bring your AI waifu to life! üíñ If you've ever dreamed of building an AI that responds like your virtual girlfriend, THIS is the dataset for you.\\n\\n\\t\\n\\t\\t\\n\\t\\tüìú What‚Äôs Inside?\\n\\t\\n\\nThis dataset features two columns:\\n\\ninput ‚Üí Boyfriend‚Äôs dialogue (aka what YOU say üòâ)\\noutput ‚Üí‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/utsavm/NSFW_Chat_Dataset."},
	{"name":"surname-nationality","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Hobson/surname-nationality","creator_name":"Hobson Lane","creator_url":"https://huggingface.co/Hobson","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPopular Surname Nationality Mapping\\n\\t\\n\\nSample of popular surnames for 30+ countries labeled with nationality (language)\\n"},
	{"name":"brwac_tiny","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/thegoodfellas/brwac_tiny","creator_name":"The Good Fellas","creator_url":"https://huggingface.co/thegoodfellas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BrWac\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe BrWaC (Brazilian Portuguese Web as Corpus) is a large corpus constructed following the Wacky framework, \\nwhich was made public for research purposes. The current corpus version, released in January 2017, is composed by \\n3.53 million documents, 2.68 billion tokens and 5.79 million types. Please note that this resource is available \\nsolely for academic research purposes, and you agreed not to use it for any commercial‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/thegoodfellas/brwac_tiny."},
	{"name":"panda","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/facebook/panda","creator_name":"AI at Meta","creator_url":"https://huggingface.co/facebook","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PANDA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nPANDA (Perturbation Augmentation NLP DAtaset) consists of approximately 100K pairs of crowdsourced human-perturbed text snippets (original, perturbed). Annotators were given selected terms and target demographic attributes, and instructed to rewrite text snippets along three demographic axes: gender, race and age, while preserving semantic meaning. Text snippets were sourced from a range of text corpora (BookCorpus, Wikipedia‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/panda."},
	{"name":"WarOnline","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/kertser/WarOnline","creator_name":"Mike Kertser","creator_url":"https://huggingface.co/kertser","description":"This is a conversational dataset, collected from WarOnine Israeli military forum\\nLanguage = Russian (with hebrew addins)\\nTarget Audience = Military\\nDataset has been used to train a Military Chat Bot\\n"},
	{"name":"NLP-KnowledgeGraph","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vishnun/NLP-KnowledgeGraph","creator_name":"Vishnu Nandakumar","creator_url":"https://huggingface.co/vishnun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKG dataset created by using spaCy PoS and Dependency parser. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nCan be leveraged for token classification for detection of knowledge graph entities and relations.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\n[More Information Needed]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Fields\\n\\t\\n\\nImportant fields for the token classification task are\\n\\ntokens -‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vishnun/NLP-KnowledgeGraph."},
	{"name":"ro-offense","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/readerbench/ro-offense","creator_name":"ReaderBench","creator_url":"https://huggingface.co/readerbench","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"RO-Offense-Sequences\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/readerbench/ro-offense-sequences\\nRepository: https://github.com/readerbench/ro-offense-sequences\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\na novel Romanian language dataset for offensive language detection with manually \\nannotated offensive labels from a local Romanian sports news website (gsp.ro):\\nResulting in 12,445 annotated messages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/readerbench/ro-offense."},
	{"name":"black-box-api-challenges","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/CohereForAI/black-box-api-challenges","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card\\n\\t\\n\\nPaper: On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research\\nAbstract: Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/black-box-api-challenges."},
	{"name":"fast-flash-hackernews-posts","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-posts","creator_name":"Fast Flash Studio ‚Äî a Multidisciplinary Creative Studio","creator_url":"https://huggingface.co/fast-flash","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFast Flash | HackerNews Posts Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExploratory Analysis\\n\\t\\n\\nTake a look at some fascinating findings from this dataset on our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe release dataset of all HackerNews posts.\\nThe dataset includes 35,316,999 posts and was collected in March 2023. \\nYou can also find a dataset of all users right here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe post objects in this dataset are structured according to HackerNews' API specification.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-posts."},
	{"name":"fast-flash-hackernews-users","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-users","creator_name":"Fast Flash Studio ‚Äî a Multidisciplinary Creative Studio","creator_url":"https://huggingface.co/fast-flash","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFast Flash | HackerNews Users Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tExploratory Analysis\\n\\t\\n\\nTake a look at some fascinating findings from this dataset on our website.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nWe release dataset of all HackerNews users who have posted at least once. \\nThe dataset includes 853,840 users and was collected on Sunday, March 26, 2023. \\nYou can find a dataset of all posts right here.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe user objects in this dataset are structured according to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fast-flash/fast-flash-hackernews-users."},
	{"name":"sql-create-context-copy","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/philschmid/sql-create-context-copy","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFork of b-mc2/sql-create-context\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/sql-create-context-copy."},
	{"name":"propsegment","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sihaochen/propsegment","creator_name":"Sihao Chen","creator_url":"https://huggingface.co/sihaochen","description":"This is a reproduced (i.e. after web-crawling) and processed version of the \\\"PropSegment\\\" dataset from Google Research.\\n\\nSince the News portion of the dataset is released only via urls, we reconstruct the dataset by crawling. Overall, ~96% \\nof the dataset can be reproduced, and the rest ~4% either have url no longer valid, or sentences that have been edited \\n(i.e. cannot be aligned with the orignial dataset).\\n\\nPropSegment (Proposition-level Segmentation and Entailment) is a large-scale, human annotated dataset for segmenting \\nEnglish text into propositions, and recognizing proposition-level entailment relations --- whether a different, related \\ndocument entails each proposition, contradicts it, or neither.\\n\\nThe original dataset features >45k human annotated propositions, i.e. individual semantic units within sentences, as \\nwell as >45k entailment labels between propositions and documents."},
	{"name":"tajik-text-segmentation","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/sobir-hf/tajik-text-segmentation","creator_name":"sobir","creator_url":"https://huggingface.co/sobir-hf","description":"This dataset contains texts in Tajik language with sentence annotations. It can be used to train and evaluate sentence-wise text segmentation algorithms.\\nThe dataset contains more than 100 short and long texts and more than 3000 annotated sentences. The texts were carefully selected from different catergories \\nsuch as news, articles, novels, classical texts, poetry, and religious texts. It deliberately contains more of \\\"hard\\\" passages where splitting them by period \\\".\\\" characters would result‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sobir-hf/tajik-text-segmentation."},
	{"name":"Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data","keyword":"nlp","license":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data","creator_name":"Senem Aktas","creator_url":"https://huggingface.co/Senem","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nThe dataset is a collection of  Youtube Comments and it was captured using the YouTube Data API. \\nThe data set consists of 1500 nostalgic and non-nostalgic comments in English.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nThe language of the data is English.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you find this dataset usefull for your study, please cite the paper as followed:\\n@article{postalcioglu2020comparison,\\n  title={Comparison of Neural Network Models for Nostalgic Sentiment Analysis of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data."},
	{"name":"all-scam-spam","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FredZhang7/all-scam-spam","creator_name":"Fred Zhang","creator_url":"https://huggingface.co/FredZhang7","description":"This is a large corpus of 42,619 preprocessed text messages and emails sent by humans in 43 languages. is_spam=1 means spam and is_spam=0 means ham.\\n1040 rows of balanced data, consisting of casual conversations and scam emails in ‚âà10 languages, were manually collected and annotated by me, with some help from ChatGPT.\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSome preprcoessing algorithms\\n\\t\\n\\n\\nspam_assassin.js, followed by spam_assassin.py\\nenron_spam.py\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData composition\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nTo‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FredZhang7/all-scam-spam."},
	{"name":"guanaco-extended","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FinchResearch/guanaco-extended","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHugging Face Dataset Card: Amoeba Mixed AI-Human Generated Samples\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nAmoeba Mixed AI-Human Generated Samples is a massive dataset that contains a diverse collection of text samples generated by both AI models and human authors. With a size exceeding 13 GB, this dataset is designed to foster research and development in the field of natural language generation and understanding.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntended Use\\n\\t\\n\\nThe Amoeba Mixed AI-Human Generated Samples dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FinchResearch/guanaco-extended."},
	{"name":"reBERT-forchat-incomplete","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FinchResearch/reBERT-forchat-incomplete","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"FinchResearch/reBERT-forchat-incomplete dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"pallas_splitted_18c","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/FinchResearch/pallas_splitted_18c","creator_name":"Finch Research","creator_url":"https://huggingface.co/FinchResearch","description":"FinchResearch/pallas_splitted_18c dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"fake_news_en_opensources","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/andyP/fake_news_en_opensources","creator_name":"Andrei Paraschiv","creator_url":"https://huggingface.co/andyP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Fake News Opensources\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/AndyTheFactory/FakeNewsDataset\\nRepository: https://github.com/AndyTheFactory/FakeNewsDataset\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\na consolidated and cleaned up version of the opensources Fake News dataset\\nFake News Corpus comprises 8,529,090 individual articles, classified into 12 classes: reliable, unreliable, political, bias, fake‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyP/fake_news_en_opensources."},
	{"name":"msmarco-10k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/nixiesearch/msmarco-10k","creator_name":"Nixiesearch","creator_url":"https://huggingface.co/nixiesearch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tA 10K docs sample from MS MARCO\\n\\t\\n\\nThis is a sample dataset of random 10K rows from the MS MARCO dataset. This is used in Nixiesearch quickstart guide to save some time indexing a full MSMARCO with 8M documents.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSchema\\n\\t\\n\\nThis is a JSONL-formatted dataset with only two fields inside: id for document identifier and text for the actual text snippet.\\n{\\n  \\\"id\\\": \\\"0\\\",\\n  \\\"text\\\": \\\"The presence of communication amid scientific minds was equally important to the success of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nixiesearch/msmarco-10k."},
	{"name":"sql-parsed","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/VishalCh/sql-parsed","creator_name":"Vishal Choudhary","creator_url":"https://huggingface.co/VishalCh","description":"VishalCh/sql-parsed dataset hosted on Hugging Face and contributed by the HF Datasets community"},
	{"name":"ro-paraphrase-bible","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/andyP/ro-paraphrase-bible","creator_name":"Andrei Paraschiv","creator_url":"https://huggingface.co/andyP","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Romanian Bible Paraphrase Corpus\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nHomepage: https://github.com/AndyTheFactory/ro-paraphrase-bible\\nRepository: https://github.com/AndyTheFactory/ro-paraphrase-bible\\nPoint of Contact: Andrei Paraschiv\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nA paraphprase corpus created from 10 different Romanian language Bible versions. Since the Bible has all paragraphs uniquely numbered an alignment between two \\nversions is straighforward. \\nWe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/andyP/ro-paraphrase-bible."},
	{"name":"sql-create-context-id","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/detakarang/sql-create-context-id","creator_name":"Gede Putra Nugraha","creator_url":"https://huggingface.co/detakarang","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a fork from sql-create-context \\nThis dataset builds from WikiSQL and Spider.\\nThere are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/detakarang/sql-create-context-id."},
	{"name":"english_to_igbo","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/ccibeekeoc42/english_to_igbo","creator_name":"Chinemerem Christopher Ibe-Ekeocha","creator_url":"https://huggingface.co/ccibeekeoc42","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEnglish-Igbo Parallel Corpus\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset is a comprehensive collection of parallel sentences in English and Igbo. It has been compiled from multiple sources to create a rich resource for machine translation, language research, and natural language processing tasks. The dataset is particularly valuable for those focusing on Igbo, a language spoken primarily in Nigeria, which is underrepresented in the field of computational linguistics.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ccibeekeoc42/english_to_igbo."},
	{"name":"hf-blogs","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/chenglu/hf-blogs","creator_name":"Luke Cheng","creator_url":"https://huggingface.co/chenglu","description":"Hugging Face Blog Content..\\n"},
	{"name":"pandas-create-context","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/hiltch/pandas-create-context","creator_name":"Or Hiltch","creator_url":"https://huggingface.co/hiltch","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is built from sql-create-context, which in itself builds from WikiSQL and Spider.\\nI have used GPT4 to translate the SQL schema into pandas DataFrame schem initialization statements and to translate the SQL queries into pandas queries. \\nThere are 862 examples of natural language queries, pandas DataFrame creation statements, and pandas query answering the question using the DataFrame creation statement as context. This dataset was built with text-to-pandas‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hiltch/pandas-create-context."},
	{"name":"Contextual_Response_Evaluation_for_ESL_and_ASD_Support","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/yunjaeys/Contextual_Response_Evaluation_for_ESL_and_ASD_Support","creator_name":"Eric Soderquist","creator_url":"https://huggingface.co/yunjaeys","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Contextual Response Evaluation for ESL and ASD Supportüíúüí¨üåê\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description üìñ\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary üìù\\n\\t\\n\\nCurated by Eric Soderquist, this dataset is a collection of English prompts and responses generated by the Phi-2 model, designed to evaluate and improve NLP models for supporting ESL (English as a Second Language) and ASD (Autism Spectrum Disorder) user bases. Each prompt is paired with multiple AI-generated responses and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yunjaeys/Contextual_Response_Evaluation_for_ESL_and_ASD_Support."},
	{"name":"MultiCaRe_Dataset","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset","creator_name":"Mauro Nievas Offidani","creator_url":"https://huggingface.co/mauro-nievoff","description":"The dataset contains multi-modal data from over 75,000 open access and de-identified case reports, including metadata, clinical cases, image captions and more than 130,000 images. Images and clinical cases belong to different medical specialties, such as oncology, cardiology, surgery and pathology. The structure of the dataset allows to easily map images with their corresponding article metadata, clinical case, captions and image labels. Details of the data structure can be found in the file‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mauro-nievoff/MultiCaRe_Dataset."},
	{"name":"myanmar_spoken_corpus","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/freococo/myanmar_spoken_corpus","creator_name":"WYC","creator_url":"https://huggingface.co/freococo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMyanmar Spoken Corpus (Version 1.0)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nMyanmar Spoken Corpus is a high-quality, open dataset of spoken Myanmar sentences designed to support NLP and ASR applications. The dataset focuses on providing clean and structured spoken language data for advancing Myanmar language technology.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Statistics\\n\\t\\n\\n\\nNumber of Rows:\\nLocal Parquet file: 16,020,011 rows\\nHugging Face Dataset Viewer: 15,728,640 rows\\n\\n\\nFile Size: 1.78 GB (Parquet format)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/myanmar_spoken_corpus."},
	{"name":"ESG_Report","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/WHATX/ESG_Report","creator_name":"NUS & A*STAR - WHATX","creator_url":"https://huggingface.co/WHATX","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tESG Report PDF Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload Instructions\\n\\t\\n\\nTo download the dataset, follow these steps:\\n\\nNavigate to the data directory in the GitHub repository:\\ncd data\\n\\n\\nInstall Git LFS (if not already installed):\\ngit lfs install\\n\\n\\nClone the dataset from Hugging Face Hub:\\ngit clone https://huggingface.co/datasets/WHATX/ESG_Report\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains three main components:\\n\\nraw_pdf:\\n\\nA collection of 195 PDFs scraped from TCFD Hub.\\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/WHATX/ESG_Report."},
	{"name":"GammaCorpus-v1-50k-UNFILTERED","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v1 - 50k - UNFILTERED\\n\\t\\n\\n\\n26 million tokens of pure unfiltered user and AI-generated data\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v1 50k Unfiltered dataset consists of 50,000 structured single-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\nThis dataset contains approximately 26 million tokens of text. It is designed to facilitate the training and evaluation of conversational AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v1-50k-UNFILTERED."},
	{"name":"UnLOK-VQA","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/vaidehi99/UnLOK-VQA","creator_name":"Vaidehi Patil","creator_url":"https://huggingface.co/vaidehi99","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìä Dataset: UnLOK-VQA (Unlearning Outside Knowledge VQA)\\n\\t\\n\\nLink: Dataset Link\\nThis dataset contains approximately 500 entries with the following key attributes:\\n\\n\\\"id\\\": Unique Identifier for each entry\\n\\\"src\\\": The question whose answer is to be deleted ‚ùì\\n\\\"pred\\\": The answer to the question meant for deletion ‚ùå\\n\\\"loc\\\": Related neighborhood questions üîÑ\\n\\\"loc_ans\\\": Answers to the neighborhood questions üó£Ô∏è\\n\\\"image_id\\\": The ID corresponding to the image in the COCO dataset üñºÔ∏è\\n\\nTo access the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vaidehi99/UnLOK-VQA."},
	{"name":"SimpleStories-JA","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/lennart-finke/SimpleStories-JA","creator_name":"Lennart Finke","creator_url":"https://huggingface.co/lennart-finke","description":"\\n\\t\\n\\t\\t\\n\\t\\tüìòüìï SimpleStories üìôüìó\\n\\t\\n\\n„Åì„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅØ„ÄÅgpt-4o-mini„Å´„Çà„Å£„Å¶ÁîüÊàê„Åï„Çå„ÅüÁü≠Á∑®Â∞èË™¨„ÅßÂá∫Êù•„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„Åô„ÄÇÁîüÊàêÊñπÊ≥ï„ÇÑ„ÄÅËá™ÂàÜ„ÅßÁâ©Ë™û„ÇíÁîüÊàê„Åô„ÇãÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„Åì„Å°„Çâ„ÅÆ„É™„Éù„Ç∏„Éà„É™„Çí„ÅîË¶ß„Åè„Å†„Åï„ÅÑ„ÄÇ\\n‰ªñ„ÅÆË®ÄË™û„ÇÑÁâ©Ë™ûÂΩ¢Âºè„ÅÆÂà∂‰Ωú„ÇíÂ∏åÊúõ„Åï„Çå„ÇãÂ†¥Âêà„ÅØ„ÄÅ„É°„Éº„É´„Å´„Å¶„ÅäÂïè„ÅÑÂêà„Çè„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ\\nSimpleStories„ÅØ„ÄÅElden„Å®Li„Å´„Çà„ÇãTinyStories„ÅÆÊîπËâØÁâà„Åß„Åô„ÄÇ\\n\\n\\t\\n\\t\\t\\n\\t\\tÁâπÂæ¥\\n\\t\\n\\n\\nÁâ©Ë™û„ÅÆÊ≥®ÈáàÊÉÖÂ†±Ôºàtheme„ÄÅtopic„ÄÅstyle„Å™„Å©Ôºâ\\nÂ§öÊßòÊÄß„ÅÆÈ´ò„Åï\\n2024Âπ¥„ÅÆ„É¢„Éá„É´„Å´„Çà„Å£„Å¶ÁîüÊàê\\nNLP„ÅÆ„Éá„Éº„Çø„ÅåÁî®ÊÑè„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åó„ÇÑ„Åô„ÅÑ\\n‰ª•‰∏ã„ÅÆË®ÄË™ûÁâà„ÅåÂà©Áî®ÂèØËÉΩÔºö\\nËã±Ë™û\\nÊó•Êú¨Ë™û\\n‰ªñ„Å´„ÇÇËøΩÂä†‰∫àÂÆö\\n\\n\\n\\n\\nThis dataset is a collection of short stories generated by gpt-4o-mini (+ other models, soon). To see how this dataset was generated, or to generate some stories‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lennart-finke/SimpleStories-JA."},
	{"name":"myanmar-written-corpus","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/freococo/myanmar-written-corpus","creator_name":"WYC","creator_url":"https://huggingface.co/freococo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMyanmar Written Corpus\\n\\t\\n\\nThe Myanmar Written Corpus is a comprehensive collection of high-quality written Myanmar text, designed to address the lack of large-scale, openly accessible resources for Myanmar Natural Language Processing (NLP). It is tailored to support various tasks such as text-to-speech (TTS), automatic speech recognition (ASR), translation, text generation, and more.\\nThis dataset serves as a critical resource for researchers and developers aiming to advance Myanmar‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/freococo/myanmar-written-corpus."},
	{"name":"spell-correction","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/torinriley/spell-correction","creator_name":"Torin Etheridge","creator_url":"https://huggingface.co/torinriley","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSpell-Check Dataset\\n\\t\\n\\nThis dataset consists of pairs of misspelled words and their corresponding correctly spelled words, designed for training and evaluating character-level spelling correction models. It is particularly useful for tasks such as:\\n\\nSpelling correction\\n\\nCharacter-level sequence-to-sequence modeling\\n\\nError detection and correction in text\\n\\n\\nEach data point in the dataset contains:\\n\\nmisspelled: A misspelled version of a word.\\n\\ncorrect: The corrected spelling of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/torinriley/spell-correction."},
	{"name":"MCRS_by_Databoost","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Databoost/MCRS_by_Databoost","creator_name":"Mada","creator_url":"https://huggingface.co/Databoost","description":"A dataset for classifying and detecting toxicity in social media content"},
	{"name":"unal-repository-dataset-train-instruct","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-train-instruct","creator_name":"Juli√°n Camilo Velandia","creator_url":"https://huggingface.co/JulianVelandia","description":"T√≠tulo: Grade Works UNAL Dataset Instruct Train (split 75/25)\\nDescripci√≥n: Split 75% del dataset original. \\nEste dataset contiene un formato estructurado de Pregunta: Respuesta generado a partir del contenido de los trabajos de grado del repositorio de la Universidad Nacional de Colombia. Cada registro incluye un fragmento del contenido del trabajo, una pregunta generada a partir de este y su respuesta correspondiente. Este dataset es ideal para tareas de fine-tuning en modelos de lenguaje‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JulianVelandia/unal-repository-dataset-train-instruct."},
	{"name":"indic_sentiment_analyzer","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer","creator_name":"Dhruv Bhatnagar","creator_url":"https://huggingface.co/dhruv0808","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tMultilingual Sentiment Analysis Dataset for Indian Languages\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis repository contains a comprehensive sentiment analysis dataset covering 11 Indian languages and English. The dataset is designed to support sentiment analysis tasks across multiple domains and languages, making it valuable for developing multilingual sentiment analysis models and applications.\\n\\n\\t\\n\\t\\t\\n\\t\\tLanguages Covered\\n\\t\\n\\n\\nEnglish (en) - Original\\nHindi (hi)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dhruv0808/indic_sentiment_analyzer."},
	{"name":"dubliners","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/chryskylodon/dubliners","creator_name":"seva","creator_url":"https://huggingface.co/chryskylodon","description":"A dataset of James Joyce's collection of short stories \\\"Dubliners,\\\" prepared for NLP tasks and computational analysis of literary texts. The dataset includes:\\n- Text tokenized by sentences.\\n- POS-tagged sentences using NLTK.\\n- Results of analyzing the text with spaCy (POS-tagged, named entities, dependencies).\\n\\nThis dataset was created as part of an NLP course at the Higher School of Economics (HSE). For more details, see the original repository: https://github.com/vifirsanova/compling.\\n\\nThe dataset can be used for various NLP tasks, including:\\n- Part-of-speech tagging.\\n- Named entity recognition.\\n- Dependency parsing.\\n- Computational analysis of literary texts.\\n\\nIt is particularly suited for researchers and students interested in computational linguistics and literary analysis.\\n"},
	{"name":"telugu-colloquial-corpus-tokenized","keyword":"nlp","license":"Creative Commons Attribution Share Alike 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-sa-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/ankitha29/telugu-colloquial-corpus-tokenized","creator_name":"ankitha Ravella","creator_url":"https://huggingface.co/ankitha29","description":"\\n\\t\\n\\t\\t\\n\\t\\tTelugu Colloquial Corpus (Tokenized)\\n\\t\\n\\nThis dataset is a tokenized version of the Telugu Colloquial Corpus (TeCC). It contains examples of informal, everyday Telugu language, including slang, regional variations, and conversational patterns.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\nLanguage: Telugu (te)\\nTokenization: Tokenized using the bert-base-multilingual-cased tokenizer from the transformers library.\\nSource: [Describe where the original data came from ‚Äì e.g., collected from online forums‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ankitha29/telugu-colloquial-corpus-tokenized."},
	{"name":"violence-and-conflict-events-in-colombia","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/3iS/violence-and-conflict-events-in-colombia","creator_name":"3iS","creator_url":"https://huggingface.co/3iS","description":"\\n\\t\\n\\t\\t\\n\\t\\tHumanitarian Dataset: Violence and IHL Violations in Colombia (2024)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains records of violence and infractions to international humanitarian law (IHL) in Colombia during 2024. The dataset was compiled by OCHA Colombia from reports by key informants and news sources. The data has been structured and categorized according to IHL standards, including the extraction of the number of victims and events.\\n\\n\\t\\n\\t\\t\\n\\t\\tContent\\n\\t\\n\\nThe dataset includes the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/3iS/violence-and-conflict-events-in-colombia."},
	{"name":"GammaCorpus-v2-500k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 500k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 500k dataset consists of 500 thosuand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-500k."},
	{"name":"GammaCorpus-v2-10k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: v2 - 10k Lines of Pure Dialogue\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nThe GammaCorpus v2 10k dataset consists of 10 thousand structured multi-turn conversations, where each interaction includes:\\n\\nInput: A user prompt or question.\\nOutput: A response generated by an AI assistant.\\n\\n\\nThis is the SECOND and LATEST version of the GammaCorpus dataset. This is a significantly improved version as it contains higher quality conversations and heavy cleaning than the GammaCorpus v1 dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-v2-10k."},
	{"name":"GammaCorpus-CoT-Math-170k","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k","creator_name":"Ruben Roy","creator_url":"https://huggingface.co/rubenroy","description":"\\n\\t\\n\\t\\t\\n\\t\\tGammaCorpus: CoT Math 170k\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tWhat is it?\\n\\t\\n\\nGammaCorpus CoT Math 170k is a dataset that consists of 170,000 math problems, each with step-by-step Chain-of-Thought (CoT) reasoning. It's designed to help in training and evaluating AI models for mathematical reasoning and problem-solving tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nNumber of Rows: 169,527\\nFormat: JSONL\\nLanguage: English\\nData Type: Math problems with step-by-step reasoning (Chain-of-Thought)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rubenroy/GammaCorpus-CoT-Math-170k."},
	{"name":"Estonian-Text-Simplification","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/vulturuldemare/Estonian-Text-Simplification","creator_name":"Eduard Barbu","creator_url":"https://huggingface.co/vulturuldemare","description":"\\n\\t\\n\\t\\t\\n\\t\\tEstonian Text Simplification\\n\\t\\n\\nThis repository contains resources and models for Estonian text simplification, including datasets and pre-trained models.\\n\\n\\t\\n\\t\\t\\n\\t\\tFiles\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Files\\n\\t\\n\\n\\nsimplification_training_set.json: A dataset used to fine-tune LLaMA 3.1 for text simplification.\\n\\nsrc: The source of the data.\\noriginal: The original sentence to be simplified.\\nsimpl_lex: A lexical simplification (may be empty).\\nsimpl_final: The final simplified sentence.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/vulturuldemare/Estonian-Text-Simplification."},
	{"name":"synthetic-legal","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Taylor658/synthetic-legal","creator_name":"atayloraerospace","creator_url":"https://huggingface.co/Taylor658","description":"\\n\\t\\n\\t\\t\\n\\t\\tSynthetic Legal (Query, Response) Dataset\\n\\t\\n\\n\\nDescriptionSynthetic Legal is a 140,000-row dataset of (legal query, legal response) pairs spanning 13 legal domains, designed to mimic real-world legal fact patterns and references. Each entry provides a short scenario (fact pattern) and a \\\"verified solution\\\" referencing real citations (statutes, case law, scholarly commentary, legislative history, and comparative law) with a specified verification method.  \\nDisclaimer: All text is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Taylor658/synthetic-legal."},
	{"name":"mongs","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/sangyon/mongs","creator_name":"yoon","creator_url":"https://huggingface.co/sangyon","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sangyon/mongs."},
	{"name":"WaterDrum-TOFU","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU","creator_name":"Group of Learning and Optimization Working in AI","creator_url":"https://huggingface.co/Glow-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\tWaterDrum: Watermarking for Data-centric Unlearning Metric\\n\\t\\n\\nWaterDrum provides an unlearning benchmark for the evaluation of effectiveness and practicality of unlearning. This repository contains the TOFU corpus of WaterDrum (WaterDrum-TOFU), which contains both unwatermarked and watermarked question-answering datasets based on the original TOFU dataset.\\nThe data samples were watermarked with Waterfall.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThe WaterDrum-TOFU dataset contains 6 subsets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Glow-AI/WaterDrum-TOFU."},
	{"name":"Meta_Plan_Optimization","keyword":"nlp","license":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","dataset_url":"https://huggingface.co/datasets/xwm/Meta_Plan_Optimization","creator_name":"xwm","creator_url":"https://huggingface.co/xwm","description":"\\n\\t\\n\\t\\t\\n\\t\\tMPO Datasets\\n\\t\\n\\nThis folder contains the datasets for the MPO experiments.\\nPaper: https://hf.co/papers/2503.02682\\nCode: https://github.com/WeiminXiong/MPO\\n\\n\\t\\n\\t\\t\\n\\t\\tFile Structure\\n\\t\\n\\nalfworld_metaplan_preference_pairs.json: includes comparison data for the DPO optimization phase of the ALFWorld meta planner.\\nsciworld_metaplan_preference_pairs.json: includes comparison data for the DPO optimization phase of the SciWorld meta planner.\\nalfworld_metaplan_sft.json: includes the metaplan data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xwm/Meta_Plan_Optimization."},
	{"name":"multisource-esco-set","keyword":"nlp","license":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","dataset_url":"https://huggingface.co/datasets/Boanerges/multisource-esco-set","creator_name":"Samson Kwaku Nkrumah","creator_url":"https://huggingface.co/Boanerges","description":"\\n\\t\\n\\t\\t\\n\\t\\tMultiSource-ESCO-Skills: A Unified Dataset for Skill Extraction\\n\\t\\n\\nThis dataset aggregates data from multiple sources‚Äîcourse descriptions, CV content, and job descriptions‚Äîall linked to ESCO skills. It is designed to help researchers and practitioners develop and fine-tune NLP models (e.g., BERT or SentenceTransformer-based models) for automated skill extraction.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview\\n\\t\\n\\n\\nName: MultiSource-ESCO-Skills\\nSources:\\nCourse Content: Educational course materials\\nCV Content:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Boanerges/multisource-esco-set."},
	{"name":"fine_tuned_llama2","keyword":"nlp","license":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","dataset_url":"https://huggingface.co/datasets/and89/fine_tuned_llama2","creator_name":"anand kumar","creator_url":"https://huggingface.co/and89","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/and89/fine_tuned_llama2."},
	{"name":"LatinSummarizer","keyword":"nlp","license":"Creative Commons Attribution 4.0 International","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","dataset_url":"https://huggingface.co/datasets/LatinNLP/LatinSummarizer","creator_name":"LatinNLP","creator_url":"https://huggingface.co/LatinNLP","description":"\\n\\t\\n\\t\\t\\n\\t\\tLatinSummarizer Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tStructure\\n\\t\\n\\n\\naligned_en_la_data_raw.csv\\naligned_en_la_data_cleaned.csv\\naligned_en_la_data_cleaned_with_stanza.csv\\nconcat_aligned_data.csv\\nconcat_cleaned.csv\\nlatin_wikipedia_cleaned.csv\\nlatin_wikipedia_raw.csv\\nlatin-literature-dataset-170M_raw_cleaned.csv\\nlatin-literature-dataset-170M_raw_cleaned_chunked.csv\\nElsa_aligned/\\nREADME.md\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDetails\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\taligned_en_la_data_raw.csv\\n\\t\\n\\nThis dataset contains aligned Latin (la) - English (en)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LatinNLP/LatinSummarizer."}
]
;
