var data_for_domain_chemistry = [

  {"name":"MsitralTrix-test-dpo","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/CultriX/MsitralTrix-test-dpo","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","description":"CultriX/MsitralTrix-test-dpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"B3DB","keyword":"chemistry","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/maomlab/B3DB","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tBlood-Brain Barrier Database (B3DB)\\n\\t\\n\\nThe Blood-Brain Barrier Database (B3DB)\\nis a large benchmark dataset compiled from 50 published resources\\n(as summarized at raw_data/raw_data_summary.tsv) and categorized based on the consistency between\\ndifferent experimental references/measurements. This dataset was\\npublished in Scientific Data and\\na mirror of the theochem/B3DB the official Github repo where it is\\noccasionally uploaded with new experimental data. We used the original datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/B3DB."},
  {"name":"AqSolDB","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/maomlab/AqSolDB","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tAqueous Solubility Database (AqSolDB)\\n\\t\\n\\nAqSolDB is created by the Autonomous Energy Materials Discovery [AMD] research group, consists of aqueous solubility values of \\n9,982 unique compounds curated from 9 different publicly available aqueous solubility datasets. This openly accessible dataset, \\nwhich is the largest of its kind, and will not only serve as a useful reference source of measured solubility data, but also \\nas a much improved and generalizable training data source for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/AqSolDB."},
  {"name":"aya_dataset_pt","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/botbot-ai/aya_dataset_pt","creator_name":"BotBot","creator_url":"https://huggingface.co/botbot-ai","description":"CohereForAI Aya Dataset filtrado para portugu√™s (PT).\\nAya Dataset Summary\\nThe Aya Dataset is a multilingual instruction fine-tuning dataset curated by an open-science community via Aya Annotation Platform from Cohere For AI. The dataset contains a total of 204k human-annotated prompt-completion pairs along with the demographics data of the annotators.\\nThis dataset can be used to train, finetune, and evaluate multilingual LLMs.\\nCurated by: Contributors of Aya Open Science Intiative.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/botbot-ai/aya_dataset_pt."},
  {"name":"slimorca-dedup-chatml","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Radiantloom/slimorca-dedup-chatml","creator_name":"Radiantloom AI","creator_url":"https://huggingface.co/Radiantloom","description":"This is a chatml formatted version of original SlimOrca-Dedup dataset with few modifications to the system prompts.\\n"},
  {"name":"ChatML-SlimOrca-Dedup","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Felladrin/ChatML-SlimOrca-Dedup","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"Open-Orca/SlimOrca-Dedup in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\\nPython code used for conversion:\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"Felladrin/Llama-160M-Chat-v1\\\")\\n\\ndataset = load_dataset(\\\"Open-Orca/SlimOrca-Dedup\\\", split=\\\"train\\\")\\n\\ndef format(columns):\\n    messages = []\\n\\n    conversations = columns[\\\"conversations\\\"]\\n\\n    for i in range(len(conversations)):\\n        message =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-SlimOrca-Dedup."},
  {"name":"AttentiveSkin","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/maomlab/AttentiveSkin","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tAttentive Skin\\n\\t\\n\\nTo Predict Skin Corrosion/Irritation Potentials of Chemicals via Explainable Machine Learning Methods\\nDownload: https://github.com/BeeBeeWong/AttentiveSkin/releases/tag/v1.0\\n\\n\\t\\n\\t\\t\\n\\t\\tQuickstart Usage\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tLoad a dataset in python\\n\\t\\n\\nEach subset can be loaded into python using the Huggingface datasets library.\\nFirst, from the command line install the datasets library\\n$ pip install datasets\\n\\nthen, from within python load the datasets library\\n>>> import datasets‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/AttentiveSkin."},
  {"name":"ChatML-Capybara","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Felladrin/ChatML-Capybara","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","description":"LDJnr/Capybara in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\\nPython code used for conversion:\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"Felladrin/Llama-160M-Chat-v1\\\")\\n\\ndataset = load_dataset(\\\"LDJnr/Capybara\\\", split=\\\"train\\\")\\n\\ndef format(columns):\\n    messages = []\\n    conversationColumn = columns[\\\"conversation\\\"]\\n\\n    for i in range(len(conversationColumn)):\\n        messages.append({\\n            \\\"role\\\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-Capybara."},
  {"name":"hercules-v2.0","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Locutusque/hercules-v2.0","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Hercules-v2.0\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nDataset Name: Hercules-v2.0\\nVersion: 2.0\\nDate of Release: February 2, 2024\\nSize: 1,307,174\\nData Sources: \\nHercules-v2.0 is an enriched instruction dataset derived from OpenHermes-2.5, aimed at enhancing its diversity and scope. The dataset amalgamates contributions from various data sources, with a strong emphasis on Biology, Physics, Medicine, Math, Computer Science, Instruction Following, Function Calling, and Roleplay. The data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/hercules-v2.0."},
  {"name":"HematoxLong2023","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/maomlab/HematoxLong2023","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tHematotoxicity Dataset (HematoxLong2023)\\n\\t\\n\\nA hematotoxicity dataset containing 1772 chemicals was obtained, which includes a positive set with 589 molecules and a negative set with 1183 molecules.\\nThe molecules were divided into a training set of 1330 molecules and a test set of 442 molecules according to their Murcko scaffolds. \\nAdditionally, 610 new molecules from related research and databases were compiled as the external validation set.\\nThe train and test datasets uploaded to our‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/HematoxLong2023."},
  {"name":"MolCorefData","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Ozymandias314/MolCorefData","creator_name":"Vincent Fan","creator_url":"https://huggingface.co/Ozymandias314","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMolDetect and MolCoref Data\\n\\t\\n\\nThe MolDetect and MolCoref models can be found in this github repository, as well as additional instructions for testing or running the models.\\nThe reaction diagrams are located at images.zip.\\nAdditionally, we use a 70-10-20 split in our experiments. The full train/dev/test split for each task is available in this repository as well.\\nThis notebook shows how to visualize the diagram and the ground truth.\\n"},
  {"name":"OpenChemIEData","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Ozymandias314/OpenChemIEData","creator_name":"Vincent Fan","creator_url":"https://huggingface.co/Ozymandias314","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenChemIE Data\\n\\t\\n\\nComplete instructions for running OpenChemIE can be found at this github repository.\\nAdditionally, the web interface is located at https://mit.openchemie.info/. \\nThe images associated with the annotated dataset for R-group resolution can be downloaded here and the annotations are located at\\nr_group_resolution_data.json. \\nThe annotations take the following format: \\n[\\n  {\\n    \\\"file_name\\\": \\\"acs.joc.2c00176 example 1.png\\\",\\n    \\\"reaction_template\\\": {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ozymandias314/OpenChemIEData."},
  {"name":"HLM_RLM","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/maomlab/HLM_RLM","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tHuman & Rat Liver Microsomal Stability\\n\\t\\n\\n3345 RLM and 6420 HLM compounds were initially collected from the ChEMBL bioactivity database. \\n(HLM ID: 613373, 2367379, and 612558; RLM ID: 613694, 2367428, and 612558)\\nFinally, the RLM stability data set contains 3108 compounds, and the HLM stability data set contains 5902 compounds.\\nFor the RLM stability data set, 1542 (49.6%) compounds were classified as stable, and 1566 (50.4%) compounds were classified as unstable, \\namong which the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/HLM_RLM."},
  {"name":"ocr-data-tnpsc","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Hemanth-thunder/ocr-data-tnpsc","creator_name":"Hemanth-thunder","creator_url":"https://huggingface.co/Hemanth-thunder","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTamil Public Domain Books (Tamil)\\n\\t\\n\\nThe dataset comprises over 30 school textbooks and certain TNPSC (Tamil Nadu Public Service Commission) materials in Tamil medium, presumed to be in the public domain.\\n"},
  {"name":"rnacentral_synthetic","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/rnacentral_synthetic","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 226729 datapoints\\nstructure: 226729 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 226729 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 226729 valid datapoints\\nINCLUDED: 0 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n0 invalid datapoints (ex: sequence with non-regular characters)\\n0 datapoints with bad‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/rnacentral_synthetic."},
  {"name":"slimorca-dedup-chatml-100k","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/philschmid/slimorca-dedup-chatml-100k","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopy of Open-Orca/SlimOrca-Dedup in ChatML format downsample to 100k\\n\\t\\n\\n\\n\\\"SlimOrca Dedup\\\" is a deduplicated, unfiltered subset of the SlimOrca dataset, excluding RLHF instances, resulting in 363k unique examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\nRemoval of RLHF instances.\\nDeduplication using minhash and Jaccard similarity techniques.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDemo Models\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote: These models were trained on the full SlimOrca dataset, not the deduplicated, unfiltered version.\\n*‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/slimorca-dedup-chatml-100k."},
  {"name":"slimorca-dedup-chatml","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/philschmid/slimorca-dedup-chatml","creator_name":"Philipp Schmid","creator_url":"https://huggingface.co/philschmid","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCopy of Open-Orca/SlimOrca-Dedup in ChatML format\\n\\t\\n\\n\\n\\\"SlimOrca Dedup\\\" is a deduplicated, unfiltered subset of the SlimOrca dataset, excluding RLHF instances, resulting in 363k unique examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\nRemoval of RLHF instances.\\nDeduplication using minhash and Jaccard similarity techniques.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDemo Models\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote: These models were trained on the full SlimOrca dataset, not the deduplicated, unfiltered version.\\n*‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/philschmid/slimorca-dedup-chatml."},
  {"name":"MutagenLou2023","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/maomlab/MutagenLou2023","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\tMutagenicity Optimization (MutagenLou2023)\\n\\t\\n\\nIn the original paper, they collected the Ames records from Hansen‚Äôs benchmark (6512 compounds) and the ISSSTY database (6052 compounds). \\nAfter data preparation, a total of 8576 compounds with structural diversity were obtained, including 4643 Ames positives and 3933 Ames negatives. \\nThe comprehensive data set was then split into a training set including 7720 compounds and a test set containing 856 compounds. \\nOverall, the numbers of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/MutagenLou2023."},
  {"name":"bpRNA-1m","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/bpRNA-1m","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 66715 datapoints\\nstructure: 66715 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 102318 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 66715 valid datapoints\\nINCLUDED: 1482 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n5064 invalid datapoints (ex: sequence with non-regular characters)\\n0 datapoints with bad‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/bpRNA-1m."},
  {"name":"distilabel-capybara-kto-15k-binarized","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-KTO 15K binarized\\n\\t\\n\\n\\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy KTO?\\n\\t\\n\\nThe KTO paper states:\\n\\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized."},
  {"name":"lncRNA","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/lncRNA","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 30 datapoints\\nstructure: 30 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 30 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 30 valid datapoints\\nINCLUDED: 6 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n0 invalid datapoints (ex: sequence with non-regular characters)\\n0 datapoints with bad structures\\n0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/lncRNA."},
  {"name":"Test-dataset-1","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Rashedul12/Test-dataset-1","creator_name":"Rashedul Hasan Rijul","creator_url":"https://huggingface.co/Rashedul12","description":"\\nThis is a test dataset\\n\\n"},
  {"name":"BookBasedQAGen_Petrochemical","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/lorinma/BookBasedQAGen_Petrochemical","creator_name":"Nuo","creator_url":"https://huggingface.co/lorinma","description":"Â¶ÇÊûú‰Ω†ÊúâÈ¢ÜÂüüÁõ∏ÂÖ≥ÁöÑ‰∏Ä‰∫õÊñáÊú¨ÊùêÊñôÔºàÂèØ‰ª•ÊòØOCRÂá∫Êù•ËøòÊØîËæÉËÑèÁöÑÊï∞ÊçÆÔºâÔºåÊÉ≥ËΩ¨ÂåñÊàêÂçïËΩÆ alpaca ÂΩ¢ÂºèÁöÑÁöÑQAÂØπÔºåËøôÊòØ‰∏Ä‰∏™ÁÆÄÊòìÁöÑÊïôÁ®ã„ÄÇ‰πüÂ∞±Âõæ‰∏Ä‰πêÔºåÂõ†‰∏∫ÁúüÊ≠£È´òË¥®ÈáèÁöÑÈ¢ÜÂüüQAËøòÊòØÂæóÊù•Ëá™‰∫éË°å‰∏ö‰∏ìÂÆ∂ÔºåÂπ∂‰∏îÂêëLLMÊ≥®ÂÖ•È¢ÜÂüüÁü•ËØÜÂ∫îËØ•ÈÄöËøápretrainËÄå‰∏çÊòØSFT‰πüÂ∑≤ÁªèÊòØÂÖ±ËØÜ‰∫Ü„ÄÇ\\nbookgenÊñá‰ª∂Â§π‰∏≠ÂåÖÂê´‰∫ÜÊ†∑‰æãÊñáÊú¨ÂíåpyÊñá‰ª∂ÔºåÊ≥®ÊÑèpromptÊ†πÊçÆÈ¢ÜÂüüË∞ÉÊï¥‰∏Ä‰∏ãÔºåÂç≥‰ΩøÂéüÂßãËØ≠ÊñôÊØîËæÉËÑèLLM‰πüÂü∫Êú¨ÂèØ‰ª•ÁêÜËß£„ÄÇÂπ∂Ê≤°ÊúâÊ†∏Êü•Ëøá‰ºö‰∏ç‰ºöÂá∫Áé∞ÂπªËßâÁé∞Ë±°„ÄÇ\\nÂè™Ë¶ÅÊòØÂíåOpenAI SDKÂÖºÂÆπÁöÑAPIÊúçÂä°ÈÉΩÂèØ‰ª•Âπ≥ÊªëËøÅÁßª„ÄÇËøôÈáå‰ΩøÁî®‰∫ÜÈõ∂‰∏Ä‰∏áÁâ©ÁöÑyi-large API„ÄÇhttps://platform.lingyiwanwu.com/\\n2024Âπ¥8Êúà2Êó•Êõ¥Êñ∞ÔºåÊõ¥Êñ∞OpenAI SDK 1.0ÁöÑË∞ÉÁî®ÊñπÂºèÔºåÊõ¥Êñ∞‰ΩøÁî®yi-large APIÔºåÊõ¥Êñ∞‰∏∫ÂçïÁ∫øÁ®ãÊ®°Âºè„ÄÇ\\n‚ö†Ô∏èÊ≥®ÊÑèÔºåÂçïÁ∫øÁ®ãÊ®°ÂºèÊòØ‰∏∫‰∫ÜÊõ¥Â•ΩÁöÑdebugÔºåÁúüÊ≠£ÁîüÊàêÊï∞ÊçÆÈúÄË¶ÅËá™Â∑±‰øÆÊîπÊàêÂ§öÁ∫øÁ®ãÊ®°ÂºèÔºåÂπ∂‰∏îrate limitÂπ∂Ê≤°ÊúâÂçïÁã¨ËøõË°åhandle„ÄÇ\\nÊ†∑‰æãÊï∞ÊçÆÊòØ‰∫é23Âπ¥6Êúà‰ΩøÁî®3.5-turboÁîüÊàêÁöÑÔºåÂè™Êîæ‰∫Ü‰∏ÄÈÉ®ÂàÜÔºåÈöè‰æøÁé©Áé©Â∞±Â•ΩÔΩû\\nIf you have some text‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lorinma/BookBasedQAGen_Petrochemical."},
  {"name":"scientific_studies","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yousefg/scientific_studies","creator_name":"Yousef Rafat Gamaleldin","creator_url":"https://huggingface.co/yousefg","description":"yousefg/scientific_studies dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"bubble_dataset_2","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/triangulum66/bubble_dataset_2","creator_name":"Atharva T","creator_url":"https://huggingface.co/triangulum66","description":"triangulum66/bubble_dataset_2 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"OpenCerebrum-SFT","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Locutusque/OpenCerebrum-SFT","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenCerebrum SFT subset\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nOpenCerebrum is my take on creating an open source version of Aether Research's proprietary Cerebrum dataset. This repository contains the SFT subset, which contains about 1,200,00 examples. Unfortunately, I was unsure about how I would compress this dataset to just 5,000 examples like in the original Cerebrum dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration\\n\\t\\n\\nThis dataset was curated using a simple and logical rationale. The goal was to use‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/OpenCerebrum-SFT."},
  {"name":"Luminia-mixture","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Nekochu/Luminia-mixture","creator_name":"Nekochu","creator_url":"https://huggingface.co/Nekochu","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Combined in Alpaca format. ‚úî\\n\\t\\n\\n\\n  Click to see V1 full list \\n\\nChangelog\\n  \\n[24/05] initial release V1 - Branch main DPO+SFT is recipes of split-v1/Combined excluding RP\\n[24/07] Add: New datasets cleaned in Alpaca format in split-v2.\\n\\n\\ndataset_info.json\\n\\n  This JSON can be used in LLaMA Factory\\n\\n  \\\"LuminiaMix-v1_Base\\\": {\\n    \\\"file_name\\\": \\\"LuminiaMix-v1_Base.json\\\",\\n    \\\"formatting\\\": \\\"alpaca\\\",\\n    \\\"columns\\\": {\\n      \\\"prompt\\\": \\\"instruction\\\",\\n      \\\"query\\\": \\\"input\\\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nekochu/Luminia-mixture."},
  {"name":"SMol_RS_Filtered_825K_SMILES-MMChat","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/OpenMol/SMol_RS_Filtered_825K_SMILES-MMChat","creator_name":"OpenMol","creator_url":"https://huggingface.co/OpenMol","description":"Retrosynthesis Prediction Dataset (derived from SMolInstruct)\\n\\nmolecule representation format: 1D SMILES\\nwill further encode into 2D graph features\\n\\n\\nWe filtered out overlapping samples from the original train-split (test-set: MolInstruct-Retrosynthesis Prediction)\\nWe only include single-step retrosynthesis prediction.\\n\\nFor Detail, refer to PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes: https://arxiv.org/pdf/2406.13193\\n"},
  {"name":"scibowl-synthetic","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/AarushSah/scibowl-synthetic","creator_name":"Aarush Sah","creator_url":"https://huggingface.co/AarushSah","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tScibowl-synthetic (v0.1)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nScibowl-synthetic is a dataset of science bowl questions that have been answered by the Claude 3 Opus language model. The dataset is designed to provide a high-quality resource for teaching science concepts to language models.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\nRepository: https://huggingface.co/datasets/AarushSah/scibowl-synthetic/\\nPoint of Contact: Aarush Sah\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Composition\\n\\t\\n\\nThe dataset consists of 5‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AarushSah/scibowl-synthetic."},
  {"name":"Xerxes-Instruct-700K","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Instinct-AI/Xerxes-Instruct-700K","creator_name":"Instinct-AI","creator_url":"https://huggingface.co/Instinct-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"Xerxes-Instruct-700K\\\"\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nXerxes, named after a Persian King renowned for his wisdom and strategic prowess, is an amalgamation of four distinct datasets. This dataset has been curated to cater to the burgeoning needs of natural language processing tasks, particularly in the domain of conversation modeling and comprehension.\\nThe dataset encompasses conversations sourced from a variety of sources, ranging from generative models to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Instinct-AI/Xerxes-Instruct-700K."},
  {"name":"alpaca-cleaned-gpt4-turbo","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo","creator_name":"myles bruce","creator_url":"https://huggingface.co/mylesgoose","description":"I downloaded the dataset from Alpaca at https://huggingface.co/datasets/yahma/alpaca-cleaned and processed it using a script to convert the text to hashes. I removed any duplicate hashes along with their corresponding input and output columns. Subsequently, I utilized the GPT-4 Turbo API to feed each message, instruction, and input to the model for generating responses.\\nHere are the outputs generated by the GPT-4 Turbo model. The information in the input column and instruction column should be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mylesgoose/alpaca-cleaned-gpt4-turbo."},
  {"name":"ProteinLMBench","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/tsynbio/ProteinLMBench","creator_name":"Toursun Synbio","creator_url":"https://huggingface.co/tsynbio","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProteinLMBench\\n\\t\\n\\nA benchmark dataset for LLMs in protein related tasks.\\n"},
  {"name":"pretoxtm-dataset","keyword":"chemistry","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/javicorvi/pretoxtm-dataset","creator_name":"Javier Corvi","creator_url":"https://huggingface.co/javicorvi","description":"javicorvi/pretoxtm-dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"hercules-v4.5","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Locutusque/hercules-v4.5","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","description":"This dataset is a slight modification to hercules-v4.0, I have removed the MetaMathQA dataset, and I am solely using orca-math-word-problems now. I have also not shuffled this dataset, as it may seem to improve performance.\\n"},
  {"name":"OpenCerebrum-2.0-SFT","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Locutusque/OpenCerebrum-2.0-SFT","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenCerebrum SFT subset\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nOpenCerebrum is my take on creating an open source version of Aether Research's proprietary Cerebrum dataset. This repository contains the SFT subset, which contains about 6,400 examples. To compress this dataset to such a small size, I used an in-house curation technique at Cognitive Computations. More information about this technique will come in the future. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration\\n\\t\\n\\nAs mentioned earlier, I used an in-house‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/OpenCerebrum-2.0-SFT."},
  {"name":"OpenCerebrum-2.0-DPO","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Locutusque/OpenCerebrum-2.0-DPO","creator_name":"Sebastian Gabarain","creator_url":"https://huggingface.co/Locutusque","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOpenCerebrum DPO subset\\n\\t\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nOpenCerebrum is my take on creating an open source version of Aether Research's proprietary Cerebrum dataset. This repository contains the DPO subset, which contains about 720 examples. To compress this dataset to such a small size, I used an in-house curation technique at Cognitive Computations. More information about this technique will come in the future. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCuration\\n\\t\\n\\nAs mentioned earlier, I used an in-house‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Locutusque/OpenCerebrum-2.0-DPO."},
  {"name":"SLM4CRP_with_RTs","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/liupf/SLM4CRP_with_RTs","creator_name":"Pengfei Liu","creator_url":"https://huggingface.co/liupf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSLM4CRP_with_RTs Dataset\\n\\t\\n\\n \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe SLM4CRP_with_RTs dataset is a chemical reaction predictions (CRPs) dataset featuring reaction type (RT) labels, developed from the Mol-Instruction. We introduce a novel knowledge elicitation approach integrating a self-feedback mechanism with data curation using large language models (LLMs). This dataset embodies domain-specific knowledge by combining reactants and products of chemical reactions with annotated RTs‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/liupf/SLM4CRP_with_RTs."},
  {"name":"efold_train","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/efold_train","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 306557 datapoints\\nstructure: 306557 datapoints\\ndms: 47464 datapoints\\nshape: 37920 datapoints\\n\\n"},
  {"name":"SE-Chatting.en","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/berwart/SE-Chatting.en","creator_name":"Berwart","creator_url":"https://huggingface.co/berwart","description":"\\n\\t\\n\\t\\t\\n\\t\\tSE.02\\n\\t\\n\\nDataset\\nHello, welcome to the official main dataset of SE.02 that's always getting updated, make sure to like to help us a lot.\\nthis dataset contains pretty much everything from math to isk what to put but pretty much anything you think an ai can say.\\nanyways this is our biggest dataset yet, and out first one that I don't even know how I managed to make this.\\nyou can use it to train your own ai if you want.\\n"},
  {"name":"terra-xplain-cc-de","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/D4ve-R/terra-xplain-cc-de","creator_name":"David","creator_url":"https://huggingface.co/D4ve-R","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Terra-Xplain-CC-de\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nA high quality text dataset in german. Scraped from https://terraxplaincommons.zdf.de/\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis dataset contains 283 items, including title, short_text and text for various domains.\\n48833 words.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nText Generation, DPO finetuning for response length preference\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nGerman only\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n{‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/D4ve-R/terra-xplain-cc-de."},
  {"name":"M4U","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/M4U-Benchmark/M4U","creator_name":"M4U-Benchmark","creator_url":"https://huggingface.co/M4U-Benchmark","description":"\\n\\t\\n\\t\\t\\n\\t\\tM4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models\\n\\t\\n\\nCode for the Paper M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models.\\n[Webpage] [Paper] [Huggingface Dataset] [Leaderboard]\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüí• News üí•\\n\\t\\n\\n\\n[2024.05.23] Our paper, dataset and code are public aviailable.\\n\\n\\n\\t\\n\\t\\n\\t\\n\\t\\tüëÄ About M4U\\n\\t\\n\\n\\n     \\n\\n\\nMultilingual multimodal reasoning is a core component to achieve human-level intelligence. However, most of the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/M4U-Benchmark/M4U."},
  {"name":"6OHOLvM38r8Kgy1","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/israel/6OHOLvM38r8Kgy1","creator_name":"Israel Abebe Azime","creator_url":"https://huggingface.co/israel","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMoleculeNet Benchmark (website)\\n\\t\\n\\nMoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/israel/6OHOLvM38r8Kgy1."},
  {"name":"ChemistryImages","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/LucasEllenberger/ChemistryImages","creator_name":"Lucas Ellenberger","creator_url":"https://huggingface.co/LucasEllenberger","description":"LucasEllenberger/ChemistryImages dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"SlimOrca-Dedup-Uzbek-cleaned","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/MLDataScientist/SlimOrca-Dedup-Uzbek-cleaned","creator_name":"Saeed","creator_url":"https://huggingface.co/MLDataScientist","description":"This is an Uzbek translated and cleaned version of https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup. \\nSpecifically, these replaced/removed records that had 'Uzbek translation|Uzbekcha tarjima|Uzbek tarjima|impossible to translate|not possible to translate|cannot fulfill your request|text is in|tilida yozilgan|Uzbek|o'zbek|ozbek|I am sorry'.\\nYou can use this dataset for chat fine-tuning of LLMs.\\nThis dataset has around 100M tokens (500M*0.8/4 = 100M assuming 4 chars are one token).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MLDataScientist/SlimOrca-Dedup-Uzbek-cleaned."},
  {"name":"chime-parent-child-relation","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/joe32140/chime-parent-child-relation","creator_name":"Chao-Chun (Joe) Hsu","creator_url":"https://huggingface.co/joe32140","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"chime-parent-child-relation\\\"\\n\\t\\n\\n Dataset in the CHIME paper.\\nMore Information needed\\n"},
  {"name":"ConBench_D","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/ConBench/ConBench_D","creator_name":"ConBench","creator_url":"https://huggingface.co/ConBench","description":"ConBench/ConBench_D dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"stem_mcqa_questions","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/mvujas/stem_mcqa_questions","creator_name":"Milos Vujasinovic","creator_url":"https://huggingface.co/mvujas","description":"This is a dataset of questions in various stem field generated using GPT-4o. The fields contain math, physics, chemistry, biology, computer_science and technical_sciences with around 400 samples in each.\\n"},
  {"name":"biochemistry-ocr","keyword":"chemistry","license":"Mozilla Public License 2.0","language":"en","url":"https://huggingface.co/datasets/maxall4/biochemistry-ocr","creator_name":"Max Campbell","creator_url":"https://huggingface.co/maxall4","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBiochemistry OCR\\n\\t\\n\\nThis is a dataset designed to fine-tune TrOCR on biochemistry terms such as Kcat/Km, various chemical names, and greek letters.\\nThe dataset contains two columns: image which is the path to the image, and label which is the ground-truth text in the image.\\n"},
  {"name":"KAR4DDI","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/liupf/KAR4DDI","creator_name":"Pengfei Liu","creator_url":"https://huggingface.co/liupf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe dataset in this study is a Drug-Drug Interaction Event (DDIE) dataset obtained from DeepDDI 2. It includes detailed information on 2,386 drugs, each represented by a 50-dimensional Principal Components Analysis (PCA) feature vector, and the corresponding SMILES strings. Additionally, drug descriptions from DDInter and DrugBank have been integrated into the dataset.\\nThe DDIE dataset comprises 222,127 drug pairs, enabling the prediction of 113 different DDIE types.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/liupf/KAR4DDI."},
  {"name":"AggregatorAdvisor","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/maomlab/AggregatorAdvisor","creator_name":"Maom Lab","creator_url":"https://huggingface.co/maomlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAggregator Advisor\\n\\t\\n\\nThe Aggregator Advisor is a web-tool hosted\\nby the Shoichet Lab at UCSF and used to assess the risk of a molecules being\\nan aggregetor, may aggregate in biochemical assays based on the chemical\\nsimilarity to known aggregators, and physical properties. The most current\\nrelease (2022/06) contains 12645 known aggregator molecules from 20 sources. \\nThe train and test datasets uploaded to our Hugging Face repository have been sanitized and split from the original‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/maomlab/AggregatorAdvisor."},
  {"name":"jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tjinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe dataset \\\"Chemical and Laboratory Equipment Pricing\\\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAssociated Model\\n\\t\\n\\nThis dataset was used to train the jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp model.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to Use\\n\\t\\n\\nTo use this dataset for model training or evaluation, you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/jinaai_jina-embeddings-v2-base-es-6122024-fv1x-webapp."},
  {"name":"ChemGPT-from-book","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/ALmonster/ChemGPT-from-book","creator_name":"The little monster will smile","creator_url":"https://huggingface.co/ALmonster","description":"ALmonster/ChemGPT-from-book dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"rqft","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/swaroop-nath/rqft","creator_name":"Swaroop Nath","creator_url":"https://huggingface.co/swaroop-nath","description":"Reliable Query-focused Summarization Tester (RQFT) is a dataset to evaluate Query-focused Summarization models. It contains 203 <query, document, summary> triples which can be used to evaluate Query-focused Summarizing models. Each document has more than 1 query on an average (1.41 to be precise). This is a design choice to tackle Topic Centralization, see Baumel et al., 2016.\\nFor more details, we refer the reader to our EMNLP 2023 paper: Reinforcement Replaces Supervision: Query focused‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/swaroop-nath/rqft."},
  {"name":"catalan","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Trimux/catalan","creator_name":"Facundo Campos","creator_url":"https://huggingface.co/Trimux","description":"Trimux/catalan dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"meudata","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/EronSamez/meudata","creator_name":"Samez","creator_url":"https://huggingface.co/EronSamez","description":"EronSamez/meudata dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"pokemon","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/shiquan1988/pokemon","creator_name":"liushiquan","creator_url":"https://huggingface.co/shiquan1988","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shiquan1988/pokemon."},
  {"name":"ssp_q3","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/ssp_q3","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Secondary Structure Prediction (Q3) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe study of a protein‚Äôs secondary structure (Sec. Struc. P.) forms a fundamental cornerstone in understanding its biological function. This secondary structure, comprising helices, strands, and various turns, bestows the protein with a specific three-dimensional configuration, which is critical for the formation of its tertiary structure. In the context of this work, a given protein sequence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/ssp_q3."},
  {"name":"fold_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/fold_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Fold Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFold class prediction is a scientific classification task that assigns protein sequences to one of 1,195 known folds. The primary application of this task lies in the identification of novel remote homologs among proteins of interest, such as emerging antibiotic-resistant genes and industrial enzymes. The study of protein fold holds great significance in fields like proteomics and structural biology, as it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/fold_prediction."},
  {"name":"antibiotic_resistance","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/antibiotic_resistance","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Antibiotic Resistance Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAntibiotic resistance refers to the ability of bacteria and other microorganisms to resist the effects of an antibiotic to which they are once sensitive. In this task, an input protein sequence is categorized according to which of 19 antibiotics it is resistant to. Thus, the scope of antibiotic drug development and research could be explored as an understanding in this topic is accumulated. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/antibiotic_resistance."},
  {"name":"cloning_clf","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/cloning_clf","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Cloning CLF Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nProtein structure determination includes a series of experimental stages to yield stable proteins for X-ray crystallography. Specifically, the proteins are first selected and expressed, then purified for crystal structure determination. Each step corresponds to a \\\"stage tag\\\" to denote whether the protein is stable under a certain stage. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/cloning_clf."},
  {"name":"enzyme_catalytic_efficiency","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/enzyme_catalytic_efficiency","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Enzyme Catalytic Efficiency Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis task is focused on predicting $k_cat$ values, which are enzymatic turnover numbers denoting the maximum chemical conversion rate of a reaction, for metabolic enzymes originating from any organism. These predictions are based on substrate structures and protein sequences. The underlying importance of this task lies in its potential to yield high-throughput and accurate $k_cat$ predictions applicable‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/enzyme_catalytic_efficiency."},
  {"name":"fitness_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/fitness_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Fitness Prediction (GB1) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Fitness Prediction (GB1) task is dedicated to anticipating the fitness landscape of the GB1 domain, a process that plays a pivotal role in understanding and enhancing the binding affinity and stability of this domain. As a prevalent protein interaction domain, GB1 finds wide usage in diverse applications such as protein purification, biosensors, and drug delivery. This task is configured as a regression‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/fitness_prediction."},
  {"name":"fluorescence_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/fluorescence_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Fluorescence Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Fluorescence Prediction task focuses on predicting the fluorescence intensity of green fluorescent protein mutants, a crucial function in biology that allows researchers to infer the presence of proteins within cell lines and living organisms. This regression task utilizes training and evaluation datasets that feature mutants with three or fewer mutations, contrasting the testing dataset, which comprises‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/fluorescence_prediction."},
  {"name":"localization_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/localization_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Localization Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe task of Protein Subcellular Localization Prediction bears substantial relevance in bioinformatics, owing to its contributions to proteomics research and its potential to augment our comprehension of protein function and disease mechanisms. In this task, the input to the model is an amino acid sequence of a protein, which is transformed into an output comprising a probability distribution over 10 unique‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/localization_prediction."},
  {"name":"material_production","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/material_production","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Material Production Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe task is to predict whether a protein sequence fails at the protein material stage.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and an integer label indicating whether a protein sequence fails at the protein material stage.  See the material production dataset viewer to explore more examples.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/material_production."},
  {"name":"metal_ion_binding","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/metal_ion_binding","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Metal Ion Binding Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMetal ion binding sites within proteins play a crucial role across a spectrum of processes, spanning from physiological to pathological, toxicological, pharmaceutical, and diagnostic. Consequently, the development of precise and efficient methods to identify and characterize these metal ion binding sites in proteins has become an imperative and intricate task for bioinformatics and structural biology.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/metal_ion_binding."},
  {"name":"optimal_ph","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/optimal_ph","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Optimal PH Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nEnzyme functions normally under a specific range of pH of the surrounding environment. However, an optimal pH for the reaction will largely boost the catalytic ability.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and a float value indicating the optimal ph for a given enzyme‚Äôs catalytic effect.  See the optimal ph dataset viewer to explore‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/optimal_ph."},
  {"name":"optimal_temperature","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/optimal_temperature","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Optimal Temperature Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGrasping the catalytic activity of enzymes is pivotal for industrial enzyme design, particularly in predicting the optimal temperature for a given enzyme‚Äôs catalytic effect. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and a float value indicating the optimal temperature for a given enzyme‚Äôs catalytic effect.  See the optimal‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/optimal_temperature."},
  {"name":"peptide_HLA_MHC_affinity","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/peptide_HLA_MHC_affinity","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Peptide-HLA/MHC Affinity  Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe human leukocyte antigen (HLA) gene encodes major histo-compatibility complex (MHC) proteins, which can bind to peptide fragments and be presented to the cell surface for subsequent T cell receptors (TCRs) recognition. Accurately predicting the interaction between peptide sequence and HLA molecule will boost the understanding of immune responses, antigen presentation, and designing therapeutic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/peptide_HLA_MHC_affinity."},
  {"name":"solubility_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/solubility_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Solubility Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis solubility prediction task involves a binary classification of a heterogenous set of proteins, assessing them as either soluble or insoluble. The solubility metric is a crucial design parameter in ensuring protein efficacy, with particular relevance in the pharmaceutical domain. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/solubility_prediction."},
  {"name":"stability_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/stability_prediction","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Stability Stability Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Stability Stability task is to predict the concentration of protease at which a protein can retain its folded state. Protease, being integral to numerous biological processes, bears significant relevance and a profound comprehension of protein stability during protease interaction can offer immense value, especially in the creation of novel therapeutics.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/stability_prediction."},
  {"name":"temperature_stability","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/temperature_stability","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Temperature Stability Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe accurate prediction of protein thermal stability has far-reaching implications in both academic and industrial spheres. This task primarily aims to predict a protein‚Äôs capacity to preserve its structural stability under a temperature condition of 65 degrees Celsius. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and an integer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/temperature_stability."},
  {"name":"contact_prediction_binary","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/contact_prediction_binary","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Contact Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nContact map prediction aims to determine whether two residues, $i$ and $j$, are in contact or not, based on their distance with a certain threshold ($<$8 Angstrom). This task is an important part of the early Alphafold version for structural prediction. \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string of the protein sequences, a sequence for the contact labels. Each of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/contact_prediction_binary."},
  {"name":"EgoExoLearn","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/hyf015/EgoExoLearn","creator_name":"hyf","creator_url":"https://huggingface.co/hyf015","description":"NOTE: Videos in huggingface are unprocessed, full-size videos. For benchmark and gaze alignment, we use processed 25fps videos. For processed data and code for benchmark, please visit the github page.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tEgoExoLearn\\n\\t\\n\\nThis repository contains the video data of the following paper:\\n\\nEgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World\\nYifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang,  Lu‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hyf015/EgoExoLearn."},
  {"name":"BALM-benchmark","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/BALM/BALM-benchmark","creator_name":"Binding Affinity Language Model","creator_url":"https://huggingface.co/BALM","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for BALM-Benchmark\\n\\t\\n\\n\\n\\nBALM-Benchmark is a curated collection of datasets designed to advance machine learning and deep learning model research for protein-ligand binding affinity prediction. This benchmark consolidates several key datasets including BindingDB, LP-PDBBind, and specific protein-ligand systems like USP7, MPro, SYK, HIF2A, and MCL1, each chosen for its distinct data characteristics and evaluation. \\nThis dataset collection has been refined and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BALM/BALM-benchmark."},
  {"name":"Kapibara","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/alban-labs/Kapibara","creator_name":"Albanian Labs","creator_url":"https://huggingface.co/alban-labs","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKapibara: Albanian Multi-turn Conversation Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nKapibara is a comprehensive Albanian language dataset designed for multi-turn conversations. It contains over 5,300 entries covering a wide range of topics including physics, biology, mathematics, chemistry, culture, and logic. The dataset is aimed at improving text generation and question-answering capabilities in the Albanian language.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks\\n\\t\\n\\nThe dataset supports the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alban-labs/Kapibara."},
  {"name":"Arabic_prompts_Mini_175","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/HeshamHaroon/Arabic_prompts_Mini_175","creator_name":"Hesham Haroon","creator_url":"https://huggingface.co/HeshamHaroon","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tArabic Prompts Dataset\\n\\t\\n\\nOverview\\nThe Arabic Prompts Dataset is a comprehensive collection of prompts designed to facilitate research and development in natural language processing (NLP), machine learning, and artificial intelligence, particularly focusing on Arabic language applications. This dataset includes a diverse range of topics and questions across various fields such as literature, science, technology, and culture, making it an invaluable resource for training models‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HeshamHaroon/Arabic_prompts_Mini_175."},
  {"name":"Celestia","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sequelbox/Celestia","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia is a dataset containing science-instruct data.\\nThe 2024-10-30 version contains:\\n\\n126k rows of synthetic science-instruct data, using synthetically generated prompts and responses generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
  {"name":"ssp_q8","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/biomap-research/ssp_q8","creator_name":"Biomap","creator_url":"https://huggingface.co/biomap-research","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Secondary Structure Prediction (Q8) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe study of a protein‚Äôs secondary structure (Sec. Struc. P.) forms a fundamental cornerstone in understanding its biological function. This secondary structure, comprising helices, strands, and various turns, bestows the protein with a specific three-dimensional configuration, which is critical for the formation of its tertiary structure. In the context of this work, a given protein sequence is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/biomap-research/ssp_q8."},
  {"name":"Dans-MemoryCore-CoreCurriculum-Small","keyword":"chemistry","license":"GNU Affero General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/PocketDoc/Dans-MemoryCore-CoreCurriculum-Small","creator_name":"PocketDoc Labs","creator_url":"https://huggingface.co/PocketDoc","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDan's Memory Core: Core Curriculum Small\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBroad strokes\\n\\t\\n\\nThis dataset aims to provide a foundation of knowledge common to a number of fields and areas of study. The question answer pairs were generated using a RAG implementation and a curated selection of source material. Ideally this will be the first in a series of datasets that will cover a wide range of topics.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNomic Atlas Visualiztion\\n\\t\\n\\nCluster visualization for the dataset available here.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PocketDoc/Dans-MemoryCore-CoreCurriculum-Small."},
  {"name":"sel_dataset","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/DeerEyeRain/sel_dataset","creator_name":"LU MUYU","creator_url":"https://huggingface.co/DeerEyeRain","description":"DeerEyeRain/sel_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"wikireading","keyword":"chemistry","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/its5Q/wikireading","creator_name":"its5Q","creator_url":"https://huggingface.co/its5Q","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Wikireading\\n\\t\\n\\nThis is a dataset of book chapters scraped from a Russian website called Wikireading.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWikireading is a collection of non-fiction educational books in various domains: Biology, Art, History, Religion and much more. The books are highly educational and provide vast knowledge in different domains, making this dataset a good choice for pretraining.\\nThe resulting dataset contains ~26M rows‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/its5Q/wikireading."},
  {"name":"SDS-Eyes-Protection-Classification","keyword":"chemistry","license":"GNU General Public License v2.0","language":"en","url":"https://huggingface.co/datasets/BASF-AI/SDS-Eyes-Protection-Classification","creator_name":"BASF","creator_url":"https://huggingface.co/BASF-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSafety Data Sheets Eyes Protection Classification\\n\\t\\n\\nThis dataset contains Safety Data Sheets (SDS) sourced from Kaggle, consisting of over 200,000 documents. SDS are detailed documents providing essential information on the properties and hazards of chemicals, ensuring user safety and compliance with regulatory standards. A subset of these documents was pre-processed, cleaned, and annotated to classify whether eye protection is required when handling materials. The labels were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BASF-AI/SDS-Eyes-Protection-Classification."},
  {"name":"crispr-bf-master","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/crispr-bf-master","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\ncrispr-bf-master:  BF_master_Xu_Feng_Tm_sup_screens_042320\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells.  The data was originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022) and is available on Figshare.\\n\\nCurated by: Feng et al., Sci. Adv. 8, eabm6638‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/crispr-bf-master."},
  {"name":"pmc_35559673_table_s2","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s2","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s2."},
  {"name":"pmc_35559673_table_s3","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s3","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s3."},
  {"name":"pmc_35559673_table_s7","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s7","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s7."},
  {"name":"lyu-wang-balius-singh-2019-ampc","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/scbirlab/lyu-wang-balius-singh-2019-ampc","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltra-large docking data: AmpC 96M compounds\\n\\t\\n\\nThese data are from John J. Irwin, Bryan L. Roth, and Brian K. Shoichet's labs. They published it as:\\n\\nLyu J, Wang S, Balius TE, Singh I, Levit A, Moroz YS, O'Meara MJ, Che T, Algaa E, Tolmachova K, Tolmachev AA, Shoichet BK, Roth BL, Irwin JJ. \\nUltra-large library docking for discovering new chemotypes. Nature. 2019 Feb;566(7743):224-229. doi: 10.1038/s41586-019-0917-9. \\nEpub 2019 Feb 6. PMID: 30728502; PMCID: PMC6383769.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/lyu-wang-balius-singh-2019-ampc."},
  {"name":"lyu-wang-balius-singh-2019-d4","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/scbirlab/lyu-wang-balius-singh-2019-d4","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUltra-large docking data: D4 receptor 115M compounds\\n\\t\\n\\nThese data are from John J. Irwin, Bryan L. Roth, and Brian K. Shoichet's labs. They published it as:\\n\\nLyu J, Wang S, Balius TE, Singh I, Levit A, Moroz YS, O'Meara MJ, Che T, Algaa E, Tolmachova K, Tolmachev AA, Shoichet BK, Roth BL, Irwin JJ. \\nUltra-large library docking for discovering new chemotypes. Nature. 2019 Feb;566(7743):224-229. doi: 10.1038/s41586-019-0917-9. \\nEpub 2019 Feb 6. PMID: 30728502; PMCID: PMC6383769.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/lyu-wang-balius-singh-2019-d4."},
  {"name":"Llamole-MolQA","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/liuganghuggingface/Llamole-MolQA","creator_name":"Gang Liu","creator_url":"https://huggingface.co/liuganghuggingface","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Info\\n\\t\\n\\nThe molecular design question-answering dataset (MolQA) is introduced in the paper: Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning.\\nüìÅ Repository: https://github.com/liugangcode/Llamole\\n"},
  {"name":"sentence-correction","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Ashraf-CK/sentence-correction","creator_name":"Chauhan","creator_url":"https://huggingface.co/Ashraf-CK","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLLM Prompt Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe LLM Prompt Dataset is designed to enhance the performance of large language models (LLMs) by transforming user inputs into structured prompts. This dataset aims to facilitate the understanding of complex queries and improve the interaction between users and LLMs.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset is organized in JSON format, where each entry consists of an input and a prompt. The input represents the original user query‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Ashraf-CK/sentence-correction."},
  {"name":"sentence-corrector","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MohamedAshraf701/sentence-corrector","creator_name":"ashraf chauhan","creator_url":"https://huggingface.co/MohamedAshraf701","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSentence Correction & Politeness Dataset\\n\\t\\n\\nThis repository contains a dataset specifically designed for sentence correction and politeness transformation. The dataset includes a set of input sentences and their corresponding polite, well-formatted outputs. It can be used to train AI models to rephrase user inputs in a more formal, polite, and grammatically correct manner.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe Sentence Correction & Politeness Dataset is designed to help improve natural‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MohamedAshraf701/sentence-corrector."},
  {"name":"protobowl-11-13","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/mgor/protobowl-11-13","creator_name":"Maharshi Gor","creator_url":"https://huggingface.co/mgor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tProgressive Quiz Bowl Clues Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains Quiz Bowl questions and their corresponding progressive clues, designed for evaluating question-answering systems.\\nThe progressive clues subset contains additional features such as GPT-3.5 generated categories and subcategories specific to each progressive clue.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Information\\n\\t\\n\\n\\nName: protobowl-11-13\\nVersion: 1.0\\nMaintainer: mgor\\nHub URL:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mgor/protobowl-11-13."},
  {"name":"chemical-documents","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/ElstnerAnalytics/chemical-documents","creator_name":"Elstner Analytics GmbH","creator_url":"https://huggingface.co/ElstnerAnalytics","description":"The dataset consists of CC-BY-4.0 licensed open access papers and the annotations on-top are provided under the same license. Train, test and validation splits are provided as separate folders containing the image files and a single json file containing the annotations in COCO format.\\nThe classes are close to the TFT-ID dataset and extend into 3 chemistry specific labels. chem_reaction contains everything with reaction arrows, chem_structures are structural formulas without reactions and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ElstnerAnalytics/chemical-documents."},
  {"name":"cyc-pep-6-12mer-650M-2024","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-650M-2024","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tcyc-pep-6-12mer-650M-2024\\n\\t\\n\\nSet of ~650 million (almost) unique cyclic and linear peptides comprising 6-12 amino acids, each with some calculated molecular properties.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\n\\nRepository: https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-650M-2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\nlinear_id: Identifier for linear peptide\\npeptide_sequence: Amino acid sequence of linear peptide\\nSMILES: SMILES string of linear‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-650M-2024."},
  {"name":"fold_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/fold_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Fold Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nFold class prediction is a scientific classification task that assigns protein sequences to one of 1,195 known folds. The primary application of this task lies in the identification of novel remote homologs among proteins of interest, such as emerging antibiotic-resistant genes and industrial enzymes. The study of protein fold holds great significance in fields like proteomics and structural biology, as it‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/fold_prediction."},
  {"name":"localization_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/localization_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Localization Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe task of Protein Subcellular Localization Prediction bears substantial relevance in bioinformatics, owing to its contributions to proteomics research and its potential to augment our comprehension of protein function and disease mechanisms. In this task, the input to the model is an amino acid sequence of a protein, which is transformed into an output comprising a probability distribution over 10‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/localization_prediction."},
  {"name":"material_production","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/material_production","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Material Production Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe task is to predict whether a protein sequence fails at the protein material stage.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and an integer label indicating whether a protein sequence fails at the protein material stage.  See the material production dataset viewer to explore more examples.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/material_production."},
  {"name":"stability_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/stability_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Stability Stability Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Stability Stability task is to predict the concentration of protease at which a protein can retain its folded state. Protease, being integral to numerous biological processes, bears significant relevance and a profound comprehension of protein stability during protease interaction can offer immense value, especially in the creation of novel therapeutics.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/stability_prediction."},
  {"name":"traindata_content","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/dsacedsfds/traindata_content","creator_name":"sss","creator_url":"https://huggingface.co/dsacedsfds","description":"dsacedsfds/traindata_content dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"test_modern_dataset","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/SerhiiLebediuk/test_modern_dataset","creator_name":"Serhii Lebediuk","creator_url":"https://huggingface.co/SerhiiLebediuk","description":"SerhiiLebediuk/test_modern_dataset dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"aBN_HSX","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/AndreiVoicuT/aBN_HSX","creator_name":"Andrei-Voicu Tomut","creator_url":"https://huggingface.co/AndreiVoicuT","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCard Description for Dataset\\n\\t\\n\\n\\nDataset Name: Atomic Structures and H/S Matrices of aBN Configurations\\nDescription:\\nThis dataset contains computationally generated atomic structures of amorphous boron nitride (aBN) with various configurations containing 2, 3, 8, 32, and 64 atoms per unit. Each structure is described by its atomic positions, lattice properties, and associated Hamiltonian (H) and overlap (S) matrices, which are commonly used in quantum mechanical simulations and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AndreiVoicuT/aBN_HSX."},
  {"name":"flemish_multimodal_exams_physician","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/jjzha/flemish_multimodal_exams_physician","creator_name":"Mike Zhang","creator_url":"https://huggingface.co/jjzha","description":"jjzha/flemish_multimodal_exams_physician dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Diverse-Knowledge","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kunu5402/Diverse-Knowledge","creator_name":"Kunal Kumar","creator_url":"https://huggingface.co/kunu5402","description":"\\n\\t\\n\\t\\t\\n\\t\\tEverything Data\\n\\t\\n\\n\\nThis data is synthetically generated by a ton of open and closed source models. This is basically a parsed version of yearly log form a small dialouge based testing to anylyze model's response on it then perform human evals on it.\\nThe data contains information about everything from every domain, most of the pairs included in this data are preferred by humans as the model's response.\\nIt can be used for topic modeling, or human preference evals etc.\\nRest anyone can do‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kunu5402/Diverse-Knowledge."},
  {"name":"ProfessionalTermsAmharicEnglish","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/admasorg/ProfessionalTermsAmharicEnglish","creator_name":"m","creator_url":"https://huggingface.co/admasorg","description":"admasorg/ProfessionalTermsAmharicEnglish dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"AcademyofEthiopianLanguagesScienceandTechnologyDictionary","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/admasorg/AcademyofEthiopianLanguagesScienceandTechnologyDictionary","creator_name":"m","creator_url":"https://huggingface.co/admasorg","description":"admasorg/AcademyofEthiopianLanguagesScienceandTechnologyDictionary dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"errors","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/katsukiai/errors","creator_name":"KatsukiAI","creator_url":"https://huggingface.co/katsukiai","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Nei]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More Information‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/katsukiai/errors."},
  {"name":"VisOnlyQA_metadata","keyword":"chemistry","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_metadata","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_metadata."},
  {"name":"CNTXTAI-Ranking-Dataset","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/CNTXTAI0/CNTXTAI-Ranking-Dataset","creator_name":"CNTXT AI","creator_url":"https://huggingface.co/CNTXTAI0","description":"General Overview\\nThis dataset is to be used for LLM Trainings, This is a sample, visit https://www.cntxt.tech/ to learn more\\nThe dataset consists of 50 rows (excluding headers) and 8 columns. The columns capture various aspects of ranked responses to prompts, including:\\nNumeric_ID (Unique Identifier - Integer)\\nPrompt (The Question or Task - Text)\\nAnswer_A / Answer_B (Response Options - Text)\\nCategory (Type of Task - Categorical)\\nBest Answer (Preferred Response - Categorical)\\nLikeRT Score‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CNTXTAI0/CNTXTAI-Ranking-Dataset."},
  {"name":"dutch-central-exam-mcq","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq","creator_name":"Mike Zhang","creator_url":"https://huggingface.co/jjzha","description":"Multiple Choice Questions of the Dutch Central Exam 1999-2024\\n\\nWhat?\\n\\nThis dataset contains only multiple choice questions from the Dutch Central Exam (High School level). From Wikipedia:\\nThe Eindexamen (Dutch pronunciation: [Àà…õiÃØnt…õksam…ôn]) or centraal examen (CE) is the matriculation exam in the Netherlands, which takes place in a student's final year of high school education (voortgezet onderwijs; \\\"continued education\\\"). The exam is regulated by the Dutch Secondary Education Act[1] and the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jjzha/dutch-central-exam-mcq."},
  {"name":"antibiotic_resistance","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/antibiotic_resistance","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Antibiotic Resistance Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nAntibiotic resistance refers to the ability of bacteria and other microorganisms to resist the effects of an antibiotic to which they are once sensitive. In this task, an input protein sequence is categorized according to which of 19 antibiotics it is resistant to. Thus, the scope of antibiotic drug development and research could be explored as an understanding in this topic is accumulated.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/antibiotic_resistance."},
  {"name":"cloning_clf","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/cloning_clf","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Cloning CLF Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nProtein structure determination includes a series of experimental stages to yield stable proteins for X-ray crystallography. Specifically, the proteins are first selected and expressed, then purified for crystal structure determination. Each step corresponds to a \\\"stage tag\\\" to denote whether the protein is stable under a certain stage. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/cloning_clf."},
  {"name":"enzyme_catalytic_efficiency","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/enzyme_catalytic_efficiency","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Enzyme Catalytic Efficiency Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis task is focused on predicting $k_cat$ values, which are enzymatic turnover numbers denoting the maximum chemical conversion rate of a reaction, for metabolic enzymes originating from any organism. These predictions are based on substrate structures and protein sequences. The underlying importance of this task lies in its potential to yield high-throughput and accurate $k_cat$ predictions‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/enzyme_catalytic_efficiency."},
  {"name":"fitness_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/fitness_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Fitness Prediction (GB1) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Fitness Prediction (GB1) task is dedicated to anticipating the fitness landscape of the GB1 domain, a process that plays a pivotal role in understanding and enhancing the binding affinity and stability of this domain. As a prevalent protein interaction domain, GB1 finds wide usage in diverse applications such as protein purification, biosensors, and drug delivery. This task is configured as a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/fitness_prediction."},
  {"name":"fluorescence_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/fluorescence_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Fluorescence Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe Fluorescence Prediction task focuses on predicting the fluorescence intensity of green fluorescent protein mutants, a crucial function in biology that allows researchers to infer the presence of proteins within cell lines and living organisms. This regression task utilizes training and evaluation datasets that feature mutants with three or fewer mutations, contrasting the testing dataset, which‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/fluorescence_prediction."},
  {"name":"metal_ion_binding","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/metal_ion_binding","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Metal Ion Binding Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nMetal ion binding sites within proteins play a crucial role across a spectrum of processes, spanning from physiological to pathological, toxicological, pharmaceutical, and diagnostic. Consequently, the development of precise and efficient methods to identify and characterize these metal ion binding sites in proteins has become an imperative and intricate task for bioinformatics and structural biology.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/metal_ion_binding."},
  {"name":"optimal_ph","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/optimal_ph","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Optimal PH Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nEnzyme functions normally under a specific range of pH of the surrounding environment. However, an optimal pH for the reaction will largely boost the catalytic ability.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and a float value indicating the optimal ph for a given enzyme‚Äôs catalytic effect.  See the optimal ph dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/optimal_ph."},
  {"name":"temperature_stability","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/temperature_stability","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Temperature Stability Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe accurate prediction of protein thermal stability has far-reaching implications in both academic and industrial spheres. This task primarily aims to predict a protein‚Äôs capacity to preserve its structural stability under a temperature condition of 65 degrees Celsius. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/temperature_stability."},
  {"name":"solubility_prediction","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/solubility_prediction","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Solubility Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThis solubility prediction task involves a binary classification of a heterogenous set of proteins, assessing them as either soluble or insoluble. The solubility metric is a crucial design parameter in ensuring protein efficacy, with particular relevance in the pharmaceutical domain. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/solubility_prediction."},
  {"name":"peptide_HLA_MHC_affinity","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/peptide_HLA_MHC_affinity","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Peptide-HLA/MHC Affinity  Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe human leukocyte antigen (HLA) gene encodes major histo-compatibility complex (MHC) proteins, which can bind to peptide fragments and be presented to the cell surface for subsequent T cell receptors (TCRs) recognition. Accurately predicting the interaction between peptide sequence and HLA molecule will boost the understanding of immune responses, antigen presentation, and designing therapeutic‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/peptide_HLA_MHC_affinity."},
  {"name":"ssp_q8","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/ssp_q8","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Secondary Structure Prediction (Q8) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe study of a protein‚Äôs secondary structure (Sec. Struc. P.) forms a fundamental cornerstone in understanding its biological function. This secondary structure, comprising helices, strands, and various turns, bestows the protein with a specific three-dimensional configuration, which is critical for the formation of its tertiary structure. In the context of this work, a given protein‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/ssp_q8."},
  {"name":"ssp_q3","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/ssp_q3","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Secondary Structure Prediction (Q3) Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nThe study of a protein‚Äôs secondary structure (Sec. Struc. P.) forms a fundamental cornerstone in understanding its biological function. This secondary structure, comprising helices, strands, and various turns, bestows the protein with a specific three-dimensional configuration, which is critical for the formation of its tertiary structure. In the context of this work, a given protein‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/ssp_q3."},
  {"name":"contact_prediction_binary","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/contact_prediction_binary","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Contact Prediction Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nContact map prediction aims to determine whether two residues, $i$ and $j$, are in contact or not, based on their distance with a certain threshold ($<$8 Angstrom). This task is an important part of the early Alphafold version for structural prediction. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string of the protein sequences, a sequence for the contact‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/contact_prediction_binary."},
  {"name":"yourbench_archived","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sumukshashidhar-archive/yourbench_archived","creator_name":"Sumuk's Archived Content","creator_url":"https://huggingface.co/sumukshashidhar-archive","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåü YourBench Y1: A Diverse Domain Benchmark Dataset\\n\\t\\n\\n\\n    \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nYourBench Y1 is a carefully curated dataset of documents from 8 different domains, specifically designed to evaluate language models on content likely generated or produced after July 2024. This dataset provides a unique benchmark for testing model performance on contemporary content across diverse professional and technical domains.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\nüìä 8 balanced domains‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sumukshashidhar-archive/yourbench_archived."},
  {"name":"MMMU_Pro","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MMMU/MMMU_Pro","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\\n\\t\\n\\t\\t\\n\\t\\tMMMU-Pro (A More Robust Multi-discipline Multimodal Understanding Benchmark)\\n\\t\\n\\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüõ†Ô∏èüõ†Ô∏è [2025-03-08] Fixed mismatch between inner image labels and shuffled options in Vision and Standard (10 options) settings. (test_Chemistry_5,94,147,216,314,345,354,461,560,570; test_Materials_450; test_Pharmacy_198; validation_Chemistry_12,26,29; validation_Materials_10,28; validation_Psychology_1)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU_Pro."},
  {"name":"crispr-qbf-master","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/crispr-qbf-master","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\ncrispr-qbf-master:  Master_QBF_293A_Xu_Feng_Tm_Sup_scr_042320\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells.  The data was originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022) and is available on Figshare.\\n\\nCurated by: Feng et al., Sci. Adv. 8, eabm6638‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/crispr-qbf-master."},
  {"name":"crispr-raw-read-counts","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/crispr-raw-read-counts","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\ncrispr-raw-read-counts:  Table_S1_Raw_read_counts_master_Xu_Feng_Tm_sup_screens\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells.  The data was originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022) and is available on Figshare.\\n\\nCurated by: Feng et al., Sci.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/crispr-raw-read-counts."},
  {"name":"pmc_35559673_table_s1","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s1","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s1."},
  {"name":"pmc_35559673_table_s5","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s5","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s5."},
  {"name":"pmc_35559673_table_s6","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s6","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for PMC_35559673_table_s datasets\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains the results of genome-wide CRISPR screens using isogenic knockout cells to uncover vulnerabilities in tumor suppressor-deficient cancer cells. The data were originally published by Feng et al., Sci. Adv. 8, eabm6638 (2022), and are available via PubMed Central (PMC). The supplementary tables included in this dataset provide detailed data on raw‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/pmc_35559673_table_s6."},
  {"name":"YakugakuQA","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/EQUES/YakugakuQA","creator_name":"EQUES Inc.","creator_url":"https://huggingface.co/EQUES","description":"\\n\\t\\n\\t\\t\\n\\t\\tYakugakuQA\\n\\t\\n\\n\\n\\nYakugakuQA is a question answering dataset, consisting of 13 years (2012-2024) of past questions and answers from the Japanese National License Examination for Pharmacists. It contains over 4K pairs of questions, answers, and commentaries.\\n2025-2-17: Image data added.  \\n2024-12-10: Dataset release.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\nCurated by: EQUES Inc.\\nFunded by [optional]: GENIAC Project\\nShared by [optional]:\\nLanguage(s) (NLP): Japanese‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/EQUES/YakugakuQA."},
  {"name":"BiST","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Mxode/BiST","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBiST\\n\\t\\n\\nEnglish | ÁÆÄ‰Ωì‰∏≠Êñá\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIntroduction\\n\\t\\n\\nBiST is a large-scale bilingual translation dataset, with \\\"BiST\\\" standing for Bilingual Synthetic Translation dataset. Currently, the dataset contains approximately 57,000,000 entries and will continue to expand in the future.\\nBiST consists of two subsets, namely en-zh and zh-en, where the former represents the source language, collected from public data as real-world content; the latter represents the target language for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Mxode/BiST."},
  {"name":"SDSEyeProtectionClassification","keyword":"chemistry","license":"GNU General Public License v2.0","language":"en","url":"https://huggingface.co/datasets/BASF-AI/SDSEyeProtectionClassification","creator_name":"BASF","creator_url":"https://huggingface.co/BASF-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSafety Data Sheets Eyes Protection Classification\\n\\t\\n\\nThis dataset is a smaller version of the SDS-Eyes-Protection-Classification dataset.\\n"},
  {"name":"SDSGlovesClassification","keyword":"chemistry","license":"GNU General Public License v2.0","language":"en","url":"https://huggingface.co/datasets/BASF-AI/SDSGlovesClassification","creator_name":"BASF","creator_url":"https://huggingface.co/BASF-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSafety Data Sheets Gloves Classification\\n\\t\\n\\nThis dataset is a smaller version of the SDS-Gloves-Classification dataset.\\n"},
  {"name":"MolParser-7M","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/AI4Industry/MolParser-7M","creator_name":"IndustryAI","creator_url":"https://huggingface.co/AI4Industry","description":"\\n\\t\\n\\t\\t\\n\\t\\tMolParser-7M\\n\\t\\n\\nAnonymous Open Source now\\nThis repo provides the training data and evaluation data for MolParser, proposed in paper ‚ÄúMolParser: End-to-end Visual Recognition of Molecule Structures in the Wild‚Äú (ICCV2025 under review)\\nMolParser-7M contains nearly 8 million paired image-SMILES data. It should be noted that the caption of image is our extended-SMILES format, which suggested in our paper.\\n\\nMolParser-Pretrain: More than 7.7M synthetic training data in pretrain_synthetic_7M‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Industry/MolParser-7M."},
  {"name":"filtered_orthologs","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/seq-to-pheno/filtered_orthologs","creator_name":"Seq-to-Pheno","creator_url":"https://huggingface.co/seq-to-pheno","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tZoonomia Filtered Orthologs Dataset\\n\\t\\n\\nThis dataset has been filtered to remove:\\n\\nProteins longer than 1000 amino acids\\nProteins with more than {MAX_NUMBER_ORTHOLOGS} orthologs in any species\\n\\nOriginal number of mapped orthologs: {len(all_mapped_ortholog_df)}\\nFiltered number of mapped orthologs: {num_examples}\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of a single CSV file with the following columns:\\n\\ntranscript: Human transcript ID\\nprotein: Human protein name\\nmapped_to:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seq-to-pheno/filtered_orthologs."},
  {"name":"steshin-2023-lohi","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/scbirlab/steshin-2023-lohi","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLo-Hi Benchmark\\n\\t\\n\\nData from Simon Steshin, Lo-Hi: Practical ML Drug Discovery Benchmark, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFrom the original README:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHit Identification\\n\\t\\n\\nThe goal of the Hit Identification task is to find novel molecules that have desirable property, but are dissimilar from the molecules‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/steshin-2023-lohi."},
  {"name":"fang-2023-biogen-adme","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tBiogen ADME dataset (public data)\\n\\t\\n\\nData from Fang et al., Prospective Validation of Machine Learning Algorithms for Absorption, Distribution, Metabolism, and Excretion Prediction: An Industrial Perspective, available from the GitHub repositiory. We used schemist (which in turn uses RDKit)\\nto add molecuar weight, Murcko scaffold, Crippen cLogP, and topological surface area, and to generate scaffold splits.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nFrom the original README:\\n\\nTo benefit the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/fang-2023-biogen-adme."},
  {"name":"ChatEnv","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/SustcZhangYX/ChatEnv","creator_name":"Zhang_YX","creator_url":"https://huggingface.co/SustcZhangYX","description":"\\nChatEnv: A Domain-Specific Instruction Dataset for Environmental Science\\n\\n\\nChatEnv is a large-scale, domain-specific instruction dataset designed to enhance large language models (LLMs) for environmental science tasks. This dataset is an integral part of the EnvGPT framework, supporting fine-tuning and evaluation processes by providing a diverse and high-quality set of instructions tailored to the unique demands of environmental science research and applications.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüìÉ Dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SustcZhangYX/ChatEnv."},
  {"name":"Hutao_furina_roleplay","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Exched/Hutao_furina_roleplay","creator_name":"Dewa Syahputra","creator_url":"https://huggingface.co/Exched","description":"---# Character Dialogue Dataset\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDescription\\n\\t\\n\\nThis dataset contains character dialogue interactions designed to train and evaluate language models. The dataset includes diverse examples of character responses and interactions, capturing the essence of character personalities and their unique ways of engaging in conversation.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLicense\\n\\t\\n\\nThe dataset is provided under the MIT License. You are free to use, modify, and distribute the dataset as long as proper attribution is‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Exched/Hutao_furina_roleplay."},
  {"name":"Firefly-Rephrased-Multiturn-300K","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Mxode/Firefly-Rephrased-Multiturn-300K","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"Mxode/Firefly-Rephrased-Multiturn-300K dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Firefly-1.1M-Rephrased","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Mxode/Firefly-1.1M-Rephrased","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"Mxode/Firefly-1.1M-Rephrased dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"test","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/hl0737/test","creator_name":"Hu Dou Dou","creator_url":"https://huggingface.co/hl0737","description":"hl0737/test dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"IndustryCorpus-Subset-zh-en","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Mxode/IndustryCorpus-Subset-zh-en","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"Mxode/IndustryCorpus-Subset-zh-en dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Spurline","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\\nThe 2024-10-30 version contains:\\n\\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\\n\\nThis dataset contains synthetically generated data and has not been subject to manual review.\\n"},
  {"name":"cyc-pep-6-12mer-70M-2024","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-70M-2024","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tcyc-pep-6-12mer-70M-2024\\n\\t\\n\\nSet of 70 million (almost) unique cyclic and linear peptides comprising 6-12 amino acids, each with some calculated molecular properties.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\n\\nRepository: https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-70M-2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\nlinear_id: Identifier for linear peptide\\npeptide_sequence: Amino acid sequence of linear peptide\\nSMILES: SMILES string of linear peptide‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/cyc-pep-6-12mer-70M-2024."},
  {"name":"CMB","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/fzkuji/CMB","creator_name":"Fu Zichuan","creator_url":"https://huggingface.co/fzkuji","description":"\\n\\t\\n\\t\\t\\n\\t\\tCMB: A Comprehensive  Medical Benchmark in Chinese\\n\\t\\n\\n\\n\\n   üåê Github ‚Ä¢ üåê Website ‚Ä¢ ü§ó HuggingFace\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåà Update\\n\\t\\n\\n\\n[2024.02.21] The answers to the CMB-Exam test has been updated and some errors caused by omissions in version management have been fixed.\\n[2024.01.08] In order to facilitate testing, we disclose the answers to the CMB-Exam test\\n[2023.09.22] CMB is included in OpenCompass.\\n[2023.08.21] Paper released.\\n[2023.08.01] üéâüéâüéâ CMB is publishedÔºÅüéâüéâüéâ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tüåê‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fzkuji/CMB."},
  {"name":"NanoExperiment-Data-Mix-10M","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Mxode/NanoExperiment-Data-Mix-10M","creator_name":"Max Zhang","creator_url":"https://huggingface.co/Mxode","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMxode/NanoExperiment-Data-Mix-10M\\n\\t\\n\\nDataset of NanoExperiment. Tokenized by Bilingual-Tokenizer-2K.\\n"},
  {"name":"COREX-18","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/laion/COREX-18","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","description":"\\n  \\n\\n\\nCOREX 18\\n\\n\\nIntroducing COREX-18, a comprehensive dataset derived from the 2018 version of the CORE dataset. Our goal is to contribute to the research community by compiling open-access scientific papers and publishing them in extensive datasets. These datasets will facilitate advanced RAG applications and enhance artificial intelligence research.\\nCOREX was developed as part of our X initiative, which aims to maintain and compile publicly available data into accessible and regularly‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laion/COREX-18."},
  {"name":"mapped_orthologs","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/seq-to-pheno/mapped_orthologs","creator_name":"Seq-to-Pheno","creator_url":"https://huggingface.co/seq-to-pheno","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tZoonomia Orthologs Dataset\\n\\t\\n\\nThis dataset contains mapped orthologs for various species from the Zoonomia Project. It includes information about protein alignments and ortholog mappings between human and other species.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThe dataset consists of a single CSV file with the following columns:\\n\\ntranscript: Human transcript ID\\nprotein: Human protein name\\nmapped_to: Non-human (query) organism transcript ID\\nspecies: Name of the query species‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/seq-to-pheno/mapped_orthologs."},
  {"name":"binding-affinity-PL","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Bindwell/binding-affinity-PL","creator_name":"Bindwell","creator_url":"https://huggingface.co/Bindwell","description":"\\n\\t\\n\\t\\t\\n\\t\\tBinding Affinity Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset is a comprehensive collection of protein-ligand binding affinity data, compiled from multiple sources. The dataset is structured with multiple splits, each corresponding to a specific source:\\n\\ntrain split\\ntest split\\nvalidation split\\ncombined split\\ndavis split\\ndavis filtered split\\nkiba split\\npdbbind 2020 combined split\\npdbbind 2020 refined split\\nbindingdb ic50 split\\nbindingdb kd split\\nbindingdb kd filtered split\\nbindingdb ki‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Bindwell/binding-affinity-PL."},
  {"name":"entrance-exams-qna","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/grafite/entrance-exams-qna","creator_name":"grafite","creator_url":"https://huggingface.co/grafite","description":"\\n\\t\\n\\t\\t\\n\\t\\tTO DO Checklist:\\n\\t\\n\\n\\n Clean Data\\n Remove duplicates\\n Handle missing values\\n Standardize data formats\\n\\n"},
  {"name":"NCERT_Chemistry_11th","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/KadamParth/NCERT_Chemistry_11th","creator_name":"Parth Kadam","creator_url":"https://huggingface.co/KadamParth","description":"KadamParth/NCERT_Chemistry_11th dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"Aurora-Think-1.0","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/naimulislam/Aurora-Think-1.0","creator_name":"Naimul Islam Nahid","creator_url":"https://huggingface.co/naimulislam","description":"naimulislam/Aurora-Think-1.0 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"cot_2250","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Yy245/cot_2250","creator_name":"yangyang","creator_url":"https://huggingface.co/Yy245","description":"Yy245/cot_2250 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"cot_75","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Yy245/cot_75","creator_name":"yangyang","creator_url":"https://huggingface.co/Yy245","description":"Yy245/cot_75 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"test","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/haggs/test","creator_name":"Dan Haggerty","creator_url":"https://huggingface.co/haggs","description":"haggs/test dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"VisOnlyQA_Eval_Synthetic","keyword":"chemistry","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Synthetic."},
  {"name":"VisOnlyQA_Train","keyword":"chemistry","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Train."},
  {"name":"VisOnlyQA_Eval_Real","keyword":"chemistry","license":"GNU General Public License v3.0","language":"en","url":"https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real","creator_name":"Ryo Kamoi","creator_url":"https://huggingface.co/ryokamoi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVisOnlyQA\\n\\t\\n\\nThis repository contains the code and data for the paper \\\"VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information\\\".\\nVisOnlyQA is designed to evaluate the visual perception capability of large vision language models (LVLMs) on geometric information of scientific figures. The evaluation set includes 1,200 mlutiple choice questions in 12 visual perception tasks on 4 categories of scientific figures. We also provide a training‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ryokamoi/VisOnlyQA_Eval_Real."},
  {"name":"Celestia2","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/sequelbox/Celestia2","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","description":"Celestia 2 is a multi-turn agent-instruct dataset containing science data.\\nThis dataset focuses on challenging multi-turn conversations and contains:\\n\\n176k rows of synthetic multi-turn science-instruct data, using Microsoft's AgentInstruct style. All prompts and responses are synthetically generated using Llama 3.1 405b Instruct. Primary subjects are physics, chemistry, biology, and computer science; secondary subjects include Earth science, astronomy, and information theory.\\n\\nThis dataset‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Celestia2."},
  {"name":"Wikipedia-Abstract","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/laion/Wikipedia-Abstract","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","description":"Wikipedia Abstract\\n\\n\\n  \\n\\n\\n\\nIntroducing Wikipedia Abstract, a comprehensive dataset encompassing abstracts, complete articles, and a popularity score index for both widely spoken and lesser-known Wikipedia subsets. Our dedication to Wikipedia-X ensures a centralized Wikipedia dataset that undergoes regular updates and adheres to the highest standards.\\nA central focus of our efforts was to include exotic languages that often lack up-to-date Wikipedia dumps or may not have any dumps at all.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laion/Wikipedia-Abstract."},
  {"name":"SDS-Gloves-Classification","keyword":"chemistry","license":"GNU General Public License v2.0","language":"en","url":"https://huggingface.co/datasets/BASF-AI/SDS-Gloves-Classification","creator_name":"BASF","creator_url":"https://huggingface.co/BASF-AI","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSafety Data Sheets Gloves Classification\\n\\t\\n\\nThis dataset contains Safety Data Sheets (SDS) sourced from Kaggle, consisting of over 200,000 documents. SDS are detailed documents providing essential information on the properties and hazards of chemicals, ensuring user safety and compliance with regulatory standards. A subset of these documents was pre-processed, cleaned, and annotated to classify whether protective gloves are required when handling materials. The labels were‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BASF-AI/SDS-Gloves-Classification."},
  {"name":"Pes2o-Abstract-X","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/laion/Pes2o-Abstract-X","creator_name":"LAION eV","creator_url":"https://huggingface.co/laion","description":"Introducing Pes2o-X, also known as Pes2o-Abstract-X, a derived dataset from the original Pes2o dataset released by Allen AI. The Pes2o dataset aimed to provide a large corpus of open-access research papers, including both abstracts and full text. However, it required pre-processing before the abstracts could be used for training or fine-tuning machine learning models.\\nAt LAION AI, we initiated a project called X, focusing on developing high-quality training corpora from scratch, reorganising‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/laion/Pes2o-Abstract-X."},
  {"name":"MolPuzzle_data","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kguo2/MolPuzzle_data","creator_name":"KehanGuo","creator_url":"https://huggingface.co/kguo2","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMolPuzzle: A Multimodal Benchmark for Molecular Structure Elucidation\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description:\\n\\t\\n\\nThe MolPuzzle dataset is a newly developed resource designed to challenge Large Language Models with multi-modal, multi-step reasoning tasks (molecular structure elucidation). This dataset consists of 217 diverse and intricate structure elucidation challenges that require LLMs to demonstrate advanced reasoning capabilities, integrating multimodal data and deep chemical‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kguo2/MolPuzzle_data."},
  {"name":"hgnc_sl_tsg_genes_202410","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/dwb2023/hgnc_sl_tsg_genes_202410","creator_name":"Don Branson","creator_url":"https://huggingface.co/dwb2023","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Synthetic Lethal Tumor Suppressor Genes (SL TSG)\\n\\t\\n\\nThis dataset was created with the goal of providing cancer researchers and bioinformaticians with a comprehensive resource for studying synthetic lethality in tumor suppressor genes (TSGs). By identifying genetic interactions that lead to cancer cell death when both a tumor suppressor gene and its synthetic lethal partner are mutated or deactivated, this dataset aims to support the development of novel cancer‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/dwb2023/hgnc_sl_tsg_genes_202410."},
  {"name":"liveideabench","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/6cf/liveideabench","creator_name":"Kai Ruan","creator_url":"https://huggingface.co/6cf","description":"\\n\\t\\n\\t\\t\\n\\t\\tüß†‚ú®üéâ News (2025/1/27): Latest Dataset Update on Hugging Face!\\n\\t\\n\\nWe are excited to announce that the latest dataset, including supplementary tests for models like deepseek-R1, deepseek-V3, minimax-01, phi-4, and Opus, has been uploaded to Hugging Face! üöÄ\\nCheck it out here: https://huggingface.co/datasets/6cf/liveideabench-DLC-250127\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tü§ñüí° LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context\\n\\t\\n\\n\\\"It's not like finding a needle in a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/6cf/liveideabench."},
  {"name":"LabSafety_Bench","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/yujunzhou/LabSafety_Bench","creator_name":"Yujun Zhou","creator_url":"https://huggingface.co/yujunzhou","description":"\\n\\t\\n\\t\\t\\n\\t\\tLabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nLabSafety Bench is a comprehensive evaluation framework designed to rigorously assess the trustworthiness of large language models in laboratory settings. The benchmark includes two main evaluation components:\\n\\nMultiple-Choice Questions (MCQs):A set of 765 questions derived from authoritative lab safety protocols, including 632 text-only questions and 133 multimodal questions. These‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yujunzhou/LabSafety_Bench."},
  {"name":"SmallThoughts","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/SmallDoge/SmallThoughts","creator_name":"Doge Face","creator_url":"https://huggingface.co/SmallDoge","description":"\\n\\t\\n\\t\\t\\n\\t\\tSmallThoughts\\n\\t\\n\\n\\n  \\n\\n\\n  \\n    \\n  \\n  \\n    \\n  \\n  \\n    \\nOpen synthetic reasoning dataset, covering math, science, code, and puzzles.\\nTo address the issue of the existing DeepSeek R1 distilled data being too long, this dataset constrains the reasoning trajectory to be more precise and concise while retaining the reflective nature.\\nWe also open-sourced the pipeline code for distilled data here, with just one command you can generate your own dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow to use\\n\\t\\n\\nYou can load‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SmallDoge/SmallThoughts."},
  {"name":"MathVista","keyword":"chemistry","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/AI4Math/MathVista","creator_name":"AI for Math Reasoning","creator_url":"https://huggingface.co/AI4Math","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MathVista\\n\\t\\n\\n\\nDataset Description\\nPaper Information\\nDataset Examples\\nLeaderboard\\nDataset Usage\\nData Downloading\\nData Format\\nData Visualization\\nData Source\\nAutomatic Evaluation\\n\\n\\nLicense\\nCitation\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nMathVista is a consolidated Mathematical reasoning benchmark within Visual contexts. It consists of three newly created datasets, IQTest, FunctionQA, and PaperQA, which address the missing visual domains and are tailored to evaluate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AI4Math/MathVista."},
  {"name":"ScienceQA","keyword":"chemistry","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/derek-thomas/ScienceQA","creator_name":"Derek Thomas","creator_url":"https://huggingface.co/derek-thomas","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card Creation Guide\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nLearn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSupported Tasks and Leaderboards\\n\\t\\n\\nMulti-modal Multiple Choice\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nExplore more samples here.\\n{'image': Image,\\n 'question': 'Which of these states is farthest north?',\\n 'choices': ['West Virginia', 'Louisiana', 'Arizona'‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/derek-thomas/ScienceQA."},
  {"name":"sumstew","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Joemgu/sumstew","creator_name":"Jonas","creator_url":"https://huggingface.co/Joemgu","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"sumstew\\\"\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTL;DR:\\n\\t\\n\\nSumstew is a abstractive, multilingual Dataset, with a balanced number of samples from a diverse set of summarization Datasets. The input sizes range up to 16384 tokens.\\nFiltered using a diverse set of heuristics to encourage high coverage, accuracy and factual consistency. Code to reproduce Dataset available at TODO\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tTask Information\\n\\t\\n\\n\\nTask Categories: The tasks covered by this dataset are primarily summarization‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Joemgu/sumstew."},
  {"name":"atlas-converse","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/AtlasUnified/atlas-converse","creator_name":"Atlas Unified","creator_url":"https://huggingface.co/AtlasUnified","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tATLAS-CONVERSE\\n\\t\\n\\nThis dataset is synthetically generated by GPT-3.5-turbo. It was generated in 1.5 hours and cost me $3.82 USD. This is a conversation dataset based that can work with FastChat, Axolotl, and ShareGPT formatting.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCategories:\\n\\t\\n\\nThe main 41 (See the repo to check the JSONL) categories below were human derived while the subcategories were synthetically generated by GPT-4.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\t1.\\tMathematics\\n\\t\\n\\n1.1.\\tArithmetic\\n1.2.\\tAlgebra\\n1.3.\\tGeometry\\n1.4.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AtlasUnified/atlas-converse."},
  {"name":"molecule_property_instruction","keyword":"chemistry","license":"Academic Free License v3.0","language":"en","url":"https://huggingface.co/datasets/haitengzhao/molecule_property_instruction","creator_name":"haiteng zhao","creator_url":"https://huggingface.co/haitengzhao","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for \\\"molecule_property_instruction\\\"\\n\\t\\n\\nMore Information needed\\n"},
  {"name":"Puffin","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/LDJnr/Puffin","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Puffin dataset. Exactly 3,000 examples with each response created using GPT-4.\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tPLEASE USE THE NEWER VERSION OF PUFFIN CALLED PURE-DOVE, IT IS NO LONGER RECCOMENDED TO USE PUFFIN\\n\\t\\n\\n\\nComprised of over 2,000 multi-turn conversations between GPT-4 and real humans.\\n\\nAverage context length per conversation is over 1,000 tokens. (will measure this more accurately soon)\\n\\nAverage turns per conversation is more than 10. (will measure this more accurately‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Puffin."},
  {"name":"LessWrong-Amplify-Instruct","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official LessWrong-Amplify-Instruct dataset. Over 500 multi-turn examples, and many more coming soon!\\n\\t\\n\\n\\nThis leverages Amplify-Instruct method to extend thousands of scraped Less-Wrong posts into advanced in-depth multi-turn conversations.\\n\\nComprised of over 500 highly filtered multi-turn synthetic conversations.\\n\\nAverage context length per conversation is over 2,000 tokens. (will measure this more accurately soon)\\n\\nSynthetically created using a newly developed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct."},
  {"name":"Pure-Dove","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/LDJnr/Pure-Dove","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Pure-Dove dataset. Over 3K multi-turn examples, and many more coming soon!\\n\\t\\n\\nThis dataset aims to be the largest highest quality cluster of real human back and forth conversations with GPT-4.\\nSteps have even been done to ensure that only the best GPT-4 conversations in comparisons are kept, there are many instances where two GPT-4 responses are rated as equal to eachother or as both bad. We exclude all such responses from Pure Dove and make sure to only‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Pure-Dove."},
  {"name":"SlimOrca-Dedup","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup","creator_name":"OpenOrca","creator_url":"https://huggingface.co/Open-Orca","description":"\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\n\\n\\\"SlimOrca Dedup\\\" is a deduplicated, unfiltered subset of the SlimOrca dataset, excluding RLHF instances, resulting in 363k unique examples.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tKey Features\\n\\t\\n\\n\\nRemoval of RLHF instances.\\nDeduplication using minhash and Jaccard similarity techniques.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDemo Models\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNote: These models were trained on the full SlimOrca dataset, not the deduplicated, unfiltered version.\\n* https://huggingface.co/openaccess-ai-collective/jackalope-7b\\n*‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup."},
  {"name":"materials-project","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/materials-toolkits/materials-project","creator_name":"Materials toolkits","creator_url":"https://huggingface.co/materials-toolkits","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset\\n\\t\\n\\nMaterials project (2019 dump)\\nThis dataset contains 133420 materials with formation energy per atom.\\nProcessed from mp.2019.04.01.json\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload\\n\\t\\n\\nDownload link: materials-project.tar.gz\\nMD5 checksum c132f3781f32cd17f3a92aa6501b9531\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tContent\\n\\t\\n\\nBundled in materials-project.tar.gz.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tIndex (index.json)\\n\\t\\n\\nlist of dict:\\n\\nindex (int) => index of the structure in data file.\\nid (str) => id of Materials Project.\\nformula (str) => formula.\\nnatoms‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/materials-toolkits/materials-project."},
  {"name":"MMMU","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/MMMU/MMMU","creator_name":"MMMU","creator_url":"https://huggingface.co/MMMU","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\\n\\t\\n\\nüåê Homepage | üèÜ Leaderboard | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüõ†Ô∏è[2024-05-30]: Fixed duplicate option issues in Materials dataset items (validation_Materials_25; test_Materials_17, 242) and content error in validation_Materials_25.\\nüõ†Ô∏è[2024-04-30]: Fixed missing \\\"-\\\" or \\\"^\\\" signs in Math dataset items (dev_Math_2, validation_Math_11, 12, 16;‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MMMU/MMMU."},
  {"name":"Capybara","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/LDJnr/Capybara","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\\n\\t\\n\\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\\nThe single-turn seeds used to initiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Capybara."},
  {"name":"jeebench","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/daman1209arora/jeebench","creator_name":"Daman","creator_url":"https://huggingface.co/daman1209arora","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tJEEBench(EMNLP 2023)\\n\\t\\n\\nRepository for the code and dataset for the paper: \\\"Have LLMs Advanced Enough? A Harder Problem Solving Benchmark For Large Language Models\\\" accepted in EMNLP 2023 as a Main conference paper. \\nhttps://aclanthology.org/2023.emnlp-main.468/\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\nIf you use our dataset in your research, please cite it using the following\\n@inproceedings{arora-etal-2023-llms,\\n    title = \\\"Have {LLM}s Advanced Enough? A Challenging Problem Solving Benchmark For‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/daman1209arora/jeebench."},
  {"name":"moses","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/katielink/moses","creator_name":"Katie Link","creator_url":"https://huggingface.co/katielink","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMolecular Sets (MOSES): A benchmarking platform for molecular generation models\\n\\t\\n\\nDeep generative models are rapidly becoming popular for the discovery of new molecules and materials. Such models learn on a large collection of molecular structures and produce novel compounds. In this work, we introduce Molecular Sets (MOSES), a benchmarking platform to support research on machine learning for drug discovery. MOSES implements several popular molecular generation models and provides‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/katielink/moses."},
  {"name":"distilabel-capybara-dpo-7k-binarized","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCapybara-DPO 7K binarized\\n\\t\\n\\n\\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\\n\\n\\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\\n\\n\\n    \\n\\n\\n\\n  \\n    \\n  \\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tWhy?\\n\\t\\n\\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized."},
  {"name":"GRBench","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/PeterJinGo/GRBench","creator_name":"Bowen","creator_url":"https://huggingface.co/PeterJinGo","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tGRBench\\n\\t\\n\\n\\n\\nGRBench is a comprehensive benchmark dataset to support the development of methodology and facilitate the evaluation of the proposed models for Augmenting Large Language Models with External Textual Graphs.\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nGRBench includes 10 real-world graphs that can serve as external knowledge sources for LLMs from five domains including academic, e-commerce, literature, healthcare, and legal domains. Each‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PeterJinGo/GRBench."},
  {"name":"ChemistryQA","keyword":"chemistry","license":"Microsoft Public License","language":"en","url":"https://huggingface.co/datasets/avaliev/ChemistryQA","creator_name":"Airat Valiev","creator_url":"https://huggingface.co/avaliev","description":"ChemistryQA is a complex QA task which cannot be solved by end-to-end neural networks. To answer chemical questions, machines need to understand questions, apply chemistry and math knowledge, and do calculation and reasoning. ChemistryQA contains about 4500 questions covering around 200 chemistry topics, which are collected from https://socratic.org/chemistry.\\nAll credits go to chemistry-qa project by Microsoft (https://github.com/microsoft/chemistry-qa)\\nTrademarks\\nThis project may contain‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/avaliev/ChemistryQA."},
  {"name":"ChemPref-DPO-for-Chemistry-data-en","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/AI4Chem/ChemPref-DPO-for-Chemistry-data-en","creator_name":"AI4Chem","creator_url":"https://huggingface.co/AI4Chem","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation\\n\\t\\n\\n@misc{zhang2024chemllm,\\n      title={ChemLLM: A Chemical Large Language Model}, \\n      author={Di Zhang and Wei Liu and Qian Tan and Jingdan Chen and Hang Yan and Yuliang Yan and Jiatong Li and Weiran Huang and Xiangyu Yue and Dongzhan Zhou and Shufei Zhang and Mao Su and Hansen Zhong and Yuqiang Li and Wanli Ouyang},\\n      year={2024},\\n      eprint={2402.06852},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.AI}\\n}\\n\\n"},
  {"name":"CareQA","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/HPAI-BSC/CareQA","creator_name":"High Performance Artificial Intelligence @ Barcelona Supercomputing Center (HPAI at BSC)","creator_url":"https://huggingface.co/HPAI-BSC","description":"\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for CareQA\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nCareQA is a healthcare QA dataset with two versions:\\n\\nClosed-Ended Version: A multichoice question answering (MCQA) dataset containing 5,621 QA pairs across six categories. Available in English and Spanish.\\nOpen-Ended Version: A free-response dataset derived from the closed version, containing 2,769 QA pairs (English only).\\n\\nThe dataset originates from official sources of the Spanish Specialized Healthcare Training (FSE)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HPAI-BSC/CareQA."},
  {"name":"RCR_CP_10K_SMILES-MMChat","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/OpenMol/RCR_CP_10K_SMILES-MMChat","creator_name":"OpenMol","creator_url":"https://huggingface.co/OpenMol","description":"Reaction Condition Prediction Dataset (Catalyst Prediction)\\n\\nmolecule representation format: 1D SMILES\\nwill further encode into 2D graph features\\n\\n\\n\\nDetail refer to PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes: https://arxiv.org/pdf/2406.13193 \\n"},
  {"name":"uzbek_ner","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/risqaliyevds/uzbek_ner","creator_name":"Riskaliev Muradjon","creator_url":"https://huggingface.co/risqaliyevds","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tUzbek NER Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tAbout the Dataset\\n\\t\\n\\nThis dataset is created for Named Entity Recognition (NER) in Uzbek texts. The dataset includes named entities from various categories such as persons, places, organizations, dates, and more.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Structure\\n\\t\\n\\nThe data is provided in JSON format with the following structure:\\n{\\n    \\\"LOC\\\": [\\\"Location names\\\"],\\n    \\\"ORG\\\": [\\\"Organization names\\\"],\\n    \\\"PERSON\\\": [\\\"Person names\\\"],\\n    \\\"DATE\\\": [\\\"Date expressions\\\"]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/risqaliyevds/uzbek_ner."},
  {"name":"semiconductor_scirepeval_v1","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/sitloboi2012/semiconductor_scirepeval_v1","creator_name":"Huy Vo","creator_url":"https://huggingface.co/sitloboi2012","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSemiconductor Dataset\\n\\t\\n\\n\\n\\nThis dataset is a collection of Scientific Report / Paper related to Semiconductor.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nThis dataset was curated from the SciRepEval dataset and only using any information that related to Semiconductor fields.\\n"},
  {"name":"SciKnowEval","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/hicai-zju/SciKnowEval","creator_name":"AI CrossX  Lab, HIC@Zhejiang University","creator_url":"https://huggingface.co/hicai-zju","description":"\\n\\n   SciKnowEval \\n Evaluating Multi-level Scientific Knowledge of Large Language Models \\n\\n\\n\\nPlease refer to our repository and paper for more details.\\n\\n\\nÂçöÂ≠¶‰πã ÔºåÂÆ°ÈóÆ‰πã ÔºåÊÖéÊÄù‰πã ÔºåÊòéËæ®‰πã ÔºåÁ¨ÉË°å‰πã„ÄÇ\\n‚Äî‚Äî „ÄäÁ§ºËÆ∞ ¬∑ ‰∏≠Â∫∏„Äã Doctrine of the Mean\\n\\n\\n\\n\\n\\n\\nThe Scientific Knowledge Evaluation (SciKnowEval) benchmark for Large Language Models (LLMs) is inspired by the profound principles outlined in the ‚ÄúDoctrine of the Mean‚Äù from ancient Chinese philosophy. This benchmark is designed to assess LLMs based on their proficiency in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hicai-zju/SciKnowEval."},
  {"name":"MassSpecGym","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/roman-bushuiev/MassSpecGym","creator_name":"Roman Bushuiev","creator_url":"https://huggingface.co/roman-bushuiev","description":"\\n  \\n\\n\\nMassSpecGym provides a dataset and benchmark for the discovery and identification of new molecules from MS/MS spectra. The provided challenges abstract the process of scientific discovery of new molecules from biological and environmental samples into well-defined machine learning problems.\\nPlease refer to the MassSpecGym GitHub page and the paper for details.\\n"},
  {"name":"InfoAlign-Data","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/liuganghuggingface/InfoAlign-Data","creator_name":"Gang Liu","creator_url":"https://huggingface.co/liuganghuggingface","description":"The dataset for the paper \\\"Learning Molecular Representation in a Cell\\\". \\nFor pretraining: Place the files from pretrain_raw into raw_data/pretrain/raw.\\n"},
  {"name":"DocGenome","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/U4R/DocGenome","creator_name":"Alpha-Innovator Lab","creator_url":"https://huggingface.co/U4R","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models\\n\\t\\n\\npaper link: DocGenome\\nWe present DocGenome, a structured document dataset constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline DocParser. DocGenome features four characteristics:\\n\\n\\nCompleteness: It is the first dataset to structure data from all modalities including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/U4R/DocGenome."},
  {"name":"MIP","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/RosettaCommons/MIP","creator_name":"Rosetta Commons","creator_url":"https://huggingface.co/RosettaCommons","description":"\\n\\t\\n\\t\\t\\n\\t\\tMicrobiome Immunity Project: Protein Universe\\n\\t\\n\\n~200,000 predicted structures for diverse protein sequences from 1,003\\nrepresentative genomes across the microbial tree of life and annotate\\nthem functionally on a per-residue basis.\\n\\n\\t\\n\\t\\t\\n\\t\\tQuickstart Usage\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tInstall HuggingFace Datasets package\\n\\t\\n\\nEach subset can be loaded into python using the Huggingface datasets library.\\nFirst, from the command line install the datasets library\\n$ pip install datasets\\n\\nOptionally set the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RosettaCommons/MIP."},
  {"name":"sonnet3.5_science_conversations","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/jeffmeloy/sonnet3.5_science_conversations","creator_name":"Jeff Meloy","creator_url":"https://huggingface.co/jeffmeloy","description":"This dataset features sharegpt structured dialogues focused on a variety of advanced scientific topics. The content reflects a high level of scientific expertise, providing in-depth information on complex subjects.\\n"},
  {"name":"MegaScale","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/RosettaCommons/MegaScale","creator_name":"Rosetta Commons","creator_url":"https://huggingface.co/RosettaCommons","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMega-scale experimental analysis of protein folding stability in biology and design\\n\\t\\n\\nThe full MegaScale dataset contains 1,841,285 thermodynamic folding stability measurements\\nusing cDNA display proteolysis of natural and designed proteins. From these 776,298 high-quality folding\\nstabilities (dataset2) cover all single amino acid variants and selected double mutants of 331 natural\\nand 148 de novo designed protein domains 40‚Äì72 amino acids in length. Of these mutations, 607,839‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RosettaCommons/MegaScale."},
  {"name":"ChartX","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/U4R/ChartX","creator_name":"Alpha-Innovator Lab","creator_url":"https://huggingface.co/U4R","description":"\\n\\nChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning\\n\\n[ Related Paper ] [ Website ] [Models ü§ó(Hugging Face)]\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChartX & ChartVLM\\n\\t\\n\\nRecently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/U4R/ChartX."},
  {"name":"serbian-llm-benchmark","keyword":"chemistry","license":"Creative Commons Attribution Share Alike 4.0 International","language":"en","url":"https://huggingface.co/datasets/datatab/serbian-llm-benchmark","creator_name":"Dean","creator_url":"https://huggingface.co/datatab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tSerbian LLM Evaluation Dataset\\n\\t\\n\\nWelcome to the Serbian LLM Evaluation Dataset, your one-stop solution for evaluating Serbian Language Models (LLMs) like never before! This comprehensive toolkit empowers you to measure model performance across diverse domains in Serbian, ensuring your models are smarter, faster, and more intuitive. Whether you're a researcher, developer, or just an enthusiast‚Äîthis dataset is tailor-made to help your LLM thrive.\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîç What's Inside?‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/datatab/serbian-llm-benchmark."},
  {"name":"naijaweb","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/saheedniyi/naijaweb","creator_name":"Saheed Azeez Ayanniyi","creator_url":"https://huggingface.co/saheedniyi","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tNaijaweb Dataset üá≥üá¨\\n\\t\\n\\nNaijaweb is a dataset that contains over 270,000+ documents, totaling approximately 230 million GPT-2 tokens. The data was web scraped from web pages popular among Nigerians, providing a rich resource for modeling Nigerian linguistic and cultural contexts.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nFeatures\\nData Types\\n\\n\\n\\t\\t\\ntext\\nstring\\n\\n\\nlink\\nstring\\n\\n\\ntoken_count\\nint64\\n\\n\\nsection\\nstring\\n\\n\\nint_score\\nint64\\n\\n\\nlanguage\\nstring\\n\\n\\nlanguage_probability\\nfloat64‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/saheedniyi/naijaweb."},
  {"name":"MMMU-Thai","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/iapp/MMMU-Thai","creator_name":"iApp Technology","creator_url":"https://huggingface.co/iapp","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU Thai (MMMU Benchmark Translated to Thai)\\n\\t\\n\\nMMMU Thai is a dataset for evaluating multimodal models on massive multi-discipline tasks requiring college-level knowledge and deliberate reasoning. This dataset is translated from MMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI) into Thai.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\nMMMU Thai consists of 11,500 meticulously collected multimodal questions from college exams, quizzes, and textbooks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/iapp/MMMU-Thai."},
  {"name":"AVEdate","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/zeroMN/AVEdate","creator_name":"zeroTT","creator_url":"https://huggingface.co/zeroMN","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/zeroMN/AVEdate."},
  {"name":"ChemGPT-2.0-Data","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ALmonster/ChemGPT-2.0-Data","creator_name":"The little monster will smile","creator_url":"https://huggingface.co/ALmonster","description":"Our team has open-sourced the training data (ChemGPT-2.0-Data) for ChemGPT 2.0. The data includes the previously open-sourced ChemGPT-from-book dataset, available at https://huggingface.co/datasets/ALmonster/ChemGPT-from-book.\\nChemGPT-2.0-Data is a dataset specifically designed for question-answering in the chemistry domain. It primarily consists of two parts: \\n\\nConversation instructions derived from real chemical books.\\nDerived data from an instruction library (inspired by WizardLM:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ALmonster/ChemGPT-2.0-Data."},
  {"name":"optimal_temperature","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/proteinglm/optimal_temperature","creator_name":"proteinglm","creator_url":"https://huggingface.co/proteinglm","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for Optimal Temperature Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nGrasping the catalytic activity of enzymes is pivotal for industrial enzyme design, particularly in predicting the optimal temperature for a given enzyme‚Äôs catalytic effect. \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData Instances\\n\\t\\n\\nFor each instance, there is a string representing the protein sequence and a float value indicating the optimal temperature for a given enzyme‚Äôs catalytic effect.  See‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proteinglm/optimal_temperature."},
  {"name":"MoleculeQA","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/hcaoaf/MoleculeQA","creator_name":"CAO He","creator_url":"https://huggingface.co/hcaoaf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for MoleculeQA\\n\\t\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\nMoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension (EMNLP 2024)\\n\\nCurated by: IDEA-XL\\nLanguage(s) (NLP): en\\nLicense: mit\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\n\\nRepository: https://github.com/IDEA-XL/MoleculeQA\\nPaper [optional]: https://arxiv.org/abs/2403.08192\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\n\\n\\n- JSON\\n  - All\\n    - train.json # 49,993\\n    - valid.json # 5,795‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hcaoaf/MoleculeQA."},
  {"name":"include-base-44","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/CohereForAI/include-base-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-base (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 22,637 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-base-44."},
  {"name":"include-lite-44","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/CohereForAI/include-lite-44","creator_name":"Cohere For AI","creator_url":"https://huggingface.co/CohereForAI","description":"\\n\\t\\n\\t\\t\\n\\t\\tINCLUDE-lite (44 languages)\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\nPaper: http://arxiv.org/abs/2411.19799\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nINCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. \\nIt contains 11,095 4-option multiple-choice-questions (MCQ) extracted from academic and professional exams, covering 57 topics, including regional‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/CohereForAI/include-lite-44."},
  {"name":"LeMat-Bulk","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/LeMaterial/LeMat-Bulk","creator_name":"LeMaterial","creator_url":"https://huggingface.co/LeMaterial","description":"Motivation: check out the blog post https://huggingface.co/blog/lematerial to hear more about the motivation behind the creation of this dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDownload and use within Python\\n\\t\\n\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset('LeMaterial/LeMat-Bulk', 'compatible_pbe')\\n\\n# convert to Pandas, if you prefer working with this type of object:\\ndf = dataset['train'].to_pandas()\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData fields\\n\\t\\n\\n\\n\\t\\n\\t\\t\\nFeature name\\nData type\\nDescription\\nOptimade required field‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LeMaterial/LeMat-Bulk."},
  {"name":"medical-o1-reasoning-SFT-orpo","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Qurtana/medical-o1-reasoning-SFT-orpo","creator_name":"Tristan Hooper","creator_url":"https://huggingface.co/Qurtana","description":"Qurtana/medical-o1-reasoning-SFT-orpo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"med-qa-orpo-dpo","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/empirischtech/med-qa-orpo-dpo","creator_name":"Empirisch Tech GmbH","creator_url":"https://huggingface.co/empirischtech","description":"\\n\\t\\n\\t\\t\\n\\t\\tMED QA ORPO-DPO Dataset\\n\\t\\n\\nThis dataset is restructured from several existing datasource on medical literature and research, hosted here on hugging face. The dataset is shaped in question, choosen \\nand rejected pairs to match the ORPO-DPO trainset requirements.\\n\\n\\t\\n\\t\\t\\n\\t\\tFeatures\\n\\t\\n\\nThe dataset consists of the following features:\\n\\nquestion: MCQ or yes/no/maybe based questions on medical questions\\ndirect-answer: correct answer to the above question\\nchosen: the correct answer along with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/empirischtech/med-qa-orpo-dpo."},
  {"name":"SciCode","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/SciCode1/SciCode","creator_name":"SciCode","creator_url":"https://huggingface.co/SciCode1","description":"This dataset was presented in SciCode: A Research Coding Benchmark Curated by Scientists.\\n"},
  {"name":"head_qa_v2","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/alesi12/head_qa_v2","creator_name":"Alexis Correa Guillen","creator_url":"https://huggingface.co/alesi12","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Summary\\n\\t\\n\\nHEAD-QA v2 is an updated version of the HEAD-QA dataset, which is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio de Sanidad, Consumo y Bienestar Social, who also provides direct access to the exams of the last 5 years (in Spanish).\\nHEAD-QA V2 expands on the original dataset by including‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alesi12/head_qa_v2."},
  {"name":"cosmetic-ingredients","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/yavuzyilmaz/cosmetic-ingredients","creator_name":"Yavuz Yilmaz","creator_url":"https://huggingface.co/yavuzyilmaz","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Cosmetic Ingredient Explanations Dataset (CIED)\\n\\t\\n\\nThis dataset card provides information about the Cosmetic Ingredient Explanations Dataset (CIED), a collection of English-language descriptions for cosmetic ingredients, designed for use in natural language processing (NLP), machine learning (ML), and cosmetic science research and development.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThe CIED dataset offers detailed explanations of cosmetic ingredients‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yavuzyilmaz/cosmetic-ingredients."},
  {"name":"cooking-knowledge-basics","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/ktiyab/cooking-knowledge-basics","creator_name":"Tiyab K.","creator_url":"https://huggingface.co/ktiyab","description":"\\n\\t\\n\\t\\t\\n\\t\\tComprehensive Cooking Knowledge Q&A Dataset\\n\\t\\n\\nThis dataset (cooking_knowledge.csv) contains a rich collection of synthetically generated Question-Answer (Q&A) pairs covering diverse aspects of cooking knowledge, with particular emphasis on food chemistry, flavor pairing, cooking techniques, dietary accommodations, and culinary traditions. The data was created using a large language model with advanced reasoning capabilities, prompted with various grounded contexts and real-world‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ktiyab/cooking-knowledge-basics."},
  {"name":"related-drugs-network","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Ariel4/related-drugs-network","creator_name":"Ariel Lubonja","creator_url":"https://huggingface.co/Ariel4","description":"Dataset created by crawling the Drugs.com database - please abide by their Terms and Conditions\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHow the Graph was Created\\n\\t\\n\\nMost drugs on Drugs.com have a Related/Similar Drugs page (e.g. here). In my graph, nodes are drugs in the database, and edges are Related/Similar Drugs linked by the drug's description page.\\nNote: Not all drugs in the dataset are part of the graph, as not all drugs have a \\\"Related/Similar Drugs\\\" section\\n"},
  {"name":"oaCamel","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/checkai/oaCamel","creator_name":"Lucas Barker","creator_url":"https://huggingface.co/checkai","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Card for oaCamel\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis is the chemistry, biology, math, and physics datasets created by CAMEL ai. https://huggingface.co/camel-ai\\nThey have been combined and converted to the Open Assistant format.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tLanguages\\n\\t\\n\\nEnglish\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Structure\\n\\t\\n\\nThis dataset follows the OA format, which is:\\n\\nINSTRUCTION (string): Instruction text\\nRESPONSE (string): Expected response to the instruction\\nSOURCE (string): Original data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/checkai/oaCamel."},
  {"name":"cifar10buqi","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/yanchao/cifar10buqi","creator_name":"YANCHAO GONG","creator_url":"https://huggingface.co/yanchao","description":"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images\\nper class. There are 50000 training images and 10000 test images."},
  {"name":"chemical_language_understanding_benchmark","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/bluesky333/chemical_language_understanding_benchmark","creator_name":"Yunsoo Kim","creator_url":"https://huggingface.co/bluesky333","description":"üß™üîã Chemical Language Understanding Benchmark üõ¢Ô∏èüß¥\\n\\n\\nBenchmark Summary\\nChemistry Language Understanding Benchmark is published in ACL2023 industry track to facilitate NLP research in chemical industry ACL2023 Industry Track.\\nFrom our understanding, it is one of the first benchmark datasets with tasks for both patent and literature articles provided by the industrial organization.\\nAll the datasets are annotated by professional chemists.\\n\\nLanguages\\nThe language of this benchmark is English.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bluesky333/chemical_language_understanding_benchmark."},
  {"name":"SaGIS","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/janPiljan/SaGIS","creator_name":"jan Piljan (Brian)","creator_url":"https://huggingface.co/janPiljan","description":"SaGIS: The Scientific and General Information (Data)Set.\\nThe information stored in the dataset is information from OpenAI GPT 3.5-Turbo, Google PaLM, and Anthropic Claude (2). The information may not be entirely factual.\\n"},
  {"name":"goodscents_leffingwell","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/seyonec/goodscents_leffingwell","creator_name":"Seyone Chithrananda","creator_url":"https://huggingface.co/seyonec","description":"seyonec/goodscents_leffingwell dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"CMB","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/FreedomIntelligence/CMB","creator_name":"FreedomAI","creator_url":"https://huggingface.co/FreedomIntelligence","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCMB: A Comprehensive  Medical Benchmark in Chinese\\n\\t\\n\\n\\n\\n   üåê Github ‚Ä¢ üåê Website ‚Ä¢ ü§ó HuggingFace\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüåà Update\\n\\t\\n\\n\\n[2024.02.21] The answers to the CMB-Exam test has been updated and some errors caused by omissions in version management have been fixed.\\n[2024.01.08] In order to facilitate testing, we disclose the answers to the CMB-Exam test\\n[2023.09.22] CMB is included in OpenCompass.\\n[2023.08.21] Paper released.\\n[2023.08.01] üéâüéâüéâ CMB is publishedÔºÅüéâüéâüéâ‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/CMB."},
  {"name":"PDB","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/PDB","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 355 datapoints\\nstructure: 355 datapoints\\n\\n"},
  {"name":"Metadata_view_test","keyword":"chemistry","license":"Creative Commons Zero v1.0 Universal","language":"en","url":"https://huggingface.co/datasets/Astr17/Metadata_view_test","creator_name":"EffyHHH","creator_url":"https://huggingface.co/Astr17","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tinstitution:\\n\\t\\n\\n\\nUniversity of Science and Technology of China\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tCitation Information:\\n\\t\\n\\n"},
  {"name":"moleculenet-benchmark","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/katielink/moleculenet-benchmark","creator_name":"Katie Link","creator_url":"https://huggingface.co/katielink","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMoleculeNet Benchmark (website)\\n\\t\\n\\nMoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license).‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/katielink/moleculenet-benchmark."},
  {"name":"testpatent","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Elliot4AI/testpatent","creator_name":"Jiang Elliot","creator_url":"https://huggingface.co/Elliot4AI","description":"test\\n"},
  {"name":"Healix-Shot","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/health360/Healix-Shot","creator_name":"Health 360","creator_url":"https://huggingface.co/health360","description":"README\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tHealix-Shot: Largest Medical Corpora by Health 360\\n\\t\\n\\nHealix-Shot, proudly presented by Health 360, stands as an emblematic milestone in the realm of medical datasets. Hosted on the HuggingFace repository, it heralds the infusion of cutting-edge AI in the healthcare domain. With an astounding 22 billion tokens, Healix-Shot provides a comprehensive, high-quality corpus of medical text, laying the foundation for unparalleled medical NLP applications.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tImportance:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/health360/Healix-Shot."},
  {"name":"Verified-Camel","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/LDJnr/Verified-Camel","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Verified Camel dataset. Just over 100 verified examples, and many more coming soon!\\n\\t\\n\\n\\nComprised of over 100 highly filtered and curated examples from specific portions of CamelAI stem datasets. \\n\\nThese examples are verified to be true by experts in the specific related field, with atleast a bachelors degree in the subject.\\n\\nRoughly 30-40% of the originally curated data from CamelAI was found to have atleast minor errors and/or incoherent questions(as‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Verified-Camel."},
  {"name":"wikipedia_20231001","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/flyingfishinwater/wikipedia_20231001","creator_name":"Flying Fish","creator_url":"https://huggingface.co/flyingfishinwater","description":"It's the English content dumped from 2023-10-01 version of Wikipedia dump site.\\nThe format is similar with \\\"datasets/wikipedia\\\". It has use same method to clean the text.\\nHowever, I ommitted the 'url' field because it follows the same format: \\\"https://en.wikipedia.org/wiki/[title]\\\". \\nAnother change is the title. I merged the \\\"REDIRECTED\\\" title with its original and use comma as seperator. \\nFor example, the title \\\"An American in Paris, AnAmericanInParis\\\" means \\\"An American in Paris\\\" and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/flyingfishinwater/wikipedia_20231001."},
  {"name":"EasyReddit","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Tonic/EasyReddit","creator_name":"Joseph [open/acc] Pollack","creator_url":"https://huggingface.co/Tonic","description":"\\n\\t\\n\\t\\t\\n\\t\\tüôãüèª‚Äç‚ôÇÔ∏èWelcome to üßëüèª‚ÄçüöÄTonic'süöÄüö∞Easyüî¥Redditüî•!\\n\\t\\n\\n\\nThis is every \\\"best reddit_question_best_answers\\\" appended and produced according to the following template :\\n{\\\"prompt\\\": \\\"This is the first prompt\\\", \\\"completion\\\": \\\"This is the first completion\\\"}\\n{\\\"prompt\\\": \\\"This is the second prompt\\\", \\\"completion\\\": \\\"This is the second completion\\\"}\\n\\n\\n\\nüåü You can use it in shards or all together !\\n\\nüåü This dataset is internally consistent !\\n\\n\\nü§îThe point is to make it easy to train models with a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Tonic/EasyReddit."},
  {"name":"gd","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kkk0001/gd","creator_name":"kkk","creator_url":"https://huggingface.co/kkk0001","description":"good\\n"},
  {"name":"Verified-Camel-KO","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/kuotient/Verified-Camel-KO","creator_name":"Jisoo Kim","creator_url":"https://huggingface.co/kuotient","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tVerified-Camel-KO\\n\\t\\n\\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ https://huggingface.co/datasets/LDJnr/Verified-Camel Ïùò ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÏûÖÎãàÎã§.\\nGPT4 TurboÎ°ú Î≤àÏó≠Ìïú Îí§, ÏïΩÍ∞ÑÏùò ÏàòÏ†ïÏùÑ Í±∞Ï≥§ÏäµÎãàÎã§.\\nÏù¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Î∞©Ïπ®ÏùÄ Ï†ÑÎ∂Ä Ïõê Ï†ÄÏûêÏùò Î∞©Ïπ®ÏùÑ Îî∞Î¶ÖÎãàÎã§.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Verified Camel dataset. Just over 100 verified examples, and many more coming soon!\\n\\t\\n\\n\\nComprised of over 100 highly filtered and curated examples from specific portions of CamelAI stem datasets. \\n\\nThese examples are verified to be true by experts in the specific related field, with atleast‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kuotient/Verified-Camel-KO."},
  {"name":"Verified-Camel-zh","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/noobmaster29/Verified-Camel-zh","creator_name":"Victor Sung","creator_url":"https://huggingface.co/noobmaster29","description":"This is a direct Chinese translation using GPT4 of the Verified-Camel dataset. I hope you find it useful. \\nhttps://huggingface.co/datasets/LDJnr/Verified-Camel\\nCitation:\\n@article{daniele2023amplify-instruct,\\n  title={Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for Effecient LLM Training.},\\n  author={Daniele, Luigi and Suphavadeeprasit},\\n  journal={arXiv preprint arXiv:(comming soon)},\\n  year={2023}\\n}\\n\\n"},
  {"name":"viral_fragments","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/viral_fragments","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 40 datapoints\\nstructure: 40 datapoints\\ndms: 25 datapoints\\nshape: 15 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 43 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 40 valid datapoints\\nINCLUDED: 0 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n0 invalid datapoints (ex: sequence with non-regular characters)\\n0‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/viral_fragments."},
  {"name":"archiveII","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/archiveII","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 3370 datapoints\\nstructure: 3370 datapoints\\n\\n"},
  {"name":"RNAstralign","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/RNAstralign","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 27125 datapoints\\nstructure: 27125 datapoints\\nfamily: 27125 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 37149 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 27125 valid datapoints\\nINCLUDED: 104 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n3949 invalid datapoints (ex: sequence with non-regular characters)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/RNAstralign."},
  {"name":"PseudoMD-1M","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/SCIR-HI/PseudoMD-1M","creator_name":"SCIR-HI","creator_url":"https://huggingface.co/SCIR-HI","description":"Pre-training dataset used in paper \\\"From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery\\\" (AAAI 2024)\\nPseudoMD-1M dataset is the first artificially-real dataset for cross-modal molecule discovery, which consists of 1,020,139 pseudo molecule-description pairs. Every molecule is represented using its Canonical SMILES notation, sourced from PubChem via the PUG View API. On average, each description within PseudoMD-1M contains 5.11‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/SCIR-HI/PseudoMD-1M."},
  {"name":"ribo500-blast","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/ribo500-blast","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 46060 datapoints\\nstructure: 46060 datapoints\\ndms: 45247 datapoints\\nshape: 38235 datapoints\\n\\n"},
  {"name":"pri_miRNA","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/pri_miRNA","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 1098 datapoints\\nstructure: 1098 datapoints\\ndms: 1098 datapoints\\n\\n"},
  {"name":"ZINC20","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/haydn-jones/ZINC20","creator_name":"Haydn Jones","creator_url":"https://huggingface.co/haydn-jones","description":"ZINC20 Dataset with SELFIES added. Any smile that could not be successfully converted was dropped from the dataset.\\nEvery tranch was downloaded, this is not the ~1B example ML subset from https://files.docking.org/zinc20-ML/.\\nThe dataset was entirely shuffled then split into 80%/10%/10% splits for train/val/test.\\nA file vocab.csv is in the root of the reposity that contains all of the SELFIES tokens found in the data, with [START], [STOP], and [PAD] added.\\n"},
  {"name":"CodegebraGPT_data","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/sr5434/CodegebraGPT_data","creator_name":"Samir R.","creator_url":"https://huggingface.co/sr5434","description":"A collection of datasets for finetuning LLMs on STEM related tasks. The dataset is formatted in the LLaVA finetuning format.\\n"},
  {"name":"tyk2_fep","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/pvrancx/tyk2_fep","creator_name":"Peter Vrancx","creator_url":"https://huggingface.co/pvrancx","description":" Molecular dataset: 10,000 TYK2 inhibitors (SMILES strings) with Docking scores and Relative Binding Free Energy (dG) \\nDataset from paper:\\nJames Thompson, W Patrick Walters, Jianwen A Feng, Nicolas A Pabon, Hongcheng Xu, Michael Maser, Brian B Goldman, Demetri Moustakas, Molly Schmidt, Forrest York,\\nOptimizing active learning for free energy calculations, Artificial Intelligence in the Life Sciences, Volume 2, 2022, 100050, ISSN 2667-3185,\\nhttps://doi.org/10.1016/j.ailsci.2022.100050.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pvrancx/tyk2_fep."},
  {"name":"Capybara-Converted","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/cfahlgren1/Capybara-Converted","creator_name":"Caleb Fahlgren","creator_url":"https://huggingface.co/cfahlgren1","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\\n\\t\\n\\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\\nThe single-turn seeds used to intiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cfahlgren1/Capybara-Converted."},
  {"name":"capybara-sharegpt","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Doctor-Shotgun/capybara-sharegpt","creator_name":"Doctor Shotgun","creator_url":"https://huggingface.co/Doctor-Shotgun","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tcapybara-sharegpt\\n\\t\\n\\nLDJnr/Capybara converted to ShareGPT format for use in common training repositories.\\nPlease refer to the original repository's dataset card for more information. All credit goes to the original creator.\\n"},
  {"name":"MMMU","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/aslessor/MMMU","creator_name":"Alex","creator_url":"https://huggingface.co/aslessor","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMMMU (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)\\n\\t\\n\\nüåê Homepage | ü§ó Dataset | ü§ó Paper | üìñ arXiv | GitHub\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tüîîNews\\n\\t\\n\\n\\nüî•[2023-12-04]: Our evaluation server for test set is now availble on EvalAI. We welcome all submissions and look forward to your participation! üòÜ\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nWe introduce MMMU: a new benchmark designed to evaluate multimodal models on massive‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/aslessor/MMMU."},
  {"name":"ChEBI-20-MM","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/liupf/ChEBI-20-MM","creator_name":"Pengfei Liu","creator_url":"https://huggingface.co/liupf","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tChEBI-20-MM Dataset\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOverview\\n\\t\\n\\nThe ChEBI-20-MM is an extensive and multi-modal benchmark developed from the ChEBI-20 dataset. It is designed to provide a comprehensive benchmark for evaluating various models' capabilities in the field of molecular science. This benchmark integrates multi-modal data, including InChI, IUPAC, SELFIES, and images, making it a versatile tool for a wide range of molecular tasks.\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Description\\n\\t\\n\\nChEBI-20-MM is an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/liupf/ChEBI-20-MM."},
  {"name":"human_mRNA","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/rouskinlab/human_mRNA","creator_name":"Rouskin Lab Harvard Medical School","creator_url":"https://huggingface.co/rouskinlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tData types\\n\\t\\n\\n\\nsequence: 1456 datapoints\\nstructure: 1456 datapoints\\ndms: 1456 datapoints\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tConversion report\\n\\t\\n\\nOver a total of 1503 datapoints, there are:\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tOUTPUT\\n\\t\\n\\n\\nALL: 1456 valid datapoints\\nINCLUDED: 0 duplicate sequences with different structure / dms / shape\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tMODIFIED\\n\\t\\n\\n\\n0 multiple sequences with the same reference (renamed reference)\\n\\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tFILTERED OUT\\n\\t\\n\\n\\n0 invalid datapoints (ex: sequence with non-regular characters)\\n0 datapoints‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/rouskinlab/human_mRNA."},
  {"name":"dawiki_categories","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/kardosdrur/dawiki_categories","creator_name":"M√°rton Kardos","creator_url":"https://huggingface.co/kardosdrur","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDanish Wikipedia Categories\\n\\t\\n\\nThe dataset was created entirely from the last Danish Wikipedia dump\\nby traversing the category hierarchy in the categorylinks table.\\nAll categories that were one level bellow the topcategories, and which had more than 30 articles assigned to them were selected.\\nIn order to see whether an article belongs to a certain category I checked, whether the article was connected to the category in the directed graph of the category hierarchy.\\nIf the length of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kardosdrur/dawiki_categories."},
  {"name":"recipe-gantt","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/pocasrocas/recipe-gantt","creator_name":"Jim Bremner","creator_url":"https://huggingface.co/pocasrocas","description":"\\n\\t\\n\\t\\t\\n\\t\\tSummary\\n\\t\\n\\nA very small dataset of input recipes and output recipe gantt charts in TSV format where each column represents a method step and each row represents a single ingredient. Cells of the output TSV are populated with X if that ingredient is used in that step.\\nIt was used to fine-tune pocasrocas/recipe-gantt-v0.1.\\n\\n\\t\\n\\t\\t\\n\\t\\tFormat\\n\\t\\n\\nIt follows the alpaca instruction/input/response format, shared here in .jsonl format for easy use with libraries such as axolotl.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pocasrocas/recipe-gantt."},
  {"name":"RSL_Maran","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/ae5115242430e13/RSL_Maran","creator_name":"R.s.L Maran","creator_url":"https://huggingface.co/ae5115242430e13","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ae5115242430e13/RSL_Maran."},
  {"name":"MX-CHAT","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/berwart/MX-CHAT","creator_name":"Berwart","creator_url":"https://huggingface.co/berwart","description":"MX-CHAT 01\\n"},
  {"name":"testing-upload","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/ausanoff/testing-upload","creator_name":"Anton","creator_url":"https://huggingface.co/ausanoff","description":"Hello, world!\\n"},
  {"name":"resmo","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/aibrahiam/resmo","creator_name":"Ahmed Ibrahim","creator_url":"https://huggingface.co/aibrahiam","description":"aibrahiam/resmo dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"NCERT_Chemistry_12th","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/KadamParth/NCERT_Chemistry_12th","creator_name":"Parth Kadam","creator_url":"https://huggingface.co/KadamParth","description":"KadamParth/NCERT_Chemistry_12th dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"b595_Simeonov2008","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/vmsavla/b595_Simeonov2008","creator_name":"Varunika","creator_url":"https://huggingface.co/vmsavla","description":"This dataset contains fluorescence profiling data for 7,152 active compounds filtered from the PubChem BioAssay database (AIDs: 587-594). The data includes CIDs, SMILES strings, and fluorescence activity outcomes, enabling researchers to explore fluorescence properties of compounds in high-throughput screening (HTS) assays.\\n"},
  {"name":"USPTO_Condition","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/tjuDavidWang/USPTO_Condition","creator_name":"David Wang","creator_url":"https://huggingface.co/tjuDavidWang","description":"\\n\\t\\n\\t\\t\\n\\t\\tUSPTO_Condition\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tOverview\\n\\t\\n\\nThis dataset contains reaction data in tabular format, with the following fields:\\n\\nsource: Source or reference ID of the reaction.\\ncanonical_rxn: The canonical SMILES representation of the reaction (reactants >> products).\\ncatalyst1: The catalyst used in the reaction (if available).\\nsolvent1, solvent2: Solvents used in the reaction (if available).\\nreagent1, reagent2: Reagents involved in the reaction (if available).\\ndataset: Dataset category or‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tjuDavidWang/USPTO_Condition."},
  {"name":"bioinf595-L04","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/marissadolorfino/bioinf595-L04","creator_name":"Marissa Dolorfino","creator_url":"https://huggingface.co/marissadolorfino","description":"marissadolorfino/bioinf595-L04 dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"tmc_fewshot_initalSample","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/PeterOldriver/tmc_fewshot_initalSample","creator_name":"Zhangde Song","creator_url":"https://huggingface.co/PeterOldriver","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PeterOldriver/tmc_fewshot_initalSample."},
  {"name":"hudson-2023-dosedo","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/scbirlab/hudson-2023-dosedo","creator_name":"SCBIR Lab","creator_url":"https://huggingface.co/scbirlab","description":"\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\thudson-2023-dosedo\\n\\t\\n\\nSMILES of ~3.7 million diversity-oriented synthesis (DOS) compounds from a reported DNA-encoded library, in:\\n\\nHudson, L., Mason, J.W., Westphal, M.V. et al. Diversity-oriented synthesis encoded by deoxyoligonucleotides. \\nNat Commun 14, 4930 (2023). https://doi.org/10.1038/s41467-023-40575-5\\n\\nThe SMILES strings have been canonicalized, and split into training (70%), validation (15%), and test (15%) sets by Murcko scaffold. Additional features like molecular‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/scbirlab/hudson-2023-dosedo."},
  {"name":"aurora_programmer_data","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/naimulislam/aurora_programmer_data","creator_name":"Naimul Islam Nahid","creator_url":"https://huggingface.co/naimulislam","description":"\\n\\t\\n\\t\\t\\n\\t\\tMy Awesome Dataset\\n\\t\\n\\nA comprehensive description of my awesome dataset.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\nThis dataset contains images of cats and dogs.  The images were collected from [mention data source(s), e.g., a specific website, scraped from the internet]. It is intended for use in image classification tasks.  The dataset consists of [number] images, with approximately [percentage]% allocated to the training set and [percentage]% to the test set. [Add more details about the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/naimulislam/aurora_programmer_data."},
  {"name":"scicode","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/Zilinghan/scicode","creator_name":"Zilinghan Li","creator_url":"https://huggingface.co/Zilinghan","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for Dataset Name\\n\\t\\n\\n\\n\\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Details\\n\\t\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Description\\n\\t\\n\\n\\n\\n\\n\\n\\nCurated by: [More Information Needed]\\nFunded by [optional]: [More Information Needed]\\nShared by [optional]: [More Information Needed]\\nLanguage(s) (NLP): [More Information Needed]\\nLicense: [More Information Needed]\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources [optional]\\n\\t\\n\\n\\n\\n\\nRepository: [More‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Zilinghan/scicode."},
  {"name":"luna-4B","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/katsukiai/luna-4B","creator_name":"KatsukiAI","creator_url":"https://huggingface.co/katsukiai","description":"\\n\\t\\n\\t\\t\\n\\t\\tlunaXL\\n\\t\\n\\nPlease see discussion (including announcement)\\n"},
  {"name":"MuMOInstruct","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/NingLab/MuMOInstruct","creator_name":"Ning Lab","creator_url":"https://huggingface.co/NingLab","description":"\\n\\t\\n\\t\\t\\n\\t\\tDataset Card for ECInstruct\\n\\t\\n\\nMuMOInstruct comprises 63 tasks, with 42 tasks aiming to improve at least 3 molecular properties simultaneously, \\nout of which 10 tasks are divided into IND and OOD tasks for evaluation in in- and out-of-domain settings.\\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Sources\\n\\t\\n\\n\\n\\n\\nRepository: GitHub\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tUsage\\n\\t\\n\\nAs detailed in the paper, \\nfor each IND task, we conduct evaluation under 2 settings: with seen and unseen instruction.\\nFor example, instr_setting = seen, task =‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/NingLab/MuMOInstruct."},
  {"name":"myrunning","keyword":"chemistry","license":"MIT License","language":"en","url":"https://huggingface.co/datasets/aide-julius/myrunning","creator_name":"Aiden","creator_url":"https://huggingface.co/aide-julius","description":"\\n\\t\\n\\t\\t\\n\\t\\tView it on Data Table\\n\\t\\n\\n"},
  {"name":"augmented_canonical_druglike_QED_43M","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Derify/augmented_canonical_druglike_QED_43M","creator_name":"Derify","creator_url":"https://huggingface.co/Derify","description":"\\n\\t\\n\\t\\t\\n\\t\\tDruglike QED 36M - Augmented SMILES Dataset\\n\\t\\n\\nThis dataset is derived from Druglike molecule datasets for drug discovery and has been canonicalized using RDKit (2024.9.4) to ensure structural consistency.  \\nTo enhance molecular diversity, 33% of the dataset was randomly sampled and augmented using RDKit‚Äôs Chem.MolToRandomSmilesVect function, following an approach similar to NVIDIA‚Äôs molmim method for SMILES augmentation.  \\n\\n\\t\\n\\t\\t\\n\\t\\n\\t\\n\\t\\tDataset Overview:\\n\\t\\n\\n\\nSource: \\nCanonicalization:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Derify/augmented_canonical_druglike_QED_43M."},
  {"name":"V1Q","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/YuRiVeRTi/V1Q","creator_name":"YuRiVeRTical","creator_url":"https://huggingface.co/YuRiVeRTi","description":"from datasets import load_dataset\\nds = load_dataset(\\\"b3x0m/Chinese-H-Novels\\\")\\nimport sagemaker\\nimport boto3\\nfrom sagemaker.huggingface import HuggingFaceModel\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client('iam')\\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\\n\\n\\t\\n\\t\\t\\n\\t\\tHub Model configuration. https://huggingface.co/models\\n\\t\\n\\nhub = {\\n    'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',\\n    'HF_TASK':'any-to-any'\\n}\\n\\n\\t\\n\\t\\t\\n\\t\\tcreate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/YuRiVeRTi/V1Q."},
  {"name":"lunaxl2","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/katsukiai/lunaxl2","creator_name":"KatsukiAI","creator_url":"https://huggingface.co/katsukiai","description":"\\nLunaXL 2 - Friends from Hugging Face\\nThis is the sister of the LunaXL Dataset, Luna thanks you for downloading KatsukiAI's LunaXL with nearly 4 billion parameters! ü´∞üèª "},
  {"name":"safe_druglike_QED_33M","keyword":"chemistry","license":"Creative Commons Attribution 4.0 International","language":"en","url":"https://huggingface.co/datasets/Derify/safe_druglike_QED_33M","creator_name":"Derify","creator_url":"https://huggingface.co/Derify","description":"\\n\\t\\n\\t\\t\\n\\t\\tDruglike QED 36M - SAFE Dataset\\n\\t\\n\\nThis dataset is derived from Druglike molecule datasets for drug discovery. SAFE (sequential attachment-based fragment embedding) representations were generated using safe-mol (0.1.13).  \\n\\n\\t\\n\\t\\t\\n\\t\\tDataset Overview:\\n\\t\\n\\n\\nSource: \\nSAFE representation: safe-mol (0.1.13) \\nTotal Entries: 33M SMILES\\n\\n"},
  {"name":"einstein_answers","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/aliMohammad16/einstein_answers","creator_name":"Mohammad Ali","creator_url":"https://huggingface.co/aliMohammad16","description":"\\n\\t\\n\\t\\t\\n\\t\\tWhat would Einstein Say?\\n\\t\\n\\nThis dataset contains a set of questions and answers, mimicking Einstein's approach to answer general scientific and philosophical queries. \\nThe data points have been generated synthetically, however the factual correctness of the data is ensured, not guaranteed whatsoever.\\n"},
  {"name":"cot_2000_new","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/Yy245/cot_2000_new","creator_name":"yangyang","creator_url":"https://huggingface.co/Yy245","description":"Yy245/cot_2000_new dataset hosted on Hugging Face and contributed by the HF Datasets community"},
  {"name":"aurora-think-1.5","keyword":"chemistry","license":"Apache License 2.0","language":"en","url":"https://huggingface.co/datasets/naimulislam/aurora-think-1.5","creator_name":"Naimul Islam Nahid","creator_url":"https://huggingface.co/naimulislam","description":"dataset_description: \\n\\\"Aurora Think 1.5 is a meticulously crafted dataset containing a vast collection of questions and answers spanning a wide range of domains, including world knowledge, history, science, technology, philosophy, and more.  It is specifically designed to be used for fine-tuning large language models (LLMs) to enhance their ability to understand and respond to complex, knowledge-intensive queries.\\nKey Features:\\n\\nExtensive Coverage:  The dataset encompasses a broad spectrum of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/naimulislam/aurora-think-1.5."}
];
