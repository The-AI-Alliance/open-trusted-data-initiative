const data_for_domain_logic = 
[
	{"name":"probability_words_nli","keyword":"logic","description":"Probing neural language models for understanding of words of estimative probability","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"probability_words_nli","keyword":"logical reasoning","description":"Probing neural language models for understanding of words of estimative probability","url":"https://huggingface.co/datasets/sileod/probability_words_nli","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","multiple-choice","question-answering","open-domain-qa","multiple-choice-qa"],"keywords_longer_than_N":true},
	{"name":"Puffin","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official Puffin dataset. Exactly 3,000 examples with each response created using GPT-4.\n\t\n\n\n\t\n\t\t\n\t\tPLEASE USE THE NEWER VERSION OF PUFFIN CALLED PURE-DOVE, IT IS NO LONGER RECCOMENDED TO USE PUFFIN\n\t\n\n\nComprised of over 2,000 multi-turn conversations between GPT-4 and real humans.\n\nAverage context length per conversation is over 1,000 tokens. (will measure this more accurately soon)\n\nAverage turns per conversation is more than 10. (will measure this more accurately soon)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Puffin.","url":"https://huggingface.co/datasets/LDJnr/Puffin","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"autotrain-data-lojban-translation","keyword":"lojban","description":"This is a very early experiment of Lojban machine translation. For a larger dataset, see https://huggingface.co/datasets/smuske/Korpora\n","url":"https://huggingface.co/datasets/woctordho/autotrain-data-lojban-translation","creator_name":"woctordho","creator_url":"https://huggingface.co/woctordho","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["translation","English","Lojban","mit","üá∫üá∏ Region: US"],"keywords_longer_than_N":false},
	{"name":"distilabel-capybara-kto-15k-binarized","keyword":"logic","description":"\n\t\n\t\t\n\t\tCapybara-KTO 15K binarized\n\t\n\n\nA KTO signal transformed version of the highly loved Capybara-DPO 7K binarized, A DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n\t\n\t\t\n\t\tWhy KTO?\n\t\n\nThe KTO paper states:\n\nKTO matches or exceeds DPO performance at scales from 1B to 30B parameters.1 That is, taking a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-kto-15k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench-French","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tüß† SLR-Bench-French: Scalable Logical Reasoning Benchmark (French Edition)\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSLR-Bench Versions:\n\t\n\n\n\n\n\nSLR-Bench-French is the French-language pendant of the original SLR-Bench dataset.\nIt follows the same symbolic structure, evaluation framework, and curriculum as the English version but provides all natural-language task prompts translated into French.\nThis enables systematic evaluation and training of Large Language Models (LLMs) in logical reasoning in French‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-French.","url":"https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-French","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"Capybara","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\n\t\n\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\nThe single-turn seeds used to initiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Capybara.","url":"https://huggingface.co/datasets/LDJnr/Capybara","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tapaco","keyword":"lojban","description":"\n\t\n\t\t\n\t\tDataset Card for TaPaCo Corpus\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nA freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. \nTatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences \nand translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a \ngraph with Tatoeba sentences and equivalence links between sentences ‚Äúmeaning the same thing‚Äù. This‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/community-datasets/tapaco.","url":"https://huggingface.co/datasets/community-datasets/tapaco","creator_name":"Community Datasets","creator_url":"https://huggingface.co/community-datasets","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":"en","first_N":5,"first_N_keywords":["translation","text-classification","semantic-similarity-classification","machine-generated","crowdsourced"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-2","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-2\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2.","url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-2","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-4","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-4\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=4. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4.","url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-4","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"arabic-reasoning-dataset-logic","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tArabic Logical Reasoning Tasks Dataset (Maximum 1000 Tasks)\n\t\n\n\n\t\n\t\t\n\t\tOverview\n\t\n\nThis dataset comprises a series of logical reasoning tasks designed to evaluate and train artificial intelligence models on understanding and generating logical inferences in the Arabic language. Each task includes a unique identifier, the task type, the task text (a question and a proposed answer), and a detailed solution that outlines the thinking steps and the final answer.\n\n\t\n\t\t\n\t\tData Format\n\t\n\nThe‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/beetleware/arabic-reasoning-dataset-logic.","url":"https://huggingface.co/datasets/beetleware/arabic-reasoning-dataset-logic","creator_name":"beetleware","creator_url":"https://huggingface.co/beetleware","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Arabic","mit","1K - 10K","json","Text"],"keywords_longer_than_N":true},
	{"name":"finepdfs","keyword":"lojban","description":"\n\nLiberating 3T of the finest tokens from PDFs\n\n\n\t\n\t\t\n\t\tWhat is this?\n\t\n\nAs we run out of web pages to process, the natural question has always been: what to do next? Only a few knew about a data source that everyone avoided for ages, due to its incredible extraction cost and complexity: PDFs.\nüìÑ FinePDFs is exactly that. It is the largest publicly available corpus sourced exclusively from PDFs, containing about 3 trillion tokens across 475 million documents in 1733 languages.\nCompared to HTML‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/finepdfs.","url":"https://huggingface.co/datasets/HuggingFaceFW/finepdfs","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"LogicHaystacks","keyword":"logic","description":"\n\t\n\t\t\n\t\tEvaluation code:\n\t\n\n\ndef parse(x):\n    if '<answer>' in x and '</answer>' in x:\n        start = x.find('<answer>') + len('<answer>')\n        end = x.find('</answer>')\n        x = x[start:end]\n    \n    lines = [i.lstrip('L').strip() for i in x.strip().strip('.').split(',')]\n    return [int(i) for i in lines if i.isnumeric()]\n\ndef jaccard(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(set(list1)) + len(set(list2))) - intersection\n    return‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sileod/LogicHaystacks.","url":"https://huggingface.co/datasets/sileod/LogicHaystacks","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Spurline","keyword":"logic","description":"Spurline is a dataset containing complex responses, emphasizing logical reasoning and using Shining Valiant's friendly, magical personality style.\nThe 2024-10-30 version contains:\n\n10.7k rows of synthetic queries, using randomly selected prompts from migtissera/Synthia-v1.5-I and responses generated using Llama 3.1 405b Instruct.\n\nThis dataset contains synthetically generated data and has not been subject to manual review.\n","url":"https://huggingface.co/datasets/sequelbox/Spurline","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","csv","Text"],"keywords_longer_than_N":true},
	{"name":"Geoperception","keyword":"logical-reasoning","description":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions\n\n\t\n\t\t\n\t\tDataset Card for Geoperception\n\t\n\nA Benchmark for Low-level Geometric Perception\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nGeoperception is a benchmark focused specifically on accessing model's low-level visual perception ability in 2D geometry.\nIt is sourced from the Geometry-3K corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/euclid-multimodal/Geoperception.","url":"https://huggingface.co/datasets/euclid-multimodal/Geoperception","creator_name":"Euclid Multimodal LLM","creator_url":"https://huggingface.co/euclid-multimodal","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","visual-question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"rc1","keyword":"logic","description":"\n\t\n\t\t\n\t\tReasoning Core ‚óâ\n\t\n\nPaper: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning\nCode: GitHub Repository\nreasoning-core is a text-based RLVR for LLM reasoning training.\nIt is centered on expressive symbolic tasks, including full fledged FOL, formal mathematics with TPTP, formal planning with novel domains, and syntax tasks.\n\n\t\n\t\t\n\t\n\t\n\t\tAbstract\n\t\n\nWe introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/reasoning-core/rc1.","url":"https://huggingface.co/datasets/reasoning-core/rc1","creator_name":"Reasoning Core","creator_url":"https://huggingface.co/reasoning-core","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"LoGiPT-data","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tDataset Details\n\t\n\nThese are the training data for LoGiPT from NAACL'24 paper: \"Language Models can be Deductive Solvers\".\n\nLoGiPT-data-ProofWriter.json: Instruction-tuning data for LoGiPT constructed from ProofWriter.\nLoGiPT-data-PrOntoQA.json: Instruction-tuning data for LoGiPT constructed from PrOntoQA.\n\nAll training examples are organised in Json-format and Vicuna-style.\n\n\t\n\t\t\n\t\n\t\n\t\tIf you find this data helpful, please cite our NAACL'24 paper: (or Arxiv version:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jzfeng/LoGiPT-data.","url":"https://huggingface.co/datasets/jzfeng/LoGiPT-data","creator_name":"Jamie Jiazhan Feng","creator_url":"https://huggingface.co/jzfeng","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","10K - 100K","json"],"keywords_longer_than_N":true},
	{"name":"PuzzleClone","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tPuzzleClone Dataset\n\t\n\nA comprehensive, diverse, and verifiable dataset of over 83k logical puzzles generated by PuzzleClone.\n\nüìú Paper: https://arxiv.org/abs/2508.15180\n\n\n\t\n\t\t\n\t\tüåü Dataset Overview\n\t\n\nPuzzleCloneData contains 83,657 unique logical reasoning puzzles procedurally generated from 86 seed puzzles. \nThe dataset spans:\n\nVarious applications of Satisfiability Modulo Theories (SMT) and SMT-like puzzles,\nClassic logical puzzles like Sudoku, the Knapsack problem, and linear‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hithink-ai/PuzzleClone.","url":"https://huggingface.co/datasets/hithink-ai/PuzzleClone","creator_name":"hithink-ai","creator_url":"https://huggingface.co/hithink-ai","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","Chinese","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Tengentoppa-grpo-v1.0","keyword":"logic","description":"\n\t\n\t\t\n\t\tTengentoppa-grpo-v1.0\n\t\n\n\n\t\n\t\t\n\t\t1. „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆË™≠„ÅøËæº„Åø\n\t\n\nfrom datasets import load_dataset\n\n# Hugging Face„Åã„ÇâÁõ¥Êé•Ë™≠„ÅøËæº„Åø\ndataset = load_dataset(\"your-username/japanese-edu-problems-tex\")\n\n# „Åæ„Åü„ÅØ„É≠„Éº„Ç´„É´„Éï„Ç°„Ç§„É´„Åã„Çâ\nimport json\nwith open(\"corrected_problems_tex.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n","url":"https://huggingface.co/datasets/DeL-TaiseiOzaki/Tengentoppa-grpo-v1.0","creator_name":"Taisei Ozaki","creator_url":"https://huggingface.co/DeL-TaiseiOzaki","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Japanese","cc-by-4.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"syllogistic-logic","keyword":"logic","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nSyllogistic-logic is a synthetic dataset designed to evaluate the logical reasoning abilities of LLMs. It focuses on the task of logical premise selection ‚Äî identifying the minimal set of premises in a knowledge base that entails a given hypothesis. The dataset is built on the syllogistic fragment of first-order logic and supports systematic generalization experiments, including generalization to unseen knowledge bases and reasoning with longer or shorter inference‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/leobertolazzi/syllogistic-logic.","url":"https://huggingface.co/datasets/leobertolazzi/syllogistic-logic","creator_name":"Leonardo Bertolazzi","creator_url":"https://huggingface.co/leobertolazzi","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","cc-by-4.0","100K - 1M","csv"],"keywords_longer_than_N":true},
	{"name":"CleverBoi-Data-20k","keyword":"logic","description":"theprint/CleverBoi-Data-20k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/theprint/CleverBoi-Data-20k","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","10K - 100K","json","Text"],"keywords_longer_than_N":true},
	{"name":"BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","keyword":"logic","description":"\n\t\n\t\t\n\t\tBAAI_bge-large-en-v1_5-05062024-m8dn-webapp Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the BAAI_bge-large-en-v1_5-05062024-m8dn-webapp model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp.","url":"https://huggingface.co/datasets/fine-tuned/BAAI_bge-large-en-v1_5-05062024-m8dn-webapp","creator_name":"Fine-tuned Embeddings","creator_url":"https://huggingface.co/fine-tuned","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"reddit-logic","keyword":"logic","description":"\n\t\n\t\t\n\t\tReddit Logic: A Dataset for Evaluating Clear and Consistent Reasoning in Natural Language Discourse\n\t\n\nThis dataset studies how people construct and express logical arguments in everyday online discussions. \nUsing posts from Reddit's r/ChangeMyView subreddit, \nthis collection provides well-structured argument analyses that are engaging for humans and machines.\nDataset Construction & Annotation\n\nA curated subset of 10‚Äâ000 posts was selected from the \"HuggingFaceGECLM/REDDIT_comments\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/agentlans/reddit-logic.","url":"https://huggingface.co/datasets/agentlans/reddit-logic","creator_name":"Alan Tseng","creator_url":"https://huggingface.co/agentlans","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","feature-extraction","English","cc-by-4.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"DAG-Reasoning-DeepSeek-R1-0528","keyword":"logic","description":"Click here to support our open-source dataset and model releases!\nDAG-Reasoning-DeepSeek-R1-0528 is a dataset focused on analysis and reasoning, creating directed acyclic graphs testing the limits of DeepSeek R1 0528's graph-reasoning skills!\nThis dataset contains:\n\n4.08k synthetically generated prompts to create directed acyclic graphs in response to user input, with all responses generated using DeepSeek R1 0528.\nAll responses contain a multi-step thinking process to perform effective‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/DAG-Reasoning-DeepSeek-R1-0528.","url":"https://huggingface.co/datasets/sequelbox/DAG-Reasoning-DeepSeek-R1-0528","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","feature-extraction","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"ChatML-Capybara","keyword":"logic","description":"LDJnr/Capybara in ChatML format, ready to use in HuggingFace TRL's SFT Trainer.\nPython code used for conversion:\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Felladrin/Llama-160M-Chat-v1\")\n\ndataset = load_dataset(\"LDJnr/Capybara\", split=\"train\")\n\ndef format(columns):\n    messages = []\n    conversationColumn = columns[\"conversation\"]\n\n    for i in range(len(conversationColumn)):\n        messages.append({\n            \"role\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Felladrin/ChatML-Capybara.","url":"https://huggingface.co/datasets/Felladrin/ChatML-Capybara","creator_name":"Victor Nogueira","creator_url":"https://huggingface.co/Felladrin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-5","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-5\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5.","url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-5","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus-Depth-3","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPARARULE-Plus-Depth-3\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=3. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the depth is greater than‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3.","url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus-Depth-3","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"LessWrong-Amplify-Instruct","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official LessWrong-Amplify-Instruct dataset. Over 500 multi-turn examples, and many more coming soon!\n\t\n\n\nThis leverages Amplify-Instruct method to extend thousands of scraped Less-Wrong posts into advanced in-depth multi-turn conversations.\n\nComprised of over 500 highly filtered multi-turn synthetic conversations.\n\nAverage context length per conversation is over 2,000 tokens. (will measure this more accurately soon)\n\nSynthetically created using a newly developed pipeline‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct.","url":"https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"Verified-Camel","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official Verified Camel dataset. Just over 100 verified examples, and many more coming soon!\n\t\n\n\nComprised of over 100 highly filtered and curated examples from specific portions of CamelAI stem datasets. \n\nThese examples are verified to be true by experts in the specific related field, with atleast a bachelors degree in the subject.\n\nRoughly 30-40% of the originally curated data from CamelAI was found to have atleast minor errors and/or incoherent questions(as determined‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Verified-Camel.","url":"https://huggingface.co/datasets/LDJnr/Verified-Camel","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"mindgames","keyword":"logic","description":"Mindgame dataset\nCode:\nhttps://github.com/sileod/llm-theory-of-mind\nArticle (Accepted at EMNLP 2023 Findings):\nhttps://arxiv.org/abs/2305.03353\n@article{sileo2023mindgames,\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\n  author={Sileo, Damien and Lernould, Antoine},\n  journal={arXiv preprint arXiv:2305.03353},\n  year={2023}\n}\n\n","url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"mindgames","keyword":"logical-reasoning","description":"Mindgame dataset\nCode:\nhttps://github.com/sileod/llm-theory-of-mind\nArticle (Accepted at EMNLP 2023 Findings):\nhttps://arxiv.org/abs/2305.03353\n@article{sileo2023mindgames,\n  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},\n  author={Sileo, Damien and Lernould, Antoine},\n  journal={arXiv preprint arXiv:2305.03353},\n  year={2023}\n}\n\n","url":"https://huggingface.co/datasets/sileod/mindgames","creator_name":"Damien Sileo","creator_url":"https://huggingface.co/sileod","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","monolingual","English"],"keywords_longer_than_N":true},
	{"name":"v-lol-trains","keyword":"logic","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis diagnostic dataset (website, paper) is specifically designed to evaluate the visual logical learning capabilities of machine learning models.\nIt offers a seamless integration of visual and logical challenges, providing 2D images of complex visual trains,\nwhere the classification is derived from rule-based logic.\nThe fundamental idea of V-LoL remains to integrate the explicit logical learning tasks of classic symbolic AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/v-lol-trains.","url":"https://huggingface.co/datasets/AIML-TUDA/v-lol-trains","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","English","cc-by-4.0","10K<n<100K","arxiv:2306.07743"],"keywords_longer_than_N":true},
	{"name":"v-lol-trains","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\t\n\t\t\n\t\tDataset Summary\n\t\n\nThis diagnostic dataset (website, paper) is specifically designed to evaluate the visual logical learning capabilities of machine learning models.\nIt offers a seamless integration of visual and logical challenges, providing 2D images of complex visual trains,\nwhere the classification is derived from rule-based logic.\nThe fundamental idea of V-LoL remains to integrate the explicit logical learning tasks of classic symbolic AI‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/v-lol-trains.","url":"https://huggingface.co/datasets/AIML-TUDA/v-lol-trains","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":null,"first_N":5,"first_N_keywords":["image-classification","English","cc-by-4.0","10K<n<100K","arxiv:2306.07743"],"keywords_longer_than_N":true},
	{"name":"Verified-Camel-KO","keyword":"logic","description":"\n\t\n\t\t\n\t\tVerified-Camel-KO\n\t\n\nÏù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ https://huggingface.co/datasets/LDJnr/Verified-Camel Ïùò ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ÏûÖÎãàÎã§.\nGPT4 TurboÎ°ú Î≤àÏó≠Ìïú Îí§, ÏïΩÍ∞ÑÏùò ÏàòÏ†ïÏùÑ Í±∞Ï≥§ÏäµÎãàÎã§.\nÏù¥ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Î∞©Ïπ®ÏùÄ Ï†ÑÎ∂Ä Ïõê Ï†ÄÏûêÏùò Î∞©Ïπ®ÏùÑ Îî∞Î¶ÖÎãàÎã§.\n\n\t\n\t\t\n\t\tThis is the Official Verified Camel dataset. Just over 100 verified examples, and many more coming soon!\n\t\n\n\nComprised of over 100 highly filtered and curated examples from specific portions of CamelAI stem datasets. \n\nThese examples are verified to be true by experts in the specific related field, with atleast a‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/kuotient/Verified-Camel-KO.","url":"https://huggingface.co/datasets/kuotient/Verified-Camel-KO","creator_name":"Jisoo Kim","creator_url":"https://huggingface.co/kuotient","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","Korean","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"DES-Reasoning-DeepSeek-V3.1","keyword":"logic","description":"Click here to support our open-source dataset and model releases!\nDES-Reasoning-DeepSeek-V3.1 is a dataset focused on analysis and reasoning, creating discrete event simulations testing the limits of DeepSeek V3.1's simulation, Python scripting, and analysis skills!\nThis dataset contains:\n\n4.03k synthetically generated prompts to create discrete event simulations and analysis chat in response to user input, with all responses generated using DeepSeek V3.1.\nAll responses contain a multi-step‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/DES-Reasoning-DeepSeek-V3.1.","url":"https://huggingface.co/datasets/sequelbox/DES-Reasoning-DeepSeek-V3.1","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","feature-extraction","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"FOL-nli","keyword":"logic","description":"\n\t\n\t\t\n\t\tDataset Card for \"FOL-nli\"\n\t\n\nhttps://github.com/sileod/unigram/\nhttps://arxiv.org/abs/2406.11035\nCitation:\n@article{sileo2024scaling,\n  title={Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars},\n  author={Sileo, Damien},\n  journal={arXiv preprint arXiv:2406.11035},\n  year={2024}\n}\n\n","url":"https://huggingface.co/datasets/tasksource/FOL-nli","creator_name":"tasksource","creator_url":"https://huggingface.co/tasksource","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","natural-language-inference","multi-input-text-classification","original","English"],"keywords_longer_than_N":true},
	{"name":"Mitakihara-DeepSeek-R1-0528","keyword":"logic","description":"Click here to support our open-source dataset and model releases!\nMitakihara-DeepSeek-R1-0528 is a dataset focused on artificial intelligence, testing the limits of DeepSeek R1 0528's AI-reasoning skills!\nThis dataset contains:\n\n16.9k synthetically generated prompts about AI, with all responses generated using DeepSeek R1 0528.\nSubjects include computer science, artificial intelligence, MLOps, LLMs and diffusion models, math and CUDA, cutting-edge and future technologies, complex adaptive and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sequelbox/Mitakihara-DeepSeek-R1-0528.","url":"https://huggingface.co/datasets/sequelbox/Mitakihara-DeepSeek-R1-0528","creator_name":"t.d.a.g.","creator_url":"https://huggingface.co/sequelbox","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","csv"],"keywords_longer_than_N":true},
	{"name":"panlex-meanings","keyword":"lojban","description":"\n\t\n\t\t\n\t\tDataset Card for panlex-meanings\n\t\n\nThis is a dataset of words in several thousand languages, extracted from https://panlex.org.\n\n\t\n\t\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThis dataset has been extracted from https://panlex.org (the 20240301 database dump) and rearranged on the per-language basis.\nEach language subset consists of expressions (words and phrases). \nEach expression is associated with some meanings (if there is more than one meaning, they are in separate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cointegrated/panlex-meanings.","url":"https://huggingface.co/datasets/cointegrated/panlex-meanings","creator_name":"David Dale","creator_url":"https://huggingface.co/cointegrated","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["translation","Afar","Western Abnaki","Abkhazian","Abaza"],"keywords_longer_than_N":true},
	{"name":"logic-trainingset-symb-structed-reformatted","keyword":"logic","description":"\n\t\n\t\t\n\t\tLogic Reasoning and Proof Verification Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nA comprehensive dataset for training and evaluating logical reasoning capabilities in language models.\nEach example contains propositional logic problems that require formal reasoning to verify hypotheses.\n\nThe dataset includes 10427 problems with the following distribution:\n- PROVED: 4291 examples where the hypothesis can be proven from the facts\n- DISPROVED: 4262 examples where the hypothesis can be‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RedaAlami/logic-trainingset-symb-structed-reformatted.","url":"https://huggingface.co/datasets/RedaAlami/logic-trainingset-symb-structed-reformatted","creator_name":"Reda alami","creator_url":"https://huggingface.co/RedaAlami","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"rule-reasoning","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tRule Reasoning Datasets\n\t\n\nThis repository contains datasets for rule-based reasoning tasks, organized into two main categories:\n\n\t\n\t\t\n\t\tIn-Distribution (ID) Datasets\n\t\n\n\nar_lsat: Analytical Reasoning from LSAT\nclutrr: CLUTtRR (Compositional Language Understanding and Text-based Relational Reasoning)\nfolio: FOLIO (First-Order Logic in Natural Language)\nlogic_nli: Logic-based Natural Language Inference\nlogical_deduction: Logical Deduction tasks\nlogiqa: LogiQA (Logical Reasoning QA)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/RuleReasoner/rule-reasoning.","url":"https://huggingface.co/datasets/RuleReasoner/rule-reasoning","creator_name":"RuleReasoner","creator_url":"https://huggingface.co/RuleReasoner","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["reinforcement-learning","text-classification","English","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"SynLogic","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tSynLogic Dataset\n\t\n\nSynLogic is a comprehensive synthetic logical reasoning dataset designed to enhance logical reasoning capabilities in Large Language Models (LLMs) through reinforcement learning with verifiable rewards. \n\nüêô GitHub Repo: https://github.com/MiniMax-AI/SynLogic\nüìú Paper (arXiv): https://arxiv.org/abs/2505.19641\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\nSynLogic contains 35 diverse logical reasoning tasks with automatic verification capabilities, making it ideal for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MiniMaxAI/SynLogic.","url":"https://huggingface.co/datasets/MiniMaxAI/SynLogic","creator_name":"MiniMax","creator_url":"https://huggingface.co/MiniMaxAI","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","Chinese","mit","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"mm-astronomy","keyword":"logical reasoning","description":"A set of NER-related questions about multimessenger astronomy.\n","url":"https://huggingface.co/datasets/vvsotnikov/mm-astronomy","creator_name":"Vladimir Sotnikov","creator_url":"https://huggingface.co/vvsotnikov","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","open-domain-qa","multiple-choice-qa","English","mit"],"keywords_longer_than_N":true},
	{"name":"GlotCC-V1","keyword":"lojban","description":"\n\t\n\t\t\n\t\tDataset Summary\n\t\n\n \n\nGlotCC-V1.0 is a document-level, general domain dataset derived from CommonCrawl, covering more than 1000 languages.It is built using the GlotLID language identification and Ungoliant pipeline from CommonCrawl.We release our pipeline as open-source at https://github.com/cisnlp/GlotCC.  \nList of Languages: See https://datasets-server.huggingface.co/splits?dataset=cis-lmu/GlotCC-V1 to get the list of splits available.\n\n\t\t\n\t\tUsage (Huggingface Hub -- Recommended)‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cis-lmu/GlotCC-V1.","url":"https://huggingface.co/datasets/cis-lmu/GlotCC-V1","creator_name":"CIS, LMU Munich","creator_url":"https://huggingface.co/cis-lmu","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["multilingual","Abau","Amarasi","Abkhaz","Abkhazian"],"keywords_longer_than_N":true},
	{"name":"llm-complex-reasoning-train-qwen2-72b-instruct-correct","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tNote\n\t\n\n\nData Seed from Âü∫‰∫éÂ∞ÅÈó≠‰∏ñÁïåÂÅáËÆæÁöÑÂ§çÊùÇÈÄªËæëÊé®ÁêÜ\nGenerate from Qwen2-72B-Instruct with prompt\ntrain.jsonl for Êé®ÁêÜÁ≠îÊ°àÂíåÈ¢òÁõÆÁ≠îÊ°à‰∏ÄËá¥, no_train.jsonlÊé®ÁêÜÁ≠îÊ°àÂíåÈ¢òÁõÆÁ≠îÊ°à‰∏ç‰∏ÄËá¥\nÊ≥®: È¢òÁõÆÁ≠îÊ°à‰∏ç‰∏ÄÂÆöÊ≠£Á°Æ\n\n","url":"https://huggingface.co/datasets/ticoAg/llm-complex-reasoning-train-qwen2-72b-instruct-correct","creator_name":"ticoAg","creator_url":"https://huggingface.co/ticoAg","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Chinese","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"polymath","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPaper Information\n\t\n\nWe present PolyMATH, a challenging benchmark aimed at evaluating the general cognitive reasoning abilities of MLLMs. \nPolyMATH comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning. \nWe conducted a comprehensive, and quantitative evaluation of 15 MLLMs using four diverse prompting strategies, including Chain-of-Thought‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/him1411/polymath.","url":"https://huggingface.co/datasets/him1411/polymath","creator_name":"Himanshu Gupta","creator_url":"https://huggingface.co/him1411","license_name":"Academic Free License v3.0","license_url":"https://choosealicense.com/licenses/afl-3.0/","language":"en","first_N":5,"first_N_keywords":["multiple-choice","expert-generated","found","expert-generated","found"],"keywords_longer_than_N":true},
	{"name":"korpora","keyword":"lojban","description":"\n\t\n\t\t\n\t\tKorpora\n\t\n\nKorpora is a meticulously curated Lojban-English bilingual parallel corpus,\ningeniously synthesized from the widely recognized Tatoeba and Lojban Corpus available online.\nThis comprehensive dataset is further enriched by integrating partially parallel data through the augmentation of Jbovlaste's Deepseek-R1,\nwhile rigorously ensuring grammatical integrity with the assistance of Camxes and Jbofi'e.\n","url":"https://huggingface.co/datasets/smuske/korpora","creator_name":"Smuske Team","creator_url":"https://huggingface.co/smuske","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["translation","English","Lojban","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"korpora","keyword":"lojban","description":"\n\t\n\t\t\n\t\tKorpora\n\t\n\nKorpora is a meticulously curated Lojban-English bilingual parallel corpus,\ningeniously synthesized from the widely recognized Tatoeba and Lojban Corpus available online.\nThis comprehensive dataset is further enriched by integrating partially parallel data through the augmentation of Jbovlaste's Deepseek-R1,\nwhile rigorously ensuring grammatical integrity with the assistance of Camxes and Jbofi'e.\n","url":"https://huggingface.co/datasets/smuske/korpora","creator_name":"Smuske Team","creator_url":"https://huggingface.co/smuske","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["translation","English","Lojban","odc-by","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"language_tags","keyword":"lojban","description":"\n\t\n\t\t\n\t\tDescription\n\t\n\nDataset listing 27,328 languages and dialects (also includes macrolanguage names).For each language, either the ISO 639 code, the Glottolog code or both are provided.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nEnglish_Name: Language name in English (e.g. \"French\").\nNative_Name: If value is not 0, corresponds to the name of the language by native speakers (e.g. \"Fran√ßais\") which may have been found in Wikipedia's nativename field.\nGlottocode: The language tag in the Glottolog convention (e.g.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/language_tags.","url":"https://huggingface.co/datasets/lbourdois/language_tags","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Afade","Par√° Ar√°ra","Afar","Aka-Bea","Abon"],"keywords_longer_than_N":true},
	{"name":"Capybara-Converted","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official Capybara dataset. Over 10,000 multi-turn examples.\n\t\n\nCapybara is the culmination of insights derived from synthesis techniques like Evol-instruct (used for WizardLM), Alpaca, Orca, Vicuna, Lamini, FLASK and others.\nThe single-turn seeds used to intiate the Amplify-Instruct synthesis of conversations are mostly based on datasets that i've personally vetted extensively, and are often highly regarded for their diversity and demonstration of logical robustness and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cfahlgren1/Capybara-Converted.","url":"https://huggingface.co/datasets/cfahlgren1/Capybara-Converted","creator_name":"Caleb Fahlgren","creator_url":"https://huggingface.co/cfahlgren1","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"tatoeba","keyword":"lojban","description":"This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M","url":"https://huggingface.co/datasets/Helsinki-NLP/tatoeba","creator_name":"Language Technology Research Group at the University of Helsinki","creator_url":"https://huggingface.co/Helsinki-NLP","license_name":"Creative Commons Attribution 2.0","license_url":"https://scancode-licensedb.aboutcode.org/cc-by-2.0.html","language":null,"first_N":5,"first_N_keywords":["translation","found","found","multilingual","original"],"keywords_longer_than_N":true},
	{"name":"Verified-Camel-zh","keyword":"logic","description":"This is a direct Chinese translation using GPT4 of the Verified-Camel dataset. I hope you find it useful. \nhttps://huggingface.co/datasets/LDJnr/Verified-Camel\nCitation:\n@article{daniele2023amplify-instruct,\n  title={Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for Effecient LLM Training.},\n  author={Daniele, Luigi and Suphavadeeprasit},\n  journal={arXiv preprint arXiv:(comming soon)},\n  year={2023}\n}\n\n","url":"https://huggingface.co/datasets/noobmaster29/Verified-Camel-zh","creator_name":"Victor Sung","creator_url":"https://huggingface.co/noobmaster29","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","Chinese","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench-Spanish","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tüß† SLR-Bench-Spanish: Scalable Logical Reasoning Benchmark (Spanish Edition)\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSLR-Bench Versions:\n\t\n\n\n\n\nSLR-Bench-Spanish is the Spanish-language pendant of the original SLR-Bench dataset.\nIt follows the same symbolic structure, evaluation framework, and curriculum as the English version but provides all natural-language task prompts translated into Spanish.\nThis enables systematic evaluation and training of Large Language Models (LLMs) in logical reasoning in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ahmad21omar/SLR-Bench-Spanish.","url":"https://huggingface.co/datasets/ahmad21omar/SLR-Bench-Spanish","creator_name":"Ahmad Omar","creator_url":"https://huggingface.co/ahmad21omar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Spanish","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701","keyword":"logic","description":"\n\t\n\t\t\n\t\tLOGIC-701 Benchmark\n\t\n\nThis is a synthetic and filtered dataset for benchmarking large language models (LLMs). It consists of 701 medium and hard logic puzzles with solutions on 10 distinct topics.\nA feature of the dataset is that it tests exclusively logical/reasoning abilities, offering only 5 answer options. There are no or very few tasks in the dataset that require external knowledge about events, people, facts, etc.\n\n\t\n\t\t\n\t\n\t\n\t\tLanguages\n\t\n\nThis benchmark is also part of an‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/hivaze/LOGIC-701.","url":"https://huggingface.co/datasets/hivaze/LOGIC-701","creator_name":"Sergey Bratchikov","creator_url":"https://huggingface.co/hivaze","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","Russian","apache-2.0","1K - 10K","parquet"],"keywords_longer_than_N":true},
	{"name":"capybara-sharegpt","keyword":"logic","description":"\n\t\n\t\t\n\t\tcapybara-sharegpt\n\t\n\nLDJnr/Capybara converted to ShareGPT format for use in common training repositories.\nPlease refer to the original repository's dataset card for more information. All credit goes to the original creator.\n","url":"https://huggingface.co/datasets/Doctor-Shotgun/capybara-sharegpt","creator_name":"Doctor Shotgun","creator_url":"https://huggingface.co/Doctor-Shotgun","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","apache-2.0","10K - 100K"],"keywords_longer_than_N":true},
	{"name":"Maths-Grade-School","keyword":"logic","description":"Maths-Grade-School\nI am releasing large Grade School level Mathematics datatset.\nThis extensive dataset, comprising nearly one million instructions in JSON format, encapsulates a diverse array of topics fundamental to building a strong mathematical foundation.\nThis dataset is in instruction format so that model developers, researchers etc. can easily use this dataset.\nFollowing Fields & sub Fields are covered:\nCalculus\nProbability\nAlgebra\nLiner Algebra\nTrigonometry\nDifferential Equations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School.","url":"https://huggingface.co/datasets/ajibawa-2023/Maths-Grade-School","creator_name":"Feynman Innovations","creator_url":"https://huggingface.co/ajibawa-2023","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"LogicPro","keyword":"logical reasoning","description":"\n  \n  \n  LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning\n\n\n\n  [üìë Paper] ‚Ä¢\n  [ü§ó HF Dataset] ‚Ä¢\n  [üëª GitHub] ‚Ä¢\n  [üîó X/Twiiter]\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tData description\n\t\n\n{\n  \"id\": \"logicpro_lc679_225531-43120\",\n  \"title\": \"24 Game\", # Title of the original leetcode algorithm problem.\n  \"difficulty\": \"Hard\",\n  \"content\": \"...\", # The questions of the original leetcode algorithm problem.\n  \"python\": \"...\", # The original gold python solution\n  \"test_input_string\": \"...\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/jiangjin/LogicPro.","url":"https://huggingface.co/datasets/jiangjin/LogicPro","creator_name":"jiangjin","creator_url":"https://huggingface.co/jiangjin","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["English","apache-2.0","100K - 1M","json","Text"],"keywords_longer_than_N":true},
	{"name":"LOGIC-701-instruct","keyword":"logic","description":"\n\t\n\t\t\n\t\tLOGIC-701 (instruct)\n\t\n\nBased on https://huggingface.co/datasets/hivaze/LOGIC-701\nSources https://github.com/EvilFreelancer/LOGIC-701-instruct\n","url":"https://huggingface.co/datasets/evilfreelancer/LOGIC-701-instruct","creator_name":"Pavel Zloi","creator_url":"https://huggingface.co/evilfreelancer","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","Russian","English","mit","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"SWAP","keyword":"logic","description":"\n\t\n\t\t\n\t\tSWAP: A Synthetic Dataset for Complex Reasoning with Trajectories and Process Supervision\n\t\n\nThis repository contains the data for the paper Deliberate Reasoning for LLMs as Structure-aware Planning with Accurate World Model.\nSWAP (Structure-aware Planning) solves complex reasoning by introducing a Generator-Discriminator architecture, and incorporates structural information to guide the reasoning process and provides a soft verification mechanism over the steps.\nWe generate the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sxiong/SWAP.","url":"https://huggingface.co/datasets/sxiong/SWAP","creator_name":"Siheng Xiong","creator_url":"https://huggingface.co/sxiong","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text2text-generation","English","mit","100K - 1M","json"],"keywords_longer_than_N":true},
	{"name":"dpo-merged-binarized","keyword":"logic","description":"CultriX/dpo-merged-binarized dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-merged-binarized","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"SocratesEval","keyword":"logic","description":"zhx123/SocratesEval dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/zhx123/SocratesEval","creator_name":"zz","creator_url":"https://huggingface.co/zhx123","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","apache-2.0","< 1K","csv"],"keywords_longer_than_N":true},
	{"name":"SocialMaze","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tSocialMaze Benchmark\n\t\n\nThis dataset is a component of the SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models project. It specifically features the Hidden Role Deduction task, which we consider one of the most challenging scenarios for testing complex social reasoning, deception handling, and inferential capabilities in Large Language Models (LLMs).\nWe have curated and formatted this task into a convenient question-answering (QA) structure to facilitate‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MBZUAI/SocialMaze.","url":"https://huggingface.co/datasets/MBZUAI/SocialMaze","creator_name":"Mohamed Bin Zayed University of Artificial Intelligence","creator_url":"https://huggingface.co/MBZUAI","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"CoreReasoning","keyword":"logical reasoning","description":"\n  üåü Core Reasoning Dataset üåü\n  \n  Overview\n  \n    Welcome to the Core Reasoning Dataset‚Äîa meticulously crafted collection of prompts, contexts, outputs, and reasoning types. This dataset is designed to push the boundaries of text-generation models, enabling them to excel in logical reasoning, ethical problem-solving, and contextual understanding.\n  \n\n  ‚ú® Dataset Features\n  \n    \n      Input: A question or prompt requiring critical thinking or creative problem-solving.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ayjays132/CoreReasoning.","url":"https://huggingface.co/datasets/ayjays132/CoreReasoning","creator_name":"Phillip Holland","creator_url":"https://huggingface.co/ayjays132","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"SATQuest","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tSATQuest Dataset\n\t\n\nPaper: SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs\n\n\n\nTL;DR. Synthetic CNF benchmark for LLM reasoning: 140 matched SAT/UNSAT pairs with n in [3, 16] and fixed ratio m=4n. The dataset stores only CNF formulas and solver stats; use the SATQuest Python library to render prompts/answers for SATDP, SATSP, MaxSAT, MCS, and MUS in four formats (math, DIMACS, story, dual story).\n\n\t\n\t\t\n\t\tData fields\n\t\n\n\nid: unique identifier‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/sdpkjc/SATQuest.","url":"https://huggingface.co/datasets/sdpkjc/SATQuest","creator_name":"Yanxiao Zhao","creator_url":"https://huggingface.co/sdpkjc","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","mit","< 1K","parquet"],"keywords_longer_than_N":true},
	{"name":"ParaNames","keyword":"lojban","description":"\n\t\n\t\t\n\t\n\t\n\t\tDataset Card for Dataset Name\n\t\n\n\n\nThis dataset card aims to be a base template for new datasets. It has been generated using this raw template.\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Details\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Description\n\t\n\n\n\n\n\n\nCurated by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Sources [optional]‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bltlab/ParaNames.","url":"https://huggingface.co/datasets/bltlab/ParaNames","creator_name":"Broadening Linguistic Technologies Lab (BLT Lab)","creator_url":"https://huggingface.co/bltlab","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["token-classification","Nias","Kotava","Banjar","Angika"],"keywords_longer_than_N":true},
	{"name":"gliclass-v3-logic-dataset","keyword":"logic","description":"\n\n\t\n\t\t\n\t\tGLiClass‚ÄëV3 Logic Dataset\n\t\n\nRows‚ÄØ‚ÄØ7‚ÄØ776‚ÄÉ|‚ÄÉSplit‚ÄØ‚ÄØtrain only‚ÄÉ|‚ÄÉFormat‚ÄØ‚ÄØParquet‚ÄÉ|‚ÄÉLanguage‚ÄØ‚ÄØEN‚ÄÉ|‚ÄÉLicense‚ÄØ‚ÄØApache‚Äë2.0\n\n\t\n\t\t\n\t\tWhat it is\n\t\n\nA length‚Äëbalanced corpus of single‚Äësentence prompts built purely for inducing reasoning in language models.\n\n\t\n\t\t\n\t\tWhy it helps\n\t\n\n\nTeaches symbolic‚Äëlogic patterns and multi‚Äëlabel behaviour.  \nBuckets cover 15 word‚Äëlength ranges (4‚ÄØ‚Üí‚ÄØ1,024) in equal proportions, exposing models to both tiny and very long inputs.  \nEach example has 1‚Äë50 true and‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/knowledgator/gliclass-v3-logic-dataset.","url":"https://huggingface.co/datasets/knowledgator/gliclass-v3-logic-dataset","creator_name":"Knowledgator Engineering","creator_url":"https://huggingface.co/knowledgator","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","sentence-similarity","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tSLR-Bench: Scalable Logical Reasoning Benchmark for LLMs\n\t\n\n\n\n\n\nüÜï August 2025: Build your own Reasoning Problems with Verifiable Rewards. Source Code is now available! üëâ Generate your own Reasoning Task\n\n\nüÜï June 2024: Evaluation & RLVR Reward Model Released!üëâ Demo on Hugging Face Spaces\n\nSLR-Bench is a scalable, fully-automated benchmark designed to systematically evaluate and train Large Language Models (LLMs) in logical reasoning via inductive logic programming (ILP) tasks.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/SLR-Bench.","url":"https://huggingface.co/datasets/AIML-TUDA/SLR-Bench","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["English","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CleverBoi","keyword":"logic","description":"\n\n\n\t\n\t\t\n\t\tCleverBoi\n\t\n\nThe CleverBoi Collection is based on a number of data sets that emphasize logic, inference, empathy, math and coding.\nThe data set has been formatted to follow the alpaca format (instruction + input -> output) when fine tuning.\n\n\t\n\t\t\n\t\tSource Data Sets\n\t\n\nThe source data sets used in the CleverBoi Collection are listed below, ordered by size.\n\nKK04/LogicInference_OA\nmlabonne/Evol-Instruct-Python-26k\ngarage-bAInd/Open-Platypus\niamtarun/python_code_instructions_18k_alpaca‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/theprint/CleverBoi.","url":"https://huggingface.co/datasets/theprint/CleverBoi","creator_name":"Rasmus Rasmussen","creator_url":"https://huggingface.co/theprint","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"QAMLC","keyword":"logical reasoning","description":"\n\t\n\t\t\n\t\tQAMLC: Questions and Answers in Mathematics, Logical and Critical Thinking\n\t\n\nQAMLC is a living, open-source dataset of manually created questions and answers designed to train/benchmark AI in mathematical reasoning, logical thinking, and critical analysis skills for learners up to Grade 6. It currently contains 944 questions with step-by-step solutions and final answers. All content is written in English (UK).\n\n\t\n\t\t\n\t\n\t\n\t\tKey Points\n\t\n\n\nContent Focus: Mathematics, logical reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/PulkitSahu/QAMLC.","url":"https://huggingface.co/datasets/PulkitSahu/QAMLC","creator_name":"Pulkit Sahu","creator_url":"https://huggingface.co/PulkitSahu","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Pure-Dove","keyword":"logic","description":"\n\t\n\t\t\n\t\tThis is the Official Pure-Dove dataset. Over 3K multi-turn examples, and many more coming soon!\n\t\n\nThis dataset aims to be the largest highest quality cluster of real human back and forth conversations with GPT-4.\nSteps have even been done to ensure that only the best GPT-4 conversations in comparisons are kept, there are many instances where two GPT-4 responses are rated as equal to eachother or as both bad. We exclude all such responses from Pure Dove and make sure to only include‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/LDJnr/Pure-Dove.","url":"https://huggingface.co/datasets/LDJnr/Pure-Dove","creator_name":"Luigi D","creator_url":"https://huggingface.co/LDJnr","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"PARARULE-Plus","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tPARARULE-Plus\n\t\n\nThis is a branch which includes the dataset from PARARULE-Plus Depth=2, Depth=3, Depth=4 and Depth=5. PARARULE Plus is a deep multi-step reasoning dataset over natural language. It can be seen as an improvement on the dataset of PARARULE (Peter Clark et al., 2020). Both PARARULE and PARARULE-Plus follow the closed-world assumption and negation as failure. The motivation is to generate deeper PARARULE training samples. We add more training samples for the case where the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/qbao775/PARARULE-Plus.","url":"https://huggingface.co/datasets/qbao775/PARARULE-Plus","creator_name":"annonymous_user","creator_url":"https://huggingface.co/qbao775","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","question-answering","English","mit","100K - 1M"],"keywords_longer_than_N":true},
	{"name":"logic_duo","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tLogicDuo: Bilingual Logical Reasoning Tutoring Corpus\n\t\n\nCreated using this project–°–æ–∑–¥–∞–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞  \n\nüá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è / Russian version...\n\n\n\t\n\t\t\n\t\n\t\n\t\t–ö–æ—Ä–ø—É—Å \"LogicDuo\": –û–±—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º\n\t\n\n–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤–µ–¥–µ–Ω–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–∏–∞–ª–æ–≥ –º–µ–∂–¥—É‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/loim/logic_duo.","url":"https://huggingface.co/datasets/loim/logic_duo","creator_name":"Arsen Arutunan","creator_url":"https://huggingface.co/loim","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["Russian","English","mit","1K<n<10K","üá∫üá∏ Region: US"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench-German","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tüß† SLR-Bench-German: Scalable Logical Reasoning Benchmark (German Edition)\n\t\n\n\n\n\n\nSLR-Bench-German is the German-language pendant of the original SLR-Bench dataset.\nIt follows the same symbolic structure, evaluation framework, and curriculum as the English version but provides all natural-language task prompts translated into German.\nThis enables systematic evaluation and training of Large Language Models (LLMs) in logical reasoning in german, supporting both multilingual reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-German.","url":"https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-German","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["German","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench-Spanish","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tüß† SLR-Bench-Spanish: Scalable Logical Reasoning Benchmark (Spanish Edition)\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSLR-Bench Versions:\n\t\n\n\n\n\nSLR-Bench-Spanish is the Spanish-language pendant of the original SLR-Bench dataset.\nIt follows the same symbolic structure, evaluation framework, and curriculum as the English version but provides all natural-language task prompts translated into Spanish.\nThis enables systematic evaluation and training of Large Language Models (LLMs) in logical reasoning in‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-Spanish.","url":"https://huggingface.co/datasets/AIML-TUDA/SLR-Bench-Spanish","creator_name":"Artificial Intelligence & Machine Learning Lab at TU Darmstadt","creator_url":"https://huggingface.co/AIML-TUDA","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["Spanish","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"SLR-Bench-French","keyword":"logic","description":" \n\n\n\t\n\t\t\n\t\tüß† SLR-Bench-French: Scalable Logical Reasoning Benchmark (French Edition)\n\t\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tSLR-Bench Versions:\n\t\n\n\n\n\n\nSLR-Bench-French is the French-language pendant of the original SLR-Bench dataset.\nIt follows the same symbolic structure, evaluation framework, and curriculum as the English version but provides all natural-language task prompts translated into French.\nThis enables systematic evaluation and training of Large Language Models (LLMs) in logical reasoning in French‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ahmad21omar/SLR-Bench-French.","url":"https://huggingface.co/datasets/ahmad21omar/SLR-Bench-French","creator_name":"Ahmad Omar","creator_url":"https://huggingface.co/ahmad21omar","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["French","cc-by-4.0","10K - 100K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"panlex","keyword":"lojban","description":"\n\t\n\t\t\n\t\tPanLex\n\t\n\nJanuary 1, 2024 version of PanLex Language Vocabulary with 24,650,274 rows covering 6,152 languages.\n\n\t\n\t\t\n\t\tColumns\n\t\n\n\nvocab: contains the text entry.  \n639-3: contains the ISO 639-3 languages tags to allow users to filter on the language(s) of their choice.\n639-3_english_name: the English language name associated to the code ISO 639-3. \nvar_code: contains a code to differentiate language variants. In practice, this is the code 639-3 + a number.  If 000, it corresponds to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lbourdois/panlex.","url":"https://huggingface.co/datasets/lbourdois/panlex","creator_name":"Lo√Øck BOURDOIS","creator_url":"https://huggingface.co/lbourdois","license_name":"Creative Commons Zero v1.0 Universal","license_url":"https://choosealicense.com/licenses/cc0-1.0/","language":"en","first_N":5,"first_N_keywords":["Ghotuo","Alumu-Tesu","Ari","Amal","Arb√´resh√´ Albanian"],"keywords_longer_than_N":true},
	{"name":"llm-fol-reasoning-eval","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tLLM FOL Reasoning Eval\n\t\n\nThis dataset is derived from ProverQA, a First-Order Logic reasoning benchmark designed to test the ability of large language models (LLMs) to perform structured logical reasoning.It restructures and normalizes the ProverQA development and training data into a unified, clean format suitable for evaluating chain-of-thought (CoT) and symbolic reasoning capabilities in LLMs.\n\n\n\t\n\t\t\n\t\n\t\n\t\tSource\n\t\n\nOriginal dataset: ProverQA: A First-Order Logic Reasoning‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/MinaGabriel/llm-fol-reasoning-eval.","url":"https://huggingface.co/datasets/MinaGabriel/llm-fol-reasoning-eval","creator_name":"Mina Gabriel","creator_url":"https://huggingface.co/MinaGabriel","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-classification","mit","1K - 10K","parquet","Tabular"],"keywords_longer_than_N":true},
	{"name":"CoreReasoningPrime","keyword":"logical reasoning","description":"\n  üåü Core Reasoning Prime Dataset üåü\n  \n  üåå Ultimate Overview\n  \n    Welcome to the Core Reasoning Prime Dataset‚Äîa pioneering collection designed for next-generation AI models. Crafted to cultivate logical reasoning, creative problem-solving, ethical thinking, and beyond, this dataset sets the benchmark for intelligence training.\n  \n\n  ‚ú® Revolutionary Features\n  \n    \n      üîçInput: Engaging prompts designed to challenge AI‚Äôs reasoning and creativity.\n    \n    \n      üß†Context: Enriched‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/ayjays132/CoreReasoningPrime.","url":"https://huggingface.co/datasets/ayjays132/CoreReasoningPrime","creator_name":"Phillip Holland","creator_url":"https://huggingface.co/ayjays132","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","1K - 10K","json"],"keywords_longer_than_N":true},
	{"name":"fineweb-2","keyword":"lojban","description":"\n\t\n\t\t\n\t\tü•Ç FineWeb2\n\t\n\n\n    \n\n\n\nA sparkling update with 1000s of languages\n\n\n\t\n\t\t\n\t\tWhat is it?\n\t\n\nThis is the second iteration of the popular üç∑ FineWeb dataset, bringing high quality pretraining data to over 1000 üó£Ô∏è languages.\nThe ü•Ç FineWeb2 dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.\nIn particular, on the set of 9 diverse languages we used to guide our processing decisions, ü•Ç FineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-2.","url":"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2","creator_name":"FineData","creator_url":"https://huggingface.co/HuggingFaceFW","license_name":"Open Data Commons Attribution License v1.0","license_url":"https://scancode-licensedb.aboutcode.org/odc-by-1.0.html","language":"en","first_N":5,"first_N_keywords":["text-generation","Arifama-Miniafia","Ankave","Abau","Amarasi"],"keywords_longer_than_N":true},
	{"name":"SocialMaze","keyword":"logical reasoning","description":"\n‚ö†Ô∏è Notice: This dataset is no longer maintained under this repository. It has been officially migrated to the MBZUAI organization for ongoing development and updates.üëâ Access the latest version here: MBZUAI/SocialMaze\n\n\n\t\n\t\t\n\t\tSocialMaze Benchmark\n\t\n\nThis dataset is a component of the SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models project. It specifically features the Hidden Role Deduction task, which we consider one of the most challenging scenarios for‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/xzx34/SocialMaze.","url":"https://huggingface.co/datasets/xzx34/SocialMaze","creator_name":"Zixiang Xu","creator_url":"https://huggingface.co/xzx34","license_name":"Creative Commons Attribution 4.0 International Public License","license_url":"https://choosealicense.com/licenses/cc-by-4.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","cc-by-4.0","100K - 1M","parquet"],"keywords_longer_than_N":true},
	{"name":"GeneralLogic","keyword":"logic","description":"\n\t\n\t\t\n\t\tGeneralLogic\n\t\n\nGeneralLogic is a dataset of 3,807,700 unique natural language reasoning questions designed for training large language models on general-purpose logical thinking.\nThe dataset avoids math, programming, or trivia-style tasks, and instead focuses on broad, diverse reasoning expressed in plain language. Each question is aligned with one or more of 13 core reasoning types and organized into a tree-like structure.\n\n\n\t\n\t\t\n\t\n\t\n\t\tStructure\n\t\n\nEach entry is a JSON object with‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Gusarich/GeneralLogic.","url":"https://huggingface.co/datasets/Gusarich/GeneralLogic","creator_name":"Daniil Sedov","creator_url":"https://huggingface.co/Gusarich","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","text2text-generation","mit","1M - 10M"],"keywords_longer_than_N":true},
	{"name":"pubmedqa-fol-light","keyword":"logic","description":"Question-FOL-Answer dataset based on PubMedQA","url":"https://huggingface.co/datasets/hesamation/pubmedqa-fol-light","creator_name":"‚ÑèŒµsam","creator_url":"https://huggingface.co/hesamation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"Intermediate-Thinking-130k","keyword":"logical-reasoning","description":"\n\t\n\t\t\n\t\tIntermediate-Thinking-130k\n\t\n\nA comprehensive dataset of 135,000 high-quality samples designed to advance language model reasoning capabilities through structured intermediate thinking processes. This dataset enables training and evaluation of models with sophisticated self-correction and iterative reasoning abilities across 42 languages.\nOG Link\n\n\t\n\t\t\n\t\n\t\n\t\tOverview\n\t\n\nIntermediate-Thinking-130k addresses a fundamental limitation in current language models: their inability to pause‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/mlx-community/Intermediate-Thinking-130k.","url":"https://huggingface.co/datasets/mlx-community/Intermediate-Thinking-130k","creator_name":"MLX Community","creator_url":"https://huggingface.co/mlx-community","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","Afrikaans","Arabic","Bengali","Bulgarian"],"keywords_longer_than_N":true},
	{"name":"distilabel-capybara-dpo-7k-binarized","keyword":"logic","description":"\n\t\n\t\t\n\t\tCapybara-DPO 7K binarized\n\t\n\n\nA DPO dataset built with distilabel atop the awesome LDJnr/Capybara\n\n\nThis is a preview version to collect feedback from the community. v2 will include the full base dataset and responses from more powerful models.\n\n\n    \n\n\n\n  \n    \n  \n\n\n\n\t\n\t\n\t\n\t\tWhy?\n\t\n\nMulti-turn dialogue data is key to fine-tune capable chat models. Multi-turn preference data has been used by the most relevant RLHF works (Anthropic, Meta Llama2, etc.). Unfortunately, there are very few‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized.","url":"https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized","creator_name":"Argilla","creator_url":"https://huggingface.co/argilla","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["question-answering","text-generation","English","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"Maths-Grade-School","keyword":"logic","description":"Maths-Grade-School\nI am releasing large Grade School level Mathematics datatset.\nThis extensive dataset, comprising nearly one million instructions in JSON format, encapsulates a diverse array of topics fundamental to building a strong mathematical foundation.\nThis dataset is in instruction format so that model developers, researchers etc. can easily use this dataset.\nFollowing Fields & sub Fields are covered:\nCalculus\nProbability\nAlgebra\nLiner Algebra\nTrigonometry\nDifferential Equations‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/pt-sk/Maths-Grade-School.","url":"https://huggingface.co/datasets/pt-sk/Maths-Grade-School","creator_name":"Sathish Kumar","creator_url":"https://huggingface.co/pt-sk","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","text2text-generation","English","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"short_COT_48k","keyword":"logic","description":"FrankL/short_COT_48k dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/FrankL/short_COT_48k","creator_name":"FrankLiu","creator_url":"https://huggingface.co/FrankL","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","text2text-generation","Chinese","apache-2.0","1K - 10K"],"keywords_longer_than_N":true},
	{"name":"flo","keyword":"logic","description":"\n\t\n\t\t\n\t\tflo Dataset\n\t\n\n\n\t\n\t\t\n\t\tDataset Description\n\t\n\nThe dataset \"general domain\" is a generated dataset designed to support the development of domain specific embedding models for retrieval tasks.\n\n\t\n\t\t\n\t\tAssociated Model\n\t\n\nThis dataset was used to train the flo model.\n\n\t\n\t\t\n\t\tHow to Use\n\t\n\nTo use this dataset for model training or evaluation, you can load it using the Hugging Face datasets library as follows:\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"florianhoenicke/flo\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/florianhoenicke/flo.","url":"https://huggingface.co/datasets/florianhoenicke/flo","creator_name":"Florian H√∂nicke","creator_url":"https://huggingface.co/florianhoenicke","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["feature-extraction","sentence-similarity","English","apache-2.0","< 1K"],"keywords_longer_than_N":true},
	{"name":"pubmedqa_cot_fol","keyword":"logic","description":"Question-FOL-Answer dataset based on PubMedQA","url":"https://huggingface.co/datasets/hesamation/pubmedqa_cot_fol","creator_name":"‚ÑèŒµsam","creator_url":"https://huggingface.co/hesamation","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["question-answering","English","mit","< 1K","json"],"keywords_longer_than_N":true},
	{"name":"dpo-merged","keyword":"logic","description":"CultriX/dpo-merged dataset hosted on Hugging Face and contributed by the HF Datasets community","url":"https://huggingface.co/datasets/CultriX/dpo-merged","creator_name":"CultriX","creator_url":"https://huggingface.co/CultriX","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","apache-2.0","10K - 100K","parquet"],"keywords_longer_than_N":true},
	{"name":"Kapibara","keyword":"logic","description":"\n\t\n\t\t\n\t\n\t\n\t\tKapibara: Albanian Multi-turn Conversation Dataset\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tDataset Summary\n\t\n\nKapibara is a comprehensive Albanian language dataset designed for multi-turn conversations. It contains over 5,300 entries covering a wide range of topics including physics, biology, mathematics, chemistry, culture, and logic. The dataset is aimed at improving text generation and question-answering capabilities in the Albanian language.\n\n\t\n\t\t\n\t\n\t\n\t\tSupported Tasks\n\t\n\nThe dataset supports the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/alban-labs/Kapibara.","url":"https://huggingface.co/datasets/alban-labs/Kapibara","creator_name":"Albanian Labs","creator_url":"https://huggingface.co/alban-labs","license_name":"Apache License 2.0","license_url":"https://choosealicense.com/licenses/apache-2.0/","language":"en","first_N":5,"first_N_keywords":["text-generation","question-answering","English","Albanian","apache-2.0"],"keywords_longer_than_N":true},
	{"name":"RelatLogic","keyword":"logic","description":"RelatLogic: A Dataset for Comparative and Conditional Reasoning\nThis is a comparative logic and conditional reasoning dataset. \nEach data point has a premise, question, answer, reasoning and attribute.\nMore about the generation process here.\nPlease cite this dataset using the provided BibTeX if you find it useful.\n@misc {sb_2025,\n    author       = { {SB} },\n    title        = { RelatLogic (Revision 15b1922) },\n    year         = 2025,\n    url          = {‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/shb777/RelatLogic.","url":"https://huggingface.co/datasets/shb777/RelatLogic","creator_name":"SB","creator_url":"https://huggingface.co/shb777","license_name":"MIT License","license_url":"https://choosealicense.com/licenses/mit/","language":"en","first_N":5,"first_N_keywords":["text-generation","English","mit","1M - 10M","parquet"],"keywords_longer_than_N":true}
]
;
